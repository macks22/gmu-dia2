{"129415":{"abstract":"The Internet's simple best-effort packet-switched architecture lies at the core of its tremendous success and impact. However, current Internet architecture allows neither (i) users to indicate their value choices at sufficient granularity nor (ii) providers to manage risks involved in investment for new innovative QoS technologies and business relationships with other providers as well as users. To allow these flexibilities, this project investigates \"contract-switching\" as a new paradigm for future Internet. Just like packet-switching enabled flexible and efficient multiplexing of data in the Internet, a contract-switched network will enable flexible and economically efficient management of risks and value flows.<br\/><br\/>Intellectual Merit: This project focuses on the design of a contract-switching framework in the context of multi-domain QoS contracts. It addresses the challenges involved in the development of decentralized inter-domain protocol mechanisms that can dynamically compose and price complex end-to-end contracts. The project formulates this end-to-end contract composition as a \"contract routing\" problem, resembling the QoS routing algorithms. This research also develops an appropriate abstraction necessary for pricing of QoS contracts, dynamically composable contracts in space and time. The project employs financial engineering techniques to provide risk sharing mechanisms and money-back guarantee structures for the QoS contracts.<br\/><br\/>Broader Impact: This research brings together network architecture design and financial engineering tools. This interdisciplinary work can inspire usage of economic tools for security problems like spam and DDoS attacks. The project will be especially beneficial to the Internet policy makers. Management of risks with economic efficiency tools and financial derivatives has several areas of impact such as water, electricity, insurance, real-options analysis, investment under uncertainty, and understanding the impact of open-source technologies.","title":"Collaborative Research: NeTS-FIND: Value Flows and Risk Management Architecture for Future Internet","awardID":"0721609","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["342900","531798","531799"],"PO":["565090"]},"129569":{"abstract":"This proposal centers around the notion that computing has evolved into an inter- and intra-disciplinary field of intertwined concepts that pervade society. The Georgia Institute of Technology (Georgia Tech) and other schools in the University System of Georgia have defined and adopted a number of specialized degrees and contextualized computing courses. Last Fall, Georgia Tech extended this approach to create<br\/>the Threads model includes a process for creating curricular change, an infrastructure for advising, and software to support administrators, advisors, educators and students. In parallel, Brooklyn College of the City University of New York (BC-CUNY) has developed several context-based approaches to computing education with a focus on introductory courses and the high school to college continuum, as well as created two new interdisciplinary masters degrees. The team proposes to create an alliance that validates and extends the Threads model. The proposed work encompasses a methodical approach to understanding the process of defining broad, flexible paths through a computing curriculum, and to measuring and analyzing the outcomes of this process when applied to a variety of departments and interest groups. At the heart of this process is an emphasis on context-based instruction and targeted advising that helps students crystalize career paths and realize the short- and long-term relevance of their coursework. The project explores crucial research questions that arise out of adapting and applying Threads, and evaluating the effects on students, faculty and administrators through quantitative and qualitative studies. Under the work proposed here, they will measure the impact of Georgia Tech?s implementation of the Threads model and the supporting advising mechanisms; extend and adapt Threads to a broad range of computing departments; facilitate its adoption at such departments; and evaluate its efficacy under a variety of conditions. The goal is a validated, widely deployed and broadly-evaluated model of curricular reform that is applicable to small and large departments, students with a range of backgrounds and abilities, and faculty with a range of interests. The combination of diverse experiences brought together by the project team promises to produce results with the potential to serve as national models for both computing and other STEM (science, technology, engineering and math) disciplines.","title":"CPATH EAE: Extending contextualized computing in multiple institutions using Threads","awardID":"0722177","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":[343358,"523503",343360],"PO":["565136"]},"131241":{"abstract":"This program undertakes a broad research agenda centered around the design and analysis of ``Flow-based Networks''. A flow is a collection of packets that belong to the same ``transaction'', such as a datagram, an ftp transfer, or a web download. It is the fundamental unit of data that a user cares about. Current packet-switched networks, like the Internet and Gigabit Ethernet, are designed to process packets; they are unaware of the flow to which a packet belongs. This is because flow-recognition is widely considered to be too expensive to implement. However, a switch or a router's ability to recognize flows can lead to a marked improvement in its performance, to a better use of its resources, and to much more secure networks. <br\/><br\/><br\/>The first major aim of this program is to design novel algorithms and data structures for high-speed, \"flow-aware\" networks. Such algorithms could heavily influence the design of commercial switches and routers. A second major thrust concerns the development of flow-level models of networks: models which capture the impact of packet-level decisions on flow-level bandwidth allocation and flow processing times. <br\/>An important component of the modeling work is the unification and generalization of two research enterprises: Stochastic Network Theory, and Large Random Networks. The former studies the performance of a, typically non-random, queueing network subject to \"random inputs\". The latter concerns the study of \"random networks\", usually subject to deterministic inputs. A successful outcome of these efforts can help answer questions such as the throughput and flow delay of a particular bandwidth allocation scheme, <br\/>and the effect of routing topology on end-to-end performance. In other words, the modeling effort aims to develop a realistic, simple and usable class of models for network flows.","title":"Collaborative Research: Flow level models and the design of flow-aware networks","awardID":"0729537","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["510145"],"PO":["432103"]},"131142":{"abstract":"The practice of medicine involves the science of prediction. <br\/>Prediction depends on clinical or laboratory variables or factors <br\/>that are linked to outcome. The most common predictors in cancer <br\/>medicine are the three variables: tumor size, regional lymph node <br\/>status, and distant metastasis. The three variables are combined <br\/>in a bin model to form the TNM Staging system, which is a major <br\/>tool used to predict the outcome of cancer patients and guide therapy. <br\/>However, the TNM system has only three variables. Therefore, <br\/>its predictive accuracy is limited. <br\/><br\/><br\/>This research addresses issues of numerical computing and optimization <br\/>in developing expanded cancer prognostic systems that can integrate <br\/>multiple variables. Two sets of closely related tasks will be investigated, <br\/>and they represent two major aspects of the intellectual merit of the study. <br\/>One task is focused on how censored survival times and different <br\/>types of variables are integrated into a clustering framework that <br\/>works for a large volume of cancer patient data. Another task is to <br\/>use the developed cluster analysis to establish prognostic systems, <br\/>which will provide a more accurate prediction of outcome by taking <br\/>multiple prognostic factors into account. The broader impact of this <br\/>research includes many aspects. It will have a direct impact on future <br\/>staging and classification of cancer patients. The work has general <br\/>applications and can be adapted to studies of any non-cancer health <br\/>problems. The investigator's study is expected to make significant <br\/>contributions to advances in medicine. The research will also have an <br\/>educational impact.","title":"Numerical Computing and Optimization in Developing Cancer Prognostic Systems","awardID":"0729080","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["438415"],"PO":["565223"]},"131164":{"abstract":"Title: A theory of monitoring based on identifying codes and their variants<br\/><br\/>PIs: Ari Trachtenberg and David Starobinski<br\/><br\/>Since their introduction a decade ago, identifying codes have developed a broad range of connections to both theoretical and applied problems.<br\/>On the theoretical side, these codes are intimately and deeply linked to locating-dominating sets, super-imposed codes, covering codes, and tilings. On the applied side, researchers (including the PIs) have found fundamental links between identifying codes and important monitoring problems, such as the detection of faults and failures in computer and communication networks, indoor localization, and identification of contaminant sources in the environment. The goal of this research is to properly develop and cultivate the connections between the theory of identifying codes and monitoring applications, thus, contributing to the broader efforts by the research community to develop efficient tools for monitoring the nation's critical infrastructures.<br\/><br\/>The work entails developing approximation algorithms for identifying code problems, with proven guarantees for arbitrary graphs or graphs of particular relevance to monitoring problems (e.g., geometric and sector random graphs). This is achieved by exploiting connections, recently found by the PIs, between identifying code problems and the well-studied set, multi-set and test covering problems. An important part of the research is to explore sophisticated variants of classical identifying codes, such as source identifying codes, statistical identifying codes, and disjoint identifying codes, which are important for several practical monitoring applications, with an eye towards extending the set covering connections to these new problems. On the educational front, the project involves a unique program, called GreenHack, through which students from local high schools come together with undergraduates engineers at Boston University for a two day workshop and competition around the theme of environmental monitoring and management, in line with the broader goals of the proposed research.","title":"A Theory of Monitoring Based on Identifying Codes and Their Variants","awardID":"0729158","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["486132","449083"],"PO":["564924"]},"133001":{"abstract":"The University of Huston Downtown proposes to explore the potential for an alliance of the Center for Computational Science and Advanced Distributed Simulation at the University of Huston Downtown, the San Diego Supercomputing Center, and the West Houston Center for Science and Engineering of the Huston Community College System to significantly impact the number of Hispanic students earning post secondary degrees in computing. With this award they, will assess and develop the partnerships necessary to build a broad alliance that could provide a critical pathway for minority students wishing to enter computing degree programs","title":"SGER: Alliance for Enhanced Computing Education Bridges (AECEB)","awardID":"0739194","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7584","name":"ITR-BROADENING PARTICIPATION"}}],"PIcoPI":["496824","427040",353712],"PO":["561855"]},"131076":{"abstract":"The onset of competition coupled with the increasingly networked nature of societies and their supporting economies implies that the extant static planning approaches are no longer adequate. Uncertainties, arising from climactic, political and economic changes, suggest the need for robust solutions while resource management questions have taken on a competitive overtone with multiple users vying for scarce resources. In effect, the question of interest has shifted from that of finding the best solution to one of determining an equilibrium in user strategies; a set of decisions from which no user has a desire to deviate. In a nutshell, this study focuses on the theory and algorithms for addressing problems lying at the nexus of competition, uncertainty and dynamics. The work will find application in the design, control, operation and simulation of large networked markets. <br\/><br\/>This work addresses theoretical and methodological questions underlying the solution of game-theoretic problems in settings complicated by uncertainty and dynamics. More specifically, we focus on stochastic dynamic equilibrium problems, particularly stochastic Nash and Stackelberg equilibrium problems as well as their dynamical extensions. Truly large-scale instances will be addressed by asynchronous grid-computing extensions. A strong component of this research lies in the involvement of both undergraduate students as well as those from under-represented groups. Another important component of this research program will be the timely posting of all the results (theory, algorithms and software) obtained on a dedicated website, so that the research output can be parlayed to other researchers, practitioners, and industry, and thus can have a much broader impact in educational and industrial sectors on a global scale.","title":"Addressing Competition, Dynamics and Uncertainty in Optimization Problems: Theory, Algorithms, Applications and Grid-Computing Extensions","awardID":"0728863","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["531257"],"PO":["565157"]},"125873":{"abstract":"IIS - 0705774<br\/>Arms, William Y.<br\/>Cornel University<br\/>III-CXT-Computer Science Research Using the Cornell Web Lab to Study Social and<br\/>Information Processes on the Web<br\/><br\/><br\/>The project is a collaborative effort between computer scientists and social scientists to advance understanding of the dynamics of the World Wide Web and its impact on science and society. The Web Lab enables social science scholars and others to study issues related to the diffusion of innovation - how new ideas arise and spread across large populations. The proposal addresses fundamental new computing and information science research questions and extends the Web Lab datasets to new levels of depth and complexity so that critical issues of scale can be explored. The project has four goals: (1) to develop large-scale datasets that support wide ranging research about the nature and evolution of the Web, (2) to study human interaction on the Web by combing methods of hypothesis-driven research from the social sciences with large-scale modeling of information structures from computer and information sciences, (3) to understand better how the Web evolves over time by developing models of the creation, modification, and destruction of pages and link structures, and by testing those models on Web-scale snapshots, and (4) to make the datasets available over the TeraGrid.","title":"III-CXT: Computer Science Research Using the Cornell Web Lab to Study Social and Informational Processes on the Web","awardID":"0705774","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["410331","420975","450550"],"PO":["565136"]},"134365":{"abstract":"Lead Proposal: OCI - 0749015 <br\/>PI: Dawson, Clinton N<br\/>Institution: University of Texas at Austin<br\/><br\/>Non-lead Proposal: OCI - 0749017<br\/>PI: Hanna, Darrin M<br\/>Institution: Oakland University<br\/><br\/>Non-lead Proposal: OCI ? 0746232<br\/>PI: Westerink, Joannes J<br\/>Institution: University of Notre Dame<br\/><br\/><br\/>Title: \"Collaborative Research\" NSF PetaApps: Storm Surge Modeling on Petascale Computers<br\/><br\/>ABSTRACT<br\/><br\/>The goal of this project is to investigate the use of petascale computing to significantly advance the state-of-the-art in storm surge simulation, to accurately model flows at multiple, interacting scales, at resolution never before attempted, and to demonstrate that results from these simulations can be delivered in real-time to emergency managers. To achieve this goal will require the continued development and improved understand of the mechanisms involved in tightly coupled models of wind, waves, circulation and geomorphology, improvements in the description of the physical domain and adaptive resolution of all energetic flow scales, and investigation of accurate, robust and highly parallelizable numerical algorithms. Efficient implementation of these models on emerging petascale architectures will require utilizing the latest developments in parallel data management, real-time visualization, and programming tools. In this project, the<br\/>PIs will develop high resolution, large-scale coastal inundation models coupled with regional-scale rainfall\/runoff models. Robust and highly parallelizable algorithms will be investigated for solving these systems on petascale architectures. The models will be implemented on NSF Track 2 HPC systems currently under construction; furthermore, implementation of the models on novel hybrid architectures will also be explored.<br\/><br\/>Predicting and studying coastal inundation due to hurricanes and tropical storms is a problem of critical importance to the United States. Hurricane Katrina alone was the costliest and 5th deadliest hurricane in history, with most of the devastation due to wind-driven flooding during the storm. The aftermath of this event has led to a number of federally-mandated studies to determine what failed, the causes of failure, and how to prevent such catastrophes from happening again. Critical decisions will be made in the next several years on how to design better protection systems and improve emergency management practices in the event of future storms. Storm surge is caused by wind, atmospheric pressure gradients, tides, river flow, short-crested wind-waves, and rainfall. In this project, the investigators will develop an accurate numerical model of storm surge which accounts for all of these effects. This model will be tested in predictive mode as storms approach landfall for the purposes of emergency evacuation and response, and used to study the design and implementation of improved man-made and natural protection systems for vulnerable coastal areas. While storm surge models have been developed extensively over the past decade; only within the last few years have the algorithms, computational power and resolution been available to begin to model these events with any reasonable degree of accuracy. In addition to storm surge modeling, the computational methodology and simulation tools developed under this project are applicable to other problems in coastal engineering and marine science, including water quality, shipping and ports, marine ecology, naval operations, weather and climate, and wetland degradation. Furthermore, the technology developed under this project will be disseminated to government agencies such as FEMA, the U.S. Army Corps of Engineers and NOAA.","title":"Collaborative Research: NSF PetaApps Storm Surge Modeling on Petascale Computers","awardID":"0746232","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7552","name":"COFFES"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["521159"],"PO":["565247"]},"127963":{"abstract":"Molecules of conventional and biological weapons have a characteristic absorption spectrum in the terahertz band that allows for spectroscopic fingerprinting. Because terahertz radiation can penetrate paper, clothes, and luggage, it can be used as means to detect terror material in envelopes and packages and can become useful for airport security. The compact THz source considered here has dimensions typically smaller than the wavelength of the radiated light. Thus, there exists the possibility of intrinsically coherent radiation due to the natural spatial bunching of the electrons. Besides being compact, other possible payoffs include an efficient DC to THz conversion at room temperature and the possibility of wideband interconnects due to MEMS processing. In an array of micromagnetrons, each separated by less than half wavelength, beam directionality can also be expected. Finally, because the maximum power output at a given frequency is determined by the geometry (aspect ratio), a broadband pulse can be shaped accordingly. The proposed design can become a readily available intense source of THz radiation for Improvised Explosive Device detection, as well as imaging and communication applications.","title":"A THz Micromagnetron [07U07NPSLarr]","awardID":"0715183","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"H186","name":"Defense Intelligence Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"I331","name":"Defense Intelligence Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T885","name":"DIA-MASINT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"J265","name":"Defense Intelligence Agency"}}],"PIcoPI":[339233],"PO":["565136"]},"127501":{"abstract":"Augmented reality (AR) is a technology where computer displays add (superimpose) computer-generated, graphical objects to a user's view of the physical (real) world. AR is distinguished from the better-known virtual reality (VR), wherein an observer sees an entirely computer-generated graphical scene. AR makes possible visualization techniques that have no real-world equivalent; one such technique is x-ray vision, where AR users perceive objects which are located behind solid, opaque surfaces. In this project the PI will empirically study how egocentric depth perception (the distance from an observer to an object) operates in AR. The PI will conduct a series of experiments, in which observers judge the depth of AR-presented virtual objects. These experiments will use two different categories of dependent measures: visually directed actions, and application-based tasks.<br\/><br\/>A commonly used visually directed action is blind walking, where observers view a target, cover their eyes, and then walk to the target location without sight. There are good theoretical arguments that visually directed actions measure a relatively pure percept of egocentric distance, uncontaminated by observers' cognitive knowledge. Furthermore, there is a substantial body of empirical data that describes visually directed action distance judgments of both real-world objects, and virtual objects viewed with VR display devices. These data indicate that, under full-cue conditions, real-world objects are judged without systematic error up to ~20 meters, while the distance of VR objects is systematically underestimated (a phenomenon which has been studied extensively but not yet fully explained).<br\/><br\/>The application-based tasks are motivated by compelling applications of AR technology. One such task is perceptual matching, where the depth of a virtual and a real object are matched; this task is an important component of AR applications in medicine and image-assisted surgery, AR situation awareness in urban settings and buildings, and others. Another such task is forced choice, where the depth of a virtual object is placed into one of a small number of categories relative to other objects. This task is motivated by applications such as an AR airport control tower and an AR urban situation awareness system, where observers must make decisions based on the gross spatial arrangement of virtual and real objects.<br\/><br\/>Although the egocentric depth perception of real-world objects and VR-presented virtual objects has been widely studied, currently there exists very little empirical data on the issue, an absence this research will correct. Furthermore, because the present studies will use two complimentary categories of dependent measures, they will allow measuring the degree to which phenomena such as the VR underestimation effect, which has been found by visually directed action tasks, is also present in qualitatively different dependent measures. This will help resolve controversial questions regarding the degree to which such phenomena arise from the choice of dependent measure versus deeper perceptual mechanisms.<br\/><br\/>Broader Impacts: In an applied context, a better understanding of how AR depth perception operates is necessary for many compelling AR applications to be realized, and the empirical data gathered through this activity will hasten AR application development. In addition, through this activity a series of students will receive a blend of experience in both computer graphics and human-subject empirical methods; upon graduation these students will be well-positioned to contribute to the important emerging research area of applied perception in computer graphics.","title":"HCC: Egocentric Depth Perception in Augmented Reality","awardID":"0713609","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["551043"],"PO":["565227"]},"135377":{"abstract":"This project is carrying out a series of activities intended to explore and catalyze the creation of a National Malware Collaboratory <br\/>The National Malware Collaboratory brings together two synergistic innovations ? a national-scale distributed cyberinfrastructure and a catalytic academic\/industry\/government consortium ? to provide order-of-magnitude improvement to the nation?s defenses against cyber-malware.<br\/>The Collaboratory recognizes the changing nature of the malware threat: from yesterday?s simple worms and viruses to polymorphic, evolvable, broad-spectrum attacks. It recognizes the expanding target of the malware threat, from yesterday?s web servers to today?s critical financial networks, SCADA systems, and national security infrastructure. It responds to these challenges by creating an environment and structure for academia, industry, and government to engage in large-scale, proactive, collaborative malware defense research, and to rapidly transition research results to practical development and deployment.<br\/>Currently the project is carrying out three early activities central to the establishment of the NMC:<br\/>1) Research, design, and design validation critical to the technical structure of the NMC-CI itself. Key foci include distributed resource ownership & federation under different models with varying security policies; secure containment with controlled Internet access, and dynamic configuration.<br\/>2) Identification, refinement and documentation of the research agenda and methodologies enabled by the NMC, working with research and industry communities who stand to benefit from and participate in the Collaboratory. <br\/>3) Structural planning and preliminary outreach to establish the Collaboratory, with early founding members from government, industry, and academia.","title":"SGER: National Malware Collaboratory Investigation (NMCI)","awardID":"0751027","effectiveDate":"2007-10-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7683","name":"SOFTWARE DEVELOPEMENT FOR CI"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["458851","561510"],"PO":["521752"]},"134960":{"abstract":"The goal of this project is developing PetaShake, an advanced computational research platform designed to support high-resolution earthquake simulations on a regional(< 1000 km) scale. PetaShake will extend two high-performance, open-source scientific modeling codes, the finite-difference Olsen code and the finite-element Hercules code, toward petascale capability. These operational codes scale efficiently on thousands of processors, and they are being widely applied to wave propagation simulations, dynamic fault rupture studies, physics-based seismic hazard analysis, and full 3D tomography. The researchers will improve single-processor performance through better cache usage, data localization, and platform-dependent optimizations; parallel performance to scale onto 100,000+ cores through a higher degree of parallelization and overlapping between communication and computation; and I\/O performance to support high-resolution input meshes and time-varying output volumes by parallelizing all I\/O and exploring the use of asynchronous I\/O. They will also improve fault tolerance and fault detection capabilities, and incorporate an on-demand verification and validation capability into the PetaShake platform to support rapid development and enhanced flexibility while maintaining scientific validity. Although the focus of PetaShake will be on capability computing, the research to optimize this platform will enable the petascale capacity-computing and data-intensive goals of three other CME platforms, CyberShake, DynaShake, and F3DT. Southern California, the natural laboratory for the proposed project, comprises23 million people and about half the national earthquake risk. The Southern California Earthquake Center (SCEC) coordinates a comprehensive program of earthquake system science that involves over 500 scientists at more than 50 research institutions, and it incorporates the results into practical seismic hazard analysis. This project will provide the HPC required to achieve the objectives for earthquake source physics and ground motion prediction in the SCEC research plan. Its cyberinfrastructure and simulation results will be used by the SCEC community and its partners in earthquake engineering and disaster management. A diverse set of undergraduate and graduate students from SCEC?s UseIT and ACCESS intern programs will participate, enhancing the career trajectories of women and minorities interested in high-performance computing.","title":"Enabling Earthquake System Science Through Petascale Calculations (PetaShake)","awardID":"0749313","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"7699","name":"ICER"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0603","name":"Division of EARTH SCIENCES","abbr":"EAR"},"pgm":{"id":"7255","name":"GEOINFORMATICS"}}],"PIcoPI":["539162"],"PO":["558205"]},"134971":{"abstract":"TECHNICAL SUMMARY:<br\/><br\/>This award is made on a proposal submitted to the PetaApps Solicitation. The Office of Cyberinfrastructure, the Division of Materials Research and Office of Multidisciplinary activities in the Mathematical and Physical Sciences Directorate, the Engineering Directorate, and the Computer and Information Science and Engineering Directorate contribute funds to this award. <br\/> <br\/>This PetaApps project focuses on hybrid quantum mechanical-atomistic-mesoscale simulations of ion transport and translocation of biopolymers such as DNA and RNA through nanometer scale pores and channels in silica and silicon nitride membranes. The PIs aim to develop a predictive hierarchical petascale simulation framework for: (1) Highly accurate quantum mechanical simulations to describe chemical processes in translocating biopolymers; (2) multibillion-atom molecular dynamics simulations for structural properties and dynamical processes of biopolymers in confined fluidic environments in solid state membranes, with interatomic interactions validated by quantum mechanical calculations and key experiments; (3) hybrid molecular dynamics and adaptive lattice Boltzmann simulations in which molecular dynamics is embedded close to the surfaces of nanopores\/nanochannels and lattice Boltzmann in the rest of the fluid; (4) accelerated dynamics approaches to reach macroscopic time scales for direct comparison with experimental data; (5) meta-scalable, self-tuning multicore parallel simulation algorithms; and (6) automated model transitioning to embed higher fidelity simulations inside coarser simulations on demand with controlled error propagation to quantify uncertainty.<br\/><br\/>After validation, this hierarchical petascale simulation framework will be used to study: (1) Translocation kinetics and dynamics of DNA through silica and silicon nitride nanopores; (2) electronic properties of translocating DNAs for sequential identification of nucleotides; (3) ionic screening of surface charges in nanopores\/nanochannels; (4) streaming electrical current generated by pressure-driven liquid flow in individual silica nanochannels as a function of channel height, pressure gradient, and salt concentration; (5) pressure-driven DNA transport in confined silica channels for novel diagnostic applications such as artificial gels and entropic trap arrays; and (6) surface functionalization, polarity switching, and transient response of silica nanotube, nanofluidic transistors.<br\/><br\/>This project supports training a new generation of graduate students to develop the tools needed to attack complex system level problems. They will learn to combine theory, modeling, and high performance computer simulation. Students will participate in a dual-degree program in which they will fulfill Ph.D. requirements within their own discipline and master?s degree requirements in computer science with specialization in high performance computing and simulations. <br\/>This award also supports the computational science workshops for underrepresented groups. Undergraduate students and faculty mentors from Historically Black Colleges and Universities and Minority Serving Institutions participate in a special one-week intense hands-on experience in parallel computing and immersive and interactive visualization. African American, Hispanic and Native American students will be recruited through USC?s Center for Engineering Diversity and women through USC?s Women in Science and Engineering Program. <br\/><br\/>NON-TECHNICAL SUMMARY:<br\/><br\/>This award is made on a proposal submitted to the PetaApps Solicitation. The Office of Cyberinfrastructure, the Mathematical and Physical Sciences Directorate, the Engineering Directorate, and the Computer and Information Science and Engineering Directorate contribute funds to this award. <br\/><br\/>This award supports the development of software for the most advanced, ?petascale,? high performance supercomputers that will enable simulations that can capture phenomena that span across a range of length and time scales. The PIs will focus on a problem of particular importance, how biomolecules move through nanometer-sized pores in inorganic materials like silica and silicon nitride. The simulation can capture detailed physics of the problem and may illuminate possible applications to sequencing DNA and RNA molecules. The PIs will also focus on how charged atoms and molecules move through channels with dimensions on nanometer length scales more generally. There are potential applications to evolving ?lab-on-a-chip? technologies that seek to miniaturize laboratory analysis functions to the size of electronic device chips. <br\/><br\/>Developed software will be distributed and can be used by a broad community of researchers in a variety of disciplinary and multidiscplinary research involving materials research, chemistry, engineering, physics, and nanotechnology. <br\/> <br\/>This project supports training a new generation of gr","title":"Collaborative Research: Petascale Hierarchical Simulations Of Biopolymer Translocation Through Silicon Nitride And Silica Nanopores And Nanofluidic Channels","awardID":"0749360","effectiveDate":"2007-10-01","expirationDate":"2013-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1712","name":"DMR SHORT TERM SUPPORT"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["490010","490009","490011","542054"],"PO":["565247"]},"134894":{"abstract":"Lead Proposal: OCI - 0749015 <br\/>PI: Dawson, Clinton N<br\/>Institution: University of Texas at Austin<br\/><br\/>Non-lead Proposal: OCI - 0749017<br\/>PI: Hanna, Darrin M<br\/>Institution: Oakland University<br\/><br\/>Non-lead Proposal: OCI ? 0746232<br\/>PI: Westerink, Joannes J<br\/>Institution: University of Notre Dame<br\/><br\/><br\/>Title: \"Collaborative Research\" NSF PetaApps: Storm Surge Modeling on Petascale Computers<br\/><br\/>ABSTRACT<br\/><br\/>The goal of this project is to investigate the use of petascale computing to significantly advance the state-of-the-art in storm surge simulation, to accurately model flows at multiple, interacting scales, at resolution never before attempted, and to demonstrate that results from these simulations can be delivered in real-time to emergency managers. To achieve this goal will require the continued development and improved understand of the mechanisms involved in tightly coupled models of wind, waves, circulation and geomorphology, improvements in the description of the physical domain and adaptive resolution of all energetic flow scales, and investigation of accurate, robust and highly parallelizable numerical algorithms. Efficient implementation of these models on emerging petascale architectures will require utilizing the latest developments in parallel data management, real-time visualization, and programming tools. In this project, the<br\/>PIs will develop high resolution, large-scale coastal inundation models coupled with regional-scale rainfall\/runoff models. Robust and highly parallelizable algorithms will be investigated for solving these systems on petascale architectures. The models will be implemented on NSF Track 2 HPC systems currently under construction; furthermore, implementation of the models on novel hybrid architectures will also be explored.<br\/><br\/>Predicting and studying coastal inundation due to hurricanes and tropical storms is a problem of critical importance to the United States. Hurricane Katrina alone was the costliest and 5th deadliest hurricane in history, with most of the devastation due to wind-driven flooding during the storm. The aftermath of this event has led to a number of federally-mandated studies to determine what failed, the causes of failure, and how to prevent such catastrophes from happening again. Critical decisions will be made in the next several years on how to design better protection systems and improve emergency management practices in the event of future storms. Storm surge is caused by wind, atmospheric pressure gradients, tides, river flow, short-crested wind-waves, and rainfall. In this project, the investigators will develop an accurate numerical model of storm surge which accounts for all of these effects. This model will be tested in predictive mode as storms approach landfall for the purposes of emergency evacuation and response, and used to study the design and implementation of improved man-made and natural protection systems for vulnerable coastal areas. While storm surge models have been developed extensively over the past decade; only within the last few years have the algorithms, computational power and resolution been available to begin to model these events with any reasonable degree of accuracy. In addition to storm surge modeling, the computational methodology and simulation tools developed under this project are applicable to other problems in coastal engineering and marine science, including water quality, shipping and ports, marine ecology, naval operations, weather and climate, and wetland degradation. Furthermore, the technology developed under this project will be disseminated to government agencies such as FEMA, the U.S. Army Corps of Engineers and NOAA.","title":"Collaborative Research: Hurricane Storm Surge Modeling on Petascale Computers","awardID":"0749017","effectiveDate":"2007-10-01","expirationDate":"2013-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7552","name":"COFFES"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":[358378],"PO":["565247"]},"131033":{"abstract":"Efficient Algorithms for Problems in the Next Generation of Computing<br\/><br\/>Cliff Stein<br\/>Columbia University<br\/><br\/><br\/>Computers, computer systems and the computational infrastructure<br\/>provided by the Internet are now essential for many aspects of modern<br\/>life. It is well-known and well-documented that, even as computing<br\/>power and network bandwidth increase at a rapid rate, the demands that<br\/>users and applications place on these resource increase at roughly the<br\/>same pace. Thus, no matter how much progress we make on the hardware<br\/>and network ends, we will always need efficient algorithms to manage<br\/>these resources. This research focuses on several algorithmic<br\/>problems in computer systems and networks; arising both from existing<br\/>technologies and at technologies that are evolving, or have been<br\/>proposed. By improve the efficiency with which we manage our devices<br\/>and infrastructure, there will be tremendous savings, both in time and<br\/>economically. In addition, the investigator integrates theory and<br\/>systems and encourages more collaboration between these areas, with<br\/>obvious benefits. This work has an impact on the decisions that are<br\/>being made presently about the next generation of the Internet, of<br\/>routing, and of scheduling in operating systems.<br\/><br\/>In particular, the investigator studies four concrete problem areas:<br\/>scheduling in operating systems -- finding schedules that are low overhead and use resources fairly; <br\/> scheduling in the next generation of routers -- designing algorithms<br\/>that allow packets with different levels of service to be routed<br\/>efficiently; network coding -- designing algorithms to manage this<br\/>exciting new routing technology, and power management -- designing<br\/>algorithms that schedule jobs to provide good response time and to use<br\/>power efficiently. While the details of these various areas differ,<br\/>the overall goals are to design simple, low overhead algorithms, use<br\/>algorithms with rigorously proved bounds and analysis, design new<br\/>theory and transfer existing theory closer to the systems. The<br\/>investigator also designs educational materials suitable for teaching<br\/>algorithms to a wider audience.","title":"Efficient Algorithms for Problems in the Next Generation of Computing","awardID":"0728733","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["562721"],"PO":["565251"]},"131176":{"abstract":"0729210 - Joint Source-Channel Coding for Wireless Networks<br\/><br\/>Currently, there is a significant demand for reliable, high-quality access to multimedia content over wireless networks. This demand stems from society's growing desire for ubiquitous access to multimedia, whether it be critical medical information (x-rays, real-time patient video), news, or entertainment.<br\/>Providing such services over wireless networks is challenging because wireless channels are subject to a phenomenon known as fading, due to which the instantaneous link quality from the transmitter to the receiver varies with time, frequency and location. The link quality is often not known at the transmitter, necessitating the use of transmission schemes that are robust to<br\/>changes in the link quality. This research involves the design and analysis of efficient encoding schemes for transmission of analog signals such as multimedia signals over fading wireless channels, when the instantaneous link quality is not known at the transmitter. Particular emphasis is placed on techniques which are efficient for broadcasting signals to many users with different link qualities and techniques that allow the reconstruction quality at the receiver to scale gracefully with the link quality.<br\/><br\/><br\/>This research focusses on the design and analysis of joint source-channel coding schemes for broadcast channels and non-ergodic wireless channels. Specific problems addressed in the research include - (i) the design of several novel joint source-channel coding schemes based on layered transmission with superposition coding, hybrid digital-analog coding with and without side information and the use of matched tandem-encoding. Such codes are designed for multiple-input multiple-output channels, single-input single-output Gaussian and erasure broadcast channels, and a single-input single-output broadcast channel in the presence of an interference known to the transmitter and unknown to the receiver. (ii) development of tools to understand the potentials and fundamental limits of these schemes, and in the definition of appropriate metrics that are analytically tractable as well as insightful. (iii) construction of practically realizable joint source-channel codes. Particular attention is given to the construction of a single code of moderate length that is simultaneously a good source code and a good channel code when used with practical encoding\/decoding algorithms, instead of under hypothetical maximum-likelihood decoding.","title":"Joint Source-Channel Coding for Wireless Networks","awardID":"0729210","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["551055"],"PO":["564924"]},"136643":{"abstract":"This project aims at developing mathematical reliability models for fault-tolerant energy-aware parallel disk systems. Reliability models, which are used to estimate reliability, are important tools in the design of fault-tolerant computer systems. In the past decade, various practical reliability models have been constructed for disk systems. However, most of these models were developed for non-energy-efficient disk systems, thereby making it difficult to apply the existing reliability models to energy efficient disks. Therefore, the overall objective of this project is to address the mathemetical underpinnings of modeling reliability of energy-efficient parallel disk systems, where fault tolerance and energy-saving techniques are seamlessly integrated to conserve energy without sacrificing reliability in parallel disks. The project can contribute to reliability modeling techniques for parallel disk systems by developing a reliability analysis modeling toolkit accompanied with a set of novel mathematical ability models. The innovative models include disk power consumption models, a reliability model for parallel disk systems with redundancy techniques, a reliability model for repairable and energy-efficient parallel disk systems, a fault recovery model for energy-efficient parallel disk systems. This research has three main strengths. First, it bridges the technology gap between reliability analysis and energy conservation techniques in the context of parallel disk systems. Second, the research can contribute to the implementation of an array of mathematical reliability models for energy-efficient parallel disk systems. Finally, this project creates a reliability analysis toolkit, which is the first toolkit of its kind designed specifically to study a variety of fault-tolerant and energy-saving techniques.","title":"Mathematical reliability models for energy-efficient parallel disk systems","awardID":"0757778","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["409962"],"PO":["535244"]},"134124":{"abstract":"Automated telephone dialog systems rely disproportionately on accurate transcription of the speech signal into readable text. When the system has low confidence in the automatic speech transcription<br\/>(ASR) of a caller's utterance, a typical dialog strategy requires the system to repeat its best guess, and ask for confirmation. This leads to unnatural interactions and dissatisfied callers. The current project focuses on developing better dialog strategies given current ASR capabilities by learning automatically from contrasting<br\/>corpora, and comparing the results. Using a novel methodology, wizard ablation, simulated human-system dialogs are collected that vary in controlled ways. The testbed application, an Automated Readers Advisor for New York City's Andrew Heiskell Talking Book and Braille Library, has appropriately limited complexity, and<br\/>potentially broad social benefit.<br\/><br\/>The motivation for wizard ablation is that research is needed into the problem-solving strategies humans would use if the human communication channel were restricted to be more like a machine's. In conventional wizard-of-oz studies, unsuspecting users interact with human wizards \"behind-the-screen\", thus providing data on the way humans interact with (what they believe to be) machines. Unlike a conventional wizard, an ablated wizard is restricted to seeing the ASR input to the system dialog manager. Under a further ablation<br\/>condition, the wizard must choose actions from the repertoire that the system uses, but can combine them freely. The book-borrowing scenarios for the wizard interactions have been designed to be realistic, and Heiskell Library patrons participate in the studies. The collected dialogs will be made available to the community.","title":"Incremental Wizard Ablation: A Novel WOz Paradigm for Learning, Testing and Evaluating Human-Machine Dialogue using Parameterized Corpora","awardID":"0744904","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["528943"],"PO":["565215"]},"126677":{"abstract":"All scientific fields report their results in publicly published and permanently archived papers. In a few fields, such as genetic sequencing, the community expects researchers also to publish their data sets, in a standard format, in a permanent archive. This project explores the next step in scientific publishing: capturing and publishing entire experiments that are fully encapsulated, ready for immediate replay, and open to inspection. Experiments and results could be examined, repeated, extended, and reused-- by anyone.<br\/><br\/>This planning grant focuses on assessing the breadth and depth of various communities' interest in such \"open community archives of replayable experiments,\" and their requirements, feasibility, and key design issues. Such \"active libraries\" would directly connect published research to its sources in ways that support new analyses and new executions of computer-based systems. These archives would directly benefit many research and education communities, and their construction would involve a wide array of systems research challenges. A long-term goal is to build a coalition of stakeholders that includes computer and computational scientists, educators, librarians, publishers, and professional societies.<br\/><br\/>The intellectual merit of this project includes assessing many challenging issues, including encapsulation, virtualization, large-scale data management, user interface, and scientific culture and trends. In broader impact, such archives have the potential to change the process of scientific publishing for much computation-based research.","title":"CRI: CRD: Raising the Standard of Scientific Publishing Through an Experiment Archive","awardID":"0709430","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["542054","344532","550238","557575","472196"],"PO":["565272"]},"125236":{"abstract":"Project Id: 0702831 and 0702628 <br\/>PI(s): Sarma Vrudhula and Spyros Tragoudas<br\/>Title: Synthesis, Verification and Testing for Nano-CMOS and Beyond using Threshold Logic<br\/>Institutions: Arizona State University & <br\/><br\/>ABSTRACT<br\/><br\/>By 2020, when thickness of Silicon will be less than a stack of a few atoms, the Semiconductor Industry Association roadmap predicts that further scaling CMOS circuits will not be sustainable, and expects a transition from CMOS to one or more of the presently nascent nano technologies such as resonant tunneling diodes (RTD), carbon nanotube FETs (CNFET) and carbon nanowires. Further in the future are devices such as single electron transistors (SET), and quantum cellular automata (QCA). An important and distinctive characteristic of these post-CMOS nano technologies is that they make it possible to efficiently and naturally implement threshold logic (TL). While TL concepts have been known since the 1960s, there has been no comprehensive work on synthesis and optimization of large TL networks similar to what we have witnessed over the past 30 years for traditional CMOS logic gate networks. <br\/><br\/>This is a proposal to develop a comprehensive design methodology encompassing synthesis, optimization, verification, and testing of TL networks. We propose to investigate synthesis algorithms that start with a technology independent, functional description of the circuit. Optimization of TL networks poses unique problems. Regardless of the underlying technology, TL gates are realized by comparing the weighted sum of the inputs with a given threshold. This can be a comparison of voltages or currents. Since process variations can change the outcome of such a comparison, they not only effect the performance and power but can also change the function realized by the gate. We refer to this as the functional yield (FY). We will develop new algorithms that jointly maximize the FY, power consumption, and performance of a TL network over the space of process variables, e.g. device lengths, widths, threshold voltages, oxide thicknesses, etc. Methods for testing the manufactured circuit for functional correctness and delay using new parametric fault models will also be developed. Verifying the equivalence of a TL network to a given a functional specification has not yet been addressed. This is essential for verifying the result of the synthesis procedure as well as in determining the functional yield when the design parameters are represented as statistical quantities as models of process variations. Expected outcomes of this effort include: new CMOS and post-CMOS circuit architectures for TL gates; algorithms and tools to automatically synthesize, perform functional verification and generate test patterns for TL circuits; methods to compute the parametric yield of TL networks, modeling TL network parameters as correlated random variables; methods to perform joint optimization of functional yield, power consumption and performance of TL networks over the space of process variables.","title":"Collaborative Research: Synthesis, Verification and Testing for Nano-CMOS and Beyond using Threshold Logic","awardID":"0702628","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":["523090","523091"],"PO":["562984"]},"143684":{"abstract":"Multiuser multiple input multiple output (MIMO) communication is likely to become instrumental for future wireless communication networks due to limited frequency spectrum and increasing demand for high data rate and high quality services. Research on MIMO wireless communication has been going on for about a decade. Various MIMO signal processing methods and information theoretical results have been developed using simplified or idealized fading channel models. Relatively less work has been done for the transceiver design of MIMO communications over realistic fading channels, which usually undergo space-selective, time-selective, and frequency-selective fading. This realistic fading is referred to as triply selective fading. The triply selective fading contains doubly selective as a special case but it is not a trivial extension, especially in terms of capacity-based transceiver design. <br\/><br\/>The project investigates several key problems in multiuser MIMO communications over triply selective fading channels. First, it analyzes the effects of triply selective fading and its correlation matrices on the ergodic capacity, outage capacity, network throughput, and error performance of multiuser MIMO multiple access channels (MAC) to assist in the design of capacity-based transmission policies. Secondly, it designs a cooperative multiuser MIMO MAC transceiver that requires only the correlation matrices be provided to optimize transmission policies, and investigate the design methodology for aximizing capacity gains and system performances with low system complexity. Thirdly, it develops signal processing algorithms for the information capacity-based transceivers including single carrier and multi-carrier modulations, multiuser MIMO channel estimation, and multiuser MIMO channel equalization and detection. The research in these topics will provide viable solutions for the ergodic capacity and outage capacity of multiuser MIMO triply selective fading channels that have, to date, not been successfully addressed.","title":"Signal Processing for Wireless Communications over Triply Selective Fading Channels","awardID":"0832833","effectiveDate":"2007-10-31","expirationDate":"2009-11-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["523556"],"PO":["564898"]},"134862":{"abstract":"NSF PetaApps 0749334\/0748898\/0749045: <br\/><br\/>Understanding the Dynamics of the Earth: <br\/>High-Resolution Mantle Convection Simulation on Petascale Computers <br\/><br\/>George Biros (Penn), Omar Ghattas (UT-Austin), Michael Gurnis (CalTech), Shijie Zhong (CU-Boulder) <br\/><br\/>Mantle convection is the principal control on the thermal and geological evolution of the Earth. It is central to our understanding of the origin and evolution of tectonic deformation, the evolution of the thermal and compositional states of the mantle, and ultimately the evolution of the Earth as a whole. Despite its central importance to our understanding of the dynamics of the solid Earth, simulation of global mantle convection at realistic Rayleigh numbers down to the scale of faulted plate boundaries is currently intractable, due to the wide range of time and length scales involved. <br\/><br\/>This project will capitalize on upcoming petascale computing systems to carry out the first high resolution mantle convection simulations that can resolve thermal boundary layers and faulted plate boundaries, which will enable the first inverse solutions that can incorporate historical plate motions. These simulations will lead to breakthroughs in understanding the dynamics of the solid Earth. However, to make effective use of the upcoming petascale systems, new scalable algorithms and implementations are needed. <br\/><br\/>To enable these simulations, this project will: (1) tune, improve the performance of, and scale up to the petascale the parallel open-source mantle convection code CitcomS; (2) develop, implement, robustify, and incorporate new parallel algorithms for adaptive mesh refinement and inverse solution that can scale to hundreds of thousands of processor cores; and (3) release the resulting mantle convection codes to the geosciences community via the Computational Infrastructure for Geodynamics (CIG), an NSF center that develops and maintains software for several earth science communities.","title":"Collaborative Research: Understanding the dynamics of the Earth: High-Resolution Mantle Convection Simulation on Petascale Computers","awardID":"0748898","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"7699","name":"ICER"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0603","name":"Division of EARTH SCIENCES","abbr":"EAR"},"pgm":{"id":"7255","name":"GEOINFORMATICS"}}],"PIcoPI":["560430"],"PO":["496127"]},"130297":{"abstract":"PROPOSAL NUMBER: 0725332<br\/>TITLE: Designing Scientific Software One Workflow at a Time<br\/>PI: Ewa Deelman and Yolanda Gil<br\/><br\/>Much of science today relies on software to make new discoveries. This software embodies scientific analyses that are frequently composed of several application components and created collaboratively by different researchers. Computational workflows have recently emerged as a paradigm to manage these large-scale and large-scope scientific analyses. Workflows represent computations that are often executed in geographically distributed settings, their interdependencies, their requirements and their data products. The design of these workflows is at the core of today?s scientific discovery processes and must be treated as scientific products in their own right. The focus of this research is to develop the foundations for a science of design of scientific processes embodied in the new artifact that is the computational workflow. The work will integrate best practices and lessons learned in existing workflow applications, and extend them in order to define and formalize design principles of computational workflows. This work will result in a fundamentally new approach to designing workflows that will greatly improve the scientific software design methodology by defining and formalizing design principles, and by familiarizing the scientific community with these effective workflow design processes.","title":"Designing Scientific Software one Workflow at a Time","awardID":"0725332","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7652","name":"SCIENCE OF DESIGN"}}],"PIcoPI":["535329","563687"],"PO":["564388"]},"131045":{"abstract":"Some Efficient Error Control Codes Designs for Various Error Channel Models<br\/><br\/> Abstract<br\/><br\/>In the last five decades error control codes have been successfully used to improve the reliability of modern computer, communication and multimedia systems. As the data rate of these systems keeps increasing and as more reliable and secure systems are required for many applications, it is expected that more powerful and efficient codes need to be designed and applied to these systems. This research involves in the design of some efficient error control codes for various error models.<br\/><br\/>First, some Z-channel capacity achieving codes with error free feedback are investigated. In a Z-channel model, the errors in a data word are of 1 to 0 type only. Next, based on this theory some novel diversity combining ARQ (Automatic Repeat Request) protocols are studied. Furthermore, some theory related to elementary symmetric functions is developed. Based on this theory, some new classes of codes capable of correcting t_1 1 to 0 errors and t_0 0 to 1 errors are designed. In addition, based on the elementary symmetric function theory some efficient decoding algorithms for many classes of codes - t-asymmetric error correcting codes, BCH and Goppa codes, error correcting balanced codes, higher order spectral null codes, etc. are studied. Some applications of the theory developed here to other areas are also investigated.","title":"Potentials and Implications of Timing Based Network Covert Channel","awardID":"0728771","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["390316"],"PO":["564924"]},"133135":{"abstract":"The success of ambient interactive environments for people who are blind or visually impaired is measured according to the extent to which they promote independence by endowing these individuals with equal access to the same information that is available to people who are sighted. In this project the PI will explore an innovative \"polymorphic\" framework for such systems, which integrates the user's various sensory modalities in order to assume context-specific roles appropriate to a variety of people and situations. The approach will be given concrete embodiment within the context of an interactive shopping application. Called iCARE, this prototype system will advance the technology of ambient interactive environments through its unique alignment of the system framework to the paradigm of everyday understanding in human psychology, namely sensation, perception and cognition. At the sensation level, sensors of different modalities (including RFID tags and readers, proximity sensors, accelerometers and gyroscopes) will gather data from the environment. These data will be assimilated by a centralized server and analyzed at the perception level to extract information, using techniques such as dead reckoning. Finally, at the cognitive level, the gathered information is presented in a manner that is relevant to a particular user. <br\/><br\/>Broader Impacts: While the immediate focus of the project is on providing a viable and holistic independent shopping experience for the specified target population, the techniques developed will generalize and serve as a template for future delocalized one-point solutions for other types of large public environments. Ultimately, the approach may lead to the acceptance and adoption of similar systems by the mainstream market (e.g., for e-commerce applications).","title":"HCC: The iCare Ambient Interactive Shopping Environment","awardID":"0739744","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["495194"],"PO":["565227"]},"125886":{"abstract":"Emotion and motivation are fundamental to learning; students with high intrinsic motivation often outperform students with low motivation. Yet affect and emotion are often ignored or marginalized with respect to classroom practice. This project will help redress the emotion versus cognition imbalance. The researchers will develop Affective Learning Companions, real-time computational agents that infer emotions and leverage this knowledge to increase student performance. The goal is to determine the affective state of a student, at any point in time, and to provide appropriate support to improve student learning in the long term. Emotion recognition methods include using hardware sensors and machine learning software to identify a student's state. Five independent affective variables are targeted (frustration, motivation, self-confidence, boredom and fatigue) within a research platform consisting of four sensors (skin conductance glove, pressure mouse, face recognition camera and posture sensing devices). Emotion feedback methods include using a variety of interventions (encouraging comments, graphics of past performance) varied according to type (explanation, hints, worked examples) and timing (immediately following an answer, after some elapsed time). The interventions will be evaluated as to which best increase performance and in which contexts. Machine learning optimization algorithms search for policies that further engage individual students who are involved in different affective and cognitive states. Animated agents are enhanced with appropriate gestures and empathetic feedback in relation to student achievement level and task complexity. Approximately 500 ethnically and economically diverse students in Massachusetts and Arizona will participate.<br\/><br\/>The broader impact of this research is its potential for developing computer-based tutors that better address student diversity, including underrepresented minorities and disabled students. The solution proposed here provides alternative representations of scientific content, alternative paths through material and alternative means of interaction; thus, potentially leading to highly individualized science learning. Further, the project has the potential to advance our understanding of emotion as a predictor of individual differences in learning, unveiling the extent to which emotion, cognitive ability and gender impact different forms of learning.","title":"HCC: Collaborative Research: Affective Learning Companions: Modeling and Supporting Emotion During Learning","awardID":"0705883","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["552861"],"PO":["564456"]},"126535":{"abstract":"0708962<br\/> <br\/>Collaborative Research: CRI: IAD: Electronic Testing Education, Research and Training Infrastructure<br\/><br\/>Vishwani Agrawal<br\/><br\/>This three-year project will develop capabilities for testing of digital, memory, analog, and radio frequency devices, with possible future upgrades to MEMS, optical and nanotechnology devices. The infrastructure developed under this project consists of a test laboratory and its value-added applications. The new VLSI test laboratory will have modern automatic test equipment (ATE) of open architecture. The laboratory will be located at Auburn University in Alabama and will be used through networked access by three other institutions of that state, namely, University of Alabama at Tuscaloosa, University of Alabama at Huntsville, and Tuskegee University, for advancing education, training and research applications. In the future, the test lab infrastructure will allow collaborative use by universities outside Alabama as well, making it a national resource. The applications will include new university courses with hands-on experiments on testing of digital, analog and radio frequency chips, FPGAs and system-on-a-chip devices; industry-oriented training classes including test lab exercises; and research on silicon debug methods aimed at improving the yield and reliability. Research with industry focus and practice-oriented short courses are expected to make this infrastructure fully supported through industry funds beyond the three-year infrastructure building period.","title":"Collaborative Research: CRI: IAD: Electronic Testing Education, Research and Training Infrastructure","awardID":"0708597","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7399","name":"CISE MINOR INST INFRA (MII) PR"}}],"PIcoPI":["255259","361062","426769","385237"],"PO":["565272"]},"126557":{"abstract":"0708788<br\/><br\/>CRI: IAD Keeping Pace with Growing Computing Needs: A Strategy for Enhancing Multi-Core Microprocessor Research and Education at Cornell University<br\/><br\/>David Albonesi<br\/><br\/>Due to a number of factors, the computing requirements for even a modest-size academic research group in computer architecture have risen dramatically in recent years. In order to keep pace with the rapidly growing research and educational needs, the research group at Cornell has adopted a cost-effective approach to building compute clusters for the laboratory. An opportunity exists to roughly double the present computing resources at modest cost which will have a profound impact on the research and educational activities. The computing infrastructure will greatly enhance the research capabilities in multi-core computer architecture. This research effort is on improving multi-core reliability and power-performance efficiency, and in the efficient exploration of multi-core design spaces. The additional computing capabilities enabled by this grant will permit the researchers to more comprehensively address these areas, but also to exploit the synergy among these activities. As one example, the additional computing capability will permit an understanding of the combined performance, power, and reliability tradeoffs in future multi-core microarchitectures through the use of predictive modeling. The research enabled by the new infrastructure will directly address several critical design challenges of future multi-core microprocessors, and thus will have high relevance to U.S. microprocessor manufacturers.","title":"CRI: IAD Keeping Pace with Growing Computing Needs: A Strategy for Enhancing Multi-Core Microprocessor Research and Education at Cornell University","awardID":"0708788","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["550887","359520","499395"],"PO":["550859"]},"127426":{"abstract":"Collaborative Proposal pair: 0713435 (Lead) & 0713148<br\/>\"Collaborative: RI: Feature Discovery and Benchmarks for exportable Reinforcement Learning\"<br\/>PI: Ronald Parr, Duke University<br\/>PI: Michael L. Littman, Rutgers University<br\/><br\/>ABSTRACT<br\/><br\/>This project focuses on several aspects of automated feature discovery in the context of reinforcement learning. Badly chosen features cause reinforcement-learning algorithms to fail and, as such, only individuals skilled in feature construction can create successful reinforcement-learning systems for novel tasks. This issue underscores two shortcomings in existing research. First, most existing reinforcement-learning methods cannot generate or discover features automatically and robustly. Second, existing benchmark problems and paradigms for benchmarking do not distinguish adequately between clever algorithm design and clever feature engineering.<br\/><br\/>This project addresses these challenges in two-pronged approach. The first prong aims to advance a technical agenda leading to a new approach to feature discovery and model representation. The second prong is the development of a benchmark methodology and repository with a different focus and structure from existing endeavors. The goal for the benchmarking effort will be to produce a set of fair and reproducible experiments that will help elucidate the strengths and weaknesses of existing approaches, while simultaneously introducing challenges to motivate the development of new approaches.","title":"RI: Collaborative Research: Feature Discovery and Benchmarks for Exportable Reinforcement Learning","awardID":"0713148","effectiveDate":"2007-10-01","expirationDate":"2012-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550488"],"PO":["491702"]},"136017":{"abstract":"Significant progress has been made in exploiting the practical utility of antineutrino detection for nonproliferation purposes, in particular, reactor monitoring, and for geophysics purposes, by mapping the earth?s core, crust, and mantle. Despite these remarkable successes, these inter-related communities have had only modest overlap. As a result, the fields of applied and basic antineutrino physics lack an overarching strategy that would allow efficient exploitation of the resources and strengths provided by each stakeholder. This workshop aims to develop the basis for such a strategic alignment of interests between the fundamental and applied antineutrino physics communities. <br\/>Workshop reporters will summarize the state of the art in current antineutrino detection; technologies and applications, and participants will work intensively to create an environment for collaborative research across the community. Further, participants will examine the practicality of establishing a National Center for Neutrino Study (NCNS) similar in nature to the Space Telescope Science Institute or the Advanced Photon Source. User community members will be invited to participate in the workshop and will be asked to describe their vision of utilizing neutrino detection devices nationally with the intent to integrate their use globally. These visions will be informed by a clear understanding of the prospects and limitations intrinsic in antineutrino physics provided by the US neutrino science community attendees. The importance of this workshop will be to unite disparate groups of multi-disciplinary scientists with shared goals in antineutrino detection research and applications. A goal of the workshop is to develop stronger bonds of communications between these fields. This step is necessary in order to fully understand the various the components of an antineutrino spectrum and identify correctly the sources of the signal.","title":"Workshop on Neutrino Detection for Nuclear Monitoring : 30 Oct-1 Nov 2007","awardID":"0754061","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T885","name":"DIA-MASINT"}}],"PIcoPI":["551625"],"PO":["565136"]},"129539":{"abstract":"A growing consensus among experts is that the routing system is <br\/>approaching a critical architectural breaking point. The Internet <br\/>Architecture Board has tried to identify the factors that limit <br\/>routing scalability and reached the conclusion that the most acutely <br\/>scale-limiting parameter of the current routing system is routing <br\/>table size, not so much for its memory requirements as for its <br\/>reaction to network dynamics. This conclusion is not surprising in <br\/>light of recent research demonstrating that no routing algorithm can <br\/>provide reasonable scalability bounds on dynamic Internet-like graphs. <br\/><br\/>These findings offer the ominous but definitive lesson: to scale <br\/>efficiently and indefinitely, we must learn how to route without <br\/>topology updates. Updateless routing seems impossible at first <br\/>glance, but Milgram's 1967 experiments showed that such routing is <br\/>in fact a reality of greedy search strategies in social networks. <br\/>Jon Kleinberg provided the first model formally demonstrating <br\/>efficiency of such greedy routing strategies. However, both the <br\/>Kleinberg model and its subsequent variations deal with graph <br\/>topologies vastly different from scale-free topologies of observed <br\/>complex networks, including the Internet. <br\/><br\/>This project proposes a new model of Greedy Routing on Hidden <br\/>Metrics, the GROHModel, which generalizes the Kleinberg model and <br\/>naturally yields scale-free topologies. The model employs the <br\/>concept of a hidden metric space (HMS) existing behind every complex <br\/>network, including the Internet. The project thoroughly investigates <br\/>the hypothesis that the observable scale-free structure of complex <br\/>networks is a consequence of natural evolution that maximizes the <br\/>efficiency of greedy routing on these HMSs. <br\/><br\/>The research agenda of the project is two-fold: (1) demonstrate the <br\/>existence of HMSs, thus validating the GROHModel premises; and (2) <br\/>build methodologies to explicitly re-construct the HMS for the <br\/>observable Internet topology, and more generally for any given <br\/>complex network. As soon the Internet's HMS is reconstructed, one <br\/>can use it to deliver addressing schemes for updateless, <br\/>indefinitely scalable Internet routing architectures based on greedy <br\/>routing strategies. <br\/><br\/>The intellectual merit of this project involves concerted <br\/>cross-fertilization across fields of networking, theoretical <br\/>computer science, physics, and mathematics. The project develops a <br\/>novel network modeling methodology that is elegantly generic in <br\/>nature, mathematically sound, and promises a solution to one of the <br\/>most challenging problems of future large-scale networking. <br\/><br\/>Since the Internet is just one of many complex networks that form <br\/>essential fabrics of human life, the potential advances of this work <br\/>are profound. Navigability of complex networks, i.e., efficiency of <br\/>targeted information propagation in them, has a fundamental <br\/>relationship to their structure. Proved faithful to reality, the <br\/>fundamental understanding advanced with the GROHModel may also result in <br\/>advances in understanding of structure and function of biological, social<br\/>and language networks.<br\/><br\/>Broader Impacts: the effects of scaling limits of conventional routing<br\/>on current networks include performance drops, loss of reachability and <br\/>high cost. As the networks grow, these are worsened. In present times,<br\/>operators and enterprises are attempting a transition to IPv6 in order<br\/>to allow virtually limitless sites to connect directly to the Internet.<br\/>This transition is expected to worsen routing strains. This project's <br\/>innovative reexamination of the routing solution space is relevant and<br\/>timely.","title":"NeTS-FIND: Greedy Routing on Hidden Metric Spaces as a Foundation of Scalable Routing Architectures without Topology Updates","awardID":"0722070","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["521741","560811"],"PO":["565090"]},"130562":{"abstract":"Semantic Goals for Communication<br\/><br\/>Madhu Sudan<br\/>MIT CSAIL<br\/><br\/>This project initiates the study of semantic issues underlying<br\/>communication: How do two communicating parties resolve <br\/>potential misunderstandings about the {\\em interpretation}<br\/>of the information (bits) that they are exchanging?<br\/>The principal objective of this project is to produce <br\/>a series of mathematical definitions capturing the underlying <br\/>problem. Furthermore the project seeks to discover paradigms <br\/>that lead to robust communication, without misunderstanding.<br\/><br\/>The principal insight at the core of this project is that<br\/>each communicating player has selfish goals that describe what<br\/>it would expect from the communication with the other players. <br\/>Such goals are non-trivial (can not be achieved by one player<br\/>alone), and verifiable by the player. The project explores the <br\/>possibility of using these verifiable goals to enable mutual <br\/>understanding. Specifically, it uses research from the last <br\/>several decades in complexity theory on interactive proofs and<br\/>program checking to suggest scenarios in which communicating <br\/>players could start without any prior common background and<br\/>start to converge towards a common understanding, thereby <br\/>establishing a theory of understanding. The broader impact of <br\/>this project would be to enable a mathematical approach to <br\/>understanding human communication and development of language, <br\/>as also to guide the design of robust protocols for inter-computer<br\/>communication.","title":"Semantic Goals for Communication","awardID":"0726525","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["425345"],"PO":["499399"]},"131684":{"abstract":"Proposal ID(s): 0732318 and 0732299<br\/>PI(s): Haesun Park and Moody Chu<br\/>Institition(s): GaTech and NCSU<br\/><br\/>Title: Collaborative Research: Fast Nonnegative Matrix Factorizations: Theory, Algorithms, and Applications<br\/><br\/>ABSTRACT:<br\/><br\/>Mathematical models with nonnegative data values are abounding in sciences and engineering. For the sake of physical feasibility and interpretability, the nature of nonnegative must be retained in computation and analysis. This work concerns itself with the factorization of nonnegative matrix into product of lower rank nonnegative matrices. Such a notion of the nonnegative matrix factorization plays a major role in a wide range of important applications including text mining, cheminformatics, factor retrieval, image articulation, bioinformatics, and in dimension reduction and clustering in pattern and data analysis. The discoveries from this proposed research are expected to impact not only the advanced theoretical foundations of matrix computation, but also contribute to the general areas of data mining such as dimension reduction, clustering, and visualization.<br\/><br\/>The basic question behind the nonnegative matrix factorization (NMF) is to best approximate a given nonnegative data matrix as the product of two lower dimensional and, hence, lower rank nonnegative matrices. The two lower rank matrices provides lot of essential information that, otherwise, would be difficult to retrieve from the original matrix. Many NMF techniques have been proposed in the literature, yet there is still little theory on how the NMF can be robustly and efficiently solved. In this work, development of new faster algorithms will be conducted through structured and comprehensive performance evaluation of promising research directions, including the active set and geometry based algorithms, against real-world application data to obtain valuable insights. The proposed study of the geometric structure of the NMF and theoretical properties of the NMF algorithms, such as convergence, should provide the basis of assessment for any NMF methods. Applicability of the NMF to dimension reduction and clustering will also be investigated. <br\/><br\/>Results of this research are also likely to have potential applications in database management, medical examination and diagnosis, bio-chemical selection, and biological networks.","title":"MSPA-MCS: Collaborative Research: Fast Nonnegative Matrix Factorizations: Theory, Algorithms, and Applications","awardID":"0732318","effectiveDate":"2007-10-01","expirationDate":"2013-09-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["562362"],"PO":["562984"]},"133510":{"abstract":"Many professions teach new entrants to the profession through an apprenticeship, in which they progressively build knowledge of the profession through making peripheral contributions to the professionals with whom they are apprenticing. Lave and Wenger have theorized that this learning is facilitated by legitimate peripheral participation (LPP) within a community of practice. This is a very difficult thing to achieve in an academic environment, especially in a face-to-face instructional mode. Using commercial off-the-shelf components, this project will replicate a portion of the real world in the form of an online 3D virtual world, and see if and how students form these ''communities of practice'' while solving assigned problems within this world. Much like a multiplayer online game, students will work in teams to solve problems for simulated businesses in the world, much as they would do after graduation. The only difference is that, while students are expected to collaborate with each other within the environment, they would also be collaborating with conversational ''bots'' and other objects (artifacts) as well, who represent employees, managers, and business objects (e.g., merchandise, tools, etc.), serving as containers that possess knowledge of the domain of interest. This exploratory project will research the following questions: 1) Do students learn technology problem solving better using the virtual world, than they do without it? Why? 2) Do students build better domain and conceptual models in this environment? 3) Does student involvement in virtual worlds provide a greater degree of legitimate peripheral participation than traditional course projects and exercises?, 4) Do virtual worlds facilitate the creation of mini-communities of learning, better than discussion groups and computer-mediated group projects?, 5) Do student prefer working in the virtual world over working with traditional course materials?, and 6) How should the environment and artifacts be designed to facilitate student interaction?<br\/><br\/>This project has the potential to provide important preliminary information regarding the effectiveness of virtual worlds such as Second Life as a medium for computer science instruction. Further, given that such virtual worlds appear as sophisticated games to students, instruction in such environments has the potential to increase student interest in computer science as a field.","title":"Establishing Virtual Worlds as Contextual Basis for an IT Curriculum - SGER","awardID":"0741700","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[354959],"PO":["564456"]},"131101":{"abstract":"Machine learning is increasingly used to automate many tasks, such as training email clients to recognize unwanted email messages. The knowledge gained by a machine learning algorithm must be represented internally in some form. For example, assume that an email filtering algorithm has learned that if an email message contains the phrase ``fast profits'' then it is very likely an unwanted message, while if it contains the phrase ``computational learning theory'' then it is likely a legitimate message. The algorithm might represent this knowledge by associating a large negative numeric weight with the first phrase and a large positive weight with the second. Given a new message, the algorithm could first determine which phrases were present in the message, sum the corresponding weights, and mark the message as unwanted if the sum was negative. The described knowledge representation is a form of linear threshold function; such functions are the basis for many common knowledge representations produced by machine learning programs.<br\/><br\/>This research addresses foundational questions related to the learning of linear threshold functions and other important classes of functions. <br\/> Answers to such questions should be useful to theoreticians and could lead to better applied machine learning algorithms and the identification of fundamental limitations of certain algorithmic approaches, saving wasted development efforts. The primary research methodology used is discrete Fourier analysis, and a second project goal is to develop new Fourier techniques applicable beyond learning. A third project objective is to provide a stimulating research experience to undergraduate and Master's students.","title":"RUI: Fourier-Based Learning of Fundamental Function Classes","awardID":"0728939","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[348029],"PO":["565251"]},"131134":{"abstract":"The focus of this project is integrated design and analysis of communication networks in service of coordinated control of multi-vehicle systems. Consider a set of vehicles, equipped with local controllers and wireless radios, that is set to arrange itself, to stabilize, and to control its collective motion. To achieve globally desirable formation behavior, the controller on a given vehicle must respond to the motion and state of others.<br\/><br\/>In fact, there exists a complicated coupling among system components: network architecture, communications protocols, and controller design. The integrated design of these components is the objective of this project. The fundamental challenge in designing networked control systems is that the tasks of communication and control, in general, cannot be considered decoupled from each other without loss of optimality. However, modular solutions can potentially provide significant insights into the nature of efficient solutions. This project addresses the problem of integrated communications and control from a practically viable perspective by decomposing the problem into modular tasks. The introduced degree of modularity, despite its sub-optimality, enables practical and efficient solutions as well as insights into the inherent trade-offs. Because the questions that arise lie at the intersection between communications and controls research, the components of the project bring together expertise in decentralized control, networking, and signal processing through the following specific tasks: 1) Nonlinear coordinated control over dynamic graphs, 2) Crosslayer optimization of wireless networks in service of coordinated control, 3) Physical layer solutions to decentralized communication and control, and 4) Experimental performance evaluation on a 3D autonomous underwater vehicle test-bed at the University of Washington.","title":"Collaborative Research: New Communication Infrastructures For Networked Coordinated Control","awardID":"0729060","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["553607"],"PO":["564898"]},"133576":{"abstract":"Interoperation between systems which use extensive formalized knowledge<br\/>is hampered by the wide variety of formalisms currently in use in such<br\/>systems. Attempts to create World-wide Web standards have already<br\/>yielded a collection of such formalisms, not all mutually compatible. <br\/>This situation results in part from a methodology which views deductive<br\/>efficiency of inference engines, rather than semantic clarity, as the<br\/>primary design criterion for new formalisms. Recent work (reviving an <br\/>older idea) has focussed instead on the design of a single, <br\/>highly expressive, logic into which a wide number of existing <br\/>formalisms can be straightforwardly mapped, essentially treating <br\/>them as subsets or simple ontologies (theories) within the <br\/>single common formalism. The resulting logic, <br\/>called the Interoperation Knowledge Language (IKL), <br\/>can express many representational strategies and the relationships<br\/>between them in a single formalism, in principle overcoming this <br\/>interoperation problem in many common cases. To realize this potential <br\/>in practice requires an inference engine to process IKL.<br\/><br\/>Although IKL is more expressive than first-order logic and so is<br\/>not decidable, many of the formalisms translated into it have very <br\/>tractable inference behavior. This project implements a new design <br\/>of a reasoner which is theoretically complete for IKL, while also having<br\/>the ability to recognize many of the known tractable subcases and <br\/>use efficient inference strategies on inputs which fit into these <br\/>known cases. The behavior of the engine is directed by scripts<br\/>which control its search behavior in a highly flexible multi-directional <br\/>search space combining hyper-resolution, rule-saturation and <br\/>tableaux reasoning. The engine is designed as an experimental <br\/>workbench rather than a production engine, with a focus on inventing<br\/>useful techniques for controlling its behavior, running large<br\/>experiment suites semi-automatically, and gathering information <br\/>relevant to the search process. We plan to make use of this engine<br\/>in a series of projects devoted to learning new inference strategies <br\/>from empirical ontological data. <br\/><br\/>Broader Impacts. <br\/><br\/>The code created by this project will be publicly available <br\/>under the GNU Lesser General Public Licence. <br\/><br\/>Details of the project can be found on the project web page at <br\/>http:\/\/homam.ihmc.us\/silkie\/Home.html","title":"IKL Reasoning Engine","awardID":"0742022","effectiveDate":"2007-10-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[355184,355185],"PO":["563727"]},"131046":{"abstract":"Wireless ad hoc and sensor networking has been identified as a recent major success story in the field of communications. Today, it is emerging as a promising technology with an expansive range of applications. The full potential of these networks is yet to be realized as a result of design challenges in the related networking issues. This research seeks to develop a research area crossing frontiers in random graph theory, probabilistic methods, communications and networking. Focusing on the analysis and design of finite wireless networks (i.e., networks with small or moderate number of nodes), for which both existing asymptotic analysis based on infinite number of nodes or simulation based approaches are inadequate, the work has the potential to advance knowledge and understanding across several related fields. <br\/><br\/>Mathematical study of wireless networks serves as a theoretical foundation for this area. During the last ten years a variety of network properties such as network connectivity, coverage, reliability, delay, lifetime, sleep scheduling, throughput and capacity have been analytically studied to some extent. The majority of these analytical studies have concentrated on asymptotic scenarios, i.e., when the number of nodes tends to infinity. To study finite (small or moderate-size) networks researchers often resort to computer simulations and algorithmic approaches (such as linear programming). Thus, mathematical analysis of finite networks that could result in practically-useful formulas has been largely ignored. The goal of this research is to develop a general framework for design, study, and optimization of finite wireless ad hoc and sensor networks.","title":"Collaborative Research: Study of Wireless Ad-Hoc and Sensor Networks in a Finite Regime","awardID":"0728772","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["485126"],"PO":["564924"]},"131178":{"abstract":"The economies of scale of cellular and WiFi networks are enabled by low-cost integrated circuit implementations of sophisticated digital signal processing (DSP) algorithms in wireless communication transceivers. An implicit assumption in this approach is that analog received signals can be converted to a reasonably faithful digital representation, an assumption that breaks down as link speeds increase to the point that high-precision analog-to-digital conversion (ADC) becomes too costly and power-hungry. This project involves the design of wireless networks in the latter regime: the goal is to design low-cost links operating at multiGigabit speeds (i.e., more than an order of magnitude faster than WiFi), exploiting large swaths of unlicensed spectrum in the 3-10 GHz band and the 60 GHz band. The research rethinks communication transceiver design, with the starting assumption that high-speed ADCs are ``sloppy.'' The research involves obtaining fundamental performance benchmarks using information theory, and devising DSP algorithms that achieve these performance benchmarks. The ultimate objective is to enable a quantum leap in the speed of wireless networks for the home and enterprise, while preserving the economies of scale associated with low-cost silicon implementations.<br\/><br\/>While conventional systems use 6-12 bits of ADC precision, this research considers the design of communication systems for low-resolution (1-4 bits) ADC, including Shannon theoretic benchmarks and algorithms for synchronization and equalization. Since high-speed digital-to-analog conversion is easier than ADC, precoding strategies which move complexity to the transmitter are investigated. The use of time-interleaved ADCs to attain higher precision, and hence higher dynamic range, is considered for both singlecarrier and multicarrier systems. The approach is to design receiver algorithms that jointly address mismatch between the component ADCs and the channel dispersion.","title":"Towards A Theory of Communication With Sloppy Analog-to-Digital Conversion: A Framework for Low-Cost Gigabit wireless","awardID":"0729222","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["549356"],"PO":["432103"]},"125887":{"abstract":"Project Summary<br\/>The goals of the proposed research are twofold: first, to advance the state of the art in artificial intelligence and cognitive sciences by developing novel probabilistic reasoning techniques; and second, to use these techniques in building better transportation models, which can then be used to help inform public deliberation regarding major infrastructure decisions. Problems of maintaining or replacing aging infrastructure, or adding new infrastructure to meet the needs of population growth and urban expansion of metropolitan areas, are becoming increasingly difficult to solve, in part because the cost is extremely large, and in part because the political discourse over alternative solutions is contentious and reflects divergent assumptions and values. Often, a major source of disagreement is cost; but another is rooted in differing assumptions about how people would adjust their travel in response to changed circumstances in both the short and long term, and how much congestion would result. Current transportation models used in operational analysis and planning are too behaviorally simple to be very useful in addressing these questions. Recent research advances have provided improvements in behavioral representation in these kinds of choice situations, but to date these nnovations are not integrated and are computationally not feasible for large-scale application. During the last decade, the artificial intelligence community has developed a set of techniques that enable fine-grained activity recognition from sensor data; among the most advanced and successful are approaches based on Dynamic Bayesian networks and statistical relational learning. The research team will build on this foundation, integrating these AI techniques with the Discrete Choice Models used in econometric approaches, to yield a new, hybrid reasoning system: Dynamic Discrete Choice Networks. This technique will be applied to the challenging domain of modeling dynamic travel choices of individuals, such as the number of trips, scheduled time of departure, destinations, modes, and routes and to predict how these choices change under dynamically updated travel conditions. <br\/><br\/>Intellectual Merit<br\/><br\/>The merit of this proposal is grounded in the research challenges in the artificial intelligence and urban modeling areas. This project advances the state of the art in artificial intelligence and cognitive sciences by developing novel probabilistic reasoning techniques that are well suited for modeling the complex combinations of factors involved in human decision making in the commonsense domain of daily travel. By integrating this modeling power into probabilistic temporal models, Dynamic Discrete Choice Networks will provide an extremely general and flexible framework for learning and recognizing human activities from sensor data and for understanding how everyday human decision making adapts to a constantly changing environment.<br\/><br\/>Broader Impacts<br\/><br\/>UrbanSim has the potential to significantly aid in public deliberation over major decisions regarding transportation replacement or expansion of transportation infrastructure, managing urban development, planning for response to mitigate the effects of events such as hurricane Katrina or a major earthquake, and other issues. UrbanSim is Open Source and freely available, and has already attracted considerable interest and use. Because of their improved ability to recognize and analyze human activities from raw sensor data, Dynamic Discrete Choice Networks will have applications to other significant domains as well, such as eldercare and long term health monitoring.","title":"RI: Dynamic Discrete Choice Networks -- An Artificial Intelligence Approach to Modeling Dynamic Travel Behavior","awardID":"0705898","effectiveDate":"2007-10-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["438603","550054",334124,"440155"],"PO":["543481"]},"127515":{"abstract":"This is a project to develop new methods for scientifically studying and assessing human cognitive function. It will employ sophisticated statistical multimodal data analysis techniques that will fuse contextual, behavioral, and neural information simultaneously obtained from human beings in the process of completing complex batteries of cognitive tasks. The tasks will be presented in the form of customized computer games that are designed to exhibit the crucial aspects of established cognitive assessment tests and at the same time provide a motivating and engaging environment for the subject's interactions with the game and computer agents. The tasks will involve exploiting our existing capabilities of monitoring and controlling certain enjoyable and challenging computer games that involve various combinations of cognitive tasks ranging from working memory and attention to executive functions. Multimodal information fusion will be accomplished by utilizing Bayesian inference techniques and information theoretic data analysis and dimensionality reduction methods. <br\/><br\/>The work to be carried out under this grant aims to develop sophisticated pattern analysis techniques for the purpose of analyzing the fine-grain behaviors of elderly when they are engaged in complex cognitive tasks in the form of computer games. Expected significant scientific findings from the proposed research are two-fold: (1) improved statistical signal processing and pattern recognition algorithms for EEG processing, (2) an enhanced understanding of the interplay of multiple cognitive processes and their neural signatures in EEG during the execution of complex tasks. <br\/><br\/>The approach is innovative in terms of three aspects: (1) an advanced adaptive interaction protocol that modifies the task parameters to maintain maximal sensitivity to cognitive state changes will be employed, (2) novel information theoretic techniques will be developed and utilized for the extraction of maximally discriminative features from EEG measurements for cognitive state estimation and neural activity visualization, (3) the developed closed-loop system will be utilized to study the human-agent interaction in complex cognitive tasks resulting in mathematical models of micro-behavior in realistic evolving environments as opposed to traditional stationary repetitive experimental paradigms. <br\/><br\/>The successful completion of the work will open the way to further collaborative activities in brain interface design, closed-loop collaborative augmented cognition human-agent interfaces for improved performance, and early diagnosis of cognitive decline in elderly. An interdisciplinary research environment will engage the participating graduate students in a multidisciplinary educational setting and will help them develop skills to perform collaborative interdisciplinary research.","title":"HCC: Assessing Cognitive Function from Interactive Agent Behavior","awardID":"0713690","effectiveDate":"2007-10-01","expirationDate":"2009-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[338149,"506513","564897"],"PO":["564456"]},"126679":{"abstract":"0709438<br\/><br\/>Building Human Infrastructure: A Pipeline for Minority Preparedness in High Computing (H-PIPE) <br\/><br\/>Patricia Nava<br\/><br\/>The Distributed Computing Lab (DCL) was established with NSF funding (NSF-CISE EIA-0325024). H-PIPE will utilize the DCL as a hub for educational, research and outreach activities that, in concert, result in more individuals trained in the design, operation and applications of high-performance computers. Support of this program will enhance a proven model to recruit, support, and develop talent in underrepresented minorities that are trained in the CISE related fields. The research effort will use the student-constructed 40-CPU (20-node) Computer cluster that is currently at the core of the DCL and utilize it in three interleaved efforts: (1) to perform enhanced outreach to minority and female high-school students; (2) to provide further enrichment of educational activities in CISE; and (3) to involve students in exciting relevant research and use the DCL as a catalyst to promote multi-disciplinary collaborations that can benefit from this form of computing. H-PIPE will afford students the opportunity to carry out hands-on projects in Artificial Neural Networks, Biomedical Engineering, Operating Systems, Computer Network Protocols and Computer Architecture.","title":"CRI: Building Human Infrastructure: A Pipeline for Minority Preparedness in High Performance Computing (H-PIPE)","awardID":"0709438","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7399","name":"CISE MINOR INST INFRA (MII) PR"}}],"PIcoPI":[336181,"544532","527334","463422"],"PO":["565272"]},"125117":{"abstract":"The storage devices now can be connected to hosts, servers, Storage Area Networks (SANs) and Local\/Wide-area Networks (LANs\/WANs). This proliferation of device attachments leads to flexible deployment of storage and potential pooling and sharing of storage across multiple hosts and operating systems. However, native file systems, such as Windows NTFS or Linux Ext2, expect to have exclusive access to their storage volumes. In other words, each operating system reserves storage devices for its use, and the space in a storage device owned by one operating system cannot be used by another. This static allocation seriously hampers the sharing of storage space. This lack of flexibility also hampers \"utility computing\" models where users can be charged based <br\/>on usage of resources. This lack of flexibility also limits the sharing of storage resources across different divisions of an organization. <br\/><br\/>This project plans to design and develop \"virtual allocation\" or \"on-demand allocation\" in storage systems. Virtual allocation will employ allocate-on-write policy i.e., storage space is allocated when the data is written. Such an approach separates the storage space allocation from the file system size and allows creation of virtual storage systems with many novel features. The project will explore architectural issues related to performance, device characteristics, locality and parallelism in employing virtual allocation. The project will investigate if virtual allocation can enable dynamic data redistribution or migration across shared networked storage devices in order to balance locality and load across changing workloads and usage patterns. The project will investigate if virtual allocation can enable power savings by pooling data onto a smaller number of devices at times of low demand.","title":"Dynamic allocation and data distribution in networked storage systems","awardID":"0702012","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["550709"],"PO":["565272"]},"125238":{"abstract":"CCF-0702635<br\/>TITLE Modular Static Checking of Software Design Intent Using Permissions PI John Boyland<br\/><br\/>Modularity is the key technique for controlling the complexity of large software projects. Modules often have requirements on how they communicate and collaborate with other modules. This work extends what can be expressed and checked in a modular way.<br\/><br\/>Annotations of design intent about properties such as aliasing, effects and locking are given a strong semantic foundation using the concept of ``permissions.'' Annotations avoid both the need of a type system to have a single decidable semantics, and the excessive detail of full specification. Rather, a single (in principle undecidable) semantic system can be checked using a variety of different algorithms ranging from syntax-directed analysis to theorem proving.<br\/><br\/>This work has the following objectives: (1) show soundness of a full system of permissions; (2) give a semantics for a wide-range of design-intent annotations; (3) implement permissions checkers; and (4) evaluate their usefulness on large commercial and open-source software.<br\/><br\/>This research uses the techniques of logics, type systems and program analysis. The investigators use an existing infrastructure integrated with an open-source environment (``Eclipse''). It is the intent to make the resulting analyses available in open-source and\/or commercial products.","title":"Modular Static Checking of Software Design Intent Using Permissions","awardID":"0702635","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":[332331],"PO":["564388"]},"130651":{"abstract":"Device scaling of silicon transistors has been the fundamental basis for the phenomenal success of the semiconductor industry. <br\/>Such scaling is reaching a point where it is absolutely necessary to explore new devices as replacement for traditional silicon transistors. <br\/>Otherwise, the progress of the semiconductor industry will be severely affected. Carbon Nanotube Field-Effect Transistors (CNFETs) are promising candidates as extensions to traditional Silicon transistors due to excellent device performance. While there have been significant accomplishments in scientific discovery of CNFETs in recent years at the single-device level, a major gap exists between such single-device-level results and the research required to harness the science into practical design technologies at the end of device scaling of silicon transistors. This research project targets to close this gap by developing necessary technologies required to make CNFETs practical candidates for replacing silicon transistors. <br\/><br\/>The objective of this research is to design of robust nanoscale computing fabrics using Carbon Nanotube Field Effect Transistors (CNFETs) in the presence of inherent limitations and imperfections, and to experimentally demonstrate essential components of a fabric such as a processor. This research is motivated by the fact that CNFETs are promising candidates as extensions to Silicon CMOS, yet fundamental nanoscale challenges prevent successful implementations of efficient CNFET-based circuits and systems. This project includes an interdisciplinary research team to demonstrate robust CNFET-based computing fabrics, and also to educate future generations of engineers and the general public in the emerging field of nanoscale computing.","title":"Collaborative Research: Design, Modeling, Automation and Experimentation of Nanoscale Computing Fabric using Carbon Nanotubes","awardID":"0726843","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["521343","548388"],"PO":["565157"]},"130211":{"abstract":"0724979<br\/>Don S. Batory<br\/>University of Texas at Austin<br\/><br\/>Today's programmers wrestle with a bewildering number of languages, tools, programming concepts, and representations. Instinctively we sense familiar ideas are reinvented and reimplemented in different contexts, in different languages, with different implementations, and on different platforms. The result is overwhelming complexity, steep learning curves, and the inability to create customized software cheaply. This research exposes fundamental principles that underlie different threads of research in program development and maintenance to create a general theory of automated software development that ultimately will reduce complexity, be easier to learn, and will enable customized software to be created inexpensively.<br\/><br\/>Architectural Metaprogramming is the idea that programs (or program designs and program structures) are first-class entities that are manipulated by transformations. Programs are values, transformations map programs to other programs, and operators map transformations to transformations. The result is an algebra of programs: programs can be added, subtracted, and transformed. The algebra unifies important areas of software design in a very teachable way: refactorings are behavior-preserving program transformations, feature-based and aspect-oriented software synthesis use behavior-extending program transformations, and model-driven design exploits both to map platform-independent models to platform-specific models. The result of our work is a foundation for a new paradigm, languages, and tools for low-cost automated software design and development. The short-term impact of our work will be in software engineering education, where refactorings, features, aspects, and model-driven design can be presented as a coherent and integrated discipline; the longer-term impact will be on tools and languages in commercial software development.","title":"Architectural Metaprogramming","awardID":"0724979","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7652","name":"SCIENCE OF DESIGN"}}],"PIcoPI":["515656","379633"],"PO":["564388"]},"131113":{"abstract":"The focus of this project is integrated design and analysis of communication networks in service of coordinated control of multi-vehicle systems. Consider a set of vehicles, equipped with local controllers and wireless radios, that is set to arrange itself, to stabilize, and to control its collective motion. To achieve globally desirable formation behavior, the controller on a given vehicle must respond to the motion and state of others.<br\/><br\/>In fact, there exists a complicated coupling among system components: network architecture, communications protocols, and controller design. The integrated design of these components is the objective of this project. The fundamental challenge in designing networked control systems is that the tasks of communication and control, in general, cannot be considered decoupled from each other without loss of optimality. However, modular solutions can potentially provide significant insights into the nature of efficient solutions. This project addresses the problem of integrated communications and control from a practically viable perspective by decomposing the problem into modular tasks. The introduced degree of modularity, despite its sub-optimality, enables practical and efficient solutions as well as insights into the inherent trade-offs. Because the questions that arise lie at the intersection between communications and controls research, the components of the project bring together expertise in decentralized control, networking, and signal processing through the following specific tasks: 1) Nonlinear coordinated control over dynamic graphs, 2) Crosslayer optimization of wireless networks in service of coordinated control, 3) Physical layer solutions to decentralized communication and control, and 4) Experimental performance evaluation on a 3D autonomous underwater vehicle test-bed at the University of Washington.","title":"Collaborative Research: New Communication Infrastructures For Networked Coordinated Control","awardID":"0728983","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":[348056],"PO":["564898"]},"131168":{"abstract":"Data privacy has become a fundamental problem of the modern information infrastructure. Collections of personal and sensitive data, previously the purview of governments and statistical agencies, are now ubiquitous. Increasing volumes of information are collected and archived by health networks, financial organizations, search engines, intrusion detection systems, social networking systems, retailers and other enterprises. Potential social benefits from analyzing these databases are enormous. The main challenge is to learn the properties of a database as a whole while protecting the privacy of individual contributors.<br\/><br\/>This project investigates the design of new algorithms and learning techniques for private data analysis. The focus is on the following research activities: (1) Exploring the ramifications of the Smooth Sensitivity framework. Releasing a function's value privately via this framework requires understanding a combinatorial property of the function, termed the smooth sensitivity. The investigators study the complexity of computing smooth sensitivity for a variety of functions, ranging from statistical summaries to graph properties. (2) Designing generic efficient methods based on sampling that can be applied without computing smooth sensitivity explicitly. (3) Investigating what classes of concepts and distributions can be efficiently learned, in the PAC sense, while maintaining privacy.<br\/><br\/>The most significant social impact of this research will come directly from its technical success: inasmuch as it provides a rigorous basis for limiting privacy breaches and new algorithmic techniques, it will help prevent damaging violations of personal privacy, and contribute to the more effective use of the vast amounts of information currently being accumulated.","title":"Algorithms for Private Data Analysis","awardID":"0729171","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["426508","501279"],"PO":["565157"]},"135469":{"abstract":"The purpose of this workshop is to develop a framework for moving forward with well coordinated technological, societal, and research activities needed to reach the 10-year objective of transforming the cyber-infrastructure to be resistant to attack. Three synergistic plans are suggested to start simultaneously: a near-term activity of from one to three years, a medium-term effort of two to five years, and a long-term thrust of up to ten years. Near-term activities will have almost immediate benefits in vulnerability reduction, and the medium- and long-term efforts will result in advances that can augment or replace weak segments of the existing infrastructure.<br\/><br\/>The workshop will bring together a relatively small group of experts from government, industry and academe in secure computer systems and networks. They will be required to address, at a high level, a broad range of technical areas and issues associated with the lifecycle processes for large secure systems. Concerns include elicitation of requirements, specification, design, implementation, configuration management, tests and analysis, certification, deployment and maintenance.<br\/><br\/>The primary product of the workshop will be a report describing the framework and plan developed by its participants. It is expected that the plan will include:<br\/>* Well defined objectives for the effort<br\/>* Methods for assessing overall program progress<br\/>* Milestones and graduated objectives to be met as the program progresses<br\/>* Characterization of the technological, societal, and research challenges to be addressed<br\/>* Description of how ongoing programs will relate to the initiative","title":"Cyber Defense Initiative Workshop: Research and Techology Framework and Plan","awardID":"0751375","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["528231"],"PO":["497499"]},"134755":{"abstract":"This SGER project is a feasibility study of algorithms for manipulation and recognition using haptic information. The Manipulation-and-Perceiving-Simultaneously (MAPS) paradigm borrows key ideas from recent advances in simultaneous localization and mapping (SLAM) and visual object recognition with modifications necessary to adapt them to haptic exploration.<br\/><br\/>In MAPS, exploration is driven by the objective of locating \"interest areas\" that carry high information content about the object. The haptic properties of these interest points will be encoded, together with location information, in a coordinate-system independent fashion. During a learning phase, a large set of such points will be acquired and stored in a database. During recognition, newly acquired points will be compared with those in the database to generate plausible model hypotheses. The final recognition decision will also incorporate the geometric constraints implicit in the location and orientation of the surface structure.<br\/><br\/>The key contributions of this project will be:<br\/>- The development of active exploration strategies to extract, localize and map distinctive surface features;<br\/>- The development and evaluation of haptics-based object recognition algorithms using both spatial and haptic \"appearance\" information;<br\/> - The development of an experimental environment to implement and evaluate MAPS algorithms.","title":"Manipulating and Perceiving Simultaneously (MAPS) for Haptic Object Recognition","awardID":"0748338","effectiveDate":"2007-10-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["531055","520895","522960"],"PO":["564316"]},"133787":{"abstract":"Existing end-user programming tools focus too much attention on programming. Likewise, game building applications focus on precision rather than exploring the design space. This research will investigate the possibility that the need for programming can be reduced or eliminated with the introduction of sketched behaviors. This project develops and evaluates a new approach to end-user production of simple games and simulations that builds on K-Sketch, an informal animation sketching system. This approach will foster creativity by allowing users to rapidly sketch out objects and their behaviors using the techniques in KSketch and then transform these sketches into rough games or simulations with no programming. An in-depth analysis of a range of games and simulations conducted as part of this work will determine what behaviors such a tool should support. This project will also produce initial interaction designs for such a system. These early designs will be evaluated with Wizard of Oz testing techniques to explore the impact of the design decisions on creativity.<br\/><br\/>The broader impact of this work is a new creative outlet to children, teachers, and others who wish to create games and simulations. It may also help discover tool features that can impact creativity in other domains.","title":"SGER - End-user Sketching of Games and Simulations","awardID":"0742877","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7655","name":"ITR-CreativeIT"}}],"PIcoPI":["483342"],"PO":["424970"]},"134898":{"abstract":"Proposal No: 0749032 <br\/>PI: Judith A. Hirsch<br\/><br\/>Award Abstract:<br\/><br\/>This award supports the preparation and sharing of computational neuroscience data as part of an exploratory activity aimed at catalyzing rapid and innovative advances in computational neuroscience and related fields. The data to be shared in this project are intracellular (whole-cell patch) recordings obtained in vivo from visual, auditory, somatosensory, and motor areas of the neocortex by the laboratories of Judith Hirsch, Anthony Zador, Michael DeWeese, and Michael Brecht. These data include not only spikes but also membrane voltages or currents generated by synaptic connections and intrinsic membrane channels. In addition to providing data, the investigators will develop tutorial materials describing recording methods, stimulus paradigms, and issues relevant to the interpretation of intracellular recordings. It is anticipated that this pooled data set will be useful for those wishing to study a particular sensory modality as well as those who hope to understand common features of neocortical function. It will also be of great value for the development of new methods of data analysis.","title":"SGER Collaborative Research: CRCNS Data Sharing of Intracellular Recordings from the Neocortex","awardID":"0749032","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}}],"PIcoPI":[358396],"PO":["564318"]},"130982":{"abstract":"This program undertakes a broad research agenda centered around the design and analysis of ``Flow-based Networks''. A flow is a collection of packets that belong to the same ``transaction'', such as a datagram, an ftp transfer, or a web download. It is the fundamental unit of data that a user cares about. Current packet-switched networks, like the Internet and Gigabit Ethernet, are designed to process packets; they are unaware of the flow to which a packet belongs. This is because flow-recognition is widely considered to be too expensive to implement. However, a switch or a router's ability to recognize flows can lead to a marked improvement in its performance, to a better use of its resources, and to much more secure networks. <br\/><br\/><br\/>The first major aim of this program is to design novel algorithms and data structures for high-speed, \"flow-aware\" networks. Such algorithms could heavily influence the design of commercial switches and routers. A second major thrust concerns the development of flow-level models of networks: models which capture the impact of packet-level decisions on flow-level bandwidth allocation and flow processing times. <br\/>An important component of the modeling work is the unification and generalization of two research enterprises: Stochastic Network Theory, and Large Random Networks. The former studies the performance of a, typically non-random, queueing network subject to \"random inputs\". The latter concerns the study of \"random networks\", usually subject to deterministic inputs. A successful outcome of these efforts can help answer questions such as the throughput and flow delay of a particular bandwidth allocation scheme, <br\/>and the effect of routing topology on end-to-end performance. In other words, the modeling effort aims to develop a realistic, simple and usable class of models for network flows.","title":"Collaborative Research: Flow Level Models and the Design of Flow-aware Networks","awardID":"0728554","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["524453","532110"],"PO":["432103"]},"130301":{"abstract":"0725350<br\/>Collaborative Research: Applying Hardware-Inspired Methods for Multi- Core Software Design Brian C. Demsky University of California, Irvine<br\/><br\/>0725357<br\/>Collaborative Research: Applying Hardware-Inspired Methods for Multi- Core Software Design Michael B. Taylor University of California, San Diego<br\/><br\/>In the past, improvements in microprocessor capabilities were expressed largely through a combination of clock frequency increases and microarchitectural enhancements that were invisible to the typical developer. More recently, due to power and microarchitectural scalability issues, microprocessor designs have diverged from this path and have begun to focus on exposing improved semiconductor process capabilities through the multi-core abstraction, which integrates multiple independent processors into a single chip. The deployment of such explicitly-parallel multi-core processors has deep implications on the future of software systems. While parallel software has been largely unnecessary in desktop systems, it will become essential if we are to expect continued increases in software functionality and programmer productivity like those that society has enjoyed over the last 35 years.<br\/><br\/>This research investigates a new design methodology for developing the parallel software systems that are necessary to take advantage of multi-core processors. This methodology leverages concepts from hardware chip-design methodologies, which scale to millions of communicating parallel entities. This new design process enables the software developer to create flexible system designs that easily accommodate refinement of how the computation is realized. It does this by separating the functional design of the software system from the specification of how to organize the computation. To validate this new design methodology, the research project investigates the construction of synthesis and profiling tools that can be used to develop and refine these functional and organizational specifications. These specifications are in turn used to create an executable that is optimized for the specific multi-core microprocessor.","title":"Collaborative Research: Applying Hardware-Inspired Methods for Multi-Core Software Design","awardID":"0725350","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7652","name":"SCIENCE OF DESIGN"}}],"PIcoPI":["550574"],"PO":["564388"]},"131115":{"abstract":"0728986<br\/>Rose, Kenneth<br\/>U of Cal Santa Barbara<br\/><br\/>Optimization of Distributed Coding for Sources with Memory and Applications in Sensor Networks<br\/><br\/><br\/>Distributed source coding is strongly motivated by high-density sensor networks with promising applications in numerous diverse scientific and engineering disciplines. Energy and capacity constraints led to extensive efforts to exploit inter-sensor (spatial) correlations so as to minimize resource requirements for data communications. However, practical distributed sources virtually always exhibit considerable time correlations. A major challenge emerges due to conflicts between the objectives of exploiting temporal versus spatial correlations. The degree to which distributed source coding will be practically applicable to sensor networks crucially depends on the development of effective solutions. The project focuses on joint exploitation of temporal and spatial correlations, which requires fundamental tradeoff analysis, development of new coder paradigms, and optimization tools to handle design intricacies and system complexity constraints.<br\/><br\/>The research work comprises derivation of the theoretical foundation for the approaches from source coding, estimation and information theory principles, and the development of practical algorithmic tools for optimizing distributed predictive coders to overcome several obstacles and challenges: conflicts between prediction and distributed quantization, intractability of the cost function which is riddled with local minima, instability of training procedures due to feedback through the prediction loop, impacts of channel loss on performance and design, adaptation of the cost function to account (in time and space) for significant sensed events. Relevant tools for global optimization and for stable design of predictive coders, developed by the Principal Investigator's research group, will serve as initial building blocks for the approach. Extensive multidisciplinary interest in sensor networks is leveraged for access to diverse data and real-world experimental settings, for exposure of students to a broad mix of disciplines, and for broad dissemination of results.","title":"Optimization of Distributed Coding for Sources with Memory and Applications in Sensor Networks","awardID":"0728986","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["550914"],"PO":["564924"]},"141169":{"abstract":"In the past decade, new nonlinear partial differential<br\/>equations (PDEs) have been developed for various image processing<br\/>applications, such as noise reduction, edge detection, image<br\/>segmentation and restoration. While the attention of the<br\/>scientific community in this area predominantly focused on<br\/>creating the new PDEs, very little attention was paid to<br\/>developing numerical algorithms that approximate their solutions.<br\/>The few numerical algorithms that are currently used suffer from<br\/>a variety of problems: they are not accurate enough, too slow,<br\/>and not fault-free. In this project, the investigator develops<br\/>accurate, efficient, and robust numerical algorithms for<br\/>nonlinear PDEs in image processing. The research activities are<br\/>based on the investigator's extensive work in the field of<br\/>hyperbolic conservation laws, and include numerical methods for<br\/>the Hamilton-Jacobi equations, fast algorithms for high-order<br\/>nonlinear PDEs, algorithms for computing steady-state solutions,<br\/>numerical homogenization of Hamilton-Jacobi equations and<br\/>multi-resolution analysis, analysis of nonlinear diffusion<br\/>equations, constrained morphing active contours and geodesic<br\/>flows, and \"non-blind\" algorithms for image processing. A<br\/>portion of the research activities focuses on improving existing<br\/>algorithms in order to solve a specific imaging problem in<br\/>radiation oncology treatment planning.<br\/> The investigator develops novel mathematical techniques for<br\/>image processing and uses these techniques for solving problems<br\/>in the field of radiation oncology imaging. Radiation oncology<br\/>treats cancer by delivering relatively small doses of radiation<br\/>to tumors in order to eliminate cancer without destroying or<br\/>chronically damaging healthy tissues in and around the growth.<br\/>CT and MRI scans are used as three-dimensional anatomical models<br\/>to ensure that the treatments conform geometrically to the tumor<br\/>target. This process depends critically upon identifying the<br\/>location of the tumor as well as the healthy organs (in order to<br\/>minimize the dose of radiation in these areas). Despite extended<br\/>research, the existing mathematical tools for image processing<br\/>are unsuitable for clinical medical applications. The<br\/>segmentation of the CT and MRI scans is still carried out by<br\/>manual tools, and consumes about one-half of the time required to<br\/>plan the treatments. The investigator designs accurate and<br\/>reliable automated algorithms that would significantly shorten<br\/>this time and have a big impact on radiation oncology. He<br\/>integrates into his work educational activities that demonstrate<br\/>the importance of applied mathematics in a broad spectrum of<br\/>sciences. Special emphasis is given to applications of<br\/>computational mathematics in biology and cutting-edge<br\/>technologies. The planned educational activities include<br\/>programs for junior-high, high-school, undergraduate, and<br\/>graduate students. The investigator works to increase the gender<br\/>and ethnic diversity in the mathematical sciences by encouraging<br\/>under-represented groups to study applied mathematics and choose<br\/>it as a future career.","title":"CAREER: Partial Differential Equation-based Image Processing with Applications to Radiation Oncology","awardID":"0820817","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0504","name":"Division of MICROELECTRONIC INFOR PROCESS","abbr":"MIP"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["563879"],"PO":["565027"]},"131159":{"abstract":"This research considers physical layer security in wireless networks. It serves as a counterpoint to traditional methods of security, such as computationally secure crypto-systems, by considering security in the wireless medium itself. An important motivator for this research is the rapid proliferation of wireless communication devices, technologies, and applications, in general. The very nature of wireless networks exposes not only the risks and vulnerabilities that a malicious user can exploit and severely compromise the network but also information confidentiality concerns with respect to the in-network terminals. Although wireless technologies are becoming more and more secure, attackers are becoming smarter, and sole reliance on cryptographic keys in large distributed networks, for example, where terminals can be compromised is unsustainable from the security perspective. A direct consequence of this process is the need to, additionally, tackle security at the very basic physical layer level where the (unconditional) secrecy may be embodied within the information itself, and, also, adapted to the communication medium and network conditions. Furthermore, there is a need for a secure key distribution, a process which can be performed in perfect secrecy only using physical layer techniques, even when, henceforth, relying on conventional cryptographic techniques.<br\/>This research studies how, in secure wireless networks, the application delay requirement relative to the communication channel coherence time aspects the selection of signaling strategies and the achievable communication rates at a prescribed security level. In addition, perhaps surprisingly,<br\/>successful design of practical secure nested forward error correction codes and secure adaptive incremental error correction strategies applicable to wireless channels is virtually non-existent. Hence, this research focuses on practical coding-scheme designs as essential enabling tools.","title":"Collaborative Research: Communicating Confidential Information over Wireless Networks","awardID":"0729142","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[348167],"PO":["432103"]},"126418":{"abstract":"0708962<br\/><br\/>Collaborative Research: CRI: IAD: Electronic Testing Education, Research and Training Infrastructure<br\/><br\/>Vishwani Agrawal<br\/><br\/>This three-year project will develop capabilities for testing of digital, memory, analog, and radio frequency devices, with possible future upgrades to MEMS, optical and nanotechnology devices. The infrastructure developed under this project consists of a test laboratory and its value-added applications. The new VLSI test laboratory will have modern automatic test equipment (ATE) of open architecture. The laboratory will be located at Auburn University in Alabama and will be used through networked access by three other institutions of that state, namely, University of Alabama at Tuscaloosa, University of Alabama at Huntsville, and Tuskegee University, for advancing education, training and research applications. In the future, the test lab infrastructure will allow collaborative use by universities outside Alabama as well, making it a national resource. The applications will include new university courses with hands-on experiments on testing of digital, analog and radio frequency chips, FPGAs and system-on-a-chip devices; industry-oriented training classes including test lab exercises; and research on silicon debug methods aimed at improving the yield and reliability. Research with industry focus and practice-oriented short courses are expected to make this infrastructure fully supported through industry funds beyond the three-year infrastructure building period.","title":"Collaborative Research: CRI: IAD: Electronic Testing Education, Research and Training Infrastructure","awardID":"0708003","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[335395],"PO":["565272"]},"137308":{"abstract":"This grant provides funding for research pertaining to the computation and analysis of extended Nash equilibria that arise from many applications in operations research, including pricing and design in electric power markets, internet network analysis, resource allocation in communication systems, competitive capacity expansion in uncertain environments, supply chain management, as well as open-loop differential games with state dynamics and control constraints. Specifically, the proposed work will focus on the design of fast efficient methods for (a) computing standard Nash equilibria with cost functions that are at best once differentiable, and (b) solving generalized Nash games (i.e., games with joint constraints), multi-leader-follower games (which include Stackelberg games, i.e., those with one leader and multiple followers), dynamic Nash games for which continuous-time solution trajectories and pathways of disequilibria are sought, Nash games with random elements (e.g., players' optimization problems are stochastic programs with recourse), as well as collusive games where players collude to<br\/>increase their Nash payoffs. We will also investigate the price of anarchy for some extended games on networks. The application areas provide a rich source of open problems that encompass all the challenging features mentioned here: joint constraints, hierarchical structure, dynamics, stochastics, and collusion.","title":"Extended Nash Equilibria and Their Applications","awardID":"0802022","effectiveDate":"2007-10-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"5514","name":"OPERATIONS RESEARCH"}}],"PIcoPI":["564987"],"PO":["502772"]},"131204":{"abstract":"Online groups are becoming increasingly important, for example, by creating the software that runs the Internet, building history?s largest encyclopedia, providing social support to millions, and enhancing opportunities for interpersonal relationships that span geographic boundaries. Regardless of the environments in which groups operate, to be successful all must meet three critical challenges: (1) Commitment: gaining and retaining members by managing members' commitment to the group and their motivation to exert effort on its behalf, (2) Coordination: coordinating members' actions to achieve collective goals, and (3) Control: ensuring that members adhere to important group norms. Although some online groups are very successful, many others fail. The large size, high turnover, weak social networks, and impoverished communication channels typical of many online groups inhibit their ability to overcome these challenges. The goal of this research is to help online groups deliver on their tremendous promise by (1) understanding the factors that affect the success of online groups; (2) shedding light on general group processes that also apply to offline groups; and (3) developing technological interventions that can help online groups achieve their objectives. The research will examine groups in three highly popular domains ? online health and technical support, the online-encyclopedia Wikipedia, and the most popular massively multiplayer game in history, World of Warcraft. Investigatory studies will use data-mining techniques to examine rich archival, longitudinal data about thousands of online groups using time-series methods. In addition, interventionist studies will follow up the investigatory research with computational interventions designed to improve the effectiveness of online groups. This research will develop principles to explain the factors that influence the success of online groups and groups more generally. Online groups are economically and socially important, as indicated by the fact that seven of the top ten Internet sites by traffic (according to www.alexa.com) are primarily online group sites. Because online groups have high failure rates, knowledge about the factors that underlie their effectiveness is critical for designers and managers. This project's technological interventions are designed to improve the performance of the groups in which they are deployed and to serve as exemplars for subsequent design efforts in other online groups.","title":"DHB: Collaborative Research: Solving Critical Problems in Online Groups","awardID":"0729344","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0400","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S005","name":"CIA"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7319","name":"HSD - DYNAMICS OF HUMAN BEHAVI"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["483488"],"PO":["474056"]},"134966":{"abstract":"NSF PetaApps 0749334\/0748898\/0749045:<br\/><br\/>Understanding the Dynamics of the Earth:<br\/>High-Resolution Mantle Convection Simulation on Petascale Computers<br\/><br\/>George Biros (Penn), Omar Ghattas (UT-Austin), Michael Gurnis (CalTech), Shijie Zhong (CU-Boulder)<br\/><br\/>Mantle convection is the principal control on the thermal and geological evolution of the Earth. It is central to our understanding of the origin and evolution of tectonic deformation, the evolution of the thermal and compositional states of the mantle, and ultimately the evolution of the Earth as a whole. Despite its central importance to our understanding of the dynamics of the solid Earth, simulation of global mantle convection at realistic Rayleigh numbers down to the scale of faulted plate boundaries is currently intractable, due to the wide range of time and length scales involved. <br\/><br\/>This project will capitalize on upcoming petascale computing systems to carry out the first high resolution mantle convection simulations that can resolve thermal boundary layers and faulted plate boundaries, which will enable the first inverse solutions that can incorporate historical plate motions. These simulations will lead to breakthroughs in understanding the dynamics of the solid Earth. However, to make effective use of the upcoming petascale systems, new scalable algorithms and implementations are needed. <br\/><br\/>To enable these simulations, this project will: (1) tune, improve the performance of, and scale up to the petascale the parallel open-source mantle convection code CitcomS; (2) develop, implement, robustify, and incorporate new parallel algorithms for adaptive mesh refinement and inverse solution that can scale to hundreds of thousands of processor cores; and (3) release the resulting mantle convection codes to the geosciences community via the Computational Infrastructure for Geodynamics (CIG), an NSF center that develops and maintains software for several earth science communities.","title":"Collaborative Research: Understanding the Dynamics of the Earth: High-Resolution Mantle Convection Simulation on Petascale Computers","awardID":"0749334","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"7699","name":"ICER"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0603","name":"Division of EARTH SCIENCES","abbr":"EAR"},"pgm":{"id":"7255","name":"GEOINFORMATICS"}}],"PIcoPI":["527004","559409"],"PO":["496127"]},"134889":{"abstract":"Proposal No: 0748994<br\/>PI: Anthony Zador<br\/><br\/>Award Abstract:<br\/><br\/>This award supports the preparation and sharing of computational neuroscience data as part of an exploratory activity aimed at catalyzing rapid and innovative advances in computational neuroscience and related fields. The data to be shared in this project are intracellular (whole-cell patch) recordings obtained in vivo from visual, auditory, somatosensory, and motor areas of the neocortex by the laboratories of Judith Hirsch, Anthony Zador, Michael DeWeese, and Michael Brecht. These data include not only spikes but also membrane voltages or currents generated by synaptic connections and intrinsic membrane channels. In addition to providing data, the investigators will develop tutorial materials describing recording methods, stimulus paradigms, and issues relevant to the interpretation of intracellular recordings. It is anticipated that this pooled data set will be useful for those wishing to study a particular sensory modality as well as those who hope to understand common features of neocortical function. It will also be of great value for the development of new methods of data analysis.","title":"SGER Collaborative Research: CRCNS Data Sharing of Intracellular Recordings from the Neocortex","awardID":"0748994","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}}],"PIcoPI":[358368],"PO":["564318"]},"125836":{"abstract":"Emotion and motivation are fundamental to learning; students with high intrinsic motivation often outperform students with low motivation. Yet affect and emotion are often ignored or marginalized with respect to classroom practice. This project will help redress the emotion versus cognition imbalance. The researchers will develop Affective Learning Companions, real-time computational agents that infer emotions and leverage this knowledge to increase student performance. The goal is to determine the affective state of a student, at any point in time, and to provide appropriate support to improve student learning in the long term. Emotion recognition methods include using hardware sensors and machine learning software to identify a student's state. Five independent affective variables are targeted (frustration, motivation, self-confidence, boredom and fatigue) within a research platform consisting of four sensors (skin conductance glove, pressure mouse, face recognition camera and posture sensing devices). Emotion feedback methods include using a variety of interventions (encouraging comments, graphics of past performance) varied according to type (explanation, hints, worked examples) and timing (immediately following an answer, after some elapsed time). The interventions will be evaluated as to which best increase performance and in which contexts. Machine learning optimization algorithms search for policies that further engage individual students who are involved in different affective and cognitive states. Animated agents are enhanced with appropriate gestures and empathetic feedback in relation to student achievement level and task complexity. Approximately 500 ethnically and economically diverse students in Massachusetts and Arizona will participate.<br\/><br\/>The broader impact of this research is its potential for developing computer-based tutors that better address student diversity, including underrepresented minorities and disabled students. The solution proposed here provides alternative representations of scientific content, alternative paths through material and alternative means of interaction; thus, potentially leading to highly individualized science learning. Further, the project has the potential to advance our understanding of emotion as a predictor of individual differences in learning, unveiling the extent to which emotion, cognitive ability and gender impact different forms of learning.","title":"HCC: Collaborative Research: Affective Learning Companions: Modeling and supporting emotion during learning","awardID":"0705554","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7625","name":"REESE"}}],"PIcoPI":["513288","552841","552730","510210"],"PO":["564456"]},"130633":{"abstract":"Device scaling of silicon transistors has been the fundamental basis for the phenomenal success of the semiconductor industry.<br\/>Such scaling is reaching a point where it is absolutely necessary to explore new devices as replacement for traditional silicon transistors.<br\/>Otherwise, the progress of the semiconductor industry will be severely affected. Carbon Nanotube Field-Effect Transistors (CNFETs) are promising candidates as extensions to traditional Silicon transistors due to excellent device performance. While there have been significant accomplishments in scientific discovery of CNFETs in recent years at the single-device level, a major gap exists between such single-device-level results and the research required to harness the science into practical design technologies at the end of device scaling of silicon transistors. This research project targets to close this gap by developing necessary technologies required to make CNFETs practical candidates for replacing silicon transistors.<br\/><br\/>The objective of this research is to design of robust nanoscale computing fabrics using Carbon Nanotube Field Effect Transistors (CNFETs) in the presence of inherent limitations and imperfections, and to experimentally demonstrate essential components of a fabric such as a processor. This research is motivated by the fact that CNFETs are promising candidates as extensions to Silicon CMOS, yet fundamental nanoscale challenges prevent successful implementations of efficient CNFET-based circuits and systems. This project includes an interdisciplinary research team to demonstrate robust CNFET-based computing fabrics, and also to educate future generations of engineers and the general public in the emerging field of nanoscale computing.","title":"Collaborative Research:Design, Modeling, Automation and Experimentation of Nanoscale Computing Fabric using Carbon Nanotubes","awardID":"0726791","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[346657,"559405","549505"],"PO":["565157"]},"130303":{"abstract":"0725350<br\/>Collaborative Research: Applying Hardware-Inspired Methods for Multi- Core Software Design Brian C. Demsky University of California, Irvine<br\/><br\/>0725357<br\/>Collaborative Research: Applying Hardware-Inspired Methods for Multi- Core Software Design Michael B. Taylor University of California, San Diego<br\/><br\/>In the past, improvements in microprocessor capabilities were expressed largely through a combination of clock frequency increases and microarchitectural enhancements that were invisible to the typical developer. More recently, due to power and microarchitectural scalability issues, microprocessor designs have diverged from this path and have begun to focus on exposing improved semiconductor process capabilities through the multi-core abstraction, which integrates multiple independent processors into a single chip. The deployment of such explicitly-parallel multi-core processors has deep implications on the future of software systems. While parallel software has been largely unnecessary in desktop systems, it will become essential if we are to expect continued increases in software functionality and programmer productivity like those that society has enjoyed over the last 35 years.<br\/><br\/>This research investigates a new design methodology for developing the parallel software systems that are necessary to take advantage of multi-core processors. This methodology leverages concepts from hardware chip-design methodologies, which scale to millions of communicating parallel entities. This new design process enables the software developer to create flexible system designs that easily accommodate refinement of how the computation is realized. It does this by separating the functional design of the software system from the specification of how to organize the computation. To validate this new design methodology, the research project investigates the construction of synthesis and profiling tools that can be used to develop and refine these functional and organizational specifications. These specifications are in turn used to create an executable that is optimized for the specific multi-core microprocessor.","title":"Collaborative Research: Applying Hardware-Inspired Methods for Multi-Core Software Design","awardID":"0725357","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7652","name":"SCIENCE OF DESIGN"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["557941"],"PO":["564388"]},"130798":{"abstract":"This research builds a unified computational framework for scalable and high <br\/>efficiency solution of elliptic partial differential equations. The <br\/>investigators develop a novel high-order multiscale multigrid computation <br\/>methodology, which combines high accuracy computation and fast computing methods <br\/>in a seamless way. This research work may impact many computational science <br\/>and engineering and industry modeling and simulation applications. As U.S. <br\/>high-tech industry moves from experiment-based design and development to <br\/>computer-assisted design and development, higher performance numerical <br\/>methods and faster computer simulation techniques will benefit U.S. industry <br\/>by enabling design and development engineers to conduct quick verification <br\/>to test their new ideas on computers, before committing to expensive <br\/>experiments. These technologies are essential for the U.S. industry to <br\/>maintain its leadership position in the competitive world market. Graduate <br\/>students, including members from underrepresented groups, are trained to <br\/>become the next generation researchers and educators with solid scientific <br\/>computing skills. <br\/><br\/>The technique simultaneously advances the numerical solution <br\/>of partial differential equations in two fronts. One is to compute high <br\/>accuracy solution by using high-order discretization methods, another is <br\/>to compute the discrete solution in a minimum amount of computer time <br\/>by using the fastest sparse linear system solvers. This unified framework <br\/>advances the two fronts collectively by fusing the ideas and advantages <br\/>of multiscale discretization and multigrid computations, to achieve <br\/>the ultimate goal of computing accurate numerical solution at the <br\/>minimum computer costs. It is the convergence of years of research work <br\/>by many researchers in several different areas. This computational <br\/>framework possesses high accuracy, high speed, high scalability, and <br\/>delivers optimal efficiency for computing the numerical solution of <br\/>elliptic partial differential equations.","title":"A Unified Framework for Large Scale Scientific Computing","awardID":"0727600","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["94260"],"PO":["565272"]},"134978":{"abstract":"The fifth ACM International conference on Embedded Networked Sensor Systems (SenSys 2007) will be held in Sydney, Australia from November 6-9, 2007. To help increase the representation and participation of United States-based graduate students in this conference, this proposal requests support to help cover the expenses for ten (10) students.<br\/><br\/>SenSys 2007 introduces a high caliber forum for research on systems issues in the emerging area of embedded, networked sensors. SenSys design issues span multiple disciplines, including wireless communication, networking, operating systems, architecture, low power circuits, distributed algorithms, data processing, scheduling, sensors, energy harvesting, and signal processing. SenSys seeks to provide a cross-disciplinary venue for researchers addressing these networked sensor system design issues. SenSys 2006 was a fourth in the series which followed a very successful third conference ACM Sensys 2005 sponsored by ACM (SIGCOMM, SIGMOBILE, SIGARCH, SIGMETRICS, SIGOPS, SIGBED) and NSF. The quality of the papers presented in this single track conference over the past years reflects the significant intellectual merit of the proposed activity.<br\/><br\/>The participation by graduate students will have a broad and educational impact. Support from NSF is critical to increasing the number of graduate students who can attend this premiere conference. The goal of the travel student program is to broaden the participation of students from various universities. The program will give priority to female and under-represented minority students, as well as students pursing their graduate studies in Minority Serving Institutions.","title":"Student Travel Support","awardID":"0749390","effectiveDate":"2007-10-01","expirationDate":"2008-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["534560"],"PO":["564777"]},"131117":{"abstract":"Abstract:<br\/>----------<br\/>This NSF project is investigating theoretical foundations behind non-conventional analog techniques that can be used for decoding low-density parity check (LDPC) codes. New analog decoding techniques are being developed based on elegant formulations of a bottom-up approach where computational primitives inherent in device physics are used for designing encoding and decoding algorithms. In particular, we are investigating margin propagation principles for designing high-performance decoders for LDPC codes. Margin propagation principle (MPP), which provides an intriguing analog-domain physical tool for evaluating information-theoretic measures (e.g., entropy) and other quantities (e.g., likelihood functions), utilizes only basic conservation laws of physical quantities (current, charge, mass, energy) for computing and therefore is scalable across micro\/nano devices (silicon, MEMS, microfluidics).<br\/>This research focuses on four specific areas: (a) mapping margin propagation principle to graphical methods and to develop novel algorithms for decoding LDPC codes; (b) modeling noise inherent in margin propagation devices and evaluating its impact on LDPC decoding algorithm; (c) developing analytical channel-coding tools based on density evolution for optimizing the performance of margin propagation based LDPC decoder; (d) prototyping a margin propagation LDPC decoder in silicon. The broader impact of this research includes novel algorithms and hardware for designing high-performance LDPC decoders, that can be used in present and future digital communication standards (DVB-S2, 802.11, 802.12, 802.16, 802.20). The developed algorithms and hardware are being utilized in the design of novel teaching materials, which are used to train graduate students in the interdisciplinary area of communications and electronics.","title":"Investigation into Non-conventional Analog Decoders for Low-density Parity Check Codes","awardID":"0728996","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["554511","491679"],"PO":["564924"]},"131128":{"abstract":"Robust Inference and Communication: <br\/>Theory, Algorithms and Performance Analysis <br\/><br\/>Sean P. Meyn and Venugopal V. Veeravalli<br\/><br\/>As sensors and wireless communication become increasingly pervasive, and network topologies increasingly complex, there is an urgent need for new techniques for inference and communication in complex environments, as well as techniques to evaluate their performance. <br\/><br\/>The goal of this investigation is to respond to these needs by following two complementary tracks. The first concerns methods for constructing inference and decoding algorithms for complex models, with possible modeling uncertainty, based on the geometry surrounding Kullback-Leibler (K-L) divergence and related methods developed by the investigators in their prior research. The second track treats performance evaluation and performance improvement. Both performance evaluation and algorithm selection are performed using Monte-Carlo or related sample path learning techniques. These approaches are chosen primarily because of their ease of application when compared to deterministic numerical techniques. The application of Monte-Carlo techniques comes at a price in the form of high variance. Efficient simulation and learning techniques are developed in concert with research on hypothesis testing and communication to construct faster algorithms for performance evaluation and adaptation. <br\/><br\/>In addition to theoretical research on these topics, the investigators will transfer technology to industry and community organizations, including the Motorola Communications Center at Illinois, United Technologies Research Center, Vodafone, and the community wireless group CUWiN. Both graduate and undergraduate students will be engaged in applied and theoretical research.","title":"Robust Inference and Communication: Theory, Algorithms and Performance Analysis","awardID":"0729031","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["537010","223414"],"PO":["564924"]},"131139":{"abstract":"Common examples of optimization problems include finding shortest routes in networks, managing caches, load balancing, scheduling, and computer tomography (CAT scanning). Optimization problems are fundamental to computer science, many branches of engineering, operations research, bioinformatics, and in many industrial and economic settings. Optimization problems must be solved in non-ideal conditions -- when ideal solutions are not possible due to time constraints, or because information about the problem instance is limited: for example, when the solution must be built in steps over time, with each step being taken without knowledge of future constraints, or when the solution must be built by many agents each with its own limited view. This research develops methods to deal with such non-ideal conditions. An immediate impact is that important optimization problems can be more effectively solved in practice.<br\/><br\/>The researchers are studying variants of facility location, buffer management in QoS networks, reconfigurable caching, microprocessor temperature management, network congestion control, computer tomography, and linear and integer-linear programming. The researchers are also building a unifying foundation for algorithms for such problems, and breaking new ground with algorithms that integrate control theory with worst-case analysis. The foundations are in probabilistic methods, linear programming primal-dual theory and control theory. The intellectual merit includes deepening and unifying the understanding of a technically formidable class of algorithms. This in turn has the broader impact of making the algorithms easier to understand, teach, and extend. Long-term impacts include increased economic efficiency and development and dispersion of technology.","title":"Approximation Algorithms for Combinatorial Optimization","awardID":"0729071","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["486542","517719"],"PO":["565251"]},"127509":{"abstract":"Recurrent connections and intrinsic rhythmic neural activity are ubiquitous in biological visual pathways, where they first appear in the retina. Although the functional role of these properties of the retinal circuit is not fully resolved, recent work suggests that they might help convey information about visual context or even the gist of a scene. Further, new experimental work shows that the oscillatory patterns of activity generated by retinal networks are relayed from the thalamus to the cortex. As yet, however, retinal oscillations are usually ignored in systems of computer vision, even those based on neural principles. If artificial systems matched human performance in visual perception, this lack of attention to the biological circuitry might not be worthy of note. But this is not the case. Humans do a far better job than computers in routine tasks like analyzing cluttered scenes or recognizing objects in noisy backgrounds or under different lighting conditions. Thus this project aims to develop biologically inspired models that include oscillating networks to improve scene analysis in artificial vision.<br\/><br\/>The new models will incorporate local and distributed connections in the retinal circuit and also take advantage of the scheme of efficient sparse coding, a powerful new concept for understanding sensory processing. By taking both local and spatially extensive circuits into account, the expectation is that the model will be able to encode two complementary types of information about the stimulus. Past work has shown that changes in spike rate with respect to the stimulus encode information about local features. This type of rate (or stimulus-locked) coding can be modeled by small scale circuits. The new models will include ongoing oscillatory activity that is generated internally by large recurrent networks but is also modulated by sensory input. Further, visually evoked changes in the temporal structure of these intrinsic oscillations occur at finer time scales than visually evoked changes in rate. Thus, in principle, visual information could be encoded by spike timing with respect to intrinsic rhythms. Moreover, since the retinal oscillations are generated by distributed networks, it is likely that they provide information about global feature of the stimulus. Thus, if successful, the new models will be able to capture, at once, information about local detail and the gist of scene.<br\/><br\/>Numerous applications would benefit from a deeper understanding of how information is encoded in the early visual system. For example, the models that will result from the research proposed here have value for the development of visual prosthetics as well as for technical applications that involve image-processing from new methods for image compression adapted to sensory perception to the problem of automated object segmentation, scene analysis and recognition.","title":"RI: Exploring neurobiological strategies of visual scene analysis using oscillations in recurrent neural circuitry","awardID":"0713657","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["518654"],"PO":["564316"]},"130831":{"abstract":"As the VLSI technology scales down to 45nm feature size and below, the lithography process no longer produces the ideal shape\/dimension of circuit components in a silicon wafer. Geometrical parameter variations at 70nm technology can reach as much as 35%, and become increasingly severe as the feature size continues to decrease. The corresponding electrical parameter variations will significantly affect the performance and function of a VLSI circuit. Therefore, computing and analyzing the statistical properties of parasitic parameters in a silicon wafer become inevitable to the emerging nanometer scale VLSI technology. This grant aims to develop stochastic computational methods to address more general stochastic variables with distributions more realistic in nanometer VLSI technology. <br\/>This project will develop efficient algorithms using sparse grid spectral stochastic collocation method and compute interconnect capacitance and inductance using Wiener-Askey chaos basis and construct proper stochastic computational methods for non-Gaussian random variables. The intellectual merit comes from the development of sophisticated stochastic theories and efficient computing algorithms for the state-of-the-art engineering problems. This research will lay out basic guidelines and ideas and test the methods on realistic engineering problems. The broader impact of the research opens a new research direction in computing parasitic parameters with random process variations. The result of this research will have a great impact on the parasitic parameter extraction, circuit simulation and design of future nanometer scale VLSI circuits. It will lead to practical and efficient algorithms and CAD tools. Research of this project will be integrated into the graduate education of the Ph.D. students in applied mathematics and electrical engineering, and the developed software will be made public.","title":"Collaborative Research: Numerical Computations of Parasitic Parameters with Spectral Stochastic Collocation Methods for Nano-VLSI Technologies","awardID":"0727791","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["542982"],"PO":["562984"]},"131008":{"abstract":"Voice is the preferred mode of human communication. Our mostly homogeneous network for voice communications, the Public Switched Telephone Network, is quickly being replaced with a heterogeneous collection of wired and wireless links, including the wired Internet for voice over IP, digital cellular telephone systems, and voice over wireless access points. These links\/networks are not jointly designed or standardized, and use different speech coders, so when they are interconnected, there is a significant loss of voice quality. Additionally, we are not well-positioned to take advantage of evolving networks, such as mesh and ad hoc networks, for voice communications. This research develops new speech coders that produce high quality, highly intelligible speech over a wide range of bit rates, with a full range of functionalities, and which are suitable for voice over IP, digital cellular, voice over wireless access points, and woice over wireless ad hoc\/mesh networks. The resulting speech coders are especially important for the wireless networks deployed in emergency response situations.<br\/><br\/>By combining phonetic mode detection and rate distortion theory motivated structures with backward adaptive tree codes, this research derives and develops new speech coders that are SNR and bandwidth scalable, have multiple descriptions functionality, tandem well with other codecs, and have excellent packet loss concealment. The resulting speech coders are important for ad hoc and mesh networks deployed in emergency response situations, where multicast transmissions are prevalent, SNR scalability is important to allow selective pruning of traffic, and multiple descriptions functionality is critical to take advantage of path diversity and provide enhanced reliability. This research establishes new fundamental limits on speech coder performance by combining multimodal speech models and conditional rate distortion theory.","title":"Speech Coding for Universal Voice Communications","awardID":"0728646","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["410030"],"PO":["564898"]},"131019":{"abstract":"Error-Pattern-Correcting Codes: A New Approach to Error Correction Coding for Interference-Dominant Channels<br\/><br\/>Jaekyun Moon, University of Minnesota<br\/><br\/>Abstract<br\/><br\/>Intersymbol interference (ISI) is a major source of transmission errors in a wide variety of communications channels. This research concerns error correction for such channels. Input-constrained ISI channels are of specific interest in this research, as efficient coding methods for such channels are still largely an open area. This work focuses on 1) developing low-complexity error correction methods for input-constrained ISI channels, with emphasis on high-density data storage applications, and 2) clearing a path to approaching theoretical limits of error correction for such channels with fundamentally improved complexity\/performance tradeoffs. One effective way of designing codes for input-constrained ISI channels is to target at dominant error cluster patterns inherent at the channel output, as the structure of the ISI typically give rises to a dominance of a relatively small set of distinct error cluster patterns, regardless of the channel signal-to-noise ratio. This set can easily be found using distance analysis or through empirical observation based on simulation or laboratory measurement. Focusing only on the dominant set, very high rate codes are possible with excellent overall error correction capability. Encoding and decoding are necessarily highly tailored to the few known error patterns. This type of code can be applied either as a stand-alone code or as a critical building block in a larger coding system, such as a turbo equalizer based on iterative soft decision processing. While the research addresses computationally efficient approaches, efforts are also directed to establishing a theoretical framework as well as to understanding theoretical performance limits of the channel-matched codes.","title":"Error-Pattern-Correcting Codes: A New Approach to Error Correction Coding for Interference-Dominant Channels","awardID":"0728676","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[347819],"PO":["564924"]},"131603":{"abstract":"The PI and Co-PIs propose to organize a joint conference of two Workshops: the<br\/>Workshop on Algorithms and Models for the Web-Graph (WAW 2007) and the Workshop on Internet & Network Economics (WINE 2007) at San Diego December 11-14, 2007.<br\/>The main theme of WINE focuses on the algorithmic game theory including various aspects of computational complexity and applications such as auctions, pricing, collaboration, competition, security and other Internet related economical issues in general. In the past three years, there has been a great deal of development in understanding the computational complexity of Nash equilibrium and large scale economical games. WINE workshops have played a vital role in this and have been an impetus to the developments of algorithmic game theory and numerous applications.<br\/><br\/>WAW focuses on mathematical modeling and analysis of the dynamics of the webgraph and related large complex social networks as well as algorithmic applications in declustering, information retrieval and data mining. The series of WAW workshops have contributed to the rapid developments on the graph-theoretical and algorithmic aspects of the new science of networks.","title":"Research Dissemination through Organizing Workshops","awardID":"0731753","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1264","name":"ALGEBRA,NUMBER THEORY,AND COM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["452844","452845","553607"],"PO":["565280"]},"130305":{"abstract":"The ``wireless revolution\"\" and the ever increasing need to communicate on the move is continually bringing up new engineering problems and challenges. One of these challenges is the design of communication systems capable of delivering high data rates over random time-varying channels. An extensive body of work exists on the analysis and design of mobile wireless communication systems. Almost invariably these works assume that the channel variations are relatively slow so that the channel can be considered to be approximately constant over a block of symbols. Thus, the fundamental operation of existing mobile communication systems is the same as in systems designed for time-invariant channels. Various methods such as powerful error correcting codes, channel equalizers, and spatial or temporal diversity are used post-facto to mitigate the effects of the time-varying channel. This research project is developing novel communication systems for channels which vary so rapidly that the performance of conventional communication systems is severely degraded.<br\/><br\/>A fundamental assumption underlying the design of a wide range of digital communication systems is that the receiver has the ability to estimate the radio or acoustic propagation channel, and to use that estimate in the detection process. In random rapidly time-varying channels with large Doppler and delay spreads, reliable channel estimation is not possible. This project considers the problem of designing a communication system when the channel realization is unknown and inestimable, and only the statistics of the channel are known (or can be estimated). Thus, the channel realization is only known to lie somewhere in a subspace whose dimension is the rank of the channel covariance matrix. The unit-rank case reduces this to a known class of communication problems. However, the case where the rank is greater than unity requires a significant extension of current theory. This problem formulation leads to new receiver structures and novel ways of designing communication systems which must operate in harsh time-varying environments. This project is developing optimal and sub-optimal solutions for communications through subspace channels and an analysis of their performance. Various types of communication systems are being studied including: single-input single-output systems operating through frequency selective channels, MIMO systems, and multi-user and multi-access systems.","title":"Communications through time-varying subspace channels","awardID":"0725366","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":[345777],"PO":["564898"]},"134958":{"abstract":"TECHNICAL SUMMARY:<br\/><br\/>This award is made on a proposal submitted to the PetaApps Solicitation. The Office of Cyberinfrastructure, the Computer and Information Science and Engineering Directorate, and the Divisions of Materials Research, Physics, and Chemistry and the Office of Multidisciplinary Activities within the Mathematical and Physical Sciences Directorate contribute funds to this award. <br\/> <br\/>This project performs research and education in high performance computing in the domain of nanotechnology. The five principle investigators at two universities, plus graduate student researchers and postdoctoral researchers will be engaged in atomistic simulations of entire nanoscale device elements and structures. The project will produce modeling tools that will be shared freely as a set of open source quantum simulation tools for petascale supercomputers in the broad area of nanoscience and nanotechnology. <br\/><br\/>The proposed work consists of interdependent parts that together to produce robust, high performance petascale tools for quantum simulations at nanoscale. The development of such tools requires interdisciplinary, synergistic research in (i) methodology and implementation of quantum methods, (ii) profiling, performance modeling and automatic optimization of kernels, and (iii) algorithm development and tuning. The tools will be based on existing real-space multi-grid (RMG) method, which are now well-established and successfully applied to a large number of systems. Profiling and performance modeling are required to ensure effective utilization of the petascale hardware and will be accomplished by using tools and improving tools from three projects: PAPI, KOJAK and TAU. The remaining activity, algorithm development, focuses on implicit iterative methods and preconditioners that would improve convergence while preserving linear or nearly linear scaling, on variable step size implicit methods for quantum molecular dynamics, and on sparse eigensolvers with reduced scaling.<br\/><br\/> Development of true petascale simulation tools at the quantum level is a substantial achievement with lasting intellectual impact. New research avenues will likely emerge, as results of such simulations are analyzed and new generally applicable concepts are created.<br\/><br\/> Access to the open-source quantum simulation tools for petascale computers will have a broad impact. Advances in this field are relevant to virtually every area of scientific endeavor including chemistry, biochemistry and molecular biosciences, computer science and engineering, earth sciences, engineering, environmental sciences and materials science. Students and post doctoral researchers trained in this area will have significant opportunities for advancement and contributing impact on their own.<br\/><br\/><br\/><br\/><br\/><br\/><br\/>NON-TECHNICAL SUMMARY:<br\/><br\/>This award is made on a proposal submitted to the PetaApps Solicitation. The Office of Cyberinfrastructure, the Computer and Information Science and Engineering Directorate, and the Divisions of Materials Research, Physics, and Chemistry and the Office of Multidisciplinary Activities within the Mathematical and Physical Sciences Directorate contribute funds to this award. <br\/> <br\/>This project carries out research and education in high performance computing in the domain of nanotechnology. The work produces simulations of ultra-miniaturized electronic and structural elements, nearly the size of a millionth of an inch, which are nano-sized components of nano-devices. Computer software is being developed that can build, in a virtual sense, such device on an atom by atom basis and simulate operation and predict characteristics of such nano-device components. Such basic modeling gives highly reliable design characteristics of Nano Materials and the Devices and Processes involved in making the devices.<br\/><br\/>The major work of the project is carried out by an interdisciplinary team of theoretical physicists, computer scientists and device modeling specialists. The five principle investigators at two universities, plus graduate student research and postdoctoral researchers will be engaged in atomistic simulations of entire nanoscale device elements and structures. The project will produce modeling tools that will be shared freely as a set of open source petascale quantum simulation tools in the broad area of nanoscience and nanotechnology.","title":"Collaborative Research: Multiscale Software for Quantum Simulations in Nano Science and Technology","awardID":"0749293","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"7244","name":"COMPUTATIONAL PHYSICS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1712","name":"DMR SHORT TERM SUPPORT"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1978","name":"PROJECTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["486078","460784"],"PO":["565247"]},"130833":{"abstract":"Abstract - RUI: Knowledge processing with interval methods<br\/><br\/>This research investigates knowledge processing with interval methods. Modern technologies have evolved collections of massive datasets from observations, experiments, and scientific simulation. However, it remains a significant challenge to effectively process these datasets to discover knowledge effectively and efficiently. Knowledge processing with interval methods has intrinsic merit. First, qualitative properties are often presented as ranges of data attributes rather than specific points. By grouping attribute values into meaningful intervals, insignificant quantitative differences can be ignored, allowing an increased focus on qualitative processing. More importantly, interval-valued attributes contain more information than points, representing variability and uncertainty. Finally, in practice, interval-valued computational results can be more meaningful and useful than point values. <br\/><br\/>This study involves theories and algorithms for dataset interval representation, interval ordering relations, interval matrix decomposition and principal component analysis, inner approximation of interval solutions, interval-valued rule generation, and a portable computational environment for knowledge processing with interval methods. This research expands current knowledge on interval-valued data. The theoretical and algorithmic results of this research should have broad applicability to computing, especially for handling variability and uncertainty. In addition, this research project enhances the quality of education at a predominantly undergraduate institution and produce more high quality computer science graduates for Arkansas, a state which lags behind the nation in STEM workforce training.","title":"RUI: Knowledge Processing with Interval Methods","awardID":"0727798","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[347186],"PO":["565157"]},"130537":{"abstract":"Quantum computation is a burgeoning field of tremendous promise. It is perhaps best known for two breakthrough algorithms discovered a decade ago, one for cracking public key cryptography and another for fast solutions to optimization problems. Quantum computers can directly solve the equations of quantum mechanics, and so are believed to hold the key to ab initio drug design and materials engineering. This research involves a model called adiabatic quantum computation (AQC) that has attracted the attention of various groups attempting to build quantum computers, especially using superconducting devices. The AQC approach to computation is particularly well suited to optimization problems, since unlike an ordinary \"classical\" computer, the adiabatic quantum computer can simultaneously explore a multitude of possibilities while tunneling through barriers, as it is searching for the optimal solution.<br\/><br\/>The potential of AQC is exciting, but there are crucial missing elements in AQC theory. Most importantly, the theory of AQC error correction is still primitive, even though error correction will undoubtedly be indispensable for a working AQC. The reason is that quantum computers are particularly sensitive to their interactions with the environment, through a process called decoherence, that rapidly erases their computational power by introducing errors and noise. This research involves the development of a theory of error correction for AQC, as well as detailed insight into specific physical systems that could be used to realize a fault tolerant AQC. At the same time, this research addresses AQC's potential for new algorithms and insight into physical processes like quantum phase transitions. To thoroughly explore error correction in AQC, the investigators are using their experience with noiseless subsystems, dynamical decoupling, and quantum error correcting codes.","title":"Collaborative Research: Adiabatic Quantum Computing in Open Systems: Methodology, Performance, and Error Correction","awardID":"0726439","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":["441771"],"PO":["565157"]},"134904":{"abstract":"Proposal No: 0749049 <br\/>PI: Friedrich T. Sommer<br\/><br\/>Award Abstract:<br\/><br\/>This award supports services and infrastructure for sharing of computational neuroscience data as part of an exploratory activity aimed at catalyzing rapid and innovative advances in computational neuroscience and related fields. The core facility will provide transparent access to shared resources in a manner that scales up to large data sets. Services will be designed to lessen the burden on contributors to make their data or other resources available and to optimize the ability of the user community to identify and use those resources. Community- and market-oriented mechanisms will be developed to identify resources of particular significance for the field, and to solicit feedback from relevant communities. It is anticipated that the availability of high quality data will offer unprecedented opportunities for new types of discoveries, development of new methods, and development of new interdisciplinary collaborations. This new activity will also assist and drive teaching in computational neuroscience, through the exchange of datasets, stimuli, and analysis and modeling tools among modelers, experimentalists, and students.","title":"CRCNS data sharing: Central facility and services","awardID":"0749049","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["483816","518654"],"PO":["564318"]},"131109":{"abstract":"Wireless ad hoc and sensor networking has been identified as a recent major success story in the field of communications. Today, it is emerging as a promising technology with an expansive range of applications. The full potential of these networks is yet to be realized as a result of design challenges in the related networking issues. This research seeks to develop a research area crossing frontiers in random graph theory, probabilistic methods, communications and networking. Focusing on the analysis and design of finite wireless networks (i.e., networks with small or moderate number of nodes), for which both existing asymptotic analysis based on infinite number of nodes or simulation based approaches are inadequate, the work has the potential to advance knowledge and understanding across several related fields. <br\/><br\/>Mathematical study of wireless networks serves as a theoretical foundation for this area. During the last ten years a variety of network properties such as network connectivity, coverage, reliability, delay, lifetime, sleep scheduling, throughput and capacity have been analytically studied to some extent. The majority of these analytical studies have concentrated on asymptotic scenarios, i.e., when the number of nodes tends to infinity. To study finite (small or moderate-size) networks researchers often resort to computer simulations and algorithmic approaches (such as linear programming). Thus, mathematical analysis of finite networks that could result in practically-useful formulas has been largely ignored. The goal of this research is to develop a general framework for design, study, and optimization of finite wireless ad hoc and sensor networks.","title":"Collaborative Research: Study of Wireless Ad-Hoc and Sensor Networks in a Finite Regime","awardID":"0728970","effectiveDate":"2007-10-01","expirationDate":"2012-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["390131"],"PO":["564924"]},"130637":{"abstract":"Device scaling of silicon transistors has been the fundamental basis for the phenomenal success of the semiconductor industry. <br\/>Such scaling is reaching a point where it is absolutely necessary to explore new devices as replacement for traditional silicon transistors. <br\/>Otherwise, the progress of the semiconductor industry will be severely affected. Carbon Nanotube Field-Effect Transistors (CNFETs) are promising candidates as extensions to traditional Silicon transistors due to excellent device performance. While there have been significant accomplishments in scientific discovery of CNFETs in recent years at the single-device level, a major gap exists between such single-device-level results and the research required to harness the science into practical design technologies at the end of device scaling of silicon transistors. This research project targets to close this gap by developing necessary technologies required to make CNFETs practical candidates for replacing silicon transistors. <br\/><br\/>The objective of this research is to design of robust nanoscale computing fabrics using Carbon Nanotube Field Effect Transistors (CNFETs) in the presence of inherent limitations and imperfections, and to experimentally demonstrate essential components of a fabric such as a processor. This research is motivated by the fact that CNFETs are promising candidates as extensions to Silicon CMOS, yet fundamental nanoscale challenges prevent successful implementations of efficient CNFET-based circuits and systems. This project includes an interdisciplinary research team to demonstrate robust CNFET-based computing fabrics, and also to educate future generations of engineers and the general public in the emerging field of nanoscale computing.","title":"Collaborative Research: Design, Modeling, Automation and Experimentation of Nanoscale Computing Fabric using Carbon Nanotubes","awardID":"0726807","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["359676"],"PO":["565157"]},"134905":{"abstract":"Proposal No: 0749051<br\/>PI: Michael DeWeese<br\/><br\/>Award Abstract:<br\/><br\/>This award supports the preparation and sharing of computational neuroscience data as part of an exploratory activity aimed at catalyzing rapid and innovative advances in computational neuroscience and related fields. The data to be shared in this project are intracellular (whole-cell patch) recordings obtained in vivo from visual, auditory, somatosensory, and motor areas of the neocortex by the laboratories of Judith Hirsch, Anthony Zador, Michael DeWeese, and Michael Brecht. These data include not only spikes but also membrane voltages or currents generated by synaptic connections and intrinsic membrane channels. In addition to providing data, the investigators will develop tutorial materials describing recording methods, stimulus paradigms, and issues relevant to the interpretation of intracellular recordings. It is anticipated that this pooled data set will be useful for those wishing to study a particular sensory modality as well as those who hope to understand common features of neocortical function. It will also be of great value for the development of new methods of data analysis.","title":"SGER Collaborative Research: CRCNS Data Sharing of Intracellular Recordings from the Neocortex","awardID":"0749051","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}}],"PIcoPI":["539958"],"PO":["564318"]},"130615":{"abstract":"Large-scale autonomous wireless sensor networks that provide complete situation awareness and ubiquitous computing environments will become an essential part of the future network infrastructure. Node collaboration is the key for the success of sensor networks due to the fact that each node itself is limited by sensing range, power, and processing ability. On the other hand, node competition is a consequence of networking multiple nodes with limited time, space, and frequency resources. For small-scale networks, node collaboration and competition can be optimally balanced by a centralized scheme that has the access to all the node information and further has the control over the whole network. However, in a large-scale sensor network, which may involve millions of nodes and cover a large geographic area, it is impossible to afford a centralized scheme that manages the network-wide functional collaboration and resource competition.<br\/><br\/>This research program focuses on large-scale sensor networks and investigates the fundamental mechanism that controls node collaboration and competition in a purely distributed manner, with joint considerations of the three key elements in sensor network<br\/>design: topology control, information transmission, and information processing. The design methodology is motivated by some recent biological research results on how billions of cells in our body control their growth and interaction with each other in a both<br\/>collaborative and competitive way. The deliverables are general theorems, performance bounds, and analytical system models, which capture the interactive dynamics of various aspects of large-scale networking. These models define not only the fundamental principles regarding collaboration and competition among neighboring nodes, but also the adaptation rules that control each node to learn the environment and adjust its behavior. They are crucial to the future design of distributed networking protocols that are embedded into each sensor node, which mimics the genetic code inherited in each biological cell.","title":"Biologically-Inspired Networking and Computation in Large-Scale Autonomous Sensor Networks","awardID":"0726740","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["560117"],"PO":["432103"]},"134928":{"abstract":"Project proposes a petaflop-scalable computational infrastructure for the direct simulation of particulate flows, in particular the simulation of spatio-temporal dynamics of platelet aggregation. Better understanding of microcirculation of blood and platelet rheology will impact clinical needs in thrombosis risk assessment, anti-coagulation therapy, and stroke research. The proposed method comprises two algorithmic components: (1) integral equation solvers for Stokesian flows with dynamic interfaces; and (2) scalable fast multipole algorithms. Why do we need petaflop-scale computing power to tackle this problem? One microliter of blood contains millions of red blood cells(RBCs) and a few hundred thousand platelets. Discretizations with O(100 points\/cell and O(1000) time steps result in more than a trillion space-time unknowns. Solving problems of such size will require 50K-core machines. Computational tools that achieve such scalability, will enable direct numerical simulation of several microliters of blood, once million-core computing platforms are available.","title":"Collaborative Research: Petascale Algorithms for Particulate Flows","awardID":"0749162","effectiveDate":"2007-10-01","expirationDate":"2012-11-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["550936"],"PO":["565247"]},"134939":{"abstract":"TECHNICAL SUMMARY:<br\/><br\/>This award is made on a proposal submitted to the PetaApps Solicitation. The Office of Cyberinfrastructure, the Computer and Information Science and Engineering Directorate, and the Divisions of Materials Research, Physics, and Chemistry and the Office of Multidisciplinary Activities within the Mathematical and Physical Sciences Directorate contribute funds to this award. <br\/> <br\/>This award supports developing a high-performance software implementation of First-Principles Molecular Dynamics (FPMD) for petascale computers. FPMD is an atomic-scale simulation method that combines molecular dynamics with a quantum mechanical description of electronic structure, thus leading to a very versatile and predictive simulation approach. It is increasingly used in several areas of Materials Science, Physics, Chemistry and Nanotechnology.<br\/><br\/>Scalability will be achieved by developing i) scalable parallel linear algebra algorithms specialized for the FPMD problem, ii) visualization tools for code performance analysis that will be used to analyze message traffic patterns and optimize the software for specific network architectures, iii) data compression algorithms to handle large datasets resulting from petascale simulations. The capability to compute electronic structure using new hybrid exchange-correlation density functionals will also be included in the implementation in order to improve the accuracy of the simulations.<br\/><br\/>When deployed on petascale platforms, the new software will considerably enhance the range of FPMD applications in the areas of i) simulation of large samples, ii) long simulation times, and iii) accurate simulations using hybrid density functionals. The resulting software will be made available to the research community and will be ported to major computational facilities including the TeraGrid. A dedicated web server will be setup and maintained for the dissemination of the software and its associated documentation, tutorial material, and reference results. The implementation will build on the expertise acquired with an existing parallel implementation of FPMD, ?Qbox,? that has demonstrated high performance and scalability on up to 128,000 CPUs of a BlueGene\/L platform. A modular design using C++ and XML data formats will be adopted in order to facilitate future software development and integration with modern database tools.<br\/><br\/>The infrastructure developed in this project will provide a powerful atomistic simulation tool to the Materials Science, Physics and Chemistry research communities and will extend the range of applications of first-principles simulations in terms of size, duration and accuracy. By focusing early on issues of scalability at the petascale, this project will ensure that a high-performance implementation of FPMD will be available as soon as petascale platforms are built, which will maximize the efficient utilization of petascale cyberinfrastructure. More generally, this project will help identifying successful strategies for software development and application programming on petascale architectures.<br\/>This project also supports training for students in the area of application programming for large-scale parallel computers, and in the area of first principles molecular simulation. The software developed in the project will be used as a tool for teaching first-principles molecular simulation courses. It will also serve as a validation tool for the evaluation of new simulation methods developed by other groups (e.g. linear-scaling algorithms or multiscale methods). Due to the general nature of the simulation method developed, this project will facilitate the creation of electronic databases of validated results for use by other researchers in Chemistry, Physics and Nanoscience. Students from underrepresented groups will be recruited to participate in this project through existing programs at University of California, Davis.<br\/><br\/><br\/>NON-TECHNICAL SUMMARY:<br\/><br\/>This award is made on a proposal submitted to the PetaApps Solicitation. The Office of Cyberinfrastructure, the Computer and Information Science and Engineering Directorate, and the Divisions of Materials Research, Physics, and Chemistry and the Office of Multidisciplinary Activities within the Mathematical and Physical Sciences Directorate contribute funds to this award. <br\/> <br\/>This award supports the development of software for the most advanced, ?petascale,? high performance supercomputers that will enable high accuracy simulations of the motion of atoms. The resulting software can be used by a broad community of researchers in a variety of disciplinary and multidiscplinary research involving materials research, chemistry, physics, and nanotechnology. The accurate calculation of the forces on atoms requires a lot of computational resources. The resulting code together with the high performance of petascale supercomputers will allow simulations involving larg","title":"First-Principles Molecular Dynamics for Petascale Computers","awardID":"0749217","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"7553","name":"PHYSICS AT THE INFO FRONTIER"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1712","name":"DMR SHORT TERM SUPPORT"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1978","name":"PROJECTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["460379","552243","562713",358515],"PO":["565247"]},"130825":{"abstract":"As the VLSI technology scales down to 45nm feature size and below, the lithography process no longer produces the ideal shape\/dimension of circuit components in a silicon wafer. Geometrical parameter variations at 70nm technology can reach as much as 35%, and become increasingly severe as the feature size continues to decrease. The corresponding electrical parameter variations will significantly affect the performance and function of a VLSI circuit. Therefore, computing and analyzing the statistical properties of parasitic parameters in a silicon wafer become inevitable to the emerging nanometer scale VLSI technology. This grant aims to develop stochastic computational methods to address more general stochastic variables with distributions more realistic in nanometer VLSI technology. <br\/><br\/>This project will develop efficient algorithms using sparse grid spectral stochastic collocation method and compute interconnect capacitance and inductance using Wiener-Askey chaos basis and construct proper stochastic computational methods for non-Gaussian random variables. The intellectual merit comes from the development of sophisticated stochastic theories and efficient computing algorithms for the state-of-the-art engineering problems. This research will lay out basic guidelines and ideas and test the methods on realistic engineering problems. The broader impact of the research opens a new research direction in computing parasitic parameters with random process variations. The result of this research will have a great impact on the parasitic parameter extraction, circuit simulation and design of future nanometer scale VLSI circuits. It will lead to practical and efficient algorithms and CAD tools. Research of this project will be integrated into the graduate education of the Ph.D. students in applied mathematics and electrical engineering, and the developed software will be made public.","title":"Collaborative Research: Numerical Computations of Parasitic Parameters with Spectral Stochastic Collocation Methods for Nano-VLSI Technologies","awardID":"0727751","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["485314"],"PO":["562984"]},"132829":{"abstract":"Current graphics and visualization systems have to be built such that they can handle gigantic data sets. Such data sets include large scientific simulations such as nuclear and power simulations, data relevant to national priority and homeland security, and digital models of defense and commercial equipments such as tanks, aircraft, ships, and power plants. Such large data sets cannot fit into the main memory of the machines. Hence, the performance of the visualization systems depends on how efficiently they can process this data segments and still provide a holistic visualization for efficient and correct decision making. This project involves fundamental research in the analysis of methods that process these large geometry data sets for computer graphics and visualization applications. Using this analysis we model the data access pattern of common geometry processing algorithms. Such models can be used to organize data coherently in the secondary storage so that the access time of the data can be reduced. This will improve the performance of the graphics and visualization systems. <br\/> <br\/>The coarse data analysis systems derive aggregate information from the data and hence are useful in streaming applications and out-of-core implementations; fine data analysis systems are interested in individual data points and their performance is dictated by data access patterns. In this research, we investigate if there is any natural grouping or partitioning of primitives that describes the data access patterns of most common geometry processing algorithms. We explore the existence of a function that would optimize the grouping of primitives and thus benefit a large class of geometry processing algorithms. This study will enable us to suggest an optimal layout for geometric data that would work best for common geometric algorithms.","title":"SGER: Modeling Memory Access Patterns of Geometry Processing Algorithms","awardID":"0738401","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["370962"],"PO":["565157"]},"128081":{"abstract":"The last ten years have seen substantial progress in our ability to automatically analyze source code for security properties. These advances are transitioning into practice and making a broad impact on software security. However, the success of this work has been partially limited by the fact that there are many security properties that we do not know how to analyze.<br\/><br\/>This project seeks to take several initial steps towards extending the range and scope of security properties that can be checked on large software systems. The research involves three tracks. First, we will investigate how the design of programming languages can support secure programming practices. Second, we will study techniques for analyzing memory safety, a a complex yet fundamental security property that, surprisingly, is still not statically checkable today. Third, we will study how to check that networking software is consistent with the specifications implied in standards committees' descriptions of network protocols. Integrated together, these three directions will allow us to provide end-to-end guarantees of security properties that span from low to high level. The goal of this project is to begin the work of developing a future generation of techniques for improving software security.","title":"CT-T: Collaborative Research: Complex, High-level, Integrated Properties for Security","awardID":"0715650","effectiveDate":"2007-10-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["556281"],"PO":["521752"]},"130907":{"abstract":"This research considers physical layer security in wireless networks. It serves as a counterpoint to traditional methods of security, such as computationally secure crypto-systems, by considering security in the wireless medium itself. An important motivator for this research is the rapid proliferation of wireless communication devices, technologies, and applications, in general. The very nature of wireless networks exposes not only the risks and vulnerabilities that a malicious user can exploit and severely compromise the network but also information confidentiality concerns with respect to the in-network terminals. Although wireless technologies are becoming more and more secure, attackers are becoming smarter, and sole reliance on cryptographic keys in large distributed networks, for example, where terminals can be compromised is unsustainable from the security perspective. A direct consequence of this process is the need to, additionally, tackle security at the very basic physical layer level where the (unconditional) secrecy may be embodied within the information itself, and, also, adapted to the communication medium and network conditions. Furthermore, there is a need for a secure key distribution, a process which can be performed in perfect secrecy only using physical layer techniques, even when, henceforth, relying on conventional cryptographic techniques.<br\/>This research studies how, in secure wireless networks, the application delay requirement relative to the communication channel coherence time aspects the selection of signaling strategies and the achievable communication rates at a prescribed security level. In addition, perhaps surprisingly,<br\/>successful design of practical secure nested forward error correction codes and secure adaptive incremental error correction strategies applicable to wireless channels is virtually non-existent. Hence,this research focuses on practical coding-scheme designs as essential enabling tools.","title":"Collaborative Research: Communicating confidential information over wireless networks","awardID":"0728208","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["560130"],"PO":["432103"]},"125290":{"abstract":"Project Id: 0702831 and 0702628 <br\/>PI(s): Sarma Vrudhula and Spyros Tragoudas<br\/>Title: Synthesis, Verification and Testing for Nano-CMOS and Beyond using Threshold Logic<br\/>Institutions: Arizona State University &<br\/><br\/>ABSTRACT<br\/><br\/>By 2020, when thickness of Silicon will be less than a stack of a few atoms, the Semiconductor Industry Association roadmap predicts that further scaling CMOS circuits will not be sustainable, and expects a transition from CMOS to one or more of the presently nascent nano technologies such as resonant tunneling diodes (RTD), carbon nanotube FETs (CNFET) and carbon nanowires. Further in the future are devices such as single electron transistors (SET), and quantum cellular automata (QCA). An important and distinctive characteristic of these post-CMOS nano technologies is that they make it possible to efficiently and naturally implement threshold logic (TL). While TL concepts have been known since the 1960s, there has been no comprehensive work on synthesis and optimization of large TL networks similar to what we have witnessed over the past 30 years for traditional CMOS logic gate networks. <br\/><br\/>This is a proposal to develop a comprehensive design methodology encompassing synthesis, optimization, verification, and testing of TL networks. We propose to investigate synthesis algorithms that start with a technology independent, functional description of the circuit. Optimization of TL networks poses unique problems. Regardless of the underlying technology, TL gates are realized by comparing the weighted sum of the inputs with a given threshold. This can be a comparison of voltages or currents. Since process variations can change the outcome of such a comparison, they not only effect the performance and power but can also change the function realized by the gate. We refer to this as the functional yield (FY). We will develop new algorithms that jointly maximize the FY, power consumption, and performance of a TL network over the space of process variables, e.g. device lengths, widths, threshold voltages, oxide thicknesses, etc. Methods for testing the manufactured circuit for functional correctness and delay using new parametric fault models will also be developed. Verifying the equivalence of a TL network to a given a functional specification has not yet been addressed. This is essential for verifying the result of the synthesis procedure as well as in determining the functional yield when the design parameters are represented as statistical quantities as models of process variations. Expected outcomes of this effort include: new CMOS and post-CMOS circuit architectures for TL gates; algorithms and tools to automatically synthesize, perform functional verification and generate test patterns for TL circuits; methods to compute the parametric yield of TL networks, modeling TL network parameters as correlated random variables; methods to perform joint optimization of functional yield, power consumption and performance of TL networks over the space of process variables.","title":"Collaborative Research: Synthesis, Verification and Testing for Nano-CMOS and Beyond using Threshold Logic","awardID":"0702831","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["525996"],"PO":["562984"]},"128271":{"abstract":"The last ten years have seen substantial progress in our ability to automatically analyze source code for security properties. These advances are transitioning into practice and making a broad impact on software security. However, the success of this work has been partially limited by the fact that there are many security properties that we do not know how to analyze.<br\/><br\/>This project seeks to take several initial steps towards extending the range and scope of security properties that can be checked on large software systems. The research involves three tracks. First, we will investigate how the design of programming languages can support secure programming practices. Second, we will study techniques for analyzing memory safety, a a complex yet fundamental security property that, surprisingly, is still not statically checkable today. Third, we will study how to check that networking software is consistent with the specifications implied in standards committees' descriptions of network protocols. Integrated together, these three directions will allow us to provide end-to-end guarantees of security properties that span from low to high level. The goal of this project is to begin the work of developing a future generation of techniques for improving software security.","title":"CT-T: Collaborative Research: Complex, High-level, Integrated Properties for Security","awardID":"0716695","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["507674"],"PO":["529429"]},"129184":{"abstract":"An invariant of a dynamical system is a region wherein the system remains in any execution. Invariants are useful for analyzing a system. They provide assurance that a complex, possibly safety-critical, system does not ever get into an undesirable, or unsafe, state. The twin tasks of (a) computing an invariant of a system, and (b) checking if a given expression is indeed an invariant -- are both notoriously difficult. In the context of software systems, the concept of an ``inductive invariant'' partly overcomes these two difficulties. <br\/><br\/>This project performs a systematic exploration of the concept of inductive invariants for continuous dynamical systems and hybrid systems. It builds a constructive theory of inductive invariants that provides computability and complexity results for generating inductive invariants of different forms for such systems. The challenge is to develop scalable techniques for highly nonlinear, nondeterministic, and stochastic systems with unknown parameters. Such systems arise commonly in engineering and science. <br\/>The approach for invariant discovery is based on reducing the problem to algorithmic problems in symbolic and numeric computation. <br\/><br\/>As a case study, invariant generation techniques are used to analyze pharmacodynamic and pharmacokinetic models from biomedical literature and verify the safety of medical devices such as insulin pumps. An invariant for an insulin pump would provide bounds for blood glucose concentrations. Invariants are useful for improving reliability of simulation tools. Invariant generation techniques will play a significant role in any fully or partly automated methodology for certifying complex systems.","title":"CSR--EHS: Invariants for Continuous and Hybrid Dynamical Systems","awardID":"0720721","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["451280"],"PO":["561889"]},"128272":{"abstract":"The last ten years have seen substantial progress in our ability to automatically analyze source code for security properties. These advances are transitioning into practice and making a broad impact on software security. However, the success of this work has been partially limited by the fact that there are many security properties that we do not know how to analyze.<br\/><br\/>This project seeks to take several initial steps towards extending the range and scope of security properties that can be checked on large software systems. The research involves three tracks. First, we will investigate how the design of programming languages can support secure programming practices. Second, we will study techniques for analyzing memory safety, a a complex yet fundamental security property that, surprisingly, is still not statically checkable today. Third, we will study how to check that networking software is consistent with the specifications implied in standards committees' descriptions of network protocols. Integrated together, these three directions will allow us to provide end-to-end guarantees of security properties that span from low to high level. The goal of this project is to begin the work of developing a future generation of techniques for improving software security.","title":"CT-T: Collaborative Research: Complex, High-level, Integrated Properties for Security","awardID":"0716715","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["550026"],"PO":["521752"]},"129097":{"abstract":"With the increasing advances in medical, computing and networking technologies, we can<br\/>significantly improve the quality and safety of medical device networks. In hospitals and clinics, these networks will relieve medical personnel from having to mentally correlate digital displays and paper records. In operating rooms, staff will no longer have to emulate the safety interlocks necessary for a patient?s survival. People once confined to expensive nursing homes, can potentially live longer, more productive and independent lives using these same medical device networks in their own home. To help realize this potential, we need to create the scientific foundation for the safe and easy composition of medical and health management devices. Much attention has been paid to the optimization of individual QoS requirements and their protocols. Yet, no system is truly functional without multiple protocols working together to achieve a balance of desirable qualities. Our research focus on the scientific foundation for the end-to-end composition of systems, backed by formally verifiable mathematical models, will push software architectures and systems to the next generation of safe and easily-composed medical device networks and other such Cyber-Physical Systems. QoS aware composition technologies are vital to the mass deployment of modern health care, assisted living, and other medical devices that will improve the quality of medical services, allowing people once confined to expensive nursing homes to live independent and productive lives in their own home. Although this technological foundation will be initially developed in the context of assisted living, we expect that it will be extended to help enable many more applications because QoS aware composition is a generic problem in many Cyber-Physical System application domains.","title":"CSR EHS: Formal Model Based Health and Medical System Composition","awardID":"0720482","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550264","553686",342042],"PO":["565136"]},"128130":{"abstract":"As malicious attacks on computer systems increase in severity and sophistication, developing effective methods for protecting the Internet is among the most important challenges facing computer science today.<br\/>Network-based security mechanisms offer both good coverage and the possibility of early threat detection, but they often conflict with the performance requirements of network elements because of the vast amounts of traffic data that must be analyzed. This project will apply massive-dataset (MDS) algorithmics to network security, bringing together two previously unconnected research areas. The objective is to achieve a qualitative improvement in network security by developing efficient, yet theoretically rigorous, algorithmic defenses that can be deployed at scale in modern networks.<br\/><br\/>The project addresses both fundamental algorithm-design problems and practical applications. MDS algorithmics provides a set of basic techniques for highly efficient (e.g., one-pass, small-space,<br\/>polylog-time) analysis of large amounts of data. This project will investigate how these methods can be used for (1) online classification and property testing of packet streams, including efficient inference of streams generated by a mixture of stochastic sources, (2) detection of changes and anomalies in traffic patterns, and (3) development of computationally tractable models of traffic sources that support reasoning about a wide variety of adversarial behaviors and incorporate prior knowledge such as conventional intrusion-detection rules.<br\/>The algorithmic toolkit developed in the course of the project will be applied to practical network-security problems such as recognizing denial of service activity, worm fingerprinting, and detecting botnets.","title":"CT-ISG: Collaborative Research: Massive-Dataset Algorithmics for Network Security","awardID":"0716158","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["519592"],"PO":["565239"]},"134180":{"abstract":"Automated telephone dialog systems rely disproportionately on accurate transcription of the speech signal into readable text. When the system has low confidence in the automatic speech transcription<br\/>(ASR) of a caller's utterance, a typical dialog strategy requires the system to repeat its best guess, and ask for confirmation. This leads to unnatural interactions and dissatisfied callers. The current project focuses on developing better dialog strategies given current ASR capabilities by learning automatically from contrasting<br\/>corpora, and comparing the results. Using a novel methodology, wizard ablation, simulated human-system dialogs are collected that vary in controlled ways. The testbed application, an Automated Readers Advisor for New York City's Andrew Heiskell Talking Book and Braille Library, has appropriately limited complexity, and<br\/>potentially broad social benefit.<br\/><br\/>The motivation for wizard ablation is that research is needed into the problem-solving strategies humans would use if the human communication channel were restricted to be more like a machine's. In conventional wizard-of-oz studies, unsuspecting users interact with human wizards \"behind-the-screen\", thus providing data on the way humans interact with (what they believe to be) machines. Unlike a conventional wizard, an ablated wizard is restricted to seeing the ASR input to the system dialog manager. Under a further ablation<br\/>condition, the wizard must choose actions from the repertoire that the system uses, but can combine them freely. The book-borrowing scenarios for the wizard interactions have been designed to be realistic, and Heiskell Library patrons participate in the studies. The collected dialogs will be made available to the community.","title":"HRI: Collaborative Proposal: Incremental Wizard Ablation: A Novel WOz Paradigm for Learning, Testing and Evaluating Human-Machine Dialog Strategies from Parameterized Corpora","awardID":"0745369","effectiveDate":"2007-10-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["472076"],"PO":["565215"]},"126590":{"abstract":"0708962<br\/><br\/>Collaborative Research: CRI: IAD: Electronic Testing Education, Research and Training Infrastructure<br\/><br\/>Vishwani Agrawal<br\/><br\/>This three-year project will develop capabilities for testing of digital, memory, analog, and radio frequency devices, with possible future upgrades to MEMS, optical and nanotechnology devices. The infrastructure developed under this project consists of a test laboratory and its value-added applications. The new VLSI test laboratory will have modern automatic test equipment (ATE) of open architecture. The laboratory will be located at Auburn University in Alabama and will be used through networked access by three other institutions of that state, namely, University of Alabama at Tuscaloosa, University of Alabama at Huntsville, and Tuskegee University, for advancing education, training and research applications. In the future, the test lab infrastructure will allow collaborative use by universities outside Alabama as well, making it a national resource. The applications will include new university courses with hands-on experiments on testing of digital, analog and radio frequency chips, FPGAs and system-on-a-chip devices; industry-oriented training classes including test lab exercises; and research on silicon debug methods aimed at improving the yield and reliability. Research with industry focus and practice-oriented short courses are expected to make this infrastructure fully supported through industry funds beyond the three-year infrastructure building period.","title":"Collaborative Research: CRI: IAD: Electronic Testing Education, Research and Training Infrastructure","awardID":"0708962","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[335880,"550443","485659",335883,"540565"],"PO":["565272"]},"129594":{"abstract":"This proposal centers around the notion that computing has evolved into an inter- and intra-disciplinary field of intertwined concepts that pervade society. The Georgia Institute of Technology (Georgia Tech) and other schools in the University System of Georgia have defined and adopted a number of specialized degrees and contextualized computing courses. Last Fall, Georgia Tech extended this approach to create<br\/>the Threads model includes a process for creating curricular change, an infrastructure for advising, and software to support administrators, advisors, educators and students. In parallel, Brooklyn College of the City University of New York (BC-CUNY) has developed several context-based approaches to computing education with a focus on introductory courses and the high school to college continuum, as well as created two new interdisciplinary masters degrees. The team proposes to create an alliance that validates and extends the Threads model. The proposed work encompasses a methodical approach to understanding the process of defining broad, flexible paths through a computing curriculum, and to measuring and analyzing the outcomes<br\/>of this process when applied to a variety of departments and interest groups. At the heart of this process is an emphasis on context-based instruction and targeted advising that helps students crystalize career paths and realize the short- and long-term relevance of their coursework. The project explores crucial research questions that arise out of adapting and applying Threads, and evaluating the effects on students, faculty and administrators through quantitative and qualitative studies. Under the work proposed here, they will measure the impact of Georgia Tech?s implementation<br\/>of the Threads model and the supporting advising mechanisms; extend and adapt Threads to a broad range of computing departments; facilitate its adoption at such departments; and evaluate its efficacy under a variety of conditions. The goal is a validated, widely deployed and broadly-evaluated model of curricular reform that is applicable to small and large departments, students with a range of backgrounds and abilities, and faculty with a range of interests. The combination of diverse experiences brought together by the project team promises to produce results with the potential to serve as national models for both computing and other STEM (science, technology, engineering and math) disciplines.","title":"CPATH EAE: Project Vision: Changing Computing Using Locally Meaningful Context","awardID":"0722270","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["347781",343451],"PO":["565136"]},"128197":{"abstract":"Most commercial wireless devices do not make lower-layer properties (e.g., raw waveform-level samples from an analog-to-digital converter) accessible to users. Recently, however, the research community has directed its attention towards the development of cognitive radios that will expose the lower-layers of the protocol stack to researchers and developers. Although the promise of such a flexible platform is great, there are also some serious potential security drawbacks. It is easily conceivable that cognitive radios could become an ideal platform for abuse since the lowest layers of the protocol stack will be accessible to programmers in an open-source manner. The proposed project addresses these concerns by focusing on two important building blocks needed in constructing a holistic solution to ensuring the trustworthy operation of software radios: first, the investigating team plans to develop tools to quantify the degree to which spectrum etiquette policies are abused in a network of cognitive radios and, second, the team plans to investigate methods for identifying such spectrum abuse, which is necessary in order to drive anomaly detection and response mechanisms. Overall, the broader impact of the effort is centered around the fact that cognitive radios represent an emerging technology that requires security mechanisms to be developed before these highly-programmable radios reach the public market.","title":"Collaborative Research: CT-T: TRIESTE: A Trusted Radio Infrastructure for Enforcing Spectrum Etiquettes","awardID":"0716400","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["564747","531726","466323"],"PO":["529429"]},"125040":{"abstract":"This project will develop a system infrastructure that protects existing software from low-level and high-level vulnerabilities. The goal is to synergistically combine hardware and software features in a manner that provides robustness (no false positives or false negatives), flexibility (can evolve to cover future threats), end-to-end coverage (handles user and system code), practicality (works will all types of real world binaries), simplicity (can be easily verified), and good performance (no significant runtime impact). At the hardware level, the system will combine two novel features: support for dynamic information flow tracking (DIFT) and support for isolated execution. DIFT allows the system to track the propagation of untrusted data and code during the execution of a program and prevent any unsafe uses. The goal is to develop hardware support for DIFT that reduces its overhead while allowing software to control and extend its policies. Isolated execution has been used to improve the performance by allowing optimistic parallelization. The goal is to extend isolated execution to support the use of untrusted data and code until their safety is verified. At the software level, the system will provide a runtime environment that will manage and virtualize the hardware security features based on the active security policies. It will also define the interfaces that allow software security tools (static and dynamic) to program, use, and collaborate with the hardware mechanisms. Finally, it will define a domain-specific language for security policies that abstracts out the division of labor between hardware and software security features. To demonstrate this approach, the project will develop a full-system prototype based on an open-source Sparc processor and the open-source Linux operating system. Such a prototype will facilitate an extensive security evaluation using real-world server and client software in an on-line setting.","title":"System Level Architecture for Practical and Efficient Security Analysis","awardID":"0701607","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":[331779,"556787"],"PO":["366560"]},"129562":{"abstract":"This proposal centers around the notion that computing has evolved into an inter- and intra-disciplinary field of intertwined concepts that pervade society. The Georgia Institute of Technology (Georgia Tech) and other schools in the University System of Georgia have defined and adopted a number of specialized degrees and contextualized computing courses. Last Fall, Georgia Tech extended this approach to create<br\/>the Threads model includes a process for creating curricular change, an infrastructure for advising, and software to support administrators, advisors, educators and students. In parallel, Brooklyn College of the City University of New York (BC-CUNY) has developed several context-based approaches to computing education with a focus on introductory courses and the high school to college continuum, as well as created two new interdisciplinary masters degrees. The team proposes to create an alliance that validates and extends the Threads model. The proposed work encompasses a methodical approach to understanding the process of defining broad, flexible paths through a computing curriculum, and to measuring and analyzing the outcomes<br\/>of this process when applied to a variety of departments and interest groups. At the heart of this process is an emphasis on context-based instruction and targeted advising that helps students crystalize career paths and realize the short- and long-term relevance of their coursework. The project explores crucial research questions that arise out of adapting and applying Threads, and evaluating the effects on students, faculty and administrators through quantitative and qualitative studies. Under the work proposed here, they will measure the impact of Georgia Tech?s implementation<br\/>of the Threads model and the supporting advising mechanisms; extend and adapt Threads to a broad range of computing departments; facilitate its adoption at such departments; and evaluate its efficacy under a variety of conditions. The goal is a validated, widely deployed and broadly-evaluated model of curricular reform that is applicable to small and large departments, students with a range of backgrounds and abilities, and faculty with a range of interests. The combination of diverse experiences brought together by the project team promises to produce results with the potential to serve as national models for both computing and other STEM (science, technology, engineering and math) disciplines.","title":"CPATH EAE: Extending Contextualized Computing in Multiple Institutions Using Threads","awardID":"0722157","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":[343329,343330,"509697"],"PO":["565136"]},"128143":{"abstract":"Most commercial wireless devices do not make lower-layer properties (e.g., raw waveform-level samples from an analog-to-digital converter) accessible to users. Recently, however, the research community has directed its attention towards the development of cognitive radios that will expose the lower-layers of the protocol stack to researchers and developers. Although the promise of such a flexible platform is great, there are also some serious potential security drawbacks. It is easily conceivable that cognitive radios could become an ideal platform for abuse since the lowest layers of the protocol stack will be accessible to programmers in an open-source manner. The proposed project addresses these concerns by focusing on two important building blocks needed in constructing a holistic solution to ensuring the trustworthy operation of software radios: first, the investigating team plans to develop tools to quantify the degree to which spectrum etiquette policies are abused in a network of cognitive radios and, second, the team plans to investigate methods for identifying such spectrum abuse, which is necessary in order to drive anomaly detection and response mechanisms. Overall, the broader impact of the effort is centered around the fact that cognitive radios represent an emerging technology that requires security mechanisms to be developed before these highly-programmable radios reach the public market.","title":"Collaborative Research: CT-T: TRIESTE: A Trusted Radio Infrastructure for Enforcing Spectrum Etiquettes","awardID":"0716208","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["564848","560140","548204"],"PO":["529429"]},"128264":{"abstract":"It is estimated that hardware (HW) intellectual property piracy induces almost an order of magnitude higher loss when compared to software piracy cost and greatly facilitates both software and entertainment data piracy. In addition, HW vulnerabilities can be exploited for security attacks, especially the increasing threat of HW malware attacks (malicious or unintentional alterations of design specifications that compromise the correctness of the functionality under specific conditions) due to business models moving to external foundries and assembly houses. The strategic objective of this project is to give impetus to research on the protection against HW malware attacks and piracy with a multi-pronged attack that includes the development of both the conceptual foundation and several practical HW protection techniques.<br\/><br\/>The intellectual merit of this project is the development of HW protection techniques that leverage the manufacturing variability (MV) inherent in modern fabrication processes and the resulting uniqueness of every manufactured integrated circuit (IC). This project is developing: (i) HW malware detection techniques considering only a single IC under very mild statistical assumptions about MV, and (ii) active HW metering protocols that prevent foundries from distributing unauthorized design copies by integrating MV into the functionality of the targeted design in such a way that only the designer can issue a key that unlocks the design after power up so that it becomes functional.<br\/><br\/>The broader impacts of this project are that it will form the basis for a new industry that ensures HW integrity in a complex horizontal IC business model. In addition, this project is exposing both undergraduate and graduate students to these increasingly important topics and providing research opportunities for underrepresented students.","title":"Collaborative Research: CT-T: Manufacturing Variability-based Hardware Protection Techniques","awardID":"0716674","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["485958"],"PO":["521752"]},"128133":{"abstract":"As malicious attacks on computer systems increase in severity and sophistication, developing effective methods for protecting the Internet is among the most important challenges facing computer science today.<br\/>Network-based security mechanisms offer both good coverage and the possibility of early threat detection, but they often conflict with the performance requirements of network elements because of the vast amounts of traffic data that must be analyzed. This project will apply massive-dataset (MDS) algorithmics to network security, bringing together two previously unconnected research areas. The objective is to achieve a qualitative improvement in network security by developing efficient, yet theoretically rigorous, algorithmic defenses that can be deployed at scale in modern networks.<br\/><br\/>The project addresses both fundamental algorithm-design problems and practical applications. MDS algorithmics provides a set of basic techniques for highly efficient (e.g., one-pass, small-space,<br\/>polylog-time) analysis of large amounts of data. This project will investigate how these methods can be used for (1) online classification and property testing of packet streams, including efficient inference of streams generated by a mixture of stochastic sources, (2) detection of changes and anomalies in traffic patterns, and (3) development of computationally tractable models of traffic sources that support reasoning about a wide variety of adversarial behaviors and incorporate prior knowledge such as conventional intrusion-detection rules.<br\/>The algorithmic toolkit developed in the course of the project will be applied to practical network-security problems such as recognizing denial of service activity, worm fingerprinting, and detecting botnets.","title":"CT-ISG: Collaborative Research: Massive Dataset Algorithmics for Network Security","awardID":"0716172","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["553656","382346"],"PO":["529429"]},"129288":{"abstract":"The Internet has become a part of our everyday lives. We use it to conduct much of our business, government, and social interactions. However, the scale and heterogeneity of the Internet have far surpassed anyone's expectations, and the Internet is responding by showing signs of strain. Security was not a major goal of the original design of Internet protocols, and it is now far too easy for malicious agents to engage in disruptive activities. The need to manage the network was not in mind from the beginning, making it difficult for network administrators. For the Internet to meet these challenges, we need a much deeper understanding both of the properties of our existing protocols and of the fundamental trade-offs that should guide the design of the future Internet.<br\/><br\/>The Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) is holding a 3-year special focus devoted to the study of algorithms and protocols for large-scale networks in a way that is guided by a deep understanding of the current Internet while also allowing for the possibility of radical change where this is warranted. This is an emerging cross-disciplinary area that requires expertise from several fields including networking, theory of computing, computer and communications security, and game theory.<br\/><br\/>The special focus, open to participants nationwide and worldwide, begins with a three-day tutorial designed to introduce participants to topics at the intersection of algorithms and networking. Focused working groups of researchers will concentrate on secure Internet routing; designing networks for manageability; and data structures and algorithms for network data. Workshops will be held on topics ranging from Internet tomography to pervasive networks, systems, and applications.","title":"DIMACS Special Focus on Algorithmic Foundations of the Internet","awardID":"0721113","effectiveDate":"2007-10-01","expirationDate":"2013-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["264120","540234"],"PO":["497499"]},"129564":{"abstract":"This proposal centers around the notion that computing has evolved into an inter- and intra-disciplinary field of intertwined concepts that pervade society. The Georgia Institute of Technology (Georgia Tech) and other schools in the University System of Georgia have defined and adopted a number of specialized degrees and contextualized computing courses. Last Fall, Georgia Tech extended this approach to create<br\/>the Threads model includes a process for creating curricular change, an infrastructure for advising, and software to support administrators, advisors, educators and students. In parallel, Brooklyn College of the City University of New York (BC-CUNY) has developed several context-based approaches to computing education with a focus on introductory courses and the high school to college continuum, as well as created two new interdisciplinary masters degrees. The team proposes to create an alliance that validates and extends the Threads model. The proposed work encompasses a methodical approach to understanding the process of defining broad, flexible paths through a computing curriculum, and to measuring and analyzing the outcomes<br\/>of this process when applied to a variety of departments and interest groups. At the heart of this process is an emphasis on context-based instruction and targeted advising that helps students crystalize career paths and realize the short- and long-term relevance of their coursework. The project explores crucial research questions that arise out of adapting and applying Threads, and evaluating the effects on students, faculty and administrators through quantitative and qualitative studies. Under the work proposed here, they will measure the impact of Georgia Tech?s implementation of the Threads model and the supporting advising mechanisms; extend and adapt Threads to a broad range of computing departments; facilitate its adoption at such departments; and evaluate its efficacy under a variety of conditions. The goal is a validated, widely deployed and broadly-evaluated model of curricular reform that is applicable to small and large departments, students with a range of backgrounds and abilities, and faculty with a range of interests. The combination of diverse experiences brought together by the project team promises to produce results with the potential to serve as national models for both computing and other STEM (science, technology, engineering and math) disciplines.","title":"CPATH EAE: Extending Contextualized Computing in Multiple Institutions Using Threads","awardID":"0722163","effectiveDate":"2007-10-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["527092","559402","425936","393315"],"PO":["565136"]},"127397":{"abstract":"The proposed project will apply some of the general principles of archiving and curation to digital data, with a focus on scientific data. The proposal calls for the development and refinement of a \"staging process\"<br\/>that will bring an appropriate mix of human skills and automated tools to organize and initially describe a given corpus prior to incorporation into a repository. It assumes that for the foreseeable future some level of human judgement will be needed to adequately prepare and package a raw body of data. Outstanding questions to be addressed include whether a some point the level of human effort so high as to preclude further scaling.<br\/>The proposed activity consists of two main parts: the design and deployment of a digital staging repository and the design and deployment of a metadata management architecture. The first encompasses issues of institutional policy, requirements analysis, and the selection and integration of existing software platforms (e.g., Fedora). The second will deal with complex metadata issues such as web semantics, ontology creation, and inter-domain cross-walking.","title":"III-CXT: Promoting the curation of research data through library-laboratory collaboration","awardID":"0712989","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[337870,337871],"PO":["565136"]},"128288":{"abstract":"It is estimated that hardware (HW) intellectual property piracy induces almost an order of magnitude higher loss when compared to software piracy cost and greatly facilitates both software and entertainment data piracy. In addition, HW vulnerabilities can be exploited for security attacks, especially the increasing threat of HW malware attacks (malicious or unintentional alterations of design specifications that compromise the correctness of the functionality under specific conditions) due to business models moving to external foundries and assembly houses. The strategic objective of this project is to give impetus to research on the protection against HW malware attacks and piracy with a multi-pronged attack that includes the development of both the conceptual foundation and several practical HW protection techniques.<br\/><br\/>The intellectual merit of this project is the development of HW protection techniques that leverage the manufacturing variability (MV) inherent in modern fabrication processes and the resulting uniqueness of every manufactured integrated circuit (IC). This project is developing: (i) HW malware detection techniques considering only a single IC under very mild statistical assumptions about MV, and (ii) active HW metering protocols that prevent foundries from distributing unauthorized design copies by integrating MV into the functionality of the targeted design in such a way that only the designer can issue a key that unlocks the design after power up so that it becomes functional.<br\/><br\/>The broader impacts of this project are that it will form the basis for a new industry that ensures HW integrity in a complex horizontal IC business model. In addition, this project is exposing both undergraduate and graduate students to these increasingly important topics and providing research opportunities for underrepresented students.","title":"Collaborative Research: CT-T: Manufacturing Variability-based Hardware Protection Techniques","awardID":"0716823","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["472184"],"PO":["529429"]},"133073":{"abstract":"This project provides for international technology assessments and related planning workshop support. WTEC and its predecessors have conducted over 60 such technology assessments since 1989. The objectives include seeking technologies to transfer to the U.S., evaluating the position of U.S. science and technology with respect to leading competitors, and seeking opportunities for international cooperation in research.<br\/><br\/>The project provides for six or more additional international studies and reporting of the results, plus one or more workshops in area related to international technology assessments over a three-year period of performance.<br\/><br\/>Results will be disseminated at workshops including copies of visual aids distributed in hardcopy and posted on the Web, final reports from each study tour of academic quality in hardcopy, CD, and Web media, plus at least one synthesis report on cross-cutting findings. A blog on international S&T policy will be published to report progress.","title":"Assessment of Global Leadership in Science and Engineering","awardID":"0739505","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"7244","name":"COMPUTATIONAL PHYSICS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1798","name":"SPECIAL PROJECTS (AST)"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1712","name":"DMR SHORT TERM SUPPORT"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1385","name":"SPECIAL STUDIES AND ANALYSES"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1402","name":"BIOCHEMICAL & BIOMASS ENG"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1403","name":"PROCESS & REACTION ENGINEERING"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1468","name":"Manufacturing Machines & Equip"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1491","name":"BIOTECH, BIOCHEM & BIOMASS ENG"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"5514","name":"OPERATIONS RESEARCH"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S099","name":"NATL LIBRARY OF MEDICINE"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S100","name":"NATL INSTITUTE OF BIOMEDICAL"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S101","name":"NIST"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S107","name":"ARGONNE NATL LABORATORY"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S110","name":"NATL INSTITUTE OF BIOMEDICAL"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S114","name":"DEPT OF ENERGY"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"T907","name":"NATIONAL INSTITUTE OF HEALTH"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"T908","name":"NASA-AMES RESEARCH CENTER"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1630","name":"MECHANICS OF MATERIALS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1635","name":"STRUCTURAL MATERIALS AND MECH"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7478","name":"DYNAMICAL SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7479","name":"Biomechanics & Mechanobiology"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0704","name":"Division of EMERGING FRONTIERS IN RES & IN","abbr":"EFRI"},"pgm":{"id":"7633","name":"EFRI RESEARCH PROJECTS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1765","name":"CONDENSED MATTER & MAT THEORY"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0706","name":"Division of DESIGN & MANUFACTURING INNOV","abbr":"DMI"},"pgm":{"id":"1786","name":"MANFG ENTERPRISE SYSTEMS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"5345","name":"BIOMEDICAL ENGINEERING"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1112","name":"Genetic Mechanisms"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1685","name":"NNCO ACTIVITIES"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"H364","name":"NIH"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"H365","name":"NASA"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"H366","name":"AFOSR"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"H372","name":"EPA"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"H384","name":"DEPT OF ENGERY"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"H388","name":"Defense Advanced Research Proj"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"H393","name":"Defense Advanced Research Proj"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"H424","name":"ARMY"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"HR02","name":"DARPA"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"HR03","name":"DARPA"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S020","name":"NASA"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"T747","name":"NIH - NANOTECHNOLOGY"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"T818","name":"EPA-NNCO"}},{"dir":{"id":"13","name":"Directorate for NATL NANOTECHNOLOGY COORDINATING OFFICE ","abbr":"NNCO"},"div":{"id":"1300","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"T976","name":"AIR FORCE OFFIC OF SCIENCE RES"}}],"PIcoPI":["560018"],"PO":["449419"]},"134184":{"abstract":"Research into creativity has only recently focused on process issues that have the potential to be transferable and hence provide the basis for the enhancement of creativity in design. This proposal addresses the need to bring the methods of creativity research to bear onto creative IT design through a Workshop of leading researchers from four disparate areas: design science, computer science, cognitive science, and neuroscience. Bringing together international researchers from these different communities will provide opportunities for trans-disciplinary research and for each of these four sciences to expose each to the others research and research methods within the context of creative IT. <br\/><br\/>The workshop, which will be scheduled over two days, is being held in Europe both because much of this research is being carried on outside the USA and to foster international collaboration. The Department of Cognitive Psychology at the University of Provence in Aix-en-Provence, through Professor Bonnardel, has offered to be the host for the Workshop and to provide the venue. The University of Provence is readily accessible from Marseille and from Paris.<br\/><br\/>The intellectual outcomes are expected to be both the building of community around new research areas in creativity in IT and fully edited, commercially published proceedings that will define the state-of-the-art. <br\/><br\/>The broader impacts: Creativity in IT may be pivotal to transformational products and processes. The outcomes of this workshop will form the basis of the development of a research agenda for studying creative designers and provide the basis for an education agenda to improve the creativity of IT designers. The knowledge derived from studying design creativity has the potential to form the foundations of new kinds of support tools for creative designing.","title":"Two Day Workshop: Studying Design Creativity- Design Science, Computer Science, Cognitive Science and Neuroscience Approaches: The State of the Art","awardID":"0745389","effectiveDate":"2007-10-01","expirationDate":"2008-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7655","name":"ITR-CreativeIT"}}],"PIcoPI":["564585"],"PO":["551712"]},"127430":{"abstract":"Proposal 0713166<br\/>\"RI: Probablistic Reasoning with Bounded Computational resources\"<br\/>PI: Adnan Darwiche<br\/>UCLA<br\/><br\/><br\/>ABSTRACT<br\/><br\/>Probabilistic modeling and reasoning currently underlie many real-world applications in diverse areas such as the world wide web, medical informatics, robotics, bioinformatics, and information security. This project aims at significantly improving the scale and utility of probabilistic reasoning systems in these application areas, where success has become increasingly dependent on the availability of efficient and accurate probabilistic reasoning systems. The project is focused on a particular class of probabilistic models, known as Bayesian and Markov networks; these are among the most successful models studied by computer scientists and statisticians. This project is concerned with attaining the highest accuracy of reasoning that is feasible under real-world constraints on computational resources. The project is based on new, fundamental discoveries by the PI's group, showing that the efficiency and accuracy of reasoning can be finely controlled by approximating model dependencies in a dynamic fashion driven by user queries. These discoveries have formed the basis of a new semantics, and a concrete realization, of one of the most influential theories of probabilistic reasoning during the last decade, known as generalized belief propagation (GBP). In addition to pursuing the theoretical and practical implications of the new semantics of GBP, the project also aims at producing a comprehensive software system that embodies this novel and practical realization of GBP, with the intent of making it publicly available to the broad scientific community on a web site. It is anticipated that the developed system, with its surrounding theory and practice, will significantly advance the state of the art in probabilistic reasoning, to the point of both allowing new applications to be handled efficiently, and also increasing the scale and scope of existing applications.","title":"RI: Probabilistic Reasoning with Bounded Computational Resources","awardID":"0713166","effectiveDate":"2007-10-01","expirationDate":"2011-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["486633"],"PO":["460643"]},"127475":{"abstract":"Collaborative Proposal pair: 0713435 (Lead) & 0713148<br\/>\"Collaborative: RI: Feature Discovery and Benchmarks for exportable Reinforcement Learning\"<br\/>PI: Ronald Parr, Duke University<br\/>PI: Michael L. Littman, Rutgers University<br\/><br\/>ABSTRACT<br\/><br\/>This project focuses on several aspects of automated feature discovery in the context of reinforcement learning. Badly chosen features cause reinforcement-learning algorithms to fail and, as such, only individuals skilled in feature construction can create successful reinforcement-learning systems for novel tasks. This issue underscores two shortcomings in existing research. First, most existing reinforcement-learning methods cannot generate or discover features automatically and robustly. Second, existing benchmark problems and paradigms for benchmarking do not distinguish adequately between clever algorithm design and clever feature engineering.<br\/><br\/>This project addresses these challenges in two-pronged approach. The first prong aims to advance a technical agenda leading to a new approach to feature discovery and model representation. The second prong is the development of a benchmark methodology and repository with a different focus and structure from existing endeavors. The goal for the benchmarking effort will be to produce a set of fair and reproducible experiments that will help elucidate the strengths and weaknesses of existing approaches, while simultaneously introducing challenges to motivate the development of new approaches.","title":"Collaborative: RI: Feature Discovery and Benchmarks for Exportable Reinforcement Learning","awardID":"0713435","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["518533"],"PO":["460643"]},"129224":{"abstract":"Cyber-Physical Systems (CPS) are integrations of computation with physical processes. Embedded computers and networks monitor and control physical processes in feedback loops where physical processes affect computations and vice versa. This seedling project studies the fundamental abstractions behind such ``action webs.'' Many of the abstractions for computing and networking deliberately hide essential properties of the physical world. This project evaluates and elaborates models that instead embrace such properties. The focus is on distributed software, networking, and distributed control systems with essential properties of time management, robustness, and security. The project builds on extensive experience with sensor web technology, which instrument the physical world by embedding low power computing, sensing and communication in the environment. While sensor webs can instrument the world, action webs go further by closing the loop, integrating physical dynamics with the dynamics of software and networks.<br\/><br\/>The project focuses on the interactions of computing with the physical world. It critically examines the foundations of computing that have been built over the last several decades, specifically focusing on the dynamics of networked embedded software in closed-loop cyber-physical systems. The goal of this seedling project is to shape a long-term research agenda that will have profound implications on technical leadership of the US and on engineering and computer science education. Curricula in these fields evolve remarkably slowly, and an effort to blend computation with engineering disciplines that are more deeply rooted in the physical world is required.","title":"CSR-CPS: Action Webs Seedling","awardID":"0720841","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["71498","526900"],"PO":["493916"]},"128256":{"abstract":"Society relies ever more heavily on the Internet flow of information and the lack of proper security is a pervasive threat. Current techniques tend to focus on the security of specific components; but what are ultimately needed are end-to-end security guarantees protecting an entire system against a very broad range of potential types of attacks. This research is developing new techniques, based on the common semantic framework of rewriting logic, to gain high assurance about the end-to-end security of protocol systems made up of different components within an overall system architecture. Different system aspects and security concerns will be included such as: (i) confidentiality and authenticity guarantees; (ii) preventing denial of service (DoS) attacks, and (iii) web browser systems whose graphical user interface (GUI) can be subverted; and (iv) emergent system-wide vulnerabilities associated with overall system architecture.<br\/><br\/>Transmission of vital data information across many individuals and organizations relies on protocol-mediated services whose end-to-end security is routinely compromised. The proposed research is expected to provide a basis for achieving much higher end-to-end security assurance in future systems and to have potential impact on the development of standards for end-to-end security. The project also seeks substantial educational impact through course development, graduate and undergraduate research training, promotion of broader participation, and public lectures that can raise awareness for general audiences.","title":"CT-ISG: Attacker Models and Verification Methods for End-to-End Protocol Security","awardID":"0716638","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T815","name":"NSA-SOFTWARE ROADMAP RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"S036","name":"NSA-SECURITY OF PROTOCOL SYSTE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"T919","name":"NSA-SOFT WARE SYSTEM"}}],"PIcoPI":["550264"],"PO":["561889"]},"129598":{"abstract":"Proposal Number: 0722279 <br\/>P\/I: Joel Wein <br\/>Institution: Polytechnic University of New York <br\/><br\/>Title: CPATH CB: Community Building Project: Virtualized Gaming as a Pathway to Enhanced Understanding of Complex Networked Systems<br\/><br\/><br\/>The focus of this project is on building community around educating undergraduates in complex distributed networks and systems. The educational goal is to produce graduates who can design, implement, and manage networked distributed systems. The community building goal is to involve project partners in defining best ways to accomplish the educational goals. <br\/><br\/>Using multi-player, digital game immersion as a vehicle, a community of innovators (including educators from computer and information science, management science, digital media, gaming, and social sciences) will define learning experiences for students in computing. The goal of the community building effort is to create novel approaches to teaching computer science students to build distributed applications for a global economy. <br\/><br\/>A difficult educational hurdle, identified by this team of researchers, is providing students with a real hands-on experience. To address this, the project team will build essential components of a virtual environment and toolset in order to facilitate a gaming environment. In addition the team will leverage a small amount of internal funding to facilitate their effort to: <br\/><br\/>1. Produce two prototype gaming environments (one troubleshooting scenario and one system building scenario), <br\/>2. Conduct preliminary experiments with them at Polytechnic University and Brooklyn College, and, <br\/>3. Organize a regional workshop that will bring together a diverse group of educators to discuss applications of gaming in teaching about complex computer systems.<br\/><br\/>This project addresses an important area of national need, the education of future computing practitioners who are able to develop and manage large-scale distributed systems. The outcomes of the project will permit educators to expose their students to realistic platforms so that students may gain experience with complex, distributed and networked systems projects in a globally competitive market.","title":"CPATH CB: Community Building Project: Virtualized Gaming as a Pathway to Enhanced Understanding of Complex Networked Systems","awardID":"0722279","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["512465","559652",343466,"427248","427251"],"PO":["361119"]},"131094":{"abstract":"Lattice Quantum Chromodynamics (QCD) is one of the world's top<br\/>consumers of supercomputer cycles. As with many other applications,<br\/>the computational bottlenecks are traced to numerical linear algebra<br\/>problems. Lattice QCD gives rise to Hermitian matrix problems, where<br\/>methods such as Conjugate Gradient (CG) or Lanczos converge<br\/>optimally. Yet, the matrices are of enormous size, and preconditioners<br\/>that speedup convergence by more than a factor of two or three have<br\/>been elusive. Equally important, lattice QCD involves two of the still<br\/>outstanding problems in numerical linear algebra: to find methods that<br\/>solve optimally a linear system with multiple right hand sides and an<br\/>eigenvalue problem for a large number of eigenpairs. <br\/>This research aims at developing new methods, theory, and<br\/>software that address the above numerical linear algebra problems,<br\/>both in general and as guided by the physics of the particular QCD<br\/>problems, thereby speeding considerably lattice QCD computations and<br\/>enabling further understanding of the structure of matter. <br\/><br\/><br\/>The key contribution of this research is to provide a unified view of<br\/>linear system and eigenvalue methods that leads to algorithms that<br\/>solve both problems at once in a nearly optimal way. The investigators<br\/>study new numerical techniques for efficient computation of quark<br\/>propagators in lattice QCD and extend them for improving the<br\/>efficiency of Hybrid Monte Carlo. They explore a new CG method that uses<br\/>previously developed recurrence-like restarting to obtain eigenvectors<br\/>and use the method to share information between successive correction<br\/>equations in the Jacobi-Davidson eigensolvers. Finally the investigators<br\/>develop well designed, tuned implementations of the above methods to the<br\/>Chroma package for lattice QCD, and to the state-of-the-art eigenvalue<br\/>package PRIMME.","title":"(AREA: Numerical Computing and Optimization): Numerical Linear Algebra Problems and Quantum Chromodynamics","awardID":"0728915","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["518256","518257"],"PO":["562984"]},"134185":{"abstract":"The scope of this SGER project could represent transformative research in the understanding of how societal factors influence creative design. This is an important issue in Information Technology and Computing where creativity in system design is critical.<br\/><br\/>Previous computational research in creativity, whether on simulating creativity or research on producing creativity support tools, focused primarily on the individual. There has been little research on societal factors and their influence on creativity.<br\/><br\/>This research proposal aims to address the effects of societal factors in creative designing using novel computational models of both agents and social interactions derived from computer science, cognitive science and social psychology.<br\/><br\/>This SGER project has the potential to lay the foundation for a new class of simulations of creative designing based on situated, constructive agents. It could open up an area of future research that allows the investigation of a range of factors previously not readily amenable to computational study.","title":"SGER: Preliminary Computational Studies of Societal Factors in Creative Designing","awardID":"0745390","effectiveDate":"2007-10-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7655","name":"ITR-CreativeIT"}}],"PIcoPI":["564585"],"PO":["493916"]},"125231":{"abstract":"PROPOSAL NO: 0702612<br\/>INSTITUTION: Purdue University<br\/>PRINCIPAL INVESTIGATOR: Roy, Kaushik<br\/>TITLE: A Framework for Non-Silicon Based Nano-scale Design: From Devices to System Architecture<br\/><br\/>ABSTRACT<br\/>The progress towards the scaling limits of silicon-based semiconductor technology has raised interest in non-silicon nano-devices such as carbon nano-tube field effect transistors (CNFETs). CNFETs offer tremendous opportunity due to high integration, but to successfully exploit them, designers must overcome the challenges of high variation in device parameters, high defect rates (opens or shorts) and high power density expected in CNFET based designs. Unfortunately, the device\/circuit and architecture models and simulation tools that could facilitate the development and validation of circuits and architectures to overcome these challenges do not exist. The PI proposes to (a) develop an integrated device\/circuit\/architecture models and simulation software to enable designers to evaluate the performance, power and yield of nano-scale designs and (b) evaluate the benefits of several circuit and architecture innovations that address the key challenges (process variation, defects and power) of nano-scale design.<br\/><br\/>The tools and models developed as a part of this research will be made available to the research community through the Nanohub, developed at Purdue (http:\/\/www.nanohub.purdue.edu), which provides web-based access to computational tools and currently contains an array of simulation packages for semiconductor and molecular electronics. The availability of models will spur circuit and architecture innovations in the research community. The PI will develop new nano-electronic courses that will take an integrated approach to design. The courses will be based on the research findings and will be available for use. Participation of undergraduates, minorities and women will be actively encouraged through existing outreach platforms at Purdue such as LSAMP, Indiana as well as partnerships with other universities.","title":"A Framework for Non-Silicon Based Nano-scale Design: From Devices to System Architecture","awardID":"0702612","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["551012","518316"],"PO":["562984"]},"127453":{"abstract":"The objective of this project is to develop a robotic musician that will create meaningful and inspiring musical interactions with humans, leading to novel musical experiences and outcomes. The robot will combine computational modeling of music perception, interaction, and improvisation, with the capacity to produce melodic and harmonic acoustic responses in physical and visual manners. The PI's underlying hypothesis is that real-time collaboration between humans and computer-based players can capitalize on the combination of their unique strengths to produce new and compelling music. The project, therefore, aims to combine human creativity, emotion, and aesthetic judgment with the algorithmic computational capability of computers. The PI believes that a perceptual and improvisatory robot will best facilitate such interactions by bringing the computer into the physical world both acoustically and visually. Unlike computer- and speaker-based interactive music systems, a physical anthropomorphic robot will create familiar, acoustically rich, and visual interactions with humans. In order to create intuitive as well as inspiring social collaboration with people, the robot will analyze live music based on computational models of human perception, and will generate algorithmic responses that are humanly impossible. Building on and extending the PI's previous work on a perceptual robotic drummer that was primarily focused on rhythm, this project will develop a robot that can listen to, analyze, and play melodic and harmonic music as well. It will use a four-arm mechanism to play the marimba (a melodic mallet-based instrument), and infer musical meaning from live input based on a set of cognitive models of musical percepts such as melodic attraction, tension, and similarity. Based on this analysis, the robot will generate musical responses informed by mathematical constructs such as fractals, cellular automata and genetic algorithms. The interaction schemes to be developed for the robot will include synchronous and sequential operations, and will address aspects such as beat tracking and style adaptation. Project outcomes will include fundamental contributions to music perception and cognition, human-robotic interaction, computer assisted collaboration, and improvisation. <br\/><br\/>Broader Impacts: Building on the engaging power of music, this project will help bring the exciting potential of HRI to the attention of the general public through workshops and high visibility concerts, which will be designed in particular to capture the interest and imagination of students who are not regularly drawn to music, mathematics, engineering, and the sciences. Within the HRI research community, project outcomes relating to sound perception, and to the integration of human expression, emotion, and aesthetics with robotic analytical and mechanical capabilities, will help researchers develop productive human-robot collaborations in a variety of non-musical domains. The PI expects the project will also play a pivotal role for the research center in music technology that he has initiated at Georgia Tech.","title":"HRI: The Robotic Musician - Facilitating Novel Musical Experiences and Outcomes through Human Robot Interaction","awardID":"0713269","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7632","name":"HUMAN-ROBOT INTERACTION"}}],"PIcoPI":["561054"],"PO":["565227"]},"129521":{"abstract":"Current Internet network controls are realized by an ad hoc<br\/>combination of protocols, automated configurations, and manual<br\/>configurations. This ad hoc environment makes it difficult to provide<br\/>concrete behavioral assurances. Moreover, the entire system might<br\/>exhibit harmful emergent behaviors that are difficult to anticipate,<br\/>test for, debug, or correct.<br\/><br\/>The architecture for the future Internet should thus eliminate the<br\/>current ad hoc practices and start from a clean slate. This project is<br\/>developing a new architecture in which each network control function<br\/>is realized as an application that runs on top of an operating<br\/>platform. The operating platform provides crucial abstractions,<br\/>interfaces, and services to allow applications to effect changes in<br\/>the underlying network nodes, to enable the composition and concurrent<br\/>executions of multiple applications, and to provide protection<br\/>mechanisms to guard against potentially harmful actions of<br\/>applications. Intellectual challenges such as scheduling,<br\/>synchronization, inter-application communication, resource<br\/>multiplexing, and operational invariant protection for the new breed<br\/>of network control applications are being investigated.<br\/><br\/>Broader Impacts: This project directly addresses a key challenge for<br\/>future networks, namely network manageability. Problems in network<br\/>manageability affect the health of the networks we depend on, <br\/>and they affect the manner in it is possible for innovation and <br\/>economic growth to occur with these networks. Our new approach to <br\/>manageability, the operating platform architecture, can potentially <br\/>stimulate a market for a variety of 3rd-party network control applications. <br\/>Finally, this project provides particularly exciting opportunities <br\/>for the training of undergraduate and graduate students because they <br\/>develop applications in a fundamental way, but addressing high-impact <br\/>current issues. We expect to diffuse the findings from this project <br\/>into cutting-edge courses on network management.","title":"NeTS-FIND: An Architecture for Network Control Management","awardID":"0721990","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["551076","543600"],"PO":["565090"]},"125165":{"abstract":"ID: 0702278 <br\/>Title: Adaptive IC Design via Stochastic Optimization<br\/>PI: Larry Pileggi<br\/>Inst: Carnegie Mellon University<br\/><br\/><br\/>Abstract<br\/>The research of this project addresses the robust design of integrated circuits in the presence of large-scale variations from both the manufacturing process and the operating environment. As integrated circuit technologies reach nanoscale feature sizes, the fluctuations in circuit parameters due to manufacturing and operating environment increase significantly. To address this problem for the particularly challenging design of analog and radio frequency integrated circuits, we propose a new adaptive design methodology to dynamically configure and tune the circuit to accommodate all possible variations. Namely, our methodology will attempt to create circuits that offer self-adaptation to external changes and disturbances.<br\/><br\/>The proposed research targets two major problems: (1) how to create new tunable circuit architectures that can adaptively perform self-testing and self-configuration, and (2) how to analyze and optimize such adaptive circuits to explore the tradeoff between performance and cost. We will model both manufacturing and environmental variations as random variables and borrow powerful mathematical methods from statistics to solve these unique problems as posed by our adaptive design methodology.<br\/><br\/>The success of this project will stimulate a paradigm shift in today's electronic system design toward stochastic design. Such a methodology could ultimately enable nano-chips and bio-chips, which are significantly limited by design robustness issues, to reach commercial fruition. In addition, this work will bring adaptive methods into the traditional engineering curriculum, thereby providing the students with concrete application examples for mathematical techniques that are increasingly important for their long term careers.","title":"Adaptive IC Design via Stochastic Optimization","awardID":"0702278","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["531467","549747"],"PO":["562984"]},"129412":{"abstract":"The Internet's simple best-effort packet-switched architecture lies at the core of its tremendous success and impact. However, current Internet architecture allows neither (i) users to indicate their value choices at sufficient granularity nor (ii) providers to manage risks involved in investment for new innovative QoS technologies and business relationships with other providers as well as users. To allow these flexibilities, this project investigates \"contract-switching\" as a new paradigm for future Internet. Just like packet-switching enabled flexible and efficient multiplexing of data in the Internet, a contract-switched network will enable flexible and economically efficient management of risks and value flows.<br\/><br\/>Intellectual Merit: This project focuses on the design of a contract-switching framework in the context of multi-domain QoS contracts. It addresses the challenges involved in the development of decentralized inter-domain protocol mechanisms that can dynamically compose and price complex end-to-end contracts. The project formulates this end-to-end contract composition as a \"contract routing\" problem, resembling the QoS routing algorithms. This research also develops an appropriate abstraction necessary for pricing of QoS contracts, dynamically composable contracts in space and time. The project employs financial engineering techniques to provide risk sharing mechanisms and money-back guarantee structures for the QoS contracts.<br\/><br\/>Broader Impact: This research brings together network architecture design and financial engineering tools. This interdisciplinary work can inspire usage of economic tools for security problems like spam and DDoS attacks. The project will be especially beneficial to the Internet policy makers. Management of risks with economic efficiency tools and financial derivatives has several areas of impact such as water, electricity, insurance, real-options analysis, investment under uncertainty, and understanding the impact of open-source technologies.","title":"Collaborative Research: NeTS-FIND: Value Flows and Risk Management Architecture for Future Internet","awardID":"0721600","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["551119"],"PO":["565090"]},"129225":{"abstract":"This research and education project is developing protocols and algorithms required to build high-confidence networks of embedded sensors, actuators, and controllers. At essentially every layer of the protocol stack, the protocols needed for such systems are fundamentally different than those needed for bulk data transfer or even for other ?real-time? applications such as voice-over-IP or live video streaming. In view of this, fundamental research is needed to solve multiple open problems in the area of networked embedded systems. Ad-hoc solutions without a strong theoretical underpinning will fail to find appropriate solutions to these problems. This research is expected to make significant contributions to the area of networked embedded systems. In particular, the research focuses on the following fundamental areas: (1) formal methods to model the dynamics of networks of embedded systems; (2) formal tools for the analysis and design of impulsive systems; and (3) high-confidence algorithms and protocols for the interconnection of sensing, actuation, and control nodes. The project has a strong theoretical component, but also is driven by two application areas. These are: networked embedded systems arising in automotive applications, and networks of autonomous vehicles. The activities also enable a strong educational component aimed at motivating undergraduate students to pursue advanced degrees in the engineering sciences through the introduction of research content into undergraduate courses and through a summer internships program.","title":"CSR-EHS High-confidence Algorithms and Protocols for Networked Embedded Systems","awardID":"0720842","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["523607","496514"],"PO":["561889"]},"128257":{"abstract":"National Science Foundation<br\/>CISE\/CNS<br\/>Form 7 Review Analysis and Recommendation<br\/><br\/>Proposal Number: 0716640<br\/>PI: Vern Paxson<br\/>Institution: International Computer Science Institute, University of California Berkeley<br\/>Lead<br\/><br\/><br\/><br\/>Proposal Abstract<br\/><br\/>CT-T: Establishing a Cross-Institutional Platform for<br\/> Cooperative Security Monitoring and Forensics<br\/><br\/>Although there has been much research in developing systems for globally sharing security information, often these approaches are fundamentally limited because their broad scope limits the trust that participants can place in the system. This project instead seeks to reap significantly greater utility by considering a more restricted scope: a system for coordinated security analysis based on exchanging information between a set of sites who have explicitly decided to work with each other. This more limited scope optimizes for the common case that in such an environment the participating sites will usually (but not always) act in a responsible manner.<br\/><br\/>A key focus of the project concerns automating the steps commonly involved in security monitoring and forensic analysis while still keeping an analyst \"in the loop\" for significant decisions. As security problems arise, a site detecting an incident codifies a description of the attack in an \"analysis script\" to export to other sites. Analysts receiving such scripts inspect them to determine whether they are of interest. If so, they can instruct the system to conduct both a retrospective search for the activity in the past, and refine the site's monitoring configurations to detect future instances.<br\/><br\/>As validating such an approach requires operational deployment, the project seeks to demonstrate a working system for coordinating analysis between the Lawrence Berkeley National Laboratory, the National Energy Research Scientific Computing Center, and the University of California at Berkeley.","title":"CT-T: Establishing a Cross-Institutional Platform for Cooperative Security Monitoring and Forensics","awardID":"0716640","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["562327","563433","562329"],"PO":["565239"]},"128147":{"abstract":"As malicious attacks on computer systems increase in severity and sophistication, developing effective methods for protecting the Internet is among the most important challenges facing computer science today.<br\/>Network-based security mechanisms offer both good coverage and the possibility of early threat detection, but they often conflict with the performance requirements of network elements because of the vast amounts of traffic data that must be analyzed. This project will apply massive-dataset (MDS) algorithmics to network security, bringing together two previously unconnected research areas. The objective is to achieve a qualitative improvement in network security by developing efficient, yet theoretically rigorous, algorithmic defenses that can be deployed at scale in modern networks.<br\/><br\/>The project addresses both fundamental algorithm-design problems and practical applications. MDS algorithmics provides a set of basic techniques for highly efficient (e.g., one-pass, small-space,<br\/>polylog-time) analysis of large amounts of data. This project will investigate how these methods can be used for (1) online classification and property testing of packet streams, including efficient inference of streams generated by a mixture of stochastic sources, (2) detection of changes and anomalies in traffic patterns, and (3) development of computationally tractable models of traffic sources that support reasoning about a wide variety of adversarial behaviors and incorporate prior knowledge such as conventional intrusion-detection rules.<br\/>The algorithmic toolkit developed in the course of the project will be applied to practical network-security problems such as recognizing denial of service activity, worm fingerprinting, and detecting botnets.","title":"CT-ISG: Collaborative Research: Massive-Dataset Algorithmics for Network Security","awardID":"0716223","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["450982"],"PO":["529429"]},"128279":{"abstract":"Computer security is a critical issue that affects our everyday life. For this reason, most colleges and other educational institutions are devoting an increasing amount of resources to develop courses and curricula that involve security training. Typical courses include cryptography, general computer security, network security, and specialized topics, such as security of wireless networks and web security.<br\/><br\/>Unfortunately, very few hands-on security courses are offered because of the additional difficulties of teaching practical security, which requires substantial extra effort on the part of both the instructor and the educational institution hosting the course. In particular, live exercises require a substantial amount of resources to prepare and execute.<br\/><br\/>This research will develop a framework for the creation and execution of live security exercises. The framework will reduce the time and effort needed to organize live exercises, and, in addition, it will make these exercises repeatable.<br\/><br\/>The framework will allow other Universities and educational institutions to take advantage of live exercises to provide hands-on security training to students. The framework will be usable in the context of a single course whose focus in on security, or could be use to support large-scale, multi-institution competitions. As a result, the framework will provide a novel tool for supporting the development of security skills of both undergraduate and graduate students.","title":"CT-ER: A Framework for Live Security Exercises and Challenges","awardID":"0716753","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["535035"],"PO":["565327"]},"143140":{"abstract":"Deformable models and level set methods are related techniques that have proven to be phenomenally successful computational tools across a variety of disciplines, ranging from computer vision and image processing, computer graphics and image synthesis, computer-aided design and geometric modeling, as well as in applied mathematics and physics. The two techniques, each a major investigative avenue in itself, are complementary in several fundamental ways. The goal of this project is, for the first time, to harness the complementary strengths of these two physics-based methods. The PIs will do so by unifying them under a biology-based control paradigm derived from the emerging field of artificial life. The unification will lead to a novel breed of intelligent, deformable organisms capable of performing a wide range of challenging data analysis tasks, such as image segmentation and data reconstruction, in a highly automated fashion. The intellectual merit of the research is the goal of incorporating within an ultimately symbolic control hierarchy, two fundamentally numeric methods, the first related to the continuum mechanics of solids, the second related to liquids. Hence, deformable models maintain their topological structure as they evolve, while level set methods are topologically adaptive. Enabling these methods seamlessly to join forces within a rigorous mathematical and tractable computational foundation is an intellectually challenging problem. The research team, whose expertise spans the computational and mathematical sciences and includes the inventors of deformable models and level set methods, is uniquely qualified to meet this challenge. The anticipated outcome is a new, highly automated methodology for analyzing large-scale datasets that are subject to uncertainty and noise.<br\/><br\/>Broader Impacts: This work will streamline the entire visual information processing pipeline. It will be possible to deploy the novel models to be developed by the PIs in massive datasets to automatically extract geometric boundaries and to discover the unknown topology of structures of interest within the data, with immediate applications to medical image analysis, as well as to modeling and rendering tasks. The research team will incorporate the newly developed theory and algorithms into the graduate curricula of their respective institutions, and they will also seek collaborations with industry to provide the longer-term financial support for technology transfer of the prototype software.","title":"ITR: Intelligent Deformable Models","awardID":"0830183","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["402737"],"PO":["565227"]},"131172":{"abstract":"Computation of pairwise potential functions is crucial, albeit computationally expensive, to simulating the underlying physics in many fields. To mitigate this cost, fast and approximate potential computation methods have been developed for several potential functions; for example, particle-mesh methods, Fast Fourier Transforms, Fast Multipole Method (FMM), and limiting computation to neighborhoods. These methods differ in efficiency, accuracy, and applicability. Recent work by one of the PIs provides the foundation for the development of unified, robust, accurate and parallel methods for fast computation of non-oscillatory potentials using the Accelerated Cartesian Expansion framework. <br\/><br\/>A two pronged approach undertaken herein involves the development of (i) translation operators to enable FMM based computation for different pairwise potentials, including Yukawa, Lennard Jones, Gauss, Morse, and Buckingham potentials, and (ii) parallel framework for computing individual and multiple potentials simultaneously. These techniques are to be applied to a set of practical systems involving the Poisson, diffusion, retarded and Helmholtz (sub-wavelength), and Klein-Gordon equations, and to computing van-der Waals (in mesoscopic systems). The underlying methodology requires that only translation operators change from potential to potential, and provides a mathematically exact formulation for traversal up and down the FMM tree. The unifying treatment for computing multiple potentials simplifies parallel code development, especially with regard to scalability. To ensure broad impact, portions of this research will be available as part of LAMMPS software package to ensure widespread dissemination. Graduate students will be trained across multiple disciplines, and will visit each other's institutions. Existing channels are utilized to recruit women and minorities and undergraduate students are involved through senior design projects and potential REU supplements.","title":"Collaborative Research: PACE - Parallel Accelerated Cartesian Expansions with Application to Molecular Dynamics","awardID":"0729189","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["565135"],"PO":["565272"]},"133020":{"abstract":"The purpose of this project is to improve the participation and success of African-American, Latino\/a, and Female high school students in computing. In particular, the project concentrates on the Los Angeles Unified School District (LAUSD), the second largest and one of the most diverse school districts in the country.<br\/><br\/>Most current high school computer science courses, curriculum, and pedagogy do not engage students with innovative, interdisciplinary applications in computing. The computer science pipeline is losing traditionally underrepresented students prior to college. By college, at the nations PhD-granting departments of computer science and engineering, just 7% of the computer science degrees are awarded to African-American and Latino\/as of both sexes and just 17% to women (Taulbee Survey, 2006).<br\/><br\/>This project builds on several successful activities from the past that the project team engaged in involving preparation of the targeted students for success in the computer science AP exam. While there is national recognition that the AP exam may suffer from a narrow programming view of CS, it is important to assure that there is equal access to AP CS for all students and that teachers are qualified to teach this college preparatory course. In addition, however, the project team is expanding the scope of their activities to transcend focused preparation for AP. They will participate in the design of more engaging computer science courses that will also qualify as part of a new core college-prepared\/career-ready curriculum. <br\/><br\/>The implementation plan for this project addresses what is involved in the process of expanding rigorous learning opportunities to underrepresented students; what schools must provide to maximize student success in these higher-level learning opportunities and what more is needed; the role of alliances and how they are built and sustained; and the extent to which a school district and individual schools either embrace the innovations or possibly push back against them. This project to broaden participation in computing takes into account the larger context of urban education and educational reform. This project identifies critical factors around which a national model for introducing and sustaining computer science education reform can coalesce.","title":"Into the Loop:University-K-12 Alliance for Computer Science Education for African-American, Latino\/a, and Female Students in the Second Largest School District in the Country (BPC)","awardID":"0739289","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7482","name":"BROADENING PARTIC IN COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7584","name":"ITR-BROADENING PARTICIPATION"}}],"PIcoPI":["560926","528136","528138","528135"],"PO":["560704"]},"131084":{"abstract":"Title: Non-negative Matrix and Tensor Approximations: Algorithms, Software and Applications<br\/><br\/>Applications as diverse as data mining, chemometrics, and bioinformatics<br\/>lead to the need for analysis of co-occurrence and count data.<br\/>These data sets are intrinsically composed of non-negative numbers, and<br\/>often can be represented as multi-dimensional matrices. Analysis of such<br\/>data is a major contemporary scientific challenge.<br\/><br\/>This research studies approximations of non-negative matrices and<br\/>tensors that reveal an easily interpretable parts-based representation<br\/>of the data. Fast, numerical algorithms are developed that incorporate<br\/>state-of-the-art techniques from numerical optimization. A software<br\/>toolbox is made available to the public after being tested on<br\/>applications in web data mining, bioinformatics and computer vision.<br\/>An emphasis of the project is to train graduate and undergraduate<br\/>students in mathematics and computer science, and make the software<br\/>available via a public web site.","title":"Non-Negative Matrix and Tensor Approximations: Algorithms, Software and Applications","awardID":"0728879","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["550986"],"PO":["565251"]},"125243":{"abstract":"Proposal Number: CCF-0702665<br\/><br\/>TITLE: ATS: A Language to Support Practical Programming with Theorem Proving<br\/><br\/>PI: Hongwei Xi<br\/><br\/>The immense complexity in software design and implementation is evident. In this day and age, software design is often expressed in forms of varying degree of formalism, ranging from verbal discussions to plain text descriptions to UML diagrams to specifications in languages like Z. Also, there is often an enormous gap between the design of a system and its actual implementation, making it exceedingly difficult to construct software meeting its specification. However, the very ability to construct software meeting its specification is crucial to (highly) dependable computing, and there seem to be no other shortcuts. The proposed research focuses on the design and implementation of a full-fledged programming language ATS with a type system rooted in the recently developed framework Applied Type System. In ATS, (certain) specifications in software design can be formally captured in terms of types and then be verified through type-checking. In stark contrast to pure theorem proving systems where programs are extracted from proofs, ATS is an effective programming language that contains a pure theorem-proving subsystem to support programming with theorem proving. The effectiveness of ATS is to be evaluated in the construction of real and complex systems.","title":"ATS: a Language to Support Practical Programming with Theorem Proving","awardID":"0702665","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["451820"],"PO":["564388"]},"126585":{"abstract":"0708962<br\/><br\/>Collaborative Research: CRI: IAD: Electronic Testing Education, Research and Training Infrastructure<br\/><br\/>Vishwani Agrawal<br\/><br\/>This three-year project will develop capabilities for testing of digital, memory, analog, and radio frequency devices, with possible future upgrades to MEMS, optical and nanotechnology devices. The infrastructure developed under this project consists of a test laboratory and its value-added applications. The new VLSI test laboratory will have modern automatic test equipment (ATE) of open architecture. The laboratory will be located at Auburn University in Alabama and will be used through networked access by three other institutions of that state, namely, University of Alabama at Tuscaloosa, University of Alabama at Huntsville, and Tuskegee University, for advancing education, training and research applications. In the future, the test lab infrastructure will allow collaborative use by universities outside Alabama as well, making it a national resource. The applications will include new university courses with hands-on experiments on testing of digital, analog and radio frequency chips, FPGAs and system-on-a-chip devices; industry-oriented training classes including test lab exercises; and research on silicon debug methods aimed at improving the yield and reliability. Research with industry focus and practice-oriented short courses are expected to make this infrastructure fully supported through industry funds beyond the three-year infrastructure building period.","title":"Collaborative Research: CRI: IAD: Electronic Testing Education, Research and Training Infrastructure","awardID":"0708945","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["429268"],"PO":["565272"]},"127367":{"abstract":"One of the basic requirements for understanding how the brain works is to know what the electrical activity of each neuron \"means\" with respect to an animal's behavior. The electrical signals are manifested as temporal sequences of very brief electrical impulses, commonly called spikes, which are the currency of the nervous system. Neurons respond to stimuli by spiking, and they communicate with each other by spiking. Interpreting what such spiking patterns encode has been of longstanding scientific interest because it translates into knowing what each neuron does within a circuit. The aim of this project is to go one step further. The goal is to examine how the spike pattern received by one neuron is transformed into a new, output spike pattern. Although much has been learned about what spike sequences encode, the coding transformation which occurs from neuron to neuron has rarely been quantitatively investigated. Only when this recoding of spike patterns is understood can a neuron?s functional role be considered fully characterized.<br\/><br\/>The research naturally progresses in three stages. First, spike trains will be recorded from connected pairs of neurons responding to naturalistic stimuli. The investigators have chosen to record from neurons in the primate lateral geniculate nucleus because their input, provided from retinal ganglion cells, and their output can be accessed readily. Second, they will apply a novel analytical method to determine the optimal stimulus features encoded by each cell's spike train. This involves a computationally intensive search through the stimulus space to arrive at the stimulus representations which carry the maximum amount of information. By comparing the optimal representations for the input and output spike trains, the feature transformation from one neuron to the next will be revealed. Third, they will test the predictive power of their method by using the found optimal stimuli and systematically degraded stimuli to probe neural responses in a second series of recording experiments. Such a test will serve to validate the computational basis of the feature transformation.<br\/><br\/>Having chosen a problem of very fundamental interest, the methods being developed will be valuable for studying signal communication and representation in any system of connected neurons. The outcome will provide a basis for comprehending the processing capabilities of neurons with multiple inputs, which are common throughout the brain, and could be applied to engineered systems designed for interpreting visual scenes.","title":"Signal Transformation in the Early Visual System","awardID":"0712852","effectiveDate":"2007-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[337801,"534538"],"PO":["564316"]},"129226":{"abstract":"Improving the performance of microprocessors by increasing the clock speed is no longer practical. In response, microprocessor manufacturers have chosen to increase performance through the use of multiprocessing. By populating a single die with multiple, less complicated processors, manufacturers are creating CPUs that provide higher computational performance per watt and per square millimeter. At the same time, GPU chips have used the same ideas to vastly improve their aggregate performance, although they address different forms of computational problems.<br\/><br\/>This project is examining these two complementary resources as a single cooperative execution system. It is characterizing how to best take advantage of each, and ascertain the ?sweet spot? of assigning different computational functions to the appropriate processing units based on the nature and size of what is being computed. It will not just produce an optimal point solution for one particular problem, but will generalize the overall problem in terms of: the type and size of the data computation, the speed of the CPUs and GPUs, the number of CPU and GPU processors, and the bus speed.<br\/><br\/>The result of this research will be to create a desktop ?mini-supercomputer? for solving compute-intensive scientific and rendering problems. Because of its multicore CPU\/GPU mix, it will have more combined compute power than any of its desktop predecessors, but it will look to an application developer as a single compute environment. Best of all, it will have no more cost and footprint than the graphical desktop system that the user would have bought anyway.","title":"CSR-AES: Characterizing the Cooperative Parallel Behavior of GPUs and Multicore CPUs","awardID":"0720848","effectiveDate":"2007-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[342400,"553725"],"PO":["493916"]},"129589":{"abstract":"This proposal centers around the notion that computing has evolved into an inter- and intra-disciplinary field of intertwined concepts that pervade society. The Georgia Institute of Technology (Georgia Tech) and other schools in the University System of Georgia have defined and adopted a number of specialized degrees and contextualized computing courses. Last Fall, Georgia Tech extended this approach to create the Threads model includes a process for creating curricular change, an infrastructure for advising, and software to support administrators, advisors, educators and students. In parallel, Brooklyn College of the City University of New York (BC-CUNY) has developed several context-based approaches to computing education with a focus on introductory courses and the high school to college continuum, as well as created two new interdisciplinary masters degrees. The team proposes to create an alliance that validates and extends the Threads model. The proposed work encompasses a methodical approach to understanding the process of defining broad, flexible paths through a computing curriculum, and to measuring and analyzing the outcomes of this process when applied to a variety of departments and interest groups. At the heart of this process is an emphasis on context-based instruction and targeted advising that helps students crystalize career paths and realize the short- and long-term relevance of their coursework. The project explores crucial research questions that arise out of adapting and applying Threads, and evaluating the effects on students, faculty and administrators through quantitative and qualitative studies. Under the work proposed here, they will measure the impact of Georgia Tech?s implementation<br\/>of the Threads model and the supporting advising mechanisms; extend and adapt Threads to a broad range of computing departments; facilitate its adoption at such departments; and evaluate its efficacy under a variety of conditions. The goal is a validated, widely deployed and broadly-evaluated model of curricular reform that is applicable to small and large departments, students with a range of backgrounds and abilities, and faculty with a range of interests. The combination of diverse experiences brought together by the project team promises to produce results with the potential to serve as national models for both computing and other STEM (science, technology, engineering and math) disciplines.","title":"CPATH EAE: Extending Contextualized Computing in Multiple Institutions Using Threads","awardID":"0722238","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["352588"],"PO":["565136"]},"129116":{"abstract":"The focus of this project is on the programming of multi-core chips. <br\/><br\/>The goal of the project is to find a comprehensive parallel programming environment that can substantially simplify parallel programming for multi-cores.<br\/><br\/>Current parallel programming approaches are typically too complicated for most programmers or do not deliver adequate performance. A major breakthrough is needed to simplify parallel programming to effectively utilize multi-core hardware. Reducing the complexity of parallel programming and enabling its widespread use has emerged as a Grand Research Challenge for the systems community. It is fundamental to the computer industry''s ability to continue innovating. <br\/><br\/>The scope of this project is to study two novel parallel programming models that either separately or in combination have the potential to provide dramatic improvements in programmer productivity. The two novel parallel programming models that may have the potential to simplify parallel programming and be accepted by large numbers of programmers are Colorama and IPOT.<br\/><br\/>Colorama is an architecture-supported Data Centric Synchronization (DCS) approach, and is used in explicitly parallel programs. In DCS, the programmer specifies concurrency control by associating synchronization constraints with the program data structures, typically when they are declared or allocated. These constraints determine which sets of data structures are in the same data consistency domain and, therefore, should be kept consistent with each other. At run time, when one data structure is being modified by a thread, the system automatically protects all the other data structures in the same domain from access by other threads. Compared to programming with locks and transactions, where the programmer reasons non-locally, Colorama has a significant programmability advantage: the programmer reasons locally, focusing only on what data structures should be consistent with each other; the system will automatically infer the critical sections.<br\/><br\/>Implicit Parallelism with Ordered Transactions (IPOT) supports speculative threads in a sequential thread of execution. The key idea is that the programmer is allowed to specify opportunities for parallelization within the sequential thread using a set of simple yet very powerful annotations. Unlike explicit parallelism, IPOT annotations do not require the programmer or the compiler to prove the absence of data or control dependences. There is runtime architectural support to detect violations, and squash and restart threads. Unlike conventional TLS, the system can extract much more parallelism because it leverages user information. <br\/><br\/>It is possible to use these models separately or, since they are complementary, in combination for example, by using Colorama on explicitly-parallel code where each thread is implicitly parallelized with IPOT. Both models require special hardware support in the multi-core chip.<br\/><br\/>The project teams activities address several layers of multi-core programming\/architectural frameworks:<br\/><br\/>1. Programming Models -- The project team will study the design issues in the specification of the Colorama, IPOT, and Unified programming models (Unified is the combination of Colorama and IPOT).<br\/>2. Compilation -- The team will develop compiler technology to improve the performance and guarantee the correctness of their programming models.<br\/>3. Runtime System -- Libraries and a runtime system will be developed for these models.<br\/>4. Computer Architecture -- Different levels of architectural support for these models will be explored.<br\/>5. Performance Monitoring and Debugging Tools -- A set of tools will be developed that allow programmers to debug, test, and tune programs written for the programming models.<br\/>6. Applications -- A suite of applications will be developed that use the programming models.<br\/>7. Empirical Productivity Studies -- A set of empirical programmer productivity experiments will be carried out in a course at the University of Illinois.<br\/><br\/>This work involves a collaboration between the University of Illinois and IBM Research, leveraging a large amount of existing IBM-sponsored infrastructure to release the complete software infrastructure of the programming environment that is developed by the project team.","title":"CSR---AES: Collaborative Research: Novel Programming Models and Architectures to Simplify Parallel Programming","awardID":"0720533","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[342086,342087],"PO":["551712"]},"129138":{"abstract":"The focus of this project is on the programming of multi-core chips. <br\/><br\/>The goal of the project is to find a comprehensive parallel programming environment that can substantially simplify parallel programming for multi-cores.<br\/><br\/>Current parallel programming approaches are typically too complicated for most programmers or do not deliver adequate performance. A major breakthrough is needed to simplify parallel programming to effectively utilize multi-core hardware. Reducing the complexity of parallel programming and enabling its widespread use has emerged as a Grand Research Challenge for the systems community. It is fundamental to the computer industry''s ability to continue innovating. <br\/><br\/>The scope of this project is to study two novel parallel programming models that either separately or in combination have the potential to provide dramatic improvements in programmer productivity. The two novel parallel programming models that may have the potential to simplify parallel programming and be accepted by large numbers of programmers are Colorama and IPOT.<br\/><br\/>Colorama is an architecture-supported Data Centric Synchronization (DCS) approach, and is used in explicitly parallel programs. In DCS, the programmer specifies concurrency control by associating synchronization constraints with the program data structures, typically when they are declared or allocated. These constraints determine which sets of data structures are in the same data consistency domain and, therefore, should be kept consistent with each other. At run time, when one data structure is being modified by a thread, the system automatically protects all the other data structures in the same domain from access by other threads. Compared to programming with locks and transactions, where the programmer reasons non-locally, Colorama has a significant programmability advantage: the programmer reasons locally, focusing only on what data structures should be consistent with each other; the system will automatically infer the critical sections.<br\/><br\/>Implicit Parallelism with Ordered Transactions (IPOT) supports speculative threads in a sequential thread of execution. The key idea is that the programmer is allowed to specify opportunities for parallelization within the sequential thread using a set of simple yet very powerful annotations. Unlike explicit parallelism, IPOT annotations do not require the programmer or the compiler to prove the absence of data or control dependences. There is runtime architectural support to detect violations, and squash and restart threads. Unlike conventional TLS, the system can extract much more parallelism because it leverages user information. <br\/><br\/>It is possible to use these models separately or, since they are complementary, in combination for example, by using Colorama on explicitly-parallel code where each thread is implicitly parallelized with IPOT. Both models require special hardware support in the multi-core chip.<br\/><br\/>The project teams activities address several layers of multi-core programming\/architectural frameworks:<br\/><br\/>1. Programming Models -- The project team will study the design issues in the specification of the Colorama, IPOT, and Unified programming models (Unified is the combination of Colorama and IPOT).<br\/>2. Compilation -- The team will develop compiler technology to improve the performance and guarantee the correctness of their programming models.<br\/>3. Runtime System -- Libraries and a runtime system will be developed for these models.<br\/>4. Computer Architecture -- Different levels of architectural support for these models will be explored.<br\/>5. Performance Monitoring and Debugging Tools -- A set of tools will be developed that allow programmers to debug, test, and tune programs written for the programming models.<br\/>6. Applications -- A suite of applications will be developed that use the programming models.<br\/>7. Empirical Productivity Studies -- A set of empirical programmer productivity experiments will be carried out in a course at the University of Illinois.<br\/><br\/>This work involves a collaboration between the University of Illinois and IBM Research, leveraging a large amount of existing IBM-sponsored infrastructure to release the complete software infrastructure of the programming environment that is developed by the project team.","title":"CSR---AES: Collaborative Research: Novel Programming Models and Architectures to Simplify Parallel Programming","awardID":"0720593","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["533380","485665","550505"],"PO":["493916"]},"131151":{"abstract":"Information theory is the strategic theory of communication: providing architectural guidance and fundamental bounds. For communication, there are three core quality of service parameters: probability of error, rate, and end-to-end delay. Shannon's seminal capacity theorems establish bounds on rate as the tolerated probability of error goes to zero while the acceptable end-to-end delay goes to infinity. However, for many applications like telemedicine, remote control (e.g. fly-by-wireless in UAVs), and even video-conferencing, short delays are also critical. Unfortunately, it turns out that the classical block-code oriented approaches to information theory are misleading regarding the tradeoffs involving end-to-end delay, especially when feedback is involved. This research aims to remedy that situation and thereby get a deeper understanding of the nature of information flows and their communication requirements. Having these fundamental architectural results will help guide not only the design of next generation communication systems, but is also critical for the setting of regulatory policy governing wireless spectrum since both interactive and non-interactive applications must coexist efficiently in the wireless context.<br\/><br\/>This research makes three core contributions: (a) Characterizing the tradeoff between delay and probability of error in communication systems with and without feedback in the context of both soft and hard latency constraints by leveraging our new hallucination bound and uncertainty focusing bound techniques, (b) Understanding how to exploit noisy and limited feedback in the above contexts, (c) Deepening our understanding of information flows in the context of interactive control and distributed estimation by taking a new approach to source-channel separation theorems based on the theoretical CS model of showing problem-level equivalences through explicit reductions of one communication problem to another.","title":"Delay, Feedback, and Interaction","awardID":"0729122","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["560235"],"PO":["564924"]},"127455":{"abstract":"Dialog-based technologies play an ever-increasing role in our lives. The interest in conversational interfaces stems not only from their importance for universal access, but also because they can be used in settings in which traditional hands-on interface devices are not practical. Efforts are underway to extend the use of dialog systems into new roles, including: becoming companions to help seniors remain independent; moving some aspects of healthcare outside of the clinical setting; providing entertainment and interactive theater experiences; and improving disaster response. In many of these systems, the computer conversational partner is presented as a talking head or graphical persona. Although designers of dialog systems aim to make them simple and natural to use, major gaps in our understanding of human-computer interaction create obstacles to achieving this goal.<br\/><br\/>Some of these gaps are likely to be filled by examining recent psycholinguistic models of human conversational interaction. These models explore the time-course with which information coming from the context of conversation affects language processing. This context includes the shared information, or common ground, between interlocutors, as well as the processes by which reference to items in common ground is coordinated. One property of negotiated reference concerns the establishment of conceptual pacts, which are a (usually implicit) agreement to refer to an item in common ground from the same conceptual perspective. Recent psycholinguistic findings have examined whether conceptual pacts in conversation are speaker-specific, and how quickly this information is used during comprehension. To the extent that speaker-specific conceptual pacts have significant and immediate effects on the interpretation of reference in human-human interaction, dialog systems utilizing computer-based partners should take these characteristics into account in human-computer interaction.","title":"Collaborative Research: HRI: Establishing and Breaking Conceptual Pacts with Dialog Partners","awardID":"0713287","effectiveDate":"2007-10-01","expirationDate":"2013-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[338003],"PO":["565227"]},"128204":{"abstract":"It is estimated that hardware (HW) intellectual property piracy induces almost an order of magnitude higher loss when compared to software piracy cost and greatly facilitates both software and entertainment data piracy. In addition, HW vulnerabilities can be exploited for security attacks, especially the increasing threat of HW malware attacks (malicious or unintentional alterations of design specifications that compromise the correctness of the functionality under specific conditions) due to business models moving to external foundries and assembly houses. The strategic objective of this project is to give impetus to research on the protection against HW malware attacks and piracy with a multi-pronged attack that includes the development of both the conceptual foundation and several practical HW protection techniques.<br\/><br\/>The intellectual merit of this project is the development of HW protection techniques that leverage the manufacturing variability (MV) inherent in modern fabrication processes and the resulting uniqueness of every manufactured integrated circuit (IC). This project is developing: (i) HW malware detection techniques considering only a single IC under very mild statistical assumptions about MV, and (ii) active HW metering protocols that prevent foundries from distributing unauthorized design copies by integrating MV into the functionality of the targeted design in such a way that only the designer can issue a key that unlocks the design after power up so that it becomes functional.<br\/><br\/>The broader impacts of this project are that it will form the basis for a new industry that ensures HW integrity in a complex horizontal IC business model. In addition, this project is exposing both undergraduate and graduate students to these increasingly important topics and providing research opportunities for underrepresented students.","title":"Collaborative Research: CT-T: Manufacturing Variability-based Hardware Protection Techniques","awardID":"0716443","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["527785"],"PO":["521752"]},"131251":{"abstract":"This program undertakes a broad research agenda centered around the design and analysis of ``Flow-based Networks''. A flow is a collection of packets that belong to the same ``transaction'', such as a datagram, an ftp transfer, or a web download. It is the fundamental unit of data that a user cares about. Current packet-switched networks, like the Internet and Gigabit Ethernet, are designed to process packets; they are unaware of the flow to which a packet belongs. This is because flow-recognition is widely considered to be too expensive to implement. However, a switch or a router's ability to recognize flows can lead to a marked improvement in its performance, to a better use of its resources, and to much more secure networks. <br\/><br\/><br\/>The first major aim of this program is to design novel algorithms and data structures for high-speed, \"flow-aware\" networks. Such algorithms could heavily influence the design of commercial switches and routers. A second major thrust concerns the development of flow-level models of networks: models which capture the impact of packet-level decisions on flow-level bandwidth allocation and flow processing times. An important component of the modeling work is the unification and generalization of two researce enterprises: Stochastic Network Theory, and Large Random Networks. The former studies the performance of a, typically non-random, queueing network subject to \"random inputs\". The latter concerns the study of \"random networks\", usually subject to deterministic inputs. A successful outcome of these efforts can help answer questions such as the throughput and flow delay of a particular bandwidth allocation scheme, and the effect of routing topology on end-to-end performance. In other words, the modeling effort aims to develop a realistic, simple and usable class of models for network flows.","title":"Collaborative Research: Flow Level Models and the Design of Flow-Aware Networks","awardID":"0729586","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[348451,"517313"],"PO":["432103"]},"134892":{"abstract":"Lead Proposal: OCI - 0749015 <br\/>PI: Dawson, Clinton N<br\/>Institution: University of Texas at Austin<br\/><br\/>Non-lead Proposal: OCI - 0749017<br\/>PI: Hanna, Darrin M<br\/>Institution: Oakland University<br\/><br\/>Non-lead Proposal: OCI ? 0746232<br\/>PI: Westerink, Joannes J<br\/>Institution: University of Notre Dame<br\/><br\/><br\/>Title: \"Collaborative Research\" NSF PetaApps: Storm Surge Modeling on Petascale Computers<br\/><br\/>ABSTRACT<br\/><br\/>The goal of this project is to investigate the use of petascale computing to significantly advance the state-of-the-art in storm surge simulation, to accurately model flows at multiple, interacting scales, at resolution never before attempted, and to demonstrate that results from these simulations can be delivered in real-time to emergency managers. To achieve this goal will require the continued development and improved understand of the mechanisms involved in tightly coupled models of wind, waves, circulation and geomorphology, improvements in the description of the physical domain and adaptive resolution of all energetic flow scales, and investigation of accurate, robust and highly parallelizable numerical algorithms. Efficient implementation of these models on emerging petascale architectures will require utilizing the latest developments in parallel data management, real-time visualization, and programming tools. In this project, the<br\/>PIs will develop high resolution, large-scale coastal inundation models coupled with regional-scale rainfall\/runoff models. Robust and highly parallelizable algorithms will be investigated for solving these systems on petascale architectures. The models will be implemented on NSF Track 2 HPC systems currently under construction; furthermore, implementation of the models on novel hybrid architectures will also be explored.<br\/><br\/>Predicting and studying coastal inundation due to hurricanes and tropical storms is a problem of critical importance to the United States. Hurricane Katrina alone was the costliest and 5th deadliest hurricane in history, with most of the devastation due to wind-driven flooding during the storm. The aftermath of this event has led to a number of federally-mandated studies to determine what failed, the causes of failure, and how to prevent such catastrophes from happening again. Critical decisions will be made in the next several years on how to design better protection systems and improve emergency management practices in the event of future storms. Storm surge is caused by wind, atmospheric pressure gradients, tides, river flow, short-crested wind-waves, and rainfall. In this project, the investigators will develop an accurate numerical model of storm surge which accounts for all of these effects. This model will be tested in predictive mode as storms approach landfall for the purposes of emergency evacuation and response, and used to study the design and implementation of improved man-made and natural protection systems for vulnerable coastal areas. While storm surge models have been developed extensively over the past decade; only within the last few years have the algorithms, computational power and resolution been available to begin to model these events with any reasonable degree of accuracy. In addition to storm surge modeling, the computational methodology and simulation tools developed under this project are applicable to other problems in coastal engineering and marine science, including water quality, shipping and ports, marine ecology, naval operations, weather and climate, and wetland degradation. Furthermore, the technology developed under this project will be disseminated to government agencies such as FEMA, the U.S. Army Corps of Engineers and NOAA.","title":"Collaborative Research- NSF PetaApps: Storm Surge Modeling on Petascale Computers","awardID":"0749015","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7691","name":"PetaApps"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7552","name":"COFFES"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["521182"],"PO":["565247"]},"131163":{"abstract":"Computation of pairwise potential functions is crucial, albeit computationally expensive, to simulating the underlying physics in many fields. To mitigate this cost, fast and approximate potential computation methods have been developed for several potential functions; for example, particle-mesh methods, Fast Fourier Transforms, Fast Multipole Method (FMM), and limiting computation to neighborhoods. These methods differ in efficiency, accuracy, and applicability. Recent work by one of the PIs provides the foundation for the development of unified, robust, accurate and parallel methods for fast computation of non-oscillatory potentials using the Accelerated Cartesian Expansion framework. <br\/><br\/>A two pronged approach undertaken herein involves the development of (i) translation operators to enable FMM based computation for different pairwise potentials, including Yukawa, Lennard Jones, Gauss, Morse, and Buckingham potentials, and (ii) parallel framework for computing individual and multiple potentials simultaneously. These techniques are to be applied to a set of practical systems involving the Poisson, diffusion, retarded and Helmholtz (sub-wavelength), and Klein-Gordon equations, and to computing van-der Waals (in mesoscopic systems). The underlying methodology requires that only translation operators change from potential to potential, and provides a mathematically exact formulation for traversal up and down the FMM tree. The unifying treatment for computing multiple potentials simplifies parallel code development, especially with regard to scalability. To ensure broad impact, portions of this research will be available as part of LAMMPS software package to ensure widespread dissemination. Graduate students will be trained across multiple disciplines, and will visit each other's institutions. Existing channels are utilized to recruit women and minorities and undergraduate students are involved through senior design projects and potential REU supplements.","title":"Collaborative Research: PACE-Parallel Accelerated Cartesian Expansions with Application to Molecular Dynamics","awardID":"0729157","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["532976","384151"],"PO":["565272"]},"131196":{"abstract":"Online groups are becoming increasingly important, for example, by creating the software that runs the Internet, building history?s largest encyclopedia, providing social support to millions, and enhancing opportunities for interpersonal relationships that span geographic boundaries. Regardless of the environments in which groups operate, to be successful all must meet three critical challenges: (1) Commitment: gaining and retaining members by managing members' commitment to the group and their motivation to exert effort on its behalf, (2) Coordination: coordinating members' actions to achieve collective goals, and (3) Control: ensuring that members adhere to important group norms. Although some online groups are very successful, many others fail. The large size, high turnover, weak social networks, and impoverished communication channels typical of many online groups inhibit their ability to overcome these challenges. The goal of this research is to help online groups deliver on their tremendous promise by (1) understanding the factors that affect the success of online groups; (2) shedding light on general group processes that also apply to offline groups; and (3) developing technological interventions that can help online groups achieve their objectives. The research will examine groups in three highly popular domains ? online health and technical support, the online-encyclopedia Wikipedia, and the most popular massively multiplayer game in history, World of Warcraft. Investigatory studies will use data-mining techniques to examine rich archival, longitudinal data about thousands of online groups using time-series methods. In addition, interventionist studies will follow up the investigatory research with computational interventions designed to improve the effectiveness of online groups. This research will develop principles to explain the factors that influence the success of online groups and groups more generally. Online groups are economically and socially important, as indicated by the fact that seven of the top ten Internet sites by traffic (according to www.alexa.com) are primarily online group sites. Because online groups have high failure rates, knowledge about the factors that underlie their effectiveness is critical for designers and managers. This project's technological interventions are designed to improve the performance of the groups in which they are deployed and to serve as exemplars for subsequent design efforts in other online groups.","title":"DHB: Collaborative Research: Solving Critical Problems in Online Groups","awardID":"0729286","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0400","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"S005","name":"CIA"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7319","name":"HSD - DYNAMICS OF HUMAN BEHAVI"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7326","name":"HSD - GENERAL"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["560995"],"PO":["563839"]},"131086":{"abstract":"Fundamentals of Efficient Communication in Wireless Underground Sensor Networks <br\/><br\/>Abstract<br\/><br\/>Wireless Underground Sensor Networks (WUSN) is a promising field that will enable a wide variety of novel applications that were not possible with current wired underground monitoring techniques. Compared to the current wired underground sensor networks, WUSN have several remarkable merits, such as concealment, ease of deployment, timeliness of data, reliability and coverage density. Despite its potential advantages, the realization of WUSN is challenging. The main challenge in this area is the realization of efficient and reliable underground links to establish multiple hops and efficiently disseminate data for seamless operation. Furthermore, the propagation characteristics of electromagnetic (EM) waves in soil prevent a straightforward characterization of underground wireless channel. Hence, advanced propagation and multi-path models are required to investigate the effects of soil in underground communication. <br\/><br\/>This research provides advanced models and techniques to completely characterize the underground wireless channel and lay out the foundations for efficient communication in this environment. This model aims to characterize the propagation of EM wave in soil by considering multipath, soil composition, water content, and burial depth. Based on these peculiarities of communication in soil, an optimization framework is developed to characterize cross-layer communication aspects in WUSN. This framework jointly models problems at various protocol layers such as medium access control, routing, and transport in a cross-layer fashion. Moreover, a cross-layer framework is investigated to analyze the effects of various error control techniques and determine the optimal packet size in WUSNs. This optimization framework is based on three main optimization functions in terms of energy consumption, throughput, and resource utilization. Consequently, efficient communication protocols can be developed based on the foundations of the channel model developed in this research and the most efficient error control techniques determined by this framework.","title":"Fundamentals of Efficient Communication in Wireless Underground Sensors Networks","awardID":"0728889","effectiveDate":"2007-10-01","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["562752"],"PO":["564924"]},"135387":{"abstract":"Proposal Number: 0751069<br\/>PI: Bulent Yener <br\/>Institution: Rensselaer Polytechnic Institute<br\/><br\/>Proposal Number: 0751095<br\/>PI: Aggelos Kiayias <br\/>Institution: University of Connecticut<br\/><br\/><br\/><br\/><br\/><br\/>Title: SGER: Collaborative Research: Secure and Auditable Privacy Contracts<br\/><br\/>Proposal Abstract<br\/><br\/><br\/><br\/>One of the major challenges imposed by the pervasiveness of<br\/>cyberspace is to ensure ``trust'' and design trustworthy systems to<br\/>support the critical infrastructure for official, commercial, and<br\/>personal business. Millions of everyday users pass their personnel<br\/>information over the Internet to their health-care providers, to<br\/>their banks, to insurance companies and other service providers.<br\/><br\/>Once such personnel information is transferred, in many cases, it is<br\/>outsourced to other parties (some of which may even reside in<br\/>foreign countries) for storage and processing. It may be sold or<br\/>resold for data mining.<br\/><br\/>The collection, processing and resale of private data is a<br\/>multi-million dollar industry that has serious security and privacy<br\/>breaches including inadvertently revealing or loosing the<br\/>personal information of thousands or millions of people.<br\/><br\/>While there is a clear need for knowledge discovery for commercial<br\/>(e.g., marketing, insurance design), scientific and even security<br\/>reasons (e.g., identifying public health problem outbreaks such as<br\/>epidemics, biological warfare instances), at present, ``data<br\/>producers'' (i.e., users like all of us) have no control of who,<br\/>how, and what exactly is done with such private and sensitive data.<br\/><br\/>This proposal considers the above fundamental problem by introducing<br\/>a novel mechanism called secure and auditable privacy<br\/>contracting (SAP-Contracting). An SAP-Contract can be used to<br\/>define a tradeoff between level of privacy and amount of data<br\/>mining. Such a tradeoff can be negotiated and customized between<br\/>data sources and data miners. An SAP-Contract defines precisely the<br\/>functions and permitions that can be performed in personnel records.<br\/>It aims to bridge the need for privacy with the need for data<br\/>collection, transfer, marketing and processing; thus, enable<br\/>sensitive private data to be treated as a commodity.<br\/><br\/>SAP-Contracting is different from the current state of art such as<br\/>privacy preserving data mining approaches since it does not rely on<br\/>server based data hiding. It complements and benefits from the<br\/>research on cryptographic databases.<br\/><br\/>The PIs propose to design, implement and test a prototype of this<br\/>paradigm to establish a proof of a concept and show provable<br\/>security properties of SAP-Contracts, including<br\/>confidentiality, integrity, and auditability.<br\/><br\/>If successful this project will provide provable and auditable<br\/>privacy. It will marry privacy and data mining research with<br\/>different economical models and venture into new research areas.","title":"SGER: Collaborative Research: Secure and Auditable Privacy Contracts","awardID":"0751069","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["541892"],"PO":["521752"]},"135398":{"abstract":"Proposal Number: 0751069 <br\/>PI: Bulent Yener <br\/>Institution: Rensselaer Polytechnic Institute <br\/><br\/>Proposal Number: 0751095 <br\/>PI: Aggelos Kiayias <br\/>Institution: University of Connecticut <br\/><br\/><br\/><br\/><br\/><br\/>Title: SGER: Collaborative Research: Secure and Auditable Privacy Contracts <br\/><br\/>Proposal Abstract <br\/><br\/><br\/><br\/>One of the major challenges imposed by the pervasiveness of <br\/>cyberspace is to ensure ``trust'' and design trustworthy systems to <br\/>support the critical infrastructure for official, commercial, and <br\/>personal business. Millions of everyday users pass their personnel <br\/>information over the Internet to their health-care providers, to <br\/>their banks, to insurance companies and other service providers. <br\/><br\/>Once such personnel information is transferred, in many cases, it is <br\/>outsourced to other parties (some of which may even reside in <br\/>foreign countries) for storage and processing. It may be sold or <br\/>resold for data mining. <br\/><br\/>The collection, processing and resale of private data is a <br\/>multi-million dollar industry that has serious security and privacy <br\/>breaches including inadvertently revealing or loosing the <br\/>personal information of thousands or millions of people. <br\/><br\/>While there is a clear need for knowledge discovery for commercial <br\/>(e.g., marketing, insurance design), scientific and even security <br\/>reasons (e.g., identifying public health problem outbreaks such as <br\/>epidemics, biological warfare instances), at present, ``data <br\/>producers'' (i.e., users like all of us) have no control of who, <br\/>how, and what exactly is done with such private and sensitive data. <br\/><br\/>This proposal considers the above fundamental problem by introducing <br\/>a novel mechanism called secure and auditable privacy <br\/>contracting (SAP-Contracting). An SAP-Contract can be used to <br\/>define a tradeoff between level of privacy and amount of data <br\/>mining. Such a tradeoff can be negotiated and customized between <br\/>data sources and data miners. An SAP-Contract defines precisely the <br\/>functions and permitions that can be performed in personnel records. <br\/>It aims to bridge the need for privacy with the need for data <br\/>collection, transfer, marketing and processing; thus, enable <br\/>sensitive private data to be treated as a commodity. <br\/><br\/>SAP-Contracting is different from the current state of art such as <br\/>privacy preserving data mining approaches since it does not rely on <br\/>server based data hiding. It complements and benefits from the <br\/>research on cryptographic databases. <br\/><br\/>The PIs propose to design, implement and test a prototype of this <br\/>paradigm to establish a proof of a concept and show provable <br\/>security properties of SAP-Contracts, including <br\/>confidentiality, integrity, and auditability. <br\/><br\/>If successful this project will provide provable and auditable <br\/>privacy. It will marry privacy and data mining research with <br\/>different economical models and venture into new research areas.","title":"SGER: Collaborative Research: Secure and Auditable Privacy Contracts","awardID":"0751095","effectiveDate":"2007-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["381857"],"PO":["521752"]},"125146":{"abstract":"Our modern society increasingly depends on the reliability, safety, and security of computer software. However, the compilers and other tools that analyze and translate the software into executable form can create a critical bottleneck: if a compiler has an error, then any software compiled by it may in turn be compromised.<br\/><br\/>This project addresses this fundamental problem by developing effective, efficient, and correct language and tool implementation technology. Central to the project is that program analyses and transformations, the heart of optimizing compilers and software analysis tools, are written in a specialized language, named Rhodium. By focusing on this domain, it becomes feasible to build a fully-automatic correctness checker that ensures that Rhodium analyses and transformations are guaranteed to preserve the behavior of any program they process.<br\/><br\/>Previous work developed a proof-of-concept Rhodium system, and demonstrated it on a range of intraprocedural optimizations. This project is developing new techniques that will allow the Rhodium system to scale to richer and more realistic settings, including the ability to optimize full-featured object-oriented and functional languages, perform scalable interprocedural analyses, execute with high efficiency, and cover the full range of tasks in the \"middle-end\" of optimizing compilers and software checking tools.","title":"Effective, Efficient, and Correct Software Analysis and Optimization Tools","awardID":"0702225","effectiveDate":"2007-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["517262","555936",332082],"PO":["523800"]}}