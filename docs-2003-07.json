{"82307":{"abstract":"Title: Geometric Tools for Algorithms<br\/><br\/>The goal of this project is to develop a set of general algorithmic tools<br\/>based on geometry and randomness. Four specific approaches will be<br\/>investigated -- geometric random walks, convex relaxations, random<br\/>projection and spectral projection. There are basic problems that can be<br\/>solved by each technique (i.e., yielding efficient algorithms). These<br\/>include convex optimization, approximation algorithms for NP-hard problems<br\/>and learning mixtures of distributions. The solutions to these problems<br\/>lead to several questions about the applicability and efficiency of these<br\/>techniques (e.g. What functions can be sampled efficiently by the random<br\/>walk approach? How quickly can the volume be computed? Is there a<br\/>relaxation refinement method that improves the integrality gap? What are<br\/>the limits of the spectral method?). The PI plans to address these<br\/>questions and use the answers to tackle basic open problems in algorithms.<br\/><br\/>Intellectual merit:<br\/>Geometric insights and approaches play an increasingly central role in the<br\/>discovery of polynomial-time algorithms for fundamental problems. At a<br\/>time when the field of algorithms is growing rapidly, such tools will be<br\/>crucial in crystallizing a theory of algorithms that will deepen our<br\/>understanding as well as advance the field by aiding in the solution of<br\/>key open problems.<br\/><br\/>Broad impact:<br\/>The problems this proposal sets out to explore are of a basic nature, and<br\/>originate from a variety of areas, including combinatorial optimization,<br\/>machine learning, information retrieval, and Euclidean geometry. Progress<br\/>on these problems, in addition to its potential practical impact, is sure<br\/>to unravel combinatorial\/geometric structure, and is likely to yield new<br\/>analysis tools. The research results will form the basis of an<br\/>undergraduate course on combinatorial optimization as well as two graduate<br\/>courses; course notes for all of these will be available online.","title":"Geometric Tools for Algorithms","awardID":"0307536","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["565103"],"PO":["499399"]},"83638":{"abstract":"Loosely coupled distributed systems are those in which nodes can join, leave, or fail at high rates without centralized control and network connections or topology can be highly unstable. Examples include self-organizing overlay networks, mobile ad hoc networks (MANET), and sensor networks. This proposal seeks to address issues in supporting key searching in these dynamic systems. Keyword searching is an extremely effective utility for many higher-level distributed services, such as information retrieval in overlay networks and habitat monitoring for sensor networks.<br\/><br\/>This proposal seeks to build a distributed keyword searching framework that achieves a desirable level of system scalability, search speed, content staleness, and quality of search. The heart of the proposed design is a distributed data structure called a summary index, which maintains gradually less accurate search index for content farther away to achieve scalability. This project contains three main thrusts: 1. creating a robust \"summary index\"-based keyword searching framework for loosely coupled distributed systems, 2. examining the design of various system components and studying the tradeoffs among performance metrics, and 3. incorporating system integration and application studies.","title":"ITR: Keyword Searching in Loosely Coupled Distributed Systems","awardID":"0312925","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":[218052,"550397"],"PO":["563751"]},"82439":{"abstract":"Robotics and Human Augmentation Program<br\/><br\/>ABSTRACT<br\/><br\/><br\/>Proposal #: 308078<br\/>Title: Robotic Dispensing of Cubic Phase for Crystallizing Membrane Proteins<br\/>PI: Zheng, Yuan<br\/>Ohio State Univ Res Fdn<br\/><br\/>This research addresses new robotic technologies and systems which enable the automated handling of nanoliter volumes of highly viscous bio-materials. The robotic system is urgently needed by the proteomics community following on the announcement in 2001 that the human genome had been sequenced. This landmark event has led to intense activity in structural biology aimed at determining the 3-dimensional structure of the proteins coded for by the genome. About a third of these proteins are integral to the cell's membranes and x-ray crystallography is the only reliable method for structure determination. A method that uses the lipid cubic mesophase (in meso) has recently been introduced for producing diffraction quality crystals of several important membrane proteins and a protein complex. The in meso method requires the controlled and reproducible handling in high-throughput fashion of accurate, nanoliter volumes of precious, highly viscous protein\/lipid cubic phase mixtures of defined shape. Three approaches will be used to facilitate the development of such a system as follows: a) efficient-motion planning of the dispensing tool for effective delivery of viscous materials, b) utilization of the cubic phase for coordinate measuring, and c) computer vision verification of successful dispensing. For efficient-motion planning the PIs will study the motion trajectory of the dispensing tool with respect to the receiving container to assure delivery of accurate cubic phase volumes in the nanoliter range of predetermined shape. To locate the bottom surface of the container for accurate dispensing, the PIs will use the cubic phase itself for coordinate measurement to obviate the need for expensive and complex coordinate measuring machines. Computer vision verification provides a cost effective way to monitor delivery since failure may occur occasionally when the viscous material refuses to leave the dispensing tool.<br\/><br\/>The intellectual merits of this research are in the creation of new robotic technologies and systems for the automatic handling of nanoliter volumes of highly viscous bio-materials which have never been studied before. The development of the three new technologies related to motion planning, coordinates measuring, and computer-vision inspection will be the primary intellectual merits of this activity. <br\/><br\/>The broader impacts of this research include the following: a) the project will produce a fully functional robotic system that will enable the screening of thousands of crystallization conditions daily to expedite the better understanding of the structure of membrane proteins, b) the results will create a knowledge base for the automatic handling of generic viscous materials, and c) the educational plan seeks to train students in an interdisciplinary area encompassing engineering and biochemistry.","title":"Robotic Dispensing of Cubic Phase for Crystallizing Membrane Proteins","awardID":"0308078","effectiveDate":"2003-07-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["518511",214949],"PO":["335186"]},"83209":{"abstract":"A Scalable On-Line Associative Deep Store<br\/><br\/>The deep store architecture solves the problems of being able to store data on inexpensive magnetic hard disk at high rates of compression by storing similar data based on content and compressing files using differential compression. Deep store file retrieval performance obviates the need for traditional archival storage media such as magnetic tape. The deep store computes data fingerprints, and then summaries, from files. Files are organized into data clusters by using a content-based file similarity metric; file content, which is immutable, is addressed by content and stored using hashing.<br\/><br\/>The research comprises development of a scalable system architecture, development of new algorithms, integrating existing technologies, and identifying and addressing new problems: searching for similar files in a very large corpus to improve compression, maximizing storage throughput, distributing a large system for throughput and reliability, and managing file similarity data for billions of files.<br\/><br\/>Improved archival data storage density and retrieval performance (latency and data throughput) will impact all computing environments including scientific experimentation and simulation, commercial and government document management, and expand knowledge of information retrieval by content.","title":"A Scalable On-Line Associative Deep Store","awardID":"0310888","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["540846"],"PO":["325495"]},"81845":{"abstract":"EIA-0305729<br\/>Nick McKeown<br\/>Department Electrical Engineering<br\/>Stanford University<br\/>\"Introducing hardware and software design exercises into the national networking curriculum<br\/>$460,000<br\/><br\/>This project involves the prototyping and deploying of a National Internet Infrastructure Teaching Facility to allow students at educational institutions across the nation to perform challenging new design exercises as part of their networking curriculum. The unique infrastructure developed by this project will permit students to design, deploy and test fully functioning packet switches (in software or hardware) that process real traffic in a real network. The infrastructure is based upon research done at this institution that produced test versions of two tools - the Virtual Router (permits implementation of software on a local computer that processes packets traversing a remote network), and NetFPGA (permits implementation of a packet switch in programmable hardware using an industry-standard design flow). The Virtual Router makes it possible to implement a software program in the user-space of any computer that processes packets traversing a remote network. This will permit students across the nation to implement an Internet router in user-space on their own computer that routes real traffic traversing a network inside the remote National Teaching Facility. In a similar way, the NetFPGA allows students to implement a packet switch in programmable hardware using an industry-standard hardware design flow. With this infrastructure at their disposal, students can investigate different design tradeoffs in packet switching by using realistic tools directly on real network flows. The project involves the development of fully functioning components of the Virtual Router and NetFPGA, their deployment and local testing in networking courses at Stanford, the distribution of the products for broader use, and, eventually, the creation of a National Teaching Facility that is accessible by all educational institutions.","title":"Introducing hardware and software design exercises into the national networking curriculum","awardID":"0305729","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}}],"PIcoPI":["560546"],"PO":["551712"]},"82703":{"abstract":"Fault Finder: Improving the Availability of Multiprocessor Servers<br\/><br\/>Abstract<br\/><br\/>The Duke FaultFinder Project seeks to provide the first hardware mechanisms for dynamically verifying the correctness - not just necessary properties - of shared memory multiprocessor systems. The memory consistency model determines the correctness of a design. FaultFinder will dynamically detect violations of the specified memory consistency model, which is the highest level of error detection possible in hardware. FaultFinder mechanisms will detect hardware errors at the system level (e.g., violation of consistency), unlike existing schemes that only detect localized errors (e.g., bit flip on message). Combining FaultFinder error detection with existing hardware mechanisms for checkpoint\/recovery of shared memory multiprocessor systems enables the system to guarantee correct behavior.<br\/><br\/>As society has increasingly relied upon computer systems to provide important infrastructure, computer engineers have not correspondingly improved the ability to detect faults in these systems. While recent advances in hardware checkpoint\/recovery have improved computer system availability, a system recovery mechanism can only recover from those errors that are detected. Currently, computer systems cannot detect whether a memory system is behaving correctly. The Duke FaultFinder Project seeks to provide the first hardware mechanisms for comprehensive error detection in computer systems. Achieving this goal would provide a qualitative benefit to a society that depends on computer availability.","title":"FaultFinder: Improving the Availability of Multiprocessor Servers","awardID":"0309164","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["536996"],"PO":["325495"]},"82857":{"abstract":"Abstract<br\/>0309701<br\/>Robert M. Gray<br\/>Stanford University <br\/> <br\/>The research is about the theory and design of systems for communicating and interpreting information bearing signals such as images, video, and speech. The main focus is on methods for reducing the complexity of signals while retaining or extracting essential information. The algorithms typically quantize or compress the data efficiently into a simpler signal that can be transmitted or stored efficiently and then used in place of the original. They can facilitate identification of the data as belonging to some class or type. Examples of such signal processing include analog-to-digital conversion, speech and image coding, speech recognition, and segmenting images into distinct regions of interest. The research involves the fundamental theory of such systems and draws on ideas from information theory, statistical signal processing, and statistics. It also involves applications, especially algorithms for classifying and segmenting images and for content-addressable browsing through image databases.<br\/>The basic tools come from modeling, density estimation, compression, coding, classification, and segmentation. They enable both theoretical characterizations of optimal performance and associated algorithms by which codes are optimized for specific applications, for example, systems for compression and segmentation of images for communications, analysis, and retrieval. Tools are drawn from four fundamental ideas of information theory and signal processing: vector quantization, mixture models, relative entropy (Kullback-Leibler information) measures of distortion or distance between probability distributions, and universal coding. Of particular interest is the theory of high rate vector quantization and its applications to statistical image classification and segmentation. Specific applications of interest include automatic<br\/>segmentation of multimodal images and of categorizing images taken internally in gas pipelines in order to quantify pipeline integrity.","title":"Quantization for Signal Compression, Classification, and Mixture Modeling","awardID":"0309701","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["441541",215949],"PO":["564898"]},"81889":{"abstract":"ABSTRACT<br\/><br\/>Information Technology Workforce (ITWF) - FY03<br\/><br\/>Proposal ID: EIA-0305917<br\/>Investigator: Laurie Williams, Winser Alexander, Sarah Berenson, and Mladen Vouk.<br\/>Institution: North Carolina State University<br\/>Title: Collaboration through Agile Software Development Practices: A Means for Improvement in Quality and Retention of IT Workers <br\/><br\/>North Carolina State University at Raleigh (NCSU), North Carolina A&T University and Meredith College have been awarded an ITWF grant for a 3-year study of the collaborative aspects of agile software development methodologies. The project's objective is to perform extensive, longitudinal experimentation in advanced undergraduate software engineering college classes at the three institutions to examine student success and retention in the educational and training pipelines when the classes utilize an agile software development model. The project will also involve the development of agile software development materials for software engineering classes.","title":"ITWF: Collaboration through Agile Software Development Practices: A Means for Improvement in Quality and Retention of IT Workers","awardID":"0305917","effectiveDate":"2003-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":["541775","315430","553865","549909"],"PO":["361119"]},"81813":{"abstract":"What is novel and unique in this proposed research is the proposed design of knowledge-based prediction systems based on optimal vector space representations of proteins that have previously been represented by character strings. If an optimal representation of a character string can be found by a numerical sequence, then a great variety of methodologies from disciplines such as optimization, pattern discovery, and machine learning can be readily applied to new understanding of protein tertiary structure and function.<br\/>For this, kernel based nonlinear classifiers and nonlinear dimension reduction as well as visualization methods will be developed to provide scalable and elective prediction systems.The prediction systems will be specially tailored for several problems related to protein structure discovery such as protein secondary structure, relative solvent accessibility and disulfide connectivity, as well as prediction of protein-protein interaction. <br\/>In this proposal the P.I.s describe how they intend to accomplish this, so that their preliminary results can be extended to the more general structure and protein-protein interaction prediction problem. All results obtained will be made available to the research community in order to facilitate further research activity. Using existing web servers, the results will be made available to teaching faculty and graduate and undergraduate students in a suitable tutorial form. This will allow those interested to tailor the material for use in graduate and undergraduate research and class projects. The authors will incorporate the results into current and future course material as well.<br\/>Special efforts will be made by the two women PIs to provide opportunities for women<br\/>graduate students to participate in the proposed research and for development of the<br\/>related software, which has long range social impact beyond the scientific results.","title":"ALGORITHMS: Collaborative Research:Development of Vector Space based Methods for Protein Structure Prediction","awardID":"0305567","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":["450698"],"PO":["565272"]},"81725":{"abstract":"The aim of this project is to study mathematical problems associated with controlling and analyzing the performance of stochastic networks. Since the complexity and heterogeneity of these networks usually preclude exact analysis, the focus here is on approximate models. Two levels of approximation are considered: first order (functional law of large numbers) approximations called fluid models, and second order (functional central limit theorem) approximations, which are frequently diffusion models. The interplay between these two levels is an important subtheme throughout. Three main topics are addressed: (i) Dynamic scheduling for stochastic processing networks, (ii) Analysis of processor sharing networks, (iii) Congestion control for modern communication networks. The stochastic network models considered under these topics are considerably more general than conventional multiclass queueing networks operating under a head-of-the-line (HL) service discipline. Although there is a fairly well developed theory of stability and heavy traffic diffusion approximation for the latter, there are many challenging open problems associated with the control and analysis of the more general stochastic network models being studied here. Some of the stochastic process aspects are that topic (i) involves the analysis and interpretation of the solution of an approximating constrained diffusion control problem, topic (ii) uses measure-valued processes to keep track of residual service times of all jobs in the network, and topic (iii) involves reflected Levy processes when document sizes have heavy tails. <br\/> This grant funds research on mathematical problems motivated by applications in the disciplines of operations research, computer science and electrical engineering. Specifically, problems associated with the control and analysis of stochastic networks are being studied. Such networks are used as models for complex manufacturing, telecommunications and computer systems. Two fundamental problems for such networks are (a) to understand the effects of common policies on congestion and delay, and (b) to design 'good' controllers for these systems. The networks under study are substantially more general than those that have been rigorously studied to date. Through their complexity and heterogeneity, these networks present challenging mathematical problems. The project involves the development of new theory for stochastic processes and uses techniques from a variety of mathematical disciplines. Collaborations with researchers familiar with areas of application and the training of graduate student researchers are integral parts of the project.","title":"Stochastic Networks: Analysis and Control","awardID":"0305272","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1263","name":"PROBABILITY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4097","name":"NETWORKING RESEARCH"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0706","name":"Division of DESIGN & MANUFACTURING INNOV","abbr":"DMI"},"pgm":{"id":"5514","name":"OPERATIONS RESEARCH"}}],"PIcoPI":["512491"],"PO":["452372"]},"81978":{"abstract":"Peer-to-peer (P2P) systems have recently become a very important part of the Internet. <br\/>Unprecedented popularity, low overhead, and high scalability of these networks<br\/>make them highly appealing to a wide range of end users. This proposal studies <br\/>fundamental performance limitations of the existing and recently proposed <br\/>peer-to-peer architectures and examines inherent scalability issues of these networks. <br\/>Outside the P2P community, this project provides a better understanding of <br\/>large-scale self-reconfiguring systems and leads to the development of new <br\/>algorithms for future implementation of various information-centric <br\/>routing infrastructures.","title":"Optimal-Diameter Routing and Error Resilience in Peer-to-Peer Networks","awardID":"0306246","effectiveDate":"2003-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["550669"],"PO":["560587"]},"81989":{"abstract":"0306270\/0306286<br\/>Collaborative: Exploiting Component Contracts <br\/>Robert Bruce Findler\/Matthew Flatt<br\/><br\/>Despite the central role of testing in the development process, current programming languages and environments provide scarcely any support for testing. Most testing research has focused on generating<br\/>test inputs and test oracles from program specifications, but program specifications are difficult to produce and maintain. Consequently, language and environment designers remain unmotivated to support specifications, and opportunities to support testing are lost.<br\/><br\/>Contracts offer a route around the specification problem. Contracts are a form of lightweight specification that work with the base programming language, i.e., there is no need for the programmer to learn a special logic for writing specifications. Contracts are monitored as the program executes, which means that the programmer is forced to maintain contracts with the code. Since contracts are essentially a generalization of assert statements, experience suggests that programmers will use them.<br\/><br\/>The investigators focus on contracts as a specification language for automatic test suite generation. To validate their investigations, they will extend the DrScheme programming environment to support unit<br\/>testing at component boundaries. In addition, they will use the new testing infrastructure to test DrScheme itself.","title":"Collaborative: Exploiting Component Contracts for Static Analysis and Testing","awardID":"0306286","effectiveDate":"2003-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["518130"],"PO":["564388"]},"83815":{"abstract":"ABSTRACT<br\/>0313812<br\/>Daniel Fuhrmann<br\/>Insitute of Electrical and Electronic Engineers<br\/><br\/>The 2003 IEEE Workshop on Statistical Signal Processing is planned for September 28, October 1, 2003, at the Chase Park Plaza Hotel in St. Louis, Missouri. The purpose of the workshop is bring together researchers and scientists with common interests in statistics and signal processing and their application in a wide variety of areas including radar and sonar signal processing, telecommunications, biomedical signal processing, image analysis, array signal processing, system identification, and many more. The two-and-a-half day workshop will feature 5 plenary speakers and approximately 150 contributed presentations. The workshop and its organization is being sponsored by the IEEE Signal Processing Society, and will be the 12th biannual meeting in this area.","title":"Workshop Travel Support 2003 IEEE Workshop on Statistical Signal Processing; September 28-October 1, 2003; St. Louis, MO","awardID":"0313812","effectiveDate":"2003-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["417387"],"PO":["564898"]},"81406":{"abstract":"Exploration of Nanoscale Plasmonic Circuits<br\/><br\/>For future developments in nano-technology, it is essential to provide connecting mechanisms that allow controlled information and energy transport at the nanometer-level. Because of the higher operating frequency, optical interconnects provide a much higher information carrying capacity than electronic interconnects. Unfortunately, conventional dielectric optical interconnects cannot be scaled down to the nanometer regime due to the diffraction limit of light. We propose to take advantage of plasmon-polariton excitations in metallic nanostructures to route information at optical frequencies, at approximately the speed of light, and in the nanometer scale that have been not been accessible through optical means. In particular, we will design, fabricate, and characterize a set of basic building blocks, such as nanowires, ordered arrays of nanoparticles, etc., that can be utilized to build interconnecting structures of complex architecture and function.<br\/><br\/>The success of this exploratory program could provide a fundamental breakthrough in establishing the basic feasibility of a new optical information exchange mechanism at nano-scale. This program will provide a wonderful training opportunity for graduate students in an exciting emerging field of study. We will take advantage of the existing Research Experience for Undergraduate program at Stanford University, which provides support for undergraduate students participating in research in the summer. In addition, Fan and Brongersma are currently collaborating to develop a course in micro and nanoscale photonics to be taught regularly starting this upcoming winter quarter. The course promises to bring the excitement of nano-scale photonics, from both theoretical and experimental perspectives, to students from a wide range of disciplines including Physics, Applied Physics, Chemistry, Electrical Engineering, and Material Science.","title":"NER: Exploration of Nanoscale Plasmonic Circuits","awardID":"0303884","effectiveDate":"2003-07-15","expirationDate":"2004-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["509350","275434"],"PO":["562984"]},"83629":{"abstract":"EIA-0312868<br\/>John Chen<br\/>Institution: Rowan University<br\/><br\/><br\/>Title: Integration of Handheld Wireless Computing Devices into Core Engineering Courses for Rapid Feedback and Concept Learning<br\/><br\/><br\/>This ITR small award provides support to adapt the Concept Inventory for frequent classroom use and then to implement it in a technology-enabled system to provide rapid feedback to students of their understanding of key concepts being presented. The feedback system acts as the focal point and catalyst to encourage students, working in pairs, to assist each other in correcting misconceptions or deepening each other's understanding of topics. The system allows the professor to assess the students' level of comprehension in a just-in-time fashion and thus judge the professor's pacing and coverage of the material. The system and methodology are to be implemented in two core-engineering courses at two institutions to demonstrate feasibility and adaptability in different environments.<br\/><br\/>The intellectual merit of the project in centered on its strong conceptual framework for the research. The resulting system should provide a platform for enhancing education in many disciplines and settings. The project has a sound evaluation plan that clearly will demonstrate the educational as well as technological benefits of the research to multiple audiences.<br\/><br\/>A major broad impact of the project is the integration of education, research, and faculty development, including the use of rigorous scientific research methods to establish that the environment is transferable and adaptable easily to other institutions. In addition, through the involvement of a historically black university impacts the participation of underrepresented minorities in engineering and enhances the curriculum at the institution. In addition, it provides opportunities for faculty professional development for currency in new and emerging technologies. The project is currently and timely and will impact both research and instruction as well as develop tools employing emerging technologies and technical innovations.","title":"ITR: Integration of Handheld Wireless Computing Devices into Core Engineering Courses for Rapid Feedback and Concept Learning","awardID":"0312868","effectiveDate":"2003-07-15","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["215916",218030],"PO":["564181"]},"82419":{"abstract":"Recommender and reputation systems process ratings from past users to guide <br\/>current users' behavior. Such systems provide a natural laboratory for the <br\/>study of large-scale social phenomena, including the provision of public <br\/>goods (the ratings themselves), and the role of reputation as a regulator <br\/>of trust and trustworthiness.<br\/><br\/>This project will focus on two widely used systems, eBay and Slashdot. <br\/>Usage logs provided by the two sites will be assessed in depth. This <br\/>analysis should reveal the social practices that have emerged surrounding <br\/>the provision of ratings, and the ways ratings actually guide users' <br\/>decisions about what to buy and from whom, and what messages to read. <br\/>Game-theoretic analyses of incentives will combine with the empirical <br\/>analysis to guide the design of potential system improvements.<br\/><br\/>This research should improve the functioning and public understanding of <br\/>web sites where millions of people already interact with each other. In <br\/>addition, it will provide fundamental insights into the workings of <br\/>impersonal social capital, i.e., productive social relations that emerge among <br\/>people who remain strangers rather than forming networks of long-term <br\/>relationships.","title":"Recommender and Reputation Systems: Principles and Practices","awardID":"0308006","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":[214894,"539967"],"PO":["564456"]},"81825":{"abstract":"Computational fluid dynamics (CFD) has been employed in the simulation of a wide variety of phenomenon in areas such as aerodynamics, meteorology, oceanography, combustion and biomedical flows. However the usefulness of computational fluid dynamics is limited by an inability to perform system simulations in a timely fashion. This is due to the complexity and computational expense of performing high fidelity unsteady CFD simulations. <br\/>The fundamental goal of this research is the development of a rapid, robust and efficient reduced order simulation technique for accelerating CFD simulation. The reduced order simulation environment must be<br\/>applicable to the study of fluid flow problems across a wide spectrum of applications domains. Existing techniques for improving the performance of fluids simulations are: (a) parallelization of the computations and (b) employment of reduced order modeling. Each of these techniques alone have demonstrated reductions of ten-fold to a hundred-fold in simulation time. However, the practical application of proper orthogonal decomposition (POD) has not been realized due to the inability to quantify the accuracy of the reduced order model without a comparison to a full simulation of the same scenario. This work will investigate and demonstrate feasibility of POD simulation methods and the ability to use residual tracking and related methods to validate and correct reduced order simulations. In addition we will demonstrate self-adaptive performance control of a 3D parallel CFD solver.","title":"ALGORITHMS: Collaborative Research: Parallel Reduced Order Modeling with In-Situ Error Correction and Performance Optimization","awardID":"0305640","effectiveDate":"2003-07-15","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["385726","453272"],"PO":["551992"]},"81946":{"abstract":"0306142<br\/>Sanjay Rajopadhye<br\/>Colorado State University<br\/>HiPHiPECS: High Level Prgramming of High Performance Embedded Computing Systems<br\/>$497,232<br\/><br\/>This project addresses curriculum development in the field of embedded computing which requires expertise from a number of disciplines: digital circuit design, VLSI, real time systems, applications, algorithms and programming. Of the two major classes of embedded systems, one consists of extremely large volumes of simple embedded systems (toasters, doorknobs, toys, controllers, etc.), and typically does not require high performance. The devices are inexpensive, mature, and stable, cross-compilation environments exist and so products can be designed and marketed fairly reliably. This project, HiPHiPECS, focuses on a related, more challenging and growing segment of the market (telephones, digital cameras, automobile sensor data processing, satellite control, information appliances, network routers, photocopiers, printers, scanners, fax machines, etc.), that require extremely high performance. Developing these high performance embedded applications necessitates the exploitation of parallelism, imposes increasingly severe constraints on bandwidth, power\/energy consumption, computation time, and throughput. For such systems, there is an increasing trend towards Systems-on-a-Chip (SoC) where multiple implementation fabrics (re-configurable logic, processor cores, DSP 's and ASIC 's) are combined on a single heterogeneous chip. According to the International Technology Roadmap for Semiconductors (ITRS), there is an exponential \"design gap\" (the roadmap identifies this as the gap between the ability of circuit designers to effectively use silicon resources and the number of transistors available on the chip) in this area. This design gap necessitates raising the level of abstraction involved in digital circuit\/system design. Although this has been identified as a need, few efforts are directed toward addressing the associated educational and training needs in a concerted manner. There is a critical need to train circuit\/system designers in these skills that encompass such areas as: Expertise in the application domain (signal\/image processing, networking, control systems, numerical analysis, algorithms), System engineering and design (software engineering, modularity, stepwise refinement, formal methods for correctness), Programming skills, data structures, algorithm analysis, Parallel processing (parallel algorithm theory, parallelism detection, parallel programming languages, multi-threading, data-parallelism), Operating systems, especially real-time OS, Computer architecture, instruction set architectures, DSP 's, instruction level parallelism, and SIMD architectures, Circuit level design, performance analysis, precise modeling of power\/energy consumption, throughput, area and time. The main contributions of HiPHiPECS will be the development of a curriculum for high performance embedded systems. It will target FPGA based re-configurable computing platforms, since their re-configurability allows for the exploration of other implementation fabrics in a pedagogic setting. The proposed curriculum will focus on the computation-and data-intensive parts of embedded applications, primarily because this is where high performance solutions are critical, with the goal of enhancing instruction in the area of High Level Programming for High Performance Embedded Computing Systems. The project, which involves industrial collaboration, addresses a critical national need, it is founded on current research in High Performance Computing, and it represents a major shift (as opposed to an incremental change) in education in this area.","title":"HiPHiPECS: High Level Programming of High Performance Embedded Computing Systems","awardID":"0306142","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}}],"PIcoPI":["528018"],"PO":["551712"]},"81737":{"abstract":"0305330<br\/>RUI: Amorphous Program Slicing<br\/>Dave Binkley<br\/>Loyola College in Maryland<br\/><br\/>Software is given ever increasing responsibility for safety-critical systems (e.g., the control of nuclear power plants, medical devices, electronic banking, and air traffic control), which necessitates the production of high-integrity software (i.e., software that can and must be trusted to work dependably). The growing complexity of this software requires increased comprehension, which is obtainable through improved tool support.<br\/><br\/>The primary objective of this research is to improve programmer productivity by improving comprehension through the use of semantics-based software analysis tools. This particular research will investigate the use of amorphous program slicing in semantics-based tools. A tool that understands and conveys the semantics, or meaning, of a program, provides unique and useful information to a programmer. Program slicing is a program decomposition technique that identifies semantically meaningful portions of a program. Amorphous program slicing is a recent variant of program slicing that reduces the amount of code<br\/>a programmer must consider. <br\/><br\/>Achieving these objectives requires building an amorphous slicing tool and then conducting experiments with it. An effective tool will improve programmer productivity; thus increasing programmer understanding, reducing software costs, and, perhaps more importantly, increasing the integrity of software.","title":"RUI: Amorphous Program Slicing","awardID":"0305330","effectiveDate":"2003-07-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T360","name":"NSA-STUDY OF COMPARISON OF LAN"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T777","name":"NSA-VARIOUS COMMUNICATION"}}],"PIcoPI":["409262"],"PO":["564388"]},"81506":{"abstract":"NER: Interaction Between Light and Nanoengineered Surfaces<br\/>Arizona State University<br\/><br\/>The fact that the energetic transactions between atoms and molecules, objects with characteristic dimensions ranging from a fraction of a nanometer to a few nanometers, take place in the currency of light, hundreds of nanometers in wavelength, presents us with one of the great challenges of Physics today. It is a challenge of scale. The wavelength most appropriate to study nanoscale molecular processes is too large to resolve the details of their operation. Smaller wavelengths quickly become either insensitive or damaging, while Scanning Microscope approaches are limited by their inherently low throughput. This project will lay down the foundation for a next-generation infrastructure instrumentation technology for sensing, measuring, characterizing, and testing nanostructures down to the 20nm scale and beyond, in the visible spectrum. The technology is based on the manipulation and localization of light in the nanoscale through the use of photonic antennas. <br\/>Preliminary theoretical analysis of these engineered structures, hundreds of nanometers across but possessing detailed features in the 10-nanometer range, has shown that they concentrate visible light into sub 20nm regions and amplify the reradiation by the illuminated objects in those regions by at least 3 orders of magnitude. Therefore they will allow the observation of functioning nanodevices in real time. Furthermore this degree of amplification would make it possible to observe weakly fluorescent molecules directly, enabling new, faster, and more sensitive molecular assay methods for DNA sequencing and detection of bio-hazards.<br\/>The expected outcomes of this exploratory research project are: (1) Development of a robust theoretical analysis method for modeling the interaction between photonic antennas and molecular objects. (2) Application of this model to demonstrate the capability to observe and inspect a nanodevice (a carbon nanotube transistor). (3) Experimental demonstration of the 3000-fold amplification of molecular fluorescence signals. Success in these three areas will open the way for full development of instrumentation based on this technology","title":"NER: Interaction Between Light and Nanoengineered Surfaces","awardID":"0304342","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":[212507,"329367"],"PO":["562984"]},"84829":{"abstract":"Alur<br\/><br\/>EMSOFT 2003 Workshop - the Third International Workshop on Embedded Software, is held October 13-15, 2003 in Philadelphia on the campus of the University of Pennsylvania. The proceedings of EMSOFT 2003 are published as a volume in the Lecture Notes in Computer Science series by Springer.<br\/><br\/>The purpose of EMSOFT 2003 is to bring together researchers, developers, and students from academia, industry, and governments so that the science of embedded software and the technology of their applications an be advanced. The purview of the workshop spans all areas of embedded software, including system design and integration methodologies, scheduling and execution time analysis, models of computation and formal methods, communication protocols, and fault tolerance. Currently, academic papers on embedded software are dispersed at many different conferences, and EMSOFT offers a unique venue with the central focus on principles of embedded software.","title":"Workshop On Embedded Software","awardID":"0318299","effectiveDate":"2003-07-15","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2801","name":"EMBEDDED & HYBRID SYSTEMS(EHS)"}}],"PIcoPI":["553656","497082"],"PO":["561889"]},"81915":{"abstract":"EIA-0306012<br\/>Peter Cappelli<br\/>Institution: National Bureau of Economic Research (NBER)<br\/><br\/>Title: The Impact of Information Technology Use on Employee Demography and Employer-Provided Training<br\/><br\/>This ITWF award provides support to analyze national data on establishments, their characteristics, and the experiences of their employees to examine questions concerning the relationship between information technology (IT) use by employers and employment related outcomes. The project will first analyze the distribution of women and minorities in IT jobs, as classified not only by the tasks employees perform but the IT systems in place at work. It will then explore factors that explain the incidence of women and minorities across IT jobs, including aspects such as career paths (e.g., internal promotions and temp-to-permanent tracks) and how employees are managed. Finally, it will explore how attributes of IT use relate to employer-provided training: Do IT-intensive employers provide more IT training, especially to their IT workers? When they provide IT training, do they offer credentials for it? If so, what accounts for their ability to provide such general training?<br\/><br\/>The investigators will examine these questions using unique data from the National Employer Surveys of the U.S. Bureau of the Census. These establishment surveys, conducted in 1994, 1997, and 2000, identify important characteristics about employers including their use of IT and their general management practices. The 2000 version also contains a Supplement that surveys employees in these establishments. It identifies IT-related tasks, IT-related training, and demographic attributes of employees such as whether they are women or minorities. These data will be enhanced by information from the Census of Manufacturing Supplement on Information Technology, also conducted in 2000, which has especially good measures of various IT interventions. The data from these different sources will be merged, allowing examination of relationships not only with individual-level IT tasks, such as programming, but also with the use of specific IT systems, such as Enterprise Resource Planning (ERP). The combination of more detailed measures of IT use and information about individual IT workers will allow more careful analysis than ever before of relationships between IT use and employee<br\/>characteristics and employment outcomes. In particular, rather than classifying workers crudely into \"IT\" or \"Non-IT\" jobs (in the past typically based on the employer's product market), the analyses can examine relationships between the extent of IT use and important job outcomes, especially for women and minorities.","title":"ITWF: The Effects of Information Technology on Employee Demography and Training Outcomes","awardID":"0306012","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":[213632,"495248"],"PO":["564181"]},"81816":{"abstract":"The proposed procedure MAPO(Multi-Algorithm Parallel Optimization) iteratively uses a committee of serial response surface algorithms to facilitate the generation of a sizeable number of distinctly different points for costly function evaluation in parallel. We refer to the methods as algorithm-experts. This approach can be effective because the computation time for one serial response surface optimization algorithm is very small in comparison to the CPU time required to evaluate costly f(x). Hence MAPO has a committee of experts, each one of which selects several candidate points x for costly function evaluation.The two main classes of algorithm-experts use Radial Basis Functions and Neural Nets. <br\/>The algorithm will be applied to a range of difficult test problems and to three classes of costly real engineering functions. Two of these applications come from the PI's own research projects on environmental pollution and safety of drinking water. Another project involves a finite element model of a mechanics system based on partial differential equations provided by the NSF ITR Adaptive Software Project at the Cornell Theory Center.","title":"ALGORITHMS: Multi-Algorithm Parallel Optimization of Costly Functions","awardID":"0305583","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["550886"],"PO":["565272"]},"82938":{"abstract":"Given a graph G=(V,E), an orientation O of G is an assignment<br\/>of directions to the edges of G. We may require that certain<br\/>properties must be satisfied by O. Varying the requirements,<br\/>different orientations can be defined. The orientations with<br\/>special properties often result in interesting combinatorial<br\/>structures.<br\/><br\/>Recently, several orientations of planar graphs have been <br\/>studied. They have important and sometimes unexpected applications<br\/>in fields such as graph drawing, VLSI layout, computer graphics etc.<br\/>These orientations often lead to elegant algorithms, which either<br\/>improve or simplify previously known algorithms.<br\/><br\/>The intellectual merit: The PI has recently obtained interesting<br\/>results using various orientations. The proposed research extends<br\/>these results. The proposed project will result in novel<br\/>combinatorial concepts and constructions and new algorithmic tools.<br\/>This research will make both theoretical and practical<br\/>contributions. On the theoretical side, several important<br\/>open questions in graph theory and combinatorics are among<br\/>the proposed problems. On the practical side, many problems<br\/>discussed in the proposal are motivated by applications in<br\/>graph drawing, computer graphics and VLSI layout problems.<br\/><br\/>The broader impacts: The results obtained from this research<br\/>will have impacts on practical fields such as graph drawing,<br\/>computer graphics and VLSI design, by providing better, simpler<br\/>and more efficient algorithms. It will also enhance graduate<br\/>education in computer science. The algorithms, concepts and<br\/>techniques developed in this research will be include in a new<br\/>advanced algorithm course.","title":"Graph Orientations and Applications","awardID":"0309953","effectiveDate":"2003-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["550545"],"PO":["562944"]},"81905":{"abstract":"ABSTRACT<br\/><br\/>Proposal ID: EIA-0305973<br\/>Investigator: Elizabeth Lawley and Tona Henderson<br\/>Institution: Rochester Institute of Technology<br\/>Title: Understanding Gendered Attrition in Departments of Information Technology <br\/><br\/>Rochester Institute of Technology (RIT) has received an ITWF award to study the experiences of undergraduate women in departments of Information Technology (IT). Most research to date into women's experiences in undergraduate computing programs has focused on Computer Science departments. While IT programs have cast themselves as qualitatively different from traditional CS, it is not clear whether women's experiences in these programs are more positive than in CS, where retention of female students has been consistently problematic.<br\/><br\/>The study will be done in two parts. The first will be a qualitative study of all women, and a sample of men, entering the IT department at RIT as freshmen. These women will be interviewed upon entrance into the program, at the end of their first quarter, and at the end of the academic year. Based on the information gained in that study, key factors related to women's persistence or attrition will be identified. The second part of the study will be the development of questionnaires for faculty and students intended to identify the presence and influence of those actors in academic departments. The questionnaire will then be administered at departments of IT across the US, in order to determine whether the factors identified at RIT can be generalized across institutions.","title":"ITWF: Understanding Gendered Attrition in Departments of Information Technology","awardID":"0305973","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":[213608,213609],"PO":["564181"]},"89090":{"abstract":"With the advances in silicon technology as predicted by Moore's law, we are already in deep sub-micron era of device feature size. Currently 90 nanometer (90 nm) is a reality, and very soon we will be in the tens of microns. The technology will then change drastically, with single electron transistors and such artifacts of nano-technology era. As a result, uncertainties about the device behaviors will be a common problem that engineers will have to deal with, in such miniscule quantum level of technology. Currently, while designing computer architecture components, engineers safely assume that the transistors and logic gates will behave as predicted by the theory. However, with uncertainties being rampant in nano-technology, one can only rely on the measures of the probability that a transistor will behave correctly, or a logic gate will function correctly. As a result, in order to implement a logic function and to depend on it with high degree of reliability, engineers will have to build redundancy in the design, such that if some of the gates fail, even then, the functional block will provide the correct functionality with very high degree of reliability. However, how to build redundancy for particular logic functions, and how much redundancy is enough, and at what level of redundancy, the reliability actually decreases, are questions to be answered by an engineering tool, before such redundancy is built into the system. Von Neumann looked at similar problems for logic gates since during his time, logic gates were built with valves which were quite unreliable. Information theorists also look at similar problems in terms of noise tolerance by logic functions. However, no tool exists for such evaluations, nor does a precise engineering methodology exist. This project aims at bringing in the novel technology of probabilistic model checking and create a methodology and tool set for evaluation of reliability for different alternate redundant architectures and compute reliability measures before the design is built to steer the engineers in the right directions. <br\/><br\/><br\/>This work will help prepare computer engineers in building reliable functionalities on unreliable nano-substrates, given that material scientists can calibrate nano-materials for probability of failures. This work therefore will have great impact in the future computer engineering and logic design. Also it will help educate future engineers to face the nano era and build reliable computing infrastructures.","title":"SGER: Evaluating Reliability of Defect Tolerant Architectures for Nanotechnology using Probabilistic Model Checking","awardID":"0340740","effectiveDate":"2003-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["384407"],"PO":["562984"]},"81807":{"abstract":"Computational fluid dynamics (CFD) has been employed in the simulation of a wide variety of phenomenon in areas such as aerodynamics, meteorology, oceanography, combustion and biomedical flows. However the usefulness of computational fluid dynamics is limited by an inability to perform system simulations in a timely fashion. This is due to the complexity and computational expense of performing high fidelity unsteady CFD simulations. <br\/>The fundamental goal of this research is the development of a rapid, robust and efficient reduced order simulation technique for accelerating CFD simulation. The reduced order simulation environment must be<br\/>applicable to the study of fluid flow problems across a wide spectrum of applications domains. Existing techniques for improving the performance of fluids simulations are: (a) parallelization of the computations and (b) employment of reduced order modeling. Each of these techniques alone have demonstrated reductions of ten-fold to a hundred-fold in simulation time. However, the practical application of proper orthogonal decomposition (POD) has not been realized due to theinability to quantify the accuracy of the reduced order model without a comparison to a full simulation of the same scenario. This work will investigate and demonstrate feasibility of POD simulation methods and the ability to use residual tracking and related methods to validate and correct reduced order simulations. In addition we will demonstrate self-adaptive performance control of a 3D parallel CFD solver.","title":"ALGORITHMS: Collaborative Research: Parallel Reduced Order Modeling with In-Situ Error Mitigation and Performance Optimization","awardID":"0305532","effectiveDate":"2003-07-15","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["408839","515816"],"PO":["551992"]},"81928":{"abstract":"0306064<br\/>Gautam Singh<br\/>Oakland University<br\/>Integrating Bioinformatics Concepts in Computer Science Curricula: A Cognitive Assessment Driven Strategy<br\/>$404,000<br\/>This project introduces the concepts of a new multidisciplinary field (bioinformatics) within the existing computing curriculum by incorporating research-oriented, self-contained learning modules. Prospects for majors in the field of bioinformatics are expanding, including opportunities for graduate study and employment. This project fulfills this demand by introducing the notion of self-contained modules that encapsulate not only the core biological principles necessary to fully appreciate the computational aspect of the subject, but they also provide theoretical and practical exposure to the related computational biology algorithms and research areas. Graduates who know how to identify and work on solving leading research problems gain the significant benefit of being able to apply bioinformatics to a broad range of problems. In addition to being immersed in a burgeoning research sector, graduates of this program, who have free access to the newest knowledge and skills in the field as it evolves, may enjoy a competitive advantage in the workforce. Specifically, this project involves developing five lecture series. Each lecture series is comprised of 2-3 lectures and 1-2 laboratory modules devised for integration within the existing courses. The modules are designed with the objective of serving as academic examples of some of the computer science concepts covered in courses such as databases, analysis of algorithms, networking, data structures, etc. Thus, the main goal is to have a CS major cognizant of the computational challenges in bioinformatics. These modules all have some common characteristics. They are self contained, designed to be delivered with 3 to 4 hours of student contact, present the bioinformatics research concepts within a computer science context, incorporate a hands-on laboratory component, and integrate cognitive assessment with the objective of helping students appreciate the depth of their understanding of new concepts taught. The modules include: BINF 01 - The Biological Database Lectures Module (Biological Databases, GenBank Schema, Biological Database Federation, Biological Databases and Federation Architecture); BINF 02 - The Information Retrieval Lectures Module (Sequence Similarity Algorithms in Bioinformatics, Bio-Database Indexing Strategies, Genome Database Search Algorithms, Searching Genomic and Protein Databases with BLAST); BINF 03 - The Intelligent Models for Mining Biological Data Lectures Module (Computational Models of Biological Sequence, Hidden Markov Models for Biological Patterns, Machine Learning Paradigms applied to Bioinformatics, Applying Hidden Markov Models for Sequence Analysis, Information Theoretical Measure of Surprise); BINF 04 - The Adaptive Middleware Lectures Module (The Bioinformatics Open Source Project, CORBA and BioCORBA, Java Servlets and Distributed Annotation Systems, Internet Sequence Analysis with BioJava and BioPerl, Dazzle Servlet and DAS); and, BINF 05 - The Evolutionary Tree Computation Lectures Module (Computational Complexity of Constructing Evolutionary Trees, Cluster Based Methods for Evolutionary Tree, Hamming Distance and Parsimonious Trees, PHYLIP - The Phylogenetics Analysis Program).","title":"Integrating Bioinformatics Concepts in Computer Science Curricula: A Cognitive Assessment Driven Strategy","awardID":"0306064","effectiveDate":"2003-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}}],"PIcoPI":[213661,"305903"],"PO":["551712"]},"78580":{"abstract":"This project exploits the CORBA middleware standard as a vehicle for<br\/>exploring the composition of real-time and fault tolerance for<br\/>distributed middleware applications. Although the initial approach<br\/>will apply to CORBA, the results will be applicable to other<br\/>middleware, such as Java. The primary scientific contributions of<br\/>this project are the description of the precise trade-offs that occur<br\/>when both real-time and fault tolerance support are required for a<br\/>middleware application, along with an architecture and strategies that<br\/>can be used to resolve these trade-offs. The primary technical<br\/>contributions of the project are a new set of practices and algorithms<br\/>for building real-time fault-tolerant systems, and tools for<br\/>sanitizing non-determinism in middleware applications, with the aim of<br\/>meeting both real-time predictability and fault-tolerant consistency<br\/>requirements.<br\/><br\/>This project furthers our technical understanding of the precise<br\/>trade-offs and complex interactions between ``-ilities'' such as<br\/>real-time and fault tolerance, thereby helping to reduce the number of<br\/>proprietary brittle systems that continue to be built. The project<br\/>has the potential to influence the state of the art and the<br\/>state of the practice, enabling future COTS-based mission-critical<br\/>applications to be developed with simultaneous real-time and<br\/>fault-tolerant support. Recognizing that applications vary in their<br\/>dependability needs, this project is unique in that it provides<br\/>mechanisms to mix-and-match the different fault tolerance mechanisms<br\/>in a manner that allows for the customization of the system's<br\/>perceived dependability.<br\/><br\/>Through participation in organizations such as the Object Management<br\/>Group, the results of this project will be made available to harden<br\/>ongoing and future standards on fault tolerance and real-time for<br\/>middleware. Elements of this project will be incorporated into<br\/>curricula in order to expose students to the cross-cutting concerns of<br\/>both reliability and real-time. Students who are exposed to not just<br\/>one ``-ility'', but to the conflicts and trade-offs between multiple<br\/>``-ilities'', will make better and more discriminating software\/system<br\/>architects who can be entrusted to build the mission-critical systems<br\/>of the future.","title":"CAREER: Integrated Fault Tolerance and Real-Time Support for Middleware Applications","awardID":"0238381","effectiveDate":"2003-07-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2801","name":"EMBEDDED & HYBRID SYSTEMS(EHS)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["313442"],"PO":["561889"]},"79471":{"abstract":"This project will develop a comprehensive security framework using content-based and context-aware access control models for XML-based applications in distributed heterogeneous multi-enterprise environments. Such applications include electronic commerce, finance and banking, manufacturing, corporate databases, health-care and other on-line services and businesses. For these applications, information access may need to be restricted due to the sensitivity, importance or the relevance of the content of the information, time, location and other contextual information obtained at the time the access requests are made. The proposed framework will be built upon role-based access control (RBAC) models. In this project the following tasks will be pursued:<br\/>development of a content and context-based generalized temporal RBAC model (CC-GTRBAC) for XML documents and extension of XML language for the proposed model. The extended language will be used to develop a security model that will allow protection of XML document sources at various levels including conceptual, XML schema and XML instance levels.<br\/>extending CC-GTRBAC to develop a secure multi-enterprise environment for distributed XML documents. <br\/>development of an experimental prototype of a distributed XML environment to check the efficacy and viability of this research.","title":"Content-Based, Context-Aware Role Based Access Control for Secure Distributed XML Applications","awardID":"0242419","effectiveDate":"2003-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"S097","name":"NIST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7228","name":"DATA AND APPLICATIONS SECURITY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V772","name":"NIST-SECURITY RESEARCH"}}],"PIcoPI":["438744","522969"],"PO":["427499"]},"82091":{"abstract":"Rapid increase of manufacturing costs of the integrated circuit (IC) and the widening gap between the design productivity and manufacturing capacity may prevent the IC technology from continuing to advance according to the Moore's Law. Programmable devices offer an attractive solution for significantly lowering the amortized manufacturing cost per unit and dramatically improving the design productivity through re-use of the same silicon implementation for a wide range of applications. However, existing programmable circuits and architectures are not power efficient. In some cases, they may consume 100x higher power compared to customized IC designs. This project focuses on research and development of power-efficient programmable circuit fabrics and architectures and associated synthesis tools, and is aimed to reduce the power dissipation by over 10x. <br\/>Our research will be guided by two key principles: (i) the use of a highly quantitative approach to circuit and architecture design and optimization; (ii) the integration of different levels of optimization techniques, from circuit-level to fabric-level and system-level, and finally synthesis tools, for developing novel programmable devices with the highest power efficiency. Outcome of this research includes: (1) A flexible power evaluation framework to enable accurate quantitative evaluation of programmable logic devices; (2) Novel circuits and fabrics to reduce leakage and dynamic power in programmable devices; (3) Novel implementation of clock gating and power gating for dynamic power management; and (4) effective and efficient algorithms and tools to support the aforementioned circuit-level, fabric-level, and system-level optimization techniques.<br\/><br\/>Broader impacts of this project include (1) Timely dissemination of research results and technology transfer to enable new applications of programmable devices that can significantly benefit the overall information technology (IT) industry; (2) Involvement of graduate and possibly undergraduate students in the proposed research and include the latest research methodologies\/results into teaching, which will help to train future researchers and engineers for further advancement of the information technology.","title":"Design and Synthesis of Power Efficient Programmable Fabric","awardID":"0306682","effectiveDate":"2003-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["558485","352589"],"PO":["562984"]},"83192":{"abstract":"The PI is investigating methods for jointly adapting transmitters <br\/>and receivers in a wireless network. The objective is to optimize<br\/>performance by measuring and exploiting properties of the channel <br\/>and interference. Parameters adapted at the transmitter include <br\/>data rate and power, along with transmitted signatures, which can <br\/>span dimensions across space, time, and frequency. An important<br\/>part of this effort is to account explicitly for limited feedback<br\/>from the receiver to the transmitter. Both cellular and non-cellular <br\/>(e.g., peer-to-peer) wireless networks are being considered. <br\/>In the latter case, the PI is investigating methods for allocating <br\/>rate and power across degrees of freedom associated with <br\/>multiple antennas and links in a multi-hop network.<br\/><br\/>Joint power and signature optimization problems are being studied<br\/>both with perfect channel knowledge at the transmitter, and with <br\/>limited feedback. Signature optimization serves to avoid interference while <br\/>simultaneously exploiting channel conditions.<br\/>Solutions to problems with perfect channel knowledge<br\/>provide benchmarks for comparisons with suboptimal transmission schemes, <br\/>e.g., when channels are unknown. Channel models of interest include <br\/>multiple access (reverse-link) and forward-link channels, and <br\/>the uncoordinated multi-input\/multi-output channel, which models <br\/>a peer-to-peer network. Transmitter optimization is being studied<br\/>in the context of both direct-sequence spread spectrum and<br\/>multi-carrier signaling. Implications for joint transmitter-receiver <br\/>adaptation over time-varying channels are being considered.<br\/>Transmitter designs, which can exploit link diversity in a <br\/>simple multi-hop channel model, are also being explored.","title":"Adaptive Transceivers for Wireless Networks","awardID":"0310809","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["560216"],"PO":["348215"]},"82082":{"abstract":"Abstract:<br\/>Over the last twenty years digital system design has gone from LSI circuits of about 10,000 gates to current designs with 10-20M gates. During this time the basic design abstraction has gone from graph based to language based using RT-level specifications. Despite numerous attempts, high level representations have not provided a practical mechanism for system level design abstraction. In this work, we are side stepping conventional behavioral approaches and are creating a new synthesis abstraction at the transaction architectural level. Fundamentally, transactions have constrained sequential and functional behavior but allow alternative construction and communication freedom. Using techniques developed for formal verification as a basis, the proposed framework promises to link architectural level modeling with sequentially constrained RT-level design. Thus, difficult trade-offs at the system level which require detailed sequential modeling can be represented within an optimization framework. The project will construct prototype tools producing RT-level output suitable for conventional synthesis as both driver and evaluation of the techniques.","title":"Transaction Level Modeling and Synthesis","awardID":"0306646","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["384054"],"PO":["562984"]},"78288":{"abstract":"ABSTRACT<br\/>0237037<br\/>Philip Schmiter<br\/>Ohio State University<br\/><br\/>The rapidly evolving global information infrastructure includes broadband wireless communication as a key component. Communication technology of the future will need to operate at the data rates, at high carrier frequencies, in high mobility situations, and under high levels of co-channel interference, all while maintaining reliable data transmission using small and inexpensive components. The fundamental challenge for future wireless technology stems from the harsh characteristics of the wireless propagation channel.<br\/>In specific, reflections from objects in the physical environment induce rapidly changing echo patterns which make it difficult for the receiver to recover the transmitted information without error. Especially challenging are doubly-selective channels, i.e., channels which echo or blur the transmitted information significantly in both the time and frequency domains. Since, these characteristics arise when high-rate information streams are propagated at high frequencies in highly mobile environments, the success of future wireless applications hinges on our ability to communicate information reliably over the doubly-selective channel.<br\/><br\/>This research investigates practical signaling, detection, and estimation strategies for spectrally-efficient broadband data communication over doubly-selective wireless channels. Orthogonal frequency division multiplexing, known for its advantages in time-non-selective fading, is used as a starting point in the <br\/>study of practical data detection and channel estimation methods. Extending these ideas, the investigators focus on non-orthogonal multicarrier signaling schemes which are specifically designed to optimize detection performance under receiver complexity constraints.","title":"CAREER: Signal Processing for Practical Data Communication over the Doubly-Selective Wireless Channel","awardID":"0237037","effectiveDate":"2003-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["518451"],"PO":["564898"]},"82391":{"abstract":"The unnaturalness of synthesized speech found in current interactive voice response (IVR) systems is due to the lack of natural prosodic variation. While state-of-the-art IVR systems are often highly intelligible and may sound natural for short prompts or when the text to be spoken is close to speech recorded for the system's database, once they deviate from these narrow bounds, results range from \"boring and mechanical\" to \"odd and confusing.\" To address these deficiencies, the PIs are developing a new method for learning contour assignment for dialogue systems that avoids the sparse data problem without massive new annotation. They have generated a series of hypotheses about which features of the dialogue context influence human speakers' choice of contour from corpora and are testing these hypotheses via a series of targeted laboratory experiments. By designing a carefully controlled set of production and perception studies, the PIs will be able to determine which intonational features prove to be most reliably correlated with contour choice and which are perceptually most salient for listeners.<br\/><br\/>From a practical viewpoint, an IVR system that incorporates the appropriate assignment of full intonational contours will greatly enhance the perceived naturalness of the system. From a scientific viewpoint, such a model will expand our understanding of how speakers use and hearers interpret intonational contour variation. From a social viewpoint, the creation of IVR systems that interact with users naturally will increase the acceptability of such systems, bringing the vision of ubiquitous access to information and services for all closer to reality.","title":"Dialogue Prosody in Interactive Voice Response Systems","awardID":"0307905","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7274","name":"HUMAN LANGUAGE & COMMUNICATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["374983","511614"],"PO":["565215"]},"82281":{"abstract":"Mobile, location-aware devices raise the potential for fundamentally new information services. However, this potential has not been realized. This is due in part to the absence of a firm conceptual and empirical<br\/>foundation. This proposal explores one basis for constructing the requisite foundation: socially defined places. How places (such as schools, offices, or theaters) shape behavior has been explored in environmental<br\/>psychology and architecture, but the notion of 'place' has not been operationalized for use in interactive systems. This project takes on that task. The key hypotheses are that (a) people's information and<br\/>communication needs are relative to place types, and (b) making 'place' a first-class computational object will increase the effectiveness and usability of location-based systems. This project will test the hypotheses<br\/>through a combination of ethnographic studies, development of novel algorithms and interfaces, and laboratory and field studies, leading to the following results: 1) additional empirical knowledge about the concept of place and its role in organizing people's activities; 2) a conceptual framework and guidelines useful to designers of location-based systems; 3) a general infrastructure for place-centered community information sharing systems; and 4) field studies and laboratory evaluations that demonstrate the utility<br\/>and acceptability of such systems.","title":"Collaborative Research: Mark This! - Operationalizing the Notion of \"Place\" For Interactive Community Systems","awardID":"0307459","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["518491"],"PO":["564456"]},"83271":{"abstract":"Fiber-optic links have the potential of terabit per second transmission. Long-haul links, such as transoceanic and transcontinental data-trunks, have demonstrated a need for such high data throughput. System designers are able to provide the high data throughput using a technique known as dense wavelength division multiplexing (WDM), where multiple lower rate signals are combined unto a single fiber by using several wavelengths of light. Long-haul WDM<br\/>fiber channels are performance limited by linear dispersion and nonlinear crosstalk. The nonlinear effects create a correlation between the signals received at different wavelength channels. In this research multiuser detection schemes to exploit this dependence and jointly demodulate multiple channels are developed. Multiuser detection has attracted enormous attention in RF multiuser communications in the last twenty years. They are powerful at combating the harmful effects of interference from other users in the system. In so doing they allow the channel capacity per user to<br\/>increase from that possible in single-user communications. The same advantages are suspected to persist in applying these detectors to WDM fiber systems. Many aspects of the physical model of the fiber system differ from the RF model. Because of the fiber nonlinearities and the photodetector processing, the interference and dominant noise are signal dependent, giving a very different statistical description of the received signal. Moreover, the modulation and the signal detection act only on the intensity of the signal, prohibiting the use of the signal phase at either end.<br\/>Therefore, multiuser detectors developed for RF communications cannot be used directly for the fiber problem.<br\/><br\/>In this research, optimal and suboptimal detectors such as maximum likelihood, linear, quadratic, and decision feedback detectors are designed to suit fiber-optic systems. Special attention is placed on the hardware requirements of each design to ensure that it can be implemented at 10 or even 40 gigabits per second, as required for these types of systems. A system of small simultaneous detectors over small groups of adjacent channels allows the most significant nonlinear crosstalk to be cancelled without requiring a prohibitive computational complexity. Detectors must also<br\/>operate on data with very coarse resolution. The performance of these detectors can be measured using variants of minimum distance, bounds on the error probability, calculations of the asymptotic multiuser efficiency, and simulation. By using these techniques, system designers will be able to increase the span length between fiber amplifiers to more than the 100 km currently possible, increase the channel density by decreasing the channel spacing to below the current<br\/>20 GHz limit, and improve the quality of service of each channel to below an error-rate of 10- 12 .","title":"Multiuser Detection for WDM Long-Haul Fiber-Optic Communication Systems","awardID":"0311198","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["523508"],"PO":["348215"]},"82061":{"abstract":"In this project the problem of approximating unknown functions with some shape constraints is being investigated. As a central tool properties of a class of functions known as positive polynomials are being investigated and utilized. The problem of approximation of unknown functions with shape constraints has many potential applications some of which is being developed and formulated in the context of this project. A sample of problems being investigated includes: shape constrained regression in statistics with applications in economics and finance (for instance estimating demand curve which is necessarily a decreasing function, or utility curves which are convex and decreasing); estimation of probabilities or odds of events based on some knowledge of their moments or frequency samples; visualization of data in the form of space curves and surfaces with shape constraints such as direction of curvatures or convexity. The study is related to an important and relatively new area called semidefinite programming, a field of mathematical optimization theory. While the problem of finding best positive polynomials can be cast as a semidefinite programming problem, the process may not be the most efficient way. In this project tailor made techniques are being developed to deal directly with positive polynomials leading to more efficient methods than using semidefinite programming methods directly.<br\/><br\/>In this project two mathematical objects are being investigated. One is called \"positive polynomials\" and the other \"moment cones\". These objects have applications in several areas including statistics, economics, finance, various areas of engineering and two and three dimensional computer visualization of data. The main application of the objects of this study is in approximation and computer representation of behavior of unknown objects. The new problem that is being investigated is the imposition of \"shape constraints\" on the unknown phenomena and requiring that the computed approximation conforms to these constraints. For instance, in econometrics estimating the demand curve in some market from a scatter of sample data may yield an approximate demand curve that does not uniformly move down as price goes up. The properties of \"positive polynomials\" and \"moment cones\" are investigated to assist in computer shape constrained approximation of unknown objects that arise in fields such as economics, engineering and data visualization.","title":"Optimization over Positive Polynomials and Moment Cones: an Algorithmic Study with Applications in Approximation Theory, Regression and Data Visualization","awardID":"0306558","effectiveDate":"2003-07-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["423516"],"PO":["399214"]},"83293":{"abstract":"The objective of the proposed research is the development of algorithmic <br\/>methods for design verification. Today's rapid development of complex and <br\/>safety-critical systems requires reliable verification methods. Formal <br\/>verification is the study of algorithms and structures applicable to the <br\/>verification of hardware and software designs. It combines theoretical <br\/>and experimental aspects. In the last few years, this area has seen a <br\/>dramatic expansion of activities. Verification tools are incorporated <br\/>into industrial development of new designs, forming an active and exciting <br\/>area of research where theory and practice stimulate each other. <br\/><br\/>The intellectual merit of this poject is the application of the<br\/>automata-theoretic approach to design verification, which uses the theory<br\/>of automata as a unifying paradigm for design specification,<br\/>verification, and synthesis. The automata-theoretic<br\/>perspective considers the relationships between designs and their<br\/>specifications as relationships between languages. By translating<br\/>design and logical specifications to automata, questions about<br\/>programs and their specifications can be reduced to questions about<br\/>automata. More specifically, questions such as satisfiability of<br\/>specifications and correctness of designs with respect to their<br\/>specifications can be reduced to questions such as non-emptiness and<br\/>containment of automata. The automata-theoretic approach separates <br\/>the logical and the combinatorial aspects of reasoning about systems. <br\/>The translation of specifications to automata handles the logic and shifts <br\/>all the combinatorial difficulties to automata-theoretic problems, <br\/>yielding clean and asymptotically optimal algorithms. <br\/>Furthermore, automata are very helpful for implementing<br\/>temporal-logic based verification methods, and are the key to <br\/>techniques such as on-the-fly verification<br\/>that help coping with the ``state-explosion'' problem.<br\/>Automata-theoretic methods have been implemented in both <br\/>academic and industrial automated-verification tools.<br\/><br\/>Many questions in the theory of automata on infinite objects are still<br\/>open, and more fruitful applications of automata theory in design<br\/>verification are possible. This projects aims at solving several<br\/>automata-theoretic problems with clear application to design<br\/>verification, and at develoing automata-based verification methodologies.<br\/>The broad impact of this project is the contribution to the<br\/>efforts of developing and improving formal verification methods,<br\/>constituting an additional step toward formal verification of<br\/>industrial real-life designs.","title":"Automata-Theoretic Approach to Design Verification","awardID":"0311326","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["565263"],"PO":["499399"]},"82083":{"abstract":"The research dramatically improves file system performance and reliability <br\/>by using Magnetic RAM as a permanent store for both file system indexes <br\/>(metadata) and data. Keeping metadata in MRAM increases the speed of <br\/>metadata operations by more than an order of magnitude while decreasing <br\/>software complexity and improving integrity. This research also <br\/>investigates dynamic memory-based algorithms for metadata management, <br\/>reducing metadata size and complexity while increasing metadata <br\/>flexibility. The research explores the effects of varying MRAM sizes and <br\/>performance characteristics on file system performance and the choice of <br\/>file system algorithms.<br\/><br\/>The higher file system performance and higher reliability made possible by <br\/>this research has several far-reaching impacts. First, file system <br\/>performance improvements of an order of magnitude dramatically reduces file <br\/>server costs, making storage more affordable. Second, the reduced code <br\/>complexity enabled by the use of memory-based algorithms permits faster <br\/>development of simpler file servers. Third, the increased reliability from <br\/>lower code complexity and continuous integrity checking allows the <br\/>production of much more reliable file servers. The research is also <br\/>applicable to systems built with other emerging non-volatile memory <br\/>technologies such as FeRAM and new types of Flash RAM.","title":"Increasing Storage Performance and Reliability with Magnetic RAM","awardID":"0306650","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["540845","557751"],"PO":["543507"]},"87792":{"abstract":"EIA-0333028<br\/>Edward Fredkin<br\/>CMU<br\/><br\/>SARS Think Ahead<br\/><br\/>Project Summary<br\/><br\/>SARS-TA is an analysis and forecasting system for staying ahead of a potential SARS pandemic. It is a new methodology for explaining and communicating the consequences of the various alternatives that governments will face in the battle against SARS. A set of Excel spreadsheets tracks up-to-date historical data for every country in the world. That data, along with various forecasts of the future course of the epidemic based on standard epidemiological models, are displayed in meaningful ways. Clear graphs show the predicted effects of actions and inactions that a particular governmental authority might consider taking. SARS-TA provides a new perspective as to what is likely to happen in the world and in each country. It is designed to provide both understanding and motivation to those in government who must make difficult decisions in a timely fashion. SARS-TA has the capability of demonstrating the likely consequences of a lack of preparedness or of delays in the implementation of specific recommendations.<br\/><br\/>The system is designed to access a special SARS-TA database on the web. The database is updated with the latest epidemiological data from The World Health Organization. Each day the up-to-date data can be downloaded and automatically incorporated into every user's copy of SARS-TA. In addition to giving an accurate picture of what is happening in the world and in every country, this historical data plays a crucial role in the functionality of the system. The purpose of SARS-TA is to give health officials in every country, region and city a better way of understanding the current state of the potential SARS problem, along with interactive calculations and graphical outputs that can better communicate to political leaders who must decide and approve of the actions that may be needed to economically bring a SARS outbreak under control.<br\/><br\/>Different regions of the world have differing amounts of resources and capabilities. There cannot be one best solution for how to address a SARS outbreak that will work in every part of the world. Further, many such regions are ill equipped to be able to design and carry out the best program suitable to their country's particular problems and limitations. What SARS-TA does is to allow a sophisticated numerical evaluation of proposed efforts so as to determine the likely consequent numbers of SARS cases and deaths, along with the total financial burdens, both as direct and collateral results of an epidemic, including the costs of efforts to control the epidemic.<br\/><br\/>SARS-TA addresses a hitherto unsolved intellectual problem: how to optimize various government decision making processes when there is only probabilistic data available to determine the consequences of those decision and when time is of the essence. What we know is that with sufficient resources, wealthy countries often cope very well, while those governments with scarce resources are very often unable to make efficient use of the resources they have. It is clear that systems similar to SARS-TA could be very useful in other epidemics or in the case of other situations where governments need to react quickly despite having incomplete knowledge of what they are facing and of the consequences of their actions.","title":"SARS - Think Ahead","awardID":"0333028","effectiveDate":"2003-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1994","name":"BIOINFORMATICS PROGRAM"}}],"PIcoPI":["254137"],"PO":["565136"]},"87110":{"abstract":"Robotics and Computer Vision Program<br\/><br\/>ABSTRACT<br\/><br\/><br\/>Proposal #: 0328782<br\/>Title: Study Contact States and Compliant Motion between General Objects Critical to Real and Virtual World Applications<br\/>PI: Xiao, Jing<br\/>U of NC Charlotte<br\/><br\/>For real-world robotic operations involving contacts or compliant motion as well as for certain virtual-world applications such as dynamic simulation and haptic interaction, two important and related problems regarding information of contacts among general objects (including robots) remain largely unsolved: <br\/>(1) how to obtain knowledge of contact states effectively and efficiently for handling motion involving complex contacts, and <br\/>(2) how to enable automatic on-line (real-time) identification of geometrically valid contact states in the presence of real-world uncertainties or digital approximation errors in virtual environments to ensure valid subsequent action or response. <br\/>Problem (1) is particularly crucial and challenging for tasks involving a large number of complex contact situations and high-dimensional motion, and Problem (2) is especially critical to tasks with high accuracy requirement, including robotic assembly, manipulation, dynamic simulation, virtual prototyping, and certain haptics applications. This project seeks to find principled solutions to these problems through systematic investigation of a number of fundamental issues related to contact states and compliant motion involving general objects, which include objects with curved surfaces, articulated objects, and certain deformable objects. As an integrated part or derivative of such basic investigation, the project will further study: physically accurate haptic display of contact states in virtual environments, and general compliant motion planning and execution in real world, as well as their related applications.<br\/> <br\/>The general problems addressed in this project (as introduced above) are crucial to advancing the state of the art in several related areas and their many applications, including robotics, haptics, and dynamic simulation, and yet there has been little principled and systematic research towards these problems. The PI has studied aspects of the problems restricted to contacts between only polyhedral rigid bodies. Hence, the project represents a very significant, multi-dimensional extension with new and substantial challenges, but not without the considerable foundation built upon the PI's related previous work. <br\/><br\/>The project research activities, aside from its expected contributions to both basic research and a wide range of related applications, will also have a significant impact on student education and research training. The interdisciplinary breadth of the research activities spans many areas in robotics and control, AI and intelligent systems, computational geometry, geometrical and physical modeling and simulation, haptics, computer graphics and virtual environments, as well as human-machine interaction and virtual collaboration. The project is also comprehensive with both theoretical and algorithmic components and experimental and system integration components. It will thus provide a rich and balanced environment that can accommodate students from all levels (from Ph.D. to undergraduate) to obtain knowledge and research training in the various related areas. <br\/><br\/>The research project will also provide an excellent opportunity for (a) cross-discipline research collaboration between the PI and her colleagues at the newly established College of Information Technology (COIT) at UNC Charlotte, and (b) close international collaboration between the PI's group and a leading research group on compliant motion control in Europe. Such collaborations will be instrumental to enhancing research capabilities as well as research infrastructure at the new COIT. The project will clearly enrich student education and contribute significantly to the special goal of COIT of providing students an interdisciplinary learning and research experience in the broad spectrum of Information Technology.","title":"Study Contact States and Compliant Motion Between General Objects Critical to Real and Virtual World Applications","awardID":"0328782","effectiveDate":"2003-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":["475621"],"PO":["403839"]},"89552":{"abstract":"Over the past decade, our ability to produce graphical<br\/>images has improved to the extent that we can create<br\/>imaginary scenes that are virtually indistinguishable from<br\/>reality. Digital humans have been called the last frontier<br\/>in this march to graphical realism, and the area of human<br\/>animation has also seen dramatic developments. Increasing<br\/>use of motion capture data and new techniques for<br\/>manipulating that data allow us to reproduce human motion at<br\/>an extremely high level of fidelity. Graphically generated<br\/>characters in video games and films can seem uncannily real.<br\/>Pending the development of easy-to-use tools for directing<br\/>digital humans, we should soon see digital humans as<br\/>plausible user interfaces, and animated characters will<br\/>become much more prevalent in education, demonstration, and<br\/>training applications. If digital humans are the last<br\/>frontier in realistic computer graphics, the last frontier<br\/>in realistic digital humans is generating believable hand<br\/>motion. Human hands are beautiful and complex mechanisms,<br\/>amazing in their utility and adaptability. It is argued that<br\/>it is our hands that make us human, and that hand evolution<br\/>was a primary factor in the development of intelligence.<br\/>Hand use in autonomous digital human characters, however, is<br\/>generally quite unconvincing. Hands may be placed in a<br\/>single frozen pose, and interaction between characters and<br\/>objects is avoided when possible. The main problem is that<br\/>geometric models of the human hand have far too much<br\/>flexibility. This flexibility makes working with hands<br\/>difficult even for trained animators, and it poses a<br\/>tremendous challenge for creating autonomous characters that<br\/>must interact with their environment. I believe that the key<br\/>to making further progress in hand motion for digital<br\/>characters is much more detailed consideration of the<br\/>anatomy of the human hand. In analysis of human grasps, for<br\/>example, critically important considerations include the<br\/>amount of contact between finger pads, palm, and object; the<br\/>ability of muscles to produce or resist task force; and the<br\/>stabilization roles of fingers and muscles, yet none of<br\/>these issues have been explored in grasp synthesis research<br\/>in either the robotics or computer graphics communities. In<br\/>pursuit of the goal of believable hand use for digital<br\/>characters, we propose an anatomy-based model of human<br\/>grasping. In particular, we propose a tendon-based quality<br\/>measure for humanlike enveloping grasps, and we plan to<br\/>evaluate this quality measure (1) for ability to<br\/>discriminate between grasps, (2) as a predictor of grasp<br\/>forces, and (3) for use in modeling grasp acquisition.<br\/>Because of the strong emphasis on human anatomy, this<br\/>research has the potential for additional impact outside<br\/>graphics and animation in areas including ergonomics (tool<br\/>design), robotics (robot hand design), and anthropology<br\/>(research in human hand evolution and tool use). The<br\/>educational portion of this proposal focuses on teaching and<br\/>mentoring of undergraduates. The research ideas, techniques,<br\/>and results will be incorporated into a course at Brown that<br\/>attracts both un-dergraduate and graduate students, and will<br\/>provide them with an opportunity to learn and experiment in<br\/>a problem domain that is a nice mix of computational<br\/>geometry and numerical optimization, grounded in human<br\/>anatomy and supported by data. A special effort will be made<br\/>to include undergraduate women in the research program, for<br\/>example, through the CRA Distributed Mentor Program.<br\/>Research results will include a library of example grasps<br\/>and applied forces, as well as a tool for adapting the<br\/>examples to new hand and object geometries. Once this<br\/>research is published, the data and tools will be made<br\/>available to other researchers on the web, and should serve<br\/>as a useful resource for creating digital characters for<br\/>education, entertainment, and training applications.","title":"CAREER: Quantifying Humanlike Enveloping Grasps","awardID":"0343161","effectiveDate":"2003-07-01","expirationDate":"2007-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["560865"],"PO":["532791"]},"78475":{"abstract":"0237835 <br\/>David Beazley<br\/>Univ of Chicago<br\/><br\/><br\/>Scripting languages are an increasingly important part of many software projects. A key feature of scripting languages is that they can be integrated with existing programs, typically written in C or C++. This integration of languages, commonly known as extension programming, is often achieved through the use of automatic code generation tools. Unfortunately, the development of these tools has been relatively ad-hoc and uncoordinated. The goal of this project is to provide a more formal foundation for extension programming by<br\/>recasting the problem in terms of type systems. This foundation will then be used to expand the scope of extension programming tools to include software reliability mechanisms such as contracts. Finally,<br\/>the research will investigate support tools such as mixed-language debuggers. The research will be conducted in the context of the SWIG project--an extension programming system already used by thousands of<br\/>programmers in industry, government, and academia. As a result, the work will directly impact these users by pushing the extension building problem in new directions. Moreover, the research aims to bridge the practice of extension programming to more formal aspects of programming languages, software architecture, and software engineering.","title":"CAREER:Type Systems and Next Generation Tools for Scripting Language Extension Programming","awardID":"0237835","effectiveDate":"2003-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":[204574],"PO":["564388"]},"78376":{"abstract":"ABSTRACT<br\/>0237422<br\/>Cesare Tinelli<br\/>Univ of Iowa<br\/><br\/>The unifying research and education theme of this project is improving the reliability of software by cost-effective automated reasoning techniques and tools. The project concentrates on extended<br\/>static checking (ESC), a powerful form of static checking aimed at detecting insidious run-time errors such as null-dereferencing and out-of-bounds errors. Because of its limited goal of just uncovering<br\/>as many run-time errors as possible, ESC is easier to automate effectively than full software verification. However, its wide adoption is presently hindered by the low speed of existing checkers.<br\/><br\/>The overall research objective of this project is to develop new automated reasoning techniques and tools for ESC, with the expectation that they will make ESC an attractive and cost-effective technology for software development. Routine use of ESC will help uncover more errors during the debugging phase of the software life cycle, leading to a more reliable final product.<br\/><br\/>The overall education objective of the project is to expose students to specification\/verification techniques and tools that have proved effective with real-word problems. A new generation of professionals<br\/>better prepared and predisposed to the use of these tools will facilitate their adoption by the software industry, making it possible for our society to reap the benefits of more reliable software.","title":"CAREER: Fast Provers for Extended Static Checking of Software","awardID":"0237422","effectiveDate":"2003-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["521573"],"PO":["564388"]},"83591":{"abstract":"ABSTRACT<br\/>0312696<br\/>Lazzi, Gianluca<br\/>North Carolina State<br\/><br\/>Most wireless communication systems currently employ arrays of single- or dual-polarized <br\/>antenna elements, each which is capable of measuring at most two components of the electromagnetic signal. Recent results in communication theory suggest that, in a rich scattering environment, the capacity of wireless links can be dramatically improved by employing co-located, co-polarized antennas that can detect and excite additional components of the electromagnetic field. <br\/> <br\/>Intellectual Merit: The aim of this project is to improve the performance of wireless communications through the use of arrays of vector sensors that can detect or excite up to 6 components of the underlying electromagnetic field. This project is an interdisciplinary effort between radio frequency antenna design and communication theory. Four main issues are addressed: (a) a new class of compact, planar vector-sensor antennas suitable for wireless communication, (b) new computational tools for accurately predicting the performance of such antennas, (c) an information-theoretic study of the capacity of vector-sensor communication systems, in order to quantify their advantages and extract insights into array design, and (d) new modulation, error-control coding, and receiver architectures that exploit the additional <br\/>information provided by these antennas. The ultimate goal of this work is to more fully exploit the potential of wireless radio channels, and in the process reduce the power and bandwidth requirements of wireless communication. <br\/> <br\/>Broader Impact: This project is a joint effort between two investigators with established track records in antennas and wireless communication and a successful history of collaboration. This research will have a direct impact on our undergraduate and graduate students by exposing them to an interdisciplinary view of antenna design and wireless communications, seen as an indivisible concept rather than as separate engineering disciplines. This activity further has the potential to significantly impact the next generation of commercial wireless systems, by offering new approaches to reducing power and bandwidth requirements, and by generating a wealth of new concepts in antenna and communication system design.","title":"ITR: A New Class of Vector Sensing Antennas for Wireless Communications","awardID":"0312696","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["560179","509517"],"PO":["564898"]},"82392":{"abstract":"Previous work on adaptive websites, wearable computing and intelligent user interfaces has shown that these tasks present significant challenges to the fields of machine learning, knowledge representation, and reasoning under uncertainty. This project will address the following core artificial intelligence problems using adaptive interfaces as inspiration and an experimental testbed.<br\/><br\/>1) Given a database of behavioral data for one or more users, what is the best representation for encoding a predictive model of user behavior? What are the best algorithms for learning such a model? This project will generalize Markov models and Dynamic Bayes Nets to create Relational Markov Models (RMMs) and Dynamic Probabilistic Relational Models (DPRMs) respectively. Effective inference and learning algorithms will be developed and evaluated against traditional propositional methods.<br\/><br\/>2) Representing user interfaces is a major challenge. This project will extend the work on task-centered user-interface design with ideas from the planning literature (sensory actions, exogenous events) to develop an expressive task formalism with clear semantics.<br\/><br\/>3) Adapting an interface, which is represented as an augmented plan schema, requires new methods for reasoning about actions. In addition to analyzing causal dependency structures, restructuring operations akin to partial evaluation will be necessary. Fast inference is an essential component of this project. A satisficing plan is not good enough, so the work will use a utility model combining plan length with a cognitive dissonance factor. <br\/><br\/>Methodologically, the project is composed of six coupled activities: (1) Formalize the RMM and DPRM representations; (2) Devise efficient particle-filtering inference methods; (3) Develop learning algorithms based on shrinkage; (4) Formalize a declarative, plan-based interface representation, and evaluate expressiveness on a corpus of adaptation examples; (5) Devise a comprehensive set of adaptation transformations and a utility metric; (6) Implement the methods, incorporate in a user interface platform, and perform extensive experiments. <br\/><br\/>The research will have broad impact, because progress in user interfaces has been dwarfed by the simultaneous enormous increase in the speed of computers. Artificial intelligence techniques are perhaps the most promising avenue for harnessing processing power to increase user productivity. This project will contribute to improved user interfaces not only in desktop software but also in personalized information systems for wearable computers.","title":"Representation and Reasoning about Adaptive Interfaces","awardID":"0307906","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["450900"],"PO":["387198"]},"83272":{"abstract":"DISE: A New Hardware-Software Interface for Customizing Application Execution<br\/><br\/>Abstract<br\/><br\/>The growing diversity of computing platforms (laptop, handheld, phone, etc.) has necessitated application customization functions (ACFs), utilities that customize the execution of applications to the environments in which they run. ACF examples include safety checking, profiling, dynamic optimization, decompression, and bug patching. ACFs are used to make applications more reliable, more secure, and more easily developed, essential characteristics of next generation software systems. Unfortunately, ACFs are often not used because of their runtime cost, complexity, or deployment burden.<br\/><br\/>We propose dynamic instruction stream editing (DISE) as an efficient, convenient, and practical hardware\/software mechanism for realizing ACFs. In DISE, an ACF is formulated as a set of dynamic instruction stream transformations and programmed via a set of rules (macros) for replacing instructions that match certain criteria with instruction sequences. The processor's decoder executes these specifications on the application, feeding the execution engine a modified instruction stream that includes ACF code. Inserting ACF code at the decode stage admits ACFs that modify (not just observe) application behavior, and sidesteps many of the costs of inserting ACF code into the application's static image first. DISE is a single mechanism that unifies the implementation of a large class of ACFs. <br\/><br\/>This research will prove the utility of DISE by implementing and evaluating a diverse set of ACFs, such as support for debugging, runtime code generation, and security\/safety. This unique organization will positively impact the way applications and ACFs are engineered, used, and deployed, ultimately improving the reliability, efficiency, and security of software.","title":"DISE: A New Hardware-Software Interface for Customizing Application Execution","awardID":"0311199","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["451152","236924"],"PO":["325495"]},"79466":{"abstract":"Many of today's mission critical databases have not been designed with a particular focus on security aspects such as integrity, confidentiality, and availability. Even if security mechanisms have been used during the initial design, these mechanisms are often outdated due to new requirements and applications, and do not reflect current security polices, thus leaving ways for insider misuse and intrusion. The proposed research is concerned with analyzing various security aspects of mission critical (relational)databases that are embedded in complex information system infrastructures. We propose four complementary avenues of research: (1) models and techniques to profile the behavior of mission critical data stored in databases, (2) algorithms to correlate (anomalous) data behavior to application\/user behavior, (3) techniques to determine and model user profiles and roles from behavioral descriptions, and (4) the integration of techniques, algorithms, and mechanisms into a security re-engineering workbench for (relational) databases. Two major themes build the core of the proposed approaches. First, the analysis of database vulnerabilities and violations of security paradigms is data-driven, i.e., first the behavior of the data is analyzed and modeled before it is correlated to users and applications. Second, we introduce the concept of access path model to uniformly model and correlate data flow and access behavior among relations, users, and applications. This model allows security personnel to fully inspect database security aspects in complex settings in a focused, aspect (policy) driven fashion.","title":"Security Analysis and Re-engineering of Databases","awardID":"0242414","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7228","name":"DATA AND APPLICATIONS SECURITY"}}],"PIcoPI":["521752","388358"],"PO":["469867"]},"82470":{"abstract":"Infrastructures are commonly conceptualized as large-scale projects created and developed by governments (e.g., when they build roads) or other large enterprises (e.g., when they build large systems for telecommunications). Historically, rural co-operatives also built roads and farmers were known to develop their own phones --although not very good ones. This project analyzes the development of wireless ad hoc Internet services to determine whether or not a decentralized communication infrastructure is a viable alternative to centrally developed systems. Specifically, \"Wi-Fi\" technology (also known as 802.11 networking) will be studied. The project will measure the spread of disorganized ad hoc Internet services in two neighborhoods, distinguished by their socio-economic status, in a mid-western city. The \"Wi-Fi\" networks will be mapped to record their spread and coverage over time. This project will elucidate the role of amateurs and cooperatives in the creation of communication systems, and will contribute to theories of communication infrastructure development and the comprehensible visualization of technical phenomenon. It has direct application to communication policy, and will support the cross-training of students in engineering and the social sciences.","title":"Wireless Ad Hoc Networks: Understanding Chaotic Communication Infrastructure","awardID":"0308269","effectiveDate":"2003-07-15","expirationDate":"2005-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["529185"],"PO":["564456"]},"87981":{"abstract":"The purpose of this workshop is to provide a forum for discussion of current directions in dialog research, specifically to assess the current state of the art in the area of dialogue processing and to identify key themes and directions that are driving research in the field. Progress in research often relies on a common infrastructure that includes tools and corpora, evaluation techniques, as well as some consensus on effective research paradigms. Thus one of the outcomes of the workshop will be a set of recommendations for developing and supporting infrastructure and encouraging sharable research paradigms and evaluation methodologies. The motivation to hold a workshop at this time is the need to establish the role of dialog as a core element in human-human and human-computer communication and to identify the resources that are needed to support research in the area.","title":"Workshop on Directions in Automatic Dialog Processing","awardID":"0334250","effectiveDate":"2003-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7274","name":"HUMAN LANGUAGE & COMMUNICATION"}}],"PIcoPI":["338056"],"PO":["565227"]},"82173":{"abstract":"This project continues research on the computational complexity of feasible computations, with the larger goal being to distinguish complexity classes that contain feasibly computable problems from those that do not. The emphasis is to continue investigation into fundamental areas of computational complexity theory.<br\/><br\/>The project advances research on disjoint NP pairs. Disjoint NP pairs are technically equivalent to promise problems, and they were studied previously because of their relevance to security issues in cryptography. This project investigates their relevance to propositional proof systems, and solves problems that add insight to questions about proof systems.<br\/><br\/>The project continues to investigate multivalued functions as a model of computation. Whereas it has been common practice to study the complexity of computational problems by focusing on decision problems alone, it is frequently more natural to study the computational complexity of problems directly as multivalued partial functions. <br\/><br\/>Average-case complexity theory provides a framework for the classification of problems according to average-case analyses. This project continues to develop the theory of average-time complexity, for the average-case complexity of a problem is frequently a more important measure than its worst-case complexity.<br\/><br\/>Regarding the broader impact of this activity, the PI offers advanced graduate courses and seminars that include the results of this project. The PI's graduate students participate fully in the activities of this project. All results are disseminated broadly. Results are submitted to major scientific symposia and to refereed journals","title":"Complexity of Feasible Computations","awardID":"0307077","effectiveDate":"2003-07-01","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[214271],"PO":["499399"]},"85484":{"abstract":"This project, developing, replicating, distributing, and using a new magnetic levitation haptic interface system, provides direct electrodynamic interaction with a single moving part permitting high-fidelity six-degree-of-freedom (6-DOF) sensing and force\/torque capability (unlike currently available haptic systems which resemble small, back-driven robot arms with motors, encoders, pivots, links, and transmission elements). Haptic interfaces allow computer users to interact mechanically, as well as visually, with computed information. With the new method, the user grasps the freely-levitated handle (manipulandum) of a desktop-high device, maneuvering it in 6-DOFs to provide position and force\/torque information to a physically-based 3D simulated environment with gravity, hard contact, flexible deformation, friction, and texture attributes. The running simulation provides 6-DOF force\/torque output to the manipulandum, and consequently to the hand. Both the proprioceptive (kinesthetic) senses of the fingers, hand, and wrist as well as the tactile senses in the skin are involved in the interaction. The prototype magnetic levitation haptic system provides higher bandwidths and resolutions than other existing techniques-an important consideration for conveying subtle friction and texture information to the user. Dramatically reducing cost, this project will greatly improve the performance of the current prototype system, replicating eight new systems using state-of-the-art manufacturing techniques. The developed systems will be distributed to seven haptic researchers in the nation. The new systems provide a basis for supporting seven new independent research efforts. Thus, eight projects, involving six universities, form part of the project.<br\/><br\/> Hollerback, Utah: investigating and displaying manual control dynamics, assisting in determining how humans interact to assemble, disassemble, and manipulate CAD objects<br\/> Hollis, CMU: understanding two-handed manipulation and investigating whether blind persons can benefit from haptic communication<br\/> Howe, Harvard: cost-benefit tradeoff for haptics as a function of frequency response, providing insights into fundamental tactile perception and motor control mechanisms and guidelines for cost-effective haptic interfaces<br\/> James, CMU: new deformable rendering algorithms for a large class of flexible models enabling technology for computer graphics and virtual environment simulation<br\/> Khatib, Stanford: new control algorithms for haptic display enable new desktop haptic applications <br\/> Pai, Rutgers: new kinds of audio-haptic interfaces with tightly synchronized sounds and contact forces enhancing understanding of human perception of contact rendered using a haptic interface<br\/> Tan, Purdue: understanding perceptual dimensionality, texture perception, & multimodal rendering of information<br\/><br\/>Many of the research efforts involve undergraduate students, some from under-represented groups. High-fidelity haptics has enormous potential for K-12 education.","title":"MRI: Instrument Development: High-Fidelity Magnetic Levitation Haptic Systems","awardID":"0321057","effectiveDate":"2003-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2885","name":"CISE RESEARCH INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["562786","402282","455506","524687","513354"],"PO":["565227"]},"86012":{"abstract":"0323510<br\/>Ronald Breaker<br\/>Yale University<br\/><br\/>Engineering Genetic Control Mechanisms of Riboswitches<br\/><br\/>Precision genetic control is an essential feature of living systems, as cells must respond to a multitude of biochemical signals and environmental cues by varying genetic expression patterns. Most known mechanisms of genetic control involve the use of protein factors that sense chemical or physical stimuli and then modulate gene expression by selectively interacting with the relevant DNA or messenger RNA sequence. Recently, it has been discovered that portions of certain mRNAs, called \"riboswitches\", serve as genetic regulation elements by forming direct RNA-metabolite complexes. This newly established form of genetic control is a critical mechanism by which biological information is accessed. Furthermore, the design of new RNA switches for use in diverse applications such as molecular sensing, designer genetic control and molecular computing could be enhanced by reverse engineering of natural riboswitches. <br\/>A rigorous investigation of the biochemical properties of several metabolite-dependent riboswitches is being conducted in order to expand our understanding of the various mechanisms used by mRNAs to achieve metabolite recognition and genetic control. Several powerful biochemical techniques including an \"in-line probing\" assay is being used to give a detailed assessment of the structural changes that occur when ligands interact with allosteric RNAs. This will permit the development of a deeper understanding of how RNA acts to control genetic information. Furthermore, mutation analyses and genetic expression assays is being used to define the precise mechanisms by which ligand binding events are converted into control of genetic information expression.<br\/>Finally, riboswitches are engineered to respond to other effector compounds that provide a means to more conveniently and thoroughly manipulate gene control in living systems. Such advances is providing enabling technologies for creating molecular computation systems for in vitro and in vivo uses, and are synergistic with ongoing efforts to create RNA logic gates.","title":"Engineering Genetic Control Mechanisms of Riboswitches","awardID":"0323510","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["224941"],"PO":["521045"]},"78312":{"abstract":"The theory of NP Completeness has shown that several naturally<br\/>occurring problems are unlikely to have polynomial time<br\/>algorithms. One approach to overcome this fundamental intractability<br\/>for optimization problems has been to shift the focus from exact<br\/>solutions to obtaining approximate solutions. The study of<br\/>approximation algorithms has thus emerged as a rich and exciting field<br\/>and recent advances have led to good approximation algorithms for<br\/>several fundamental optimization problems. Despite these developments,<br\/>several interesting and difficult open problems remain. A primary<br\/>focus of this career development plan is studying the approximability<br\/>of fundamental problems and attempting to close such gaps in our<br\/>understanding.<br\/><br\/>The broad research goals of this career development plan are the<br\/>following:<br\/><br\/>1. Developing new tools to devise approximation algorithms <br\/>for problems on directed graphs.<br\/>2. Developing new techniques for metric approximations and <br\/>embeddings as building blocks for approximation.<br\/>3. Investigating a systematic way to obtain and exploit<br\/>strengthened SDPs as well as the use of strengthened SDPs for <br\/>graph partitioning minimization problems and ordering problems.<br\/>4. Devise techniques to deal with scheduling problems with<br\/>precedence constraints.<br\/>5. Extend the machinery of approximation algorithms to new<br\/>settings such as information theoretic and algebraic problems.<br\/><br\/>The research program will involve students at all levels, from<br\/>undergraduate projects on understanding the quality of LP and SDP<br\/>relaxations through computational experiments, to the mathematical<br\/>research suitable for Ph.D. students. The educational component of<br\/>this project includes the development of courses geared towards<br\/>disseminating new algorithmic ideas outside the theoretical computer<br\/>science community; the techniques developed in the latest research<br\/>will be distilled into new graduate and undergraduate courses. Course<br\/>materials developed for such new courses will be made freely available<br\/>to enable similar courses to be taught elsewhere.","title":"CAREER: Approximation Algorithms - New Directions and Techniques","awardID":"0237113","effectiveDate":"2003-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["542007"],"PO":["550329"]},"78554":{"abstract":"Abstract<br\/>0238285<br\/>CAREER: Document Image Degradation Analysis<br\/>Elisa H. Barney Smith<br\/>Boise State U<br\/><br\/><br\/>Optical Character Recognition (OCR) is used to convert documents from images to text form. This allows the documents on the WWW, in company archives or for government intelligence gathering to be searched for<br\/>content. The documents that are heavily degraded by printing, scanning, photocopying and faxing may be easily readable to humans, but the recognition accuracies are very low requiring human intervention.<br\/>Understanding the image degradations introduced by printing and scanning can lead to improvements in OCR. Instead of training the computer to recognize characters by providing it with a large variety of examples of<br\/>characters under different degradation situations we will develop a more effective method based on understanding the degradation and being able to estimate the degradation characteristics for each document.<br\/><br\/>To improve the performance of OCR, the PI proposes to model the nonlinear systems of printing, scanning, photocopying and FAXing. A calibrated model can predict how a document will look after being subjected to these processes and can be used to develop products that degrade text images less. Once statistically validated, researchers will have the confidence to use these models. Estimation of the parameters to these models from a short character string allows continuous calibration to account for spatially-variant systems. These models and parameters will be used to partition the training set based on modeled degradations and match the appropriate partition to the test data at hand to improve OCR accuracy. This technical side will be combined with an educational component to develop an introduction to engineering course for education majors so they may understand, appreciate and educate all K-12 students about how math and science are used in engineering.","title":"CAREER: Document Image Degradation Analysis","awardID":"0238285","effectiveDate":"2003-07-01","expirationDate":"2010-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["551082"],"PO":["564898"]},"88267":{"abstract":"Despite 40 years of research, however, today's recognition systems are still largely unable to handle the extraordinarily wide range of appearances assumed by common objects in typical images. Our tenet is that fundamental new advances in the design and implementation of automated object recognition systems can be achieved by integrating the sophisticated geometric and physical image formation models developed in the computer vision community with the effective models of data distribution and classification procedures developed in the statistical learning theory and theoretical computer science communities. We propose to hold a three-day workshop that will bring together prominent computer vision, machine learning, and computational geometry researchers interested in the fundamental and applicative aspects of object recognition, as well as representatives of industry and funding agencies. Its goals are (1) to promote the creation of an international object recognition community, with common datasets and evaluation procedures, (2) to map the state of the art and identify the main open problems, design issues, and interdisciplinary research opportunities, and (3) to articulate the industrial and societal needs for object recognition technology worldwide. The workshop will be held in Taormina, Sicily, in September 2003. NSF support is sought to partially fund the travel of 25 US academic researchers to the workshop. Additional funds have been solicited from industry and INRIA (France) to support non-US participants and the rest of the workshop expenses. Intellectual Merit and Broader Impacts: The proposed workshop is intended to set the stage for the development of (a) unified design principles for visual object recognition systems, leading to a completely new set of capabilities, (b) fundamental advances in machine learning and computational geometry in a setting---somewhat unusual for these disciplines---where the geometric and physical origins of the input data are well understood, but an appropriate catalog of features for representing them remains elusive, and (c) the successful deployment and evaluation of the developed technology in high-impact application domains. In particular, the workshop will address fundamental scientific issues that are directly relevant to (1) industrial needs in areas as diverse as biomedical imaging, human-computer interface, telecommunications, and transportation, and (2) applications of strategic national interest such as surveillance and security, target recognition, and monitoring. A concrete outcome of the workshop will be a report outlining its conclusions and available to the community at large via the World Wide Web.","title":"Designing Tomorrow's Category-Level Object Recognition Systems: An International Workshop","awardID":"0335780","effectiveDate":"2003-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["290030"],"PO":["234178"]},"89136":{"abstract":"This work investigates some theoretical aspects of clockless digital VLSI design with emphasis on methods for assessing and optimizing the confidence level, the so-called yield, of the design. The particular system model under investigation is wave pipeline without any clock-controlled operations, the so-called, clockless asynchronous wave pipeline. <br\/><br\/>This work will specifically address and resolve the following problems: theoretical characterization of clockless-specific delay faults and the fault rate; modeling and analysis of yield; testing algorithms and fault-coverage analysis; and finally fault-tolerance algorithms. This work will ultimately establish a sound and adequate theoretical foundation for development of exploratory, yet practical test\/diagnosis\/fault-tolerance methods in early design stage of clockless wave pipeline, thereby enabling further research towards development of computer-aided design tools for clockless electronic VLSI systems.","title":"SGER: Yield Assurance and Optimization for Clockless Wave Pipeline","awardID":"0340949","effectiveDate":"2003-07-15","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":[233745],"PO":["562984"]},"79467":{"abstract":"The focus of this project is to develop access control models, mechanisms and systems for spatial databases, in particular, geospatial and moving object databases. Low-cost commercial high-resolution satellite imagery, when coupled with the publicly available data, may pose significant threats to national security and privacy. Similar security and privacy concerns exist in moving object databases when sharing the location and profile<br\/>information of mobile users. To address these issues, the proposed research will make several novel contributions including: development of a comprehensive authorization model for specifying access control policies for geospatial data based on the resolution and spatio-temporal attributes of the images, and based on the tabular data; devising a unified indexing structure to manage the geospatial authorizations as well as geospatial objects for efficient processing of access requests; development of an access control model for moving object databases to ensure the privacy of mobile consumers by protecting their profile, current location and movement information; and development of a combined index for authorizations and moving objects. The solutions to be developed, if successful, will help in building robust and powerful access control modules for geospatial and moving object databases, that may influence applications in satellite industry and mobile commerce.","title":"Access Control for Spatial Databases","awardID":"0242415","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7228","name":"DATA AND APPLICATIONS SECURITY"}}],"PIcoPI":["543481"],"PO":["469867"]},"81690":{"abstract":"EIA-0305146<br\/>Angela M. O'Donnell<br\/>Rutgers University-New Brunswick<br\/><br\/>Title: The Influence of Gender, University Majors, and Work Experiences on Perceptions and Choice of IT Careers<br\/><br\/>This ITWF award provides support to study the extent to which<br\/><br\/>1. Gender, attitudes, perceptions, work experiences, and competencies influence the choice by students of IT-related majors collectively and non-IT majors. <br\/>2. Gender, attitudes, perceptions, work experiences, and competencies influence the choice by students in different IT-related majors (Computer Science, Computer Engineering, and Information Technology). <br\/>3. Specific technology-involved work experience influences attitudes and perceptions of the IT workforce, interest in IT careers, and choices about continuing with this type of work for more than one year. <br\/>4. Recent graduates are working in jobs related to information technology and the degree to which gender, attitudes, perceptions, and work experience during college influence post-graduation career choice among the four different groups of students in this study. <br\/>5. Recent graduates believe themselves to be prepared for the IT workforce.<br\/><br\/>The project will take place over 3 years in a number of departments and on several campuses of Rutgers University. The study combines quantitative and qualitative approaches to understand how men and women make career decisions while in school.","title":"ITWF: The Influence of Gender, University Majors, and Work Experiences on Perceptions and Choice of IT Careers","awardID":"0305146","effectiveDate":"2003-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":[213072,"531416","342273",213075,213076],"PO":["361119"]},"81481":{"abstract":"Proposal No: 304218<br\/>Title: NER: Carbon Nanotube based Nano-Electromechanical Resonators<br\/>Carbon Nanotube Nanoelectromechanical Resonators<br\/><br\/>Abstract<br\/><br\/>This nanoscale exploratory research (NER) project centers around the development of high frequency nano-electro-mechanical (NEMS) resonators using carbon nanotubes. Carbon nanotubes are ideal for NEMS due to their high strength, high elastic modulus, high thermal conductivity, low density, and ability to be synthesized with different lengths and diameters. The proposed resonators are fabricated by aligning single wall carbon nanotubes on nano-lithographically patterned substrates creating a doubly clamped resonating structure, with expected resonance frequencies greater than 1 GHz. If successful, the project will lead to the development of sensors for precise detection of extremely small forces, displacements, masses, and chemical molecules. Creation of efficient antennas much smaller than a wavelength by using the mechanical resonance to accelerate charges can create a new approach for designing antenna arrays for future military, space, and wireless applications. The educational impacts of the project stem from the interdisciplinary nature of this research that involves nanotechnology, electromagnetics, and molecular structural mechanics, thereby creating a new area called \"Nano-Electromagnetics\". A graduate course on NEMS, utilization of undergraduate research programs, and collaborative opportunities between academic and industrial researchers are expected to create excellent awareness of and access to this new technology.","title":"NER: Carbon Nanotube based Nano-Electromechanical Resonators","awardID":"0304218","effectiveDate":"2003-07-01","expirationDate":"2005-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1676","name":"NANOSCALE:  EXPLORATORY RSRCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["496669","484866","524430"],"PO":["562984"]},"82471":{"abstract":"This project aims at reconciling the benefits that personalization of web-based systems has demonstrably brought about with current privacy concerns of Internet users and with upcoming privacy regulation and legislation. The project will analyze and systemize these impacts, and <br\/>develop principled solutions as to how privacy requirements and benefits through personalization can be reconciled. Specifically, an architecture for personalized systems will be developed that can be dynamically configured, so that the personalization methods in use are always in agreement with the current individual privacy preferences as well as the privacy policies and legislation that apply to the given user and the site of the application. Privacy will -- for the first time -- become a design requirement in the development of personalized software systems. The results of the project will benefit researchers in the area of privacy-enhancing technology, and developers of personalized websites who need to respect user privacy. The project outcomes will be published at academic venues, and disseminated to industry and the general public. The project will also contribute to the integration of research and teaching through the employment of undergraduate and graduate students, and through the inclusion of privacy issues into academic syllabi.","title":"Privacy-Enabling Design of Personalized Websites","awardID":"0308277","effectiveDate":"2003-07-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["432332"],"PO":["564456"]},"83450":{"abstract":"This research will develop formal mathematical conditions for reducing the search space of planning problems, and will demonstrate performance improvements in search engines of planners and other discrete searches. Based on the observation that temporal planning problems can be arranged into stages by time, they can be formulated as dynamic optimization problems with dynamic variables that evolve over time. Due to the presence of general constraints that span across multiple stages, path dominance in dynamic programming cannot be applied to reduce the search complexity. This research will seek new node-dominance conditions in each stage by developing the necessary or necessary-and-sufficient conditions for local optimality and by partitioning the conditions into distributed necessary conditions, based on local constraints in each stage and constraints between adjacent stages. By partitioning the search into stages and by finding only dominating states in each stage using the necessary conditions, the search for feasible or optimal plans can be restricted to a much smaller subspace in each stage, leading to a smaller number of combinations of possible paths that need to be searched across multiple stages.<br\/><br\/>Three research tasks will be completed in this work. Based on the discrete-space variational search implementing the stopping conditions, research will develop a new planning system that fully supports PDDL2.1 language features, using the constraint-based-interval representation. Temporal flexible scheduling will be extended by formulating a temporal constraint network with flexible time points into a constrained nonlinear optimization problem and by partitioning the search space. Research will study algorithms for partitioning satisfiability (SAT), mixed-integer optimization, and planning problems.<br\/><br\/>The research is on a new approach that reduces the computational complexity by exploiting the locality of constraints in multi-stage optimization problems. By developing node dominance conditions that identify dominating nodes in each stage, it will lead to reduced search space in each stage and a significant reduction in base of the exponential number of possible paths to be searched across multiple stages. Although a similar approach has been studied in calculus of variations in continuous space, the extension to discrete problems requires the development of a completely new foundation in the theory of Lagrange multipliers in discrete space. The approach developed is general and can be used as stopping conditions in existing planners or integrated into new search algorithms. It can also benefit the solution of other discrete optimization problems, including SAT and mixed-integer optimization.<br\/><br\/>In addition to training graduate and undergraduate students in their research, the project develops a fundamental approach that will be incorporated into courses on optimization. The results will also carry significant impact on autonomous vehicle planning and will be applied to planners currently developed at the National Aeronautics and Space Administration and Jet Propulsion Laboratory. Improved planners will allow higher dependability of space missions that will benefit society at large.","title":"ITR: Efficient Algorithms for Temporal Planning Under Nonlinear Constraints","awardID":"0312084","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["388382"],"PO":["387198"]},"82031":{"abstract":"Embedded microchip design is becoming prevalent in our daily lives and continues to play an important role in the security and health of our nation. An embedded design by definition is an electronic system that contains a computer processor (microprocessor), yet we do not think of them as computers because the computer is hidden or \"embedded\" in the product. Homes in the United States have an average of 30 to 40 microprocessors, yet only 45% of these homes have a desktop computer. The rest of these microprocessors are embedded in appliances. Examples of embedded systems include the computer controlled fuel injectors in automobiles, control mechanisms in toasters and washing machines, a remote control for an unmanned spacecraft, or liquid sensing devices such as those used to monitor biological fluids for illicit drugs or hazardous chemicals in the human body. The tiny size of these devices makes them very attractive because they are light and portable. In health applications, these devices can be injected into the human body and used to diagnose and monitor patients in remote areas that do not have a nearby hospital. They can even reduce expensive hospital stays by dispensing the correct amount of a drug in a patient on a time schedule without any human intervention.<br\/>Applications of embedded systems can be designed using many different scientific principles such as chemical, biological, optical, electronic and mechanical engineering theory all in one product. A single microchip that uses this multiple discipline design technique is called a \"MEMS\", micro-electromechanical device. Thus, the multidisciplinary nature of MEMS requires that many specialists work together and understand how his or her portion of the design will interact and interconnect with all the other parts of the design. This presents a host of problems with respect to producing low-cost, reliable and safe products. <br\/>Consider the MEMS device for monitoring illicit drugs or hazardous chemicals in the human body. This presents a very harsh environment for the device to operate in since these devices are subject to corrosion and contamination. The device could cause harm if its shelf-life and lifetime use properties were not tested. In defense systems, MEMS chips aboard missiles allow the missile to communicate with the command center and report the exact speed and position coordinates of the missile. Having this type of functionality allows the trajectory of the missile to be modified in flight to provide exact precision on a target and minimizes civilian casualties. If the device fails, it could cause erratic behavior that could result in catastrophic results. From the manufacturing perspective the prevalent issues for developing MEMS are the technology risk and cost of production. If good repeatability and reliability testing for life testing are not developed, the production cost of MEMS devices could be prohibitive and not practical for consumer products.<br\/>In summary, the major problems with developing this technology include: (a) The interdisciplinary nature of the design requiring many different skill sets of designers that understand how his or her portion of the design will interact and interconnect with all the other portions of the design. (b) Developing testing methods for reliability and safety and (c) Developing low-cost manufacturing methods that are repeatable and reliable so that the final product is affordable. <br\/>The research work will focus on developing a simulation methodology to help designers develop and perform robust testing on MEMS designs across multiple scientific and engineering disciplines. Consider an electronic control and communication system to control and correct the flight path of a missile in flight. An electrical engineer implements the electronic design, while the sensors that track the position of the missile and actuators that move the wings on the missile are developed by mechanical engineers. The boundary where these two designs connect is a known source of errors due to the lack of understanding between the design disciplines. Using simulation allows low cost experimentation on the design before any expenditure is made on real physical hardware; however, the simulation CAD tools must work across many scientific and engineering disciplines to be effective. This presents a persistent problem for today's MEMs chip designers.<br\/>The simulator software being developed by the Investigators will be able to perform thousands of experiments on software models of a MEMS device. The Investigators will develop a simulation environment that allows different engineering disciplines to use the same simulator. The simulations will find catastrophic conditions over all the operating regions and achieve this without requiring days of computer time or special supercomputers. This allows an engineer to observe the cause and effect relationships of the system parameters that interacted to create an undesira","title":"A Unified Simulation and Fault Environment for Mixed Signal Systems including MEMS Components","awardID":"0306464","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["461118",213904],"PO":["562984"]},"83362":{"abstract":"Recently introduced and forthcoming multithreading processor architectures <br\/>represent new challenges and opportunities for the compilation system.<br\/>This proposal will focus on three areas, exploiting features of a <br\/>simultaneous multithreading (SMT) processor:<br\/><br\/>1. Generating Task Threads and Helper Threads - Task-based parallelism <br\/>provides heterogeneous parallelism that is particularly effective for an <br\/>SMT processor. Helper threads assist and accelerate the execution of other <br\/>threads, without necessarily offloading any computation.<br\/><br\/>2. Simultaneous Compilation - Using spare contexts to do dynamic <br\/>compilation, optimization, and profiling provides the opportunity to <br\/>perform these functions concurrent with the running program, and <br\/>continuously, without interrupting other threads.<br\/><br\/>3. Program Placement for SMT - Efficiently using the memory hierarchy is <br\/>more difficult on an SMT processor because programs interact in <br\/>non-deterministic ways. Novel code, data, and page placement compiler <br\/>algorithms will reduce cache misses for multithreaded workloads.<br\/><br\/>This research will involve grad and undergrad students, including students <br\/>from underrepresented groups, training them in research methodology and <br\/>practice, and developing particular research expertise. This research will <br\/>create a compiler and simulation infrastructure that will be made available <br\/>widely to other academic institutions. This should most benefit <br\/>institutions that lack resources to develop such an infrastructure themselves.","title":"Compiler Optimizations to Exploit Simultaneous Multithreading","awardID":"0311683","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["542069","293079"],"PO":["565272"]},"83296":{"abstract":"A Hardware\/Software Framework for Managing On-Chip Communication Latency<br\/><br\/>In modern computing systems, there is an inherent tension between performance and correctness. Processors and compilers are becoming increasingly complicated to maintain the performance growth often characterized by Moore's Law. This complexity makes it very challenging and costly to ensure these systems operate reliably. In this project, we explore an execution paradigm that decouples performance and correctness by simultaneously performing executions of two versions of a program's binary: one responsible for performance and one for correctness. The performance execution is responsible for executing the program quickly with a high likelihood, but no guarantee, of correctness; the lack of correctness guarantees enables significant optimization opportunities of both the program and the hardware, without overheads of verification. The results of the performance execution can be used to parallelize the correctness execution, which is responsible for updating user-visible (architected) state. Parallelization enables the correctness execution to be performed by a collection of simple processors using an unoptimized version of the program. In addition to reducing verification complexity, the proposed execution paradigm appears to have potential to improve performance and reduce power consumption.","title":"A Hardware\/Software Framework for Managing On-Chip Communication Latency","awardID":"0311340","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["539055"],"PO":["325495"]},"82097":{"abstract":"This grant will support an assessment of information technologies relative to on-line voting and ballot design. This is a particularly relevant topic given the confusing processes and outcomes of the 2000 national elections. The project will bring together social\/political scientists with computing\/information scientists to study ability of voters to accurately express their intentions, voter ease in completing ballots, user interfaces, and impact on voter turnout. A variety of data collections methods will be used: laboratory experiments, expert reviews, and field tests. The project will be guided by interactions with practitioners who administer voting process, and with companies which build and market electronic voting machines.","title":"An Assessment of Voting Technology and Ballot Design","awardID":"0306698","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}}],"PIcoPI":[214067,214068,"317018","426438","552787"],"PO":["371077"]},"88752":{"abstract":"The DIALM-POMC Joint Workshop is devoted to discrete algorithms and<br\/>discrete modelling in the context of mobile and wireless computing and<br\/>communications. It is intended to be a lively meeting, covering many<br\/>of the algorithmic and discrete aspects of this field going from<br\/>operations research to radio engineering problems. It aims, in<br\/>particular, at fostering the cooperation among practitioners and<br\/>theoreticians of the field.<br\/>If this project is awarded, the funds provided by NSF to support<br\/>DIALM-POMC'2003 will be used to provide funds for inviting two renowed<br\/>speakers in the area of mobile computing to present at the workshop,<br\/>to allow for reduced registration costs for students, and to provide<br\/>travel support for students attending the workshop.","title":"DIALM-POMC Joint Workshop on Foundations of Computing","awardID":"0338509","effectiveDate":"2003-07-15","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["563358"],"PO":["309350"]},"81493":{"abstract":"Proposal Number: 304284<br\/>Principal Investigators: R. Iris Bahar, Jie Chen, Joseph Mundy<br\/>Inst: Brown University<br\/>Proposal Title: NER: Y-Junction Nanotube-based Computer Devices and Architectures<br\/><br\/>This research focuses on exploratory research in nano-scale <br\/>computation. These new forms of computation will allow for more<br\/>efficient means of exploiting nano-scale electronics and devices than<br\/>those based on traditional silicon-based computation. In particular,<br\/>this research focuses on finding an effective means of dealing with<br\/>high defect rates found in nano-scale devices and developing an<br\/>architecture that is dynamically defect tolerant.<br\/><br\/>Three areas are being developed: (1) Device models are being defined<br\/>based on Y-junction carbon nanotubes (CNTs). This work may include<br\/>use of self-assembly capabilities to program interconnections of CNT<br\/>devices and build I\/O interfaces. (2) A probabilistic-based design<br\/>methodology for nano-scale computer devices will be proposed based on<br\/>the Markov random network and brief propagation. This methodology<br\/>allows for an architecture that adapts to errors as a natural<br\/>consequence of probability maximization, thereby removing the need to<br\/>actually detect faults. (3) A new course in nano-computing will be<br\/>introduced in fall 2003. The course will offer a unique opportunity<br\/>to expose students to the wide range of disciplines that influence<br\/>nano-computing.","title":"NER: Y-Junction Nanotube-based Computer Devices and Architectures","awardID":"0304284","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["275123","550256","364777"],"PO":["562984"]},"83231":{"abstract":"The proposed research focuses on understanding the inherent computational complexity of various algorithmic questions which have arisen recently in connection with a variety of optimization, scheduling and resource allocation problems coming from Internet and telecommunications routing and usage applications, among others. Our main goal is to provide efficient (polynomial-time) algorithms for those problems, which have them, sharp approximation algorithms for those, which don't, and appropriate non-approximability results when this is appropriate. Problems of this type form a critical part of the foundations of theoretical computer science. In addition to increasing our understanding of the fundamental reasons as to why some problems are so much harder to solve than others, the discovery of efficient algorithms (when they exist) can contribute to more effective use of Internet network and processing resources, memory usage in parallel computation algorithms, more efficient pattern recognition methods in computational biology, and increased capacity for many kinds of communication networks, for example.","title":"Approximation Algorithms for identification, bin packing and scheduling problems","awardID":"0310991","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["452844"],"PO":["499399"]},"83495":{"abstract":"This project develops an integrated Bayesian framework for vision based control of Unmanned Aerial Vehicles (UAVs) through fundamental research in 1) model-based nonlinear state and parameter estimation, 2) intelligent adaptive control, and 3) image processing. We specifically address how real-time video data can be processed with ground-based sensors (and on-board avionics) to extract spatial and situational information (e.g., vehicle state and model parameters). Using only stationary video cameras, information from the sequence of images are integrated with an adaptive controller that transmits actuator commands directly to the UAV. Our research infrastructure consist of an X-Cell-60 R\/C helicopter with custom avionics, video cameras on the ground, and a PC ground-station to perform all necessary processing<br\/><br\/>A key aspect is to go beyond traditional vision based motion estimation and tracking, utilizing new approaches to recursive Bayesian estimation allowing full coupling with the control system. Heuristically, this involves the propagation of probabilistic density estimates for the state (vehicle position, attitude, and velocities) and model parameters (mass, moments of inertia, aerodynamic forces, etc.). The vision components models the ``image likelihood'' and describes the probability of observing the image given the current state. The estimation combines the vision measurements with the dynamic vehicle model in a recursive filtering procedure using a Sigma-Point Filter (SPF) framework. SPF methods are a recent development in machine learning, and are shown to be far superior to standard EKF based estimation approaches. <br\/><br\/>The intellectual merit of the research contributes to both the individual component areas as well as the integrated whole. The integration of the different components in the proposed manner represents an interdisciplinary new approach, providing new research opportunities and applications in integrated sensing, information processing, and control. Beyond basic research, the broader impact to technology includes the obvious commercial and military applications that can be studied in this controlled environment (e.g. visually assisted vertical take-off and landing for ship board helicopters, or agile maneuvering through urban environments). The core technologies can also be extended to other information technology areas from image tracking and detection, to control of complex biological systems.","title":"Collaborative Research: ITR: A Robotics-Based Computational Environment to Simulate the Human Hand","awardID":"0312271","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["384595"],"PO":["335186"]},"83154":{"abstract":"A rigorous, effectively computable theory for the analysis of swept volumes (the set occupied by objects moving in space) is developed in this project. The work involves mathematical and computing sciences in the specialty of computational topology. The goals are to make fundamental contributions to the automated computation and representation of swept volume operations, and explore generalizations and applications to timely problems in engineering and science. This will lead to new approaches and insights that advance the state-of-the-art in computer aided geometric design and related fields. Traditional methods will be integrated with advanced mathematical techniques to achieve the goals, which encompass the following tasks: (1) Develop algorithms for smoother representation of swept volumes that include effectively computable characterizations of accuracy and stability, and extend these results to more general objects. (2) Create new methods for computing and representing Boolean operations for swept volumes that are real-time executable, and include accuracy and stability algorithms. (3) Compare the new algorithms with current solid geometry methods, and develop efficient ways to interface these approaches with existing software. (4) Investigate applications to emerging problems in such areas as tissue engineering and virtual sculpting. <br\/><br\/>Swept volumes have important applications in design, manufacturing and the health sciences. A basic question concerning the representation of swept volumes and other objects is, how can one insure that computer based representations are accurate, especially when the data contains errors, and when different methods are used to render the object? This question, which concerns the shape of computer-generated objects, provides the motivation for and the main focus of the project. In some cases there are readily computable quantities characterizing shape that can be included in programs for representing objects. Such quantities, especially as they apply to swept volumes, are to be studied in depth, along with methods for obtaining smoother representations of geometric objects. Applications of the approaches developed to tissue engineering and virtual design and manufacturing will be investigated, and additional applications in the computing sciences are anticipated. Findings from the project will be used to create innovative programs for computer aided design, and engineering applications, and will be disseminated via publications, lectures, the Web, existing and new undergraduate and graduate courses, new programs in computational topology, and interactions with scientific and engineering collaborators.","title":"Accuracy and Stability of Computational Representations of Swept Volume Operations","awardID":"0310619","effectiveDate":"2003-07-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"V023","name":"DARPA-CARGO"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1106","name":"Division of HUMAN RESOURCE DEVELOPMENT","abbr":"HRD"},"pgm":{"id":"1260","name":"INFRASTRUCTURE PROGRAM"}}],"PIcoPI":["457669","63357","508452","560676"],"PO":["321058"]},"85211":{"abstract":"Theoretical and algorithmic approaches to data analysis have played a<br\/>central role in the development of modern methods for handling data.<br\/>The massive amounts of data gathered in important present-day<br\/>applications ranging from the Internet to homeland security to<br\/>astronomy and medicine have dramatically changed the requirements for<br\/>algorithms and provide ample motivation for a great deal of new<br\/>theoretical development. Traditional data analysis tools are incapable<br\/>of handling the sheer size and complexity of these gigantic data sets.<br\/><br\/>The project will run three workshops on topics related to the size and<br\/>complexity of modern data sets:<br\/><br\/>\"Data Quality, Data Cleaning and Treatment of Noisy Data\"<br\/><br\/>\"Compact Representation of Data\"<br\/><br\/>\"Geographical Information Systems: Terrain Representation and<br\/>Analysis Algorithms\"<br\/><br\/>Intellectual Merit:<br\/><br\/>The Data Quality workshop seeks to bring together experts from<br\/>different research disciplines to initiate a comprehensive technical<br\/>discussion on data quality, data cleaning and treatment of noisy data.<br\/>The objectives of the workshop are to develop quantitative ways of<br\/>measuring the quality of data and to search for an integrated,<br\/>domain-independent approach to data cleaning in the situation where<br\/>the sheer size of data collections prevents one from manually<br\/>scrubbing or even monitoring them.<br\/><br\/>The central topic of the Compact Representation workshop revolves<br\/>around the following question: Can a large and potentially complicated<br\/>data set be replaced by a short sketch such that a certain class of<br\/>queries on the original data can be answered (perhaps approximately)<br\/>by performing computations on the short sketch? The workshop will<br\/>explore metric embeddings of sampling spaces with complex structure<br\/>into spaces that are amenable to simpler analysis; space-time<br\/>tradeoffs for data structures; and algorithms for analysis of data in<br\/>the streaming model.<br\/><br\/>Geographical Information Systems (GIS) are called on to store,<br\/>combine, display, and analyze disparate types of spatially-referenced<br\/>data---locations of telephone poles, streets, rivers, mountains,<br\/>buildings, soil layers, pollution levels, voting districts---for<br\/>applications ranging from natural resource management to urban<br\/>planning. The goal of the proposed workshop is to address the<br\/>challenges in modern GIS by focusing on algorithmic issues encountered<br\/>in such systems.<br\/><br\/>The workshops will identify areas for research and focus on future<br\/>research challenges.<br\/><br\/>Broader Impact:<br\/><br\/>All workshops will seek a broad impact through a mix of theoretical<br\/>researchers and practitioners and by helping to stimulate<br\/>interdisciplinary collaborations. Theoretical progress on the research<br\/>topics in these workshops will have impact on a wide variety of areas<br\/>such as health care, homeland security, environmental management,<br\/>disaster management, marketing, bioinformatics, psychology, sociology,<br\/>chemometrics, astronomy, and credit card fraud detection.<br\/><br\/>A major theme of the project is integration of research and<br\/>education. It is addressed through participation of graduate students<br\/>in the workshops, interaction between students and DIMACS visitors<br\/>working in related areas, and tutorial programs organized in each<br\/>workshop.","title":"Three Workshops in Data Analysis and Mining","awardID":"0320022","effectiveDate":"2003-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["264120"],"PO":["279077"]},"82087":{"abstract":"The investigators are creating algorithms and software for solving optimization problems of a general yet highly practical nature. Such problems arise in aerospace, biology, engineering, economics, medicine, and many other such fields. They involve thousands of variables and constraints. The intellectual challenge is to design methods that are efficient and reliable on increasingly large problems. A key aspect of the new work is improved handling of the sparse matrices involved. New classes of problems are also targeted, including ones with a mixture of continuous and discrete variables.<br\/><br\/>A large community of researchers already depend critically upon the investigators' optimization software. They include economists exploring international trade agreements and the effect of greenhouse gases. At Boeing and NASA, shape optimization is essential for designing complex new aircraft configurations such as transonic airliners and a quiet supersonic business jet. Future Space Shuttle decisions depend on accurate computation of optimal trajectories for reentry from orbit and for emergency landings during launch. For example, new sharp-nosed vehicles are being compared against conventional blunt-nosed designs. The investigators' work is of national importance through its contribution to the saving of scarce resources and the competitive edge that it provides to US industry and defense.","title":"Large-Scale Optimization","awardID":"0306662","effectiveDate":"2003-07-15","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[214041,"526394"],"PO":["381214"]},"86003":{"abstract":"EIA-0323463<br\/>John Howell<br\/>University of Rochester<br\/><br\/>Unit Efficiency, nondestructive, photon counting detector<br\/><br\/>The startling behaviors that have led to the dramatic slowing and stopping of light also have promise in the field of quantum information. Following a theoretical proposal by Schmidt and Imamoglu, an ultra-sensitive photon-counting detector is currently under construction. At the heart of this experiment is electromagnetically induced transparency (EIT). EIT allows a laser beam (probe beam), on resonance with an atomic transition, to pass unattenuated through an atomic gas of the relevant atoms. However, when a signal beam passes through the sample the EIT effects are dramatically changed. The effects are so strong that single photons in the signal field can change the macroscopic properties such as phase and attenuation of a classical probe beam. Nondestructively counting photons with high efficiency is of great interest to many quantum communications protocols.<br\/> The experiment is in the construction phase. The electronics are being developed that will allow two New Focus Vortex lasers, one as the probe field and the other as the coupling field, to be locked to the top of a resonance peak in a Doppler-free saturated absorption spectrum of Rb 87. The effort is to achieve a linewidth stability on the order of 100 KHz or better for both lasers. The locked lasers will then be nearly collinearly passed through a relatively high buffer gas density vapor cell. <br\/>Two sources for the signal photons are in production. The first source is from a third dramatically attenuated diode laser. The detector will project the otherwise Poissonian distribution of the signal laser into pure number states, which is an exciting result in and of itself. The second source is from narrow band entangled photons. Concurrent with the construction of the detector, is a narrow-band frequency-locked source of entangled photons, which is an important part of the test of this detector. <br\/>Two women, three Hispanic and one African-American students are being trained under this project. The investigators are also heavily involved in Research Experiences for Undergraduates (REU) and undergraduate teaching.","title":"Unit Quantum Efficiency, Nondestructive, Photon Counting Detector","awardID":"0323463","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["560414","7085"],"PO":["521045"]},"86124":{"abstract":"EIA-0324045<br\/>Ronald Breaker<br\/>Yale University<br\/><br\/>The Role of Riboswitches in Bacillus Information Control<br\/><br\/>Riboswitches are a new-found form of genetic control systems wherein access to biological information is controlled by metabolite-binding messenger RNAs. This research will initiate our comprehensive efforts to establish the importance of riboswitches to metabolic control in Bacillus subtilis, which is a model organism for gram positive bacteria. Specifically, the scope of riboswitch-based genetic control networks that maintain metabolic stasis is being determined. Furthermore, whether riboswitches are indispensable regulatory components is being determined. It is possible that riboswitches are the most fundamental of genetic control elements in certain bacteria, which, if true, would prove that structured RNA molecules have the capability to control information access in sophisticated genetic networks. <br\/>It is expected that the basic understanding of riboswitch regulatory networks will establish a foundation upon which several important issues may be addressed. It is hypothesized that riboswitches are an ancient form of genetic control, and this speculation can be supported if riboswitches are deeply involved in fundamental and indispensable metabolic regulation. This potential for complex molecular sensing\/switching could be harnessed through engineering novel switches for biosensor and molecular computing applications. Most significantly, riboswitches could make excellent targets for novel antibiotic development, wherein metabolite analogs could be used to selectively down-regulate the metabolism of pathogenic or","title":"The Role of Riboswitches in Bacillus Information Control","awardID":"0324045","effectiveDate":"2003-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":[224941],"PO":["565223"]},"88126":{"abstract":"Tactual perception of graphical images is a challenging problem. In this project, the PI and his team propose to develop new image processing and understanding techniques that specifically relate to low-bandwidth tactual perception. The PI and his team will design and build a prototype of an interactive Tactile Image Editor (TIE) that will enable sighted users to translate, as automatically as possible, graphical images into highly comprehensible Braille graphics (raised dots embossed on paper) that can be viewed tactually by blind persons. The focus is on graphical images (drawings, graphs, charts, and diagrams) that appear in textbooks, research papers, and Web pages that are used by scientists, engineers, and mathematicians (students and professionals). Criteria will be developed for what makes a Braille graphic as understandable as possible. That information will be provided to graphical image transcribers to enable them to make good decisions about the transcription of graphical images either manually or with the assistance of the TIE. To achieve his goals, the PI and his team will explore the use of image processing techniques to automate and expedite the conversion of graphical images that are in print or digital format into Braille graphics. They will explore application of low-level image processing techniques such as denoising, edge detection, smoothing, and resolution reduction, to determine heuristics for producing effective Braille graphics. They will validate developed heuristics with an experimental study, in which blind participants will evaluate the readability and comprehensibility of images produced by the derived heuristics. They will explore the application of segmentation techniques to enable them to locate text and other objects that need special attention within an image (for instance, text must be extracted and placed appropriately). They will develop classification and learning algorithms to enable them to determine the type of image being processed, because different kinds of images may require different processing steps. They will develop a software prototype, the Tactile Image Editor (TIE), which will enable them to easily apply the image processing heuristics, segmentation techniques, and classification and learning algorithms in future research. They will conduct an observational study of Braille transcribers, to better understand the processes they use and to inform the future design of the TIE's interface and functionality.<br\/><br\/>Broader Impacts: This research will lead to better and more timely access by blind scientists, engineers, and mathematicians (students and professionals) to graphical images that are becoming more and more prevalent in textbooks, research papers, and on the Web. The research will also lead to a better understanding of what makes Braille graphics most understandable; this information will be useful for graphical image transcribers, whether they use the TIE tool or not. Although the focus of the research is on scientific graphical material, the TIE is potentially useful in other areas that rely on graphical images to covey information. The students, both graduate and undergraduate, who work on the project will benefit from its cross-disciplinary nature.","title":"SGER: Toward the Automated Tactilization of Graphical Images","awardID":"0335143","effectiveDate":"2003-07-01","expirationDate":"2004-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":["250258","485574"],"PO":["565227"]},"79469":{"abstract":"A fully digitized multimedia enriched world will be a reality when security and quality of the multimedia content can be guaranteed. Information hiding (watermarking, digital fingerprinting, etc.) has been gaining popularity in this quest for security. Investigation of information hiding based security techniques that suit multimedia data and networks is the goal of this project. Primarily, this project considers problems in providing multimedia security from the following different perspectives: (a) Encoder\/Content originator perspective, (b) Decoder\/Content recipient perspective, (c) Attacker's perspective, and (d) Networking perspective. Studying the multimedia data security problem from these perspectives could lead to a better understanding of security issues from an end-to-end system view point. Connections between perceptually based and information theory based information hiding, and content based watermarking will be investigated with robustness and embedding capacity being the main focus. Detector dependent embedding and game theoretic formulation to study the dynamics between the decoder and a malicious attacker are proposed. A new attack dependent definition of security and methods to compute the information hiding capacity based on this definition are also proposed in this project. Finally, joint source-channel coding and multiple description coding approaches for robust watermark systems that can handle network losses are also of interest.","title":"Perspectives on Information Hiding for Multimedia Security","awardID":"0242417","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7228","name":"DATA AND APPLICATIONS SECURITY"}}],"PIcoPI":["515465","409877"],"PO":["469867"]},"81450":{"abstract":"NUE Abstract<br\/>NUE: A Freshman-Level Introduction to Nanotechnology based on Scanning- probe Instrument<br\/><br\/>This award is part of Nanoscale Science and Engineering initiative, NSF 02-148, category NUE and is supported by the CISE\/CCR and ENG\/EEC programs. This award supports a team led by Wolfgang Porod at the University of Notre Dame to develop a freshman-level course module which provides an introduction to nanotechnology based on content and activities centered around scanning-probe instruments. This module, which also serves as a vehicle to introduce general principles in multidisciplinary engineering systems design, will consist of lectures, hand-on experimental activities, and web-based tutorials. Assessment will be put at the heart of module development. A key component of this module will be a model version of a scanning-probe instrument, and we plan to use LEGO pieces and software since the use of standard LEGO components will facilitate module dissemination. While this module will be developed within the context of Notre Dame's existing EG 111\/112 freshman course, it itself will be modular in nature, and it will be designed that the different components can be used in different combinations, as deemed appropriate for a given environment at another institution. Upon completion of this project, a workshop will be held at Notre Dame where the finished module will be introduced to interested educators.","title":"NUE: A Freshman-Level Introduction to Nanotechnology based on Scanning-Probe Instruments","awardID":"0304089","effectiveDate":"2003-07-01","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7219","name":"NANOTECHNOLOGY UNDERGRAD EDUCA"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1340","name":"ENGINEERING EDUCATION"}}],"PIcoPI":["560914","507932"],"PO":["562984"]},"82440":{"abstract":"Robotics and Human Augmentation Program<br\/><br\/>ABSTRACT<br\/><br\/><br\/>Proposal #: 308087<br\/>Title: Toward True 3D Object Recognition<br\/>PI: Ponce, Jean<br\/>U of Ill Urbana-Champaign<br\/><br\/>This proposal addresses the problem of recognizing three-dimensional (3D) objects in photographs and image sequences. It revisits viewpoint invariants as a local representation of shape and appearance. The key insight is that, although smooth surfaces are almost never planar in the large, and thus do not (in general) admit global invariants, they are always planar in the small---that is, sufficiently small surface patches can always be thought of as being comprised of coplanar points---and thus can be represented locally by planar invariants. This is the basis for a new, unified approach to object recognition where object models consist of a collection of small (planar) patches, their invariants, and a description of their 3D spatial relationship. Specifically, the local invariants used in this proposal are the affine-invariant descriptions of the image brightness pattern in the neighborhood of interest points recently developed by Lindeberg and Garding and by Mikolajczyk and Schmid. These affine-invariant patches provide a normalized representation of the local object appearance, invariant under viewpoint and illumination changes, that can be used as a local measure of image, part, or object similarity. The spatial relationship between local invariants is used to represent the global object structure and drive the recognition process. The proposed approach is applied to four fundamental instances of the 3D object recognition problem: (1) modeling rigid 3D objects from a small set of unregistered pictures and recognizing them in cluttered photographs taken from unconstrained viewpoints; (2) representing and recognizing non-uniform texture patterns under non-rigid transformations; (3) modeling and recognizing articulated objects in image sequences, with applications to the identification of shots that depict the same scene (shot matching) in video clips; and (4) learning and recognizing part-based descriptions of object classes in photographs and video clips.<br\/><br\/>Intellectual Merit: The main scientific contributions of the proposed project will be (a) a unified framework for 3D object recognition that combines the advantages of geometric and appearance-based approaches to recognition; (b) fundamental advances in object recognition technology in the four target domains, including the wide open problem of category-level recognition; (c) effective algorithms for a number of practical applications, including shot matching in video analysis. Large, representative datasets will be gathered for each of the problems addressed in the project. They will be used to systematically evaluate the algorithms developed in its course, and be made available to the computer vision community at large on the World Wide Web.<br\/><br\/>Broader Impacts: With the ever expanding array of imagery sources, some form of automatic object recognition technology must eventually be an integral part of every information system. However, today's recognition systems are still largely unable to handle the extraordinarily wide range of appearances assumed by common objects in typical images, and fundamental advances are needed before 3D object recognition fulfills its potential as a critical enabling technology in domains such as surveillance and security, image retrieval and data mining, and video analysis and annotation. The research conducted in this project will be a stepping stone in that direction.","title":"Toward True 3D Object Recognition","awardID":"0308087","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["290030"],"PO":["317663"]},"85960":{"abstract":"EIA-0323261<br\/>F. C. Wellstood<br\/>University of Maryland College Park<br\/><br\/>Demonstration of Entanglement in Coupled Josephson-Junction Qubits<br\/><br\/><br\/> This project is a first step toward a scalable solid-state quantum computer with implications for cryptography, national security, and scientific computing. Under this project, the energy levels, coherence times, state manipulation, gate operations, and quantum behavior of systems composed of current-biased Josephson junctions are being examined both examining experimentally and theoretically. Building upon prior spectroscopic studies of two capacitively coupled qubits, the entanglement of two or more qubits, not only coupled capacitively but also coupled by means of switchable active elements are being investigated. Switchable coupling may be crucial for the gate operations of a quantum computer with its large number of qubits. Theoretical analysis is aiding in designing experiments and interpreting results. This project supports training of graduate and undergraduate students and provides opportunities for women, minorities, and high-school students to participate.","title":"Demonstration of Entanglement in Coupled Josephson-Junction Qubits","awardID":"0323261","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":[224431,"31436","376955","304038"],"PO":["521045"]},"83672":{"abstract":"Abstract:<br\/><br\/>Various types of ad hoc wireless networks have been the subject of<br\/>much recent interest. For our purposes, these networks can be<br\/>characterized as a set of nodes that<br\/>communicate with each other over wireless channels using little or no<br\/>fixed<br\/>infrastructure. In such a network, to reach its<br\/>destination, a message may need to be forwarded over several<br\/>intermediate links. Examples of this type network include mobile<br\/>ad hoc networks (MANET's), sensor<br\/>networks, and various hybrid ad hoc\/cellular architectures. Such<br\/>networks<br\/>have the advantages of being rapidly deployable and requiring little or<br\/>no<br\/>fixed infrastructure. However, efficiently utilizing this type of<br\/>network is<br\/>a challenging task due, in part, to the interference between nodes, the<br\/>time-varying nature of the communication channels, the energy<br\/>limitations of untethered nodes, and the<br\/>lack of centralized control via, for example, base stations.<br\/><br\/>This project is part of ongoing research with the goal of<br\/>furthering understanding of basic performance trade-offs for ad<br\/>hoc networks. This work takes a cross layer view and<br\/>considers both physical layer limitations as well as higher layer<br\/>performance metrics such as packet throughput. Techniques from<br\/>queueing theory, optimization, and information theory will be<br\/>utilized in this work. In contrast with our previous work on problems of this type in<br\/>single-hop wireless settings, or, that is, models more appropriate<br\/>for cases such as cellular networks, where all communication is to<br\/>or from a single point, in this project, we are focusing on multi-hop<br\/>ad hoc networks, with multiple transmitters and multiple<br\/>receivers. In this environment, several new issues complicate the<br\/>analysis. These include the routing and forwarding of traffic<br\/>within the network and the scheduling of transmission and<br\/>reception by each node. Specific problems that are being addressed<br\/>include<br\/>characterizing network stability and identifying adaptive resource<br\/>allocation policies which maximize packet throughput.","title":"Collaborative Research: ITR: Fundamental Performance Trade-offs for Ad Hoc Networks","awardID":"0313183","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["524620"],"PO":["348215"]},"82000":{"abstract":"Logical frameworks are languages for specifying logics and reasoning with them formally, with machine support. Examples are logics to prove the safety of programs or authenticate access rights to protected data. The project investigates theoretical foundations and efficient implementation techniques for logical frameworks. Specifically, the project addresses performance bottlenecks in current, large-scale applications of the Twelf logical framework aimed at improving safety and security of mobile code within the national cyber infrastructure. The techniques include methods for proof compression based on intrinsic redundancy, and methods to increase the degree of automation in proof search and verification.<br\/><br\/>Logic is a key discipline in computer science because it allows us to make definitive judgments such as \"yes, this program is safe to run\" or \"yes, this agent is allowed to access these data\". The critical notion is that of a formal proof, which can convince a code recipient of its safety or an operating system of access rights. Logical frameworks represent such proofs as data structures. This project investigates methods to manipulate these data structures efficiently so they can be used in realistic, large-scale applications. The system under construction is also used to teach undergraduates the concepts of formal logic as they are applied in computer science and mathematics.","title":"Efficient Logical Frameworks","awardID":"0306313","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["339844"],"PO":["321058"]},"83694":{"abstract":"The utility of databases could be much greater if they were generally accessible across a network. But such network-available databases face serious security challenges. Users can use inference techniques on information from multiple databases to obtain data that none of these databases would directly divulge.<br\/><br\/>We propose to build a system to protect network-accessible databases form this threat by establishing an inference protection system at the directory (e.g., the standard resource description framework, RDF) site. The system will require that all requests, sent to sites that store the databases, be submitted through the RDF directory site. As a result, this site will be able to observe the information content of all requests. By keeping proper records on the requests submitted by each user, and by using knowledge about database schema, contents and patterns leading to security violations, this site will be able to detect when an individual user is attempting to use data mining techniques to infer information that he could not obtain directly.<br\/>A test bed of the proposed security violation protection system will be constructed. A series of experiments with data traces from real applications will be used to evaluate the effectiveness of the proposed system.","title":"ITR: Knowledge-Based Inference Techniques to Ensure the Security of Database Content","awardID":"0313283","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":[218209,"485932"],"PO":["563751"]},"83353":{"abstract":"The goal of this ongoing research is to find the structure of the theory of information flow in<br\/>networks. While several single-user channels, and a few multi-user channels, are completely<br\/>understood, the rate region of the general multi-user memoryless network p(y1 ym x1 xm )<br\/>remains unknown. Certain key channels and new formulations are serving to advance the<br\/>theory. This work focuses on relay networks (where passive nodes are recruited as relays),<br\/>ad hoc networks (with interference and Gaussian noise), erasure networks (in which the<br\/>nodes and symbols are erased at random), and amplification networks (in which the state<br\/>of the channel becomes the intended information). Quantum communication networks allow<br\/>physical processing not conceived by classical probability theory. In particular, this work is<br\/>investigating how quantum entanglement may be used to facilitate cooperative communica-<br\/>tion.<br\/><br\/>The general attack on classical networks is based on developing ideas for inducing cooperation<br\/>in an attempt to provide principles for practical low complexity implementation.","title":"Communication Networks","awardID":"0311633","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["321034"],"PO":["348215"]},"82088":{"abstract":"For medical, biological and chemical applications, microfluidic devices such as valves, mixers and Atomic Force Microscopy (AFM) cantilevers are of particular importance, but the simulation tools available to design these devices are quite limited. This research focuses on development of fast integral equation based solvers for the convection-diffusion equations associated with mixing and for the Navier-Stokes problems associated with microvalves and AFM cantilever tips. We plan to interface these fast nonlinear solvers with model reduction techniques so as to automatically extract system models of these microfluidic components, making it possible for designers to perform system level simulation and optimization.<br\/><br\/>The research developed in this proposal will be carried out in conjunction with an educational program. The educational objectives are to promote the involvement of minorities and women in the research area of design of Micro\/Nano systems. To create impact of the proposed research on MEMS industry, software resulting from this research will be placed in the public domain and will be available via the web.","title":"Numerical Techniques for Extracting System-Level Models of Micromachined Devices","awardID":"0306664","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["292331","276034"],"PO":["562984"]},"78535":{"abstract":"Previous power and energy research has focused almost exculsive on battery-operated devices. In contrast, this research is focused is focused on power and energy prediction and conservation for clusters of servers.<br\/><br\/>The main goals of the research are: (1) to develop a methodology and a tool that combine analytical modeling, experimentation, and optimization algorithms to predict the power\/energy conservation techniques server clusters; (2) to develop power\/energy conservation techniques that can use the prediction infrastructure to optimize consumption for both curren and future server cluster without excessive performance degradingl and (3) to design, implement, and evaluate cluster systems that exploit these techniques.<br\/><br\/>This research will allow server cluster to be engineered to a specific and low consumption. Furthermore, it allow conservation techniques to make fundamental tradeoff decisions about power, energy, and performance. These developments will have important implications, given that the computational model upon which the Internet is oragnized relies on large server clusters.","title":"CAREER: Energy Prediction and Conservation for Server Clusters","awardID":"0238182","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["556616"],"PO":["560587"]},"81561":{"abstract":"Electrochemiluminescence as a tool for microscopy at the nanoscale.<br\/><br\/>A team of University of California--Irvine engineers, led by Professor<br\/>James Brody and Professor Peter Burke, will construct a prototype of a<br\/>new nanoscale microscope. This microscope will combine two very<br\/>different technologies; one developed to manufacture<br\/>microprocessors and the other to analyze blood.<br\/><br\/>The pilot nanoscale microscope will measure the motion of tiny<br\/>micro-spheres in water. An electronic signal applied to a series of<br\/>electrodes will cause a chemical tag attached to the micro-spheres to<br\/>emit red light. This red light will be correlated with the applied<br\/>electronic signal to sense the motion of the micro-spheres.<br\/><br\/>This project leverages the considerable research and development<br\/>investment by industry to develop a revolutionary method for<br\/>nano-scale imaging. This imaging method will allow one to see some<br\/>objects as they have never seen before. Furthermore, finer and finer<br\/>detail will be observable as industry is able to produce more powerful<br\/>microprocessors.","title":"NER: Electrochemiluminescence as a Tool for Microscopy at the Nanoscale","awardID":"0304612","effectiveDate":"2003-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":[212721,"243669"],"PO":["562984"]},"83530":{"abstract":"Abstract<br\/>0312432<br\/>Douglas Jones<br\/>U of Illinois, Urbana-Champaign<br\/><br\/>Current audiovisual recording, communication, and playback provides a single, two-dimensional perspective<br\/>of the world as it varies through time. Humans live in a \\four-dimensional\" world, in which they move about<br\/>(in 3-D space over time) at their own volition to experience the world from any perspective. Advances in<br\/>sensor, computer, and networking technologies now make possible new systems that employ multiple cam-<br\/>eras and microphones together with sophisticated processing algorithms to deliver unprecedented immersive<br\/>recording and viewing capabilities. Sufficient sensing, networking, and computing power to practically ad-<br\/>dress this vision already exists; the critical gap in achieving it is the lack of the necessary signal processing<br\/>theory and algorithms.<br\/>This research will develop new signal processing techniques for reconstruction of the audio and visual<br\/>recording at an arbitrary location in space and time from multiple acoustic and video sensors, by extending<br\/>recent research in adaptive beamforming, multisensor signal processing of non-stationary signals, and fun-<br\/>damental new advances in multi-dimensional signal representation. Practical four-dimensional audiovisual<br\/>recording, transmission, and playback, or \\remote reality\", will be demonstrated with low-cost, conven-<br\/>tional sensors attached to networked computers, thus confirming the practicality of the proposed methods<br\/>and applications.","title":"ITR - Remote Reality: 4-D Audio-Visual Reconstruction and Compression from Multiple Sensors","awardID":"0312432","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["470450","518402"],"PO":["564898"]},"83651":{"abstract":"Ad-hoc wireless networks lack the infrastructure of traditional wireless networks; for example, they have no base stations or switching centers. Every node in an ad-hoc network can act as a forwarder or router to the other nodes. While ad-hoc networks were conceived primarily for military and emergency relief applications, more recently, such networks are finding applications in regular wireless packet data environments because they offer convenient deployment, improved coverage, reduced energy consumption, and higher network capacity than traditional infrastructure networks. Concurrent with the expansion of ad hoc network applications, multiple-input multiple-output (MIMO) links have drawn tremendous interest for the extremely high data rates they can support in the absence of interference. A MIMO link has multiple antennas at the transmitter and multiple antennas at the receiver. The multiple transmit antennas are used to transmit multiple parallel streams of data in the same channel; signal processing on the outputs of the multiple receiver antennas separates the parallel streams. In the presence of interference, closed-loop MIMO (CL-MIMO), which requires that the transmitter have channel state information (CSI), is known to significantly outperform open-loop MIMO, which does not use CSI. This project investigates how MIMO can be used in ad-hoc wireless networks to increase their throughput. Previous work by the authors has shown that network throughput can be increased by up to 70% by allowing some CL-MIMO links to interfere with each other, in other words, by allowing space-division multiple access (SDMA) for CL-MIMO links. Control of the number of streams is the key to this problem, because the same parallel stream feature that enables a MIMO link to carry such high data rates also makes interference coming from a MIMO transmitter especially degrading to an unintended receiver. <br\/><br\/>Such sophisticated antenna technology necessitates appropriate developments at the different layers of the network protocol stack. Previous work by the authors has shown that existing multiple access control (MAC) protocols, such as the MAC for the IEEE Standard 802.11, are incapable of attaining the maximum throughputs possible with SDMA and CL-MIMO links. The main objective of this project is to determine a cross-layer physical-layer\/medium-access-control solution for ad-hoc networks with MIMO links. \"Cross-layer\" means that more than the traditional amount of information is shared between the physical and MAC layers. For example, a receiver can estimate, at the PHY layer, how many more streams (either desired or interfering) it can tolerate without being overwhelmed. This information can then be used by the MAC protocol to allow or disallow additional co-channel streams to be transmitted in the area. Specific physical layer research activities include study of (i) the dynamics of the joint-link adaptive process, including the effects of asynchrony of packets and the constraints imposed by the MAC protocol, (ii) various learning opportunities for the transmitter in order to speed convergence, (iii) how nonlinear receiver processor algorithms effect stream control, (iv) realistic channel models, and (v) alternative joint optimization algorithms. Also, the MAC layer research activities in this project are focused on (i) formulation of the medium access control problem in ad-hoc networks with CL-MIMO links, and identification of the key optimization considerations, (ii) design and development of medium access control algorithms and schemes that help achieve the benefits of stream-control, address its drawbacks in the target environment, incorporate the key optimization considerations, and use the unique flexibility offered by the PHY layer in its operation, (iii) molding the resulting MAC protocol to operate within practical and deployable protocol frameworks, and (iv) the development of algorithms through a prototype implementation that","title":"ITR: A Cross-layer PHY\/MAC Solution for Ad-hoc Networks with Multiple-element Arrays","awardID":"0313005","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["560261","562753"],"PO":["348215"]},"82122":{"abstract":"0306845<br\/>Alan M .MacEachren<br\/>Pennsylvania State University<br\/><br\/>GeoCollaborative Crisis Management: Building Better Systems Through Advanced Technology and Deep Understanding of Technology-Enabled Group Work<br\/><br\/>This grant will bring group work and collaborative technologies (dialogue-enabled, distributed) into the area of crisis management and emergency response. Government partners include the Port Authority of New York, NSAS, the Florida Department of Community Affairs, the Pennsylvania Department of Environmental Protection, the National Imaging and Mapping Agency, the Federal Geographic Data Committee, the USGS, EPA, and the Air Force. There is also a commitment from a user interface company, Advanced Interfaces, to provide free software and to support some of the project's research staff.<br\/><br\/>Most commercial Geographic Information systems do not include collaborative technologies, nor an understanding of how emergency responders need to collaborate; this research should push the frontier in both directions. A strong component of training, outreach, and education are also included.","title":"GeoCollaborative Crisis Management(GCCM): Building better systems through advanced technology and deep understanding of technology-enabled group work","awardID":"0306845","effectiveDate":"2003-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["254458","524344","514854","236874",214144],"PO":["469867"]},"83574":{"abstract":"We propose a system, MS-Analyze, to analyze and detect patterns in Multiple Sclerosis (MS), a brain disease that can lead to loss of motor and memory skills and even death, as the disease progresses over time. Magnetic Resonance Imaging (MRI) is used to monitor brain changes at regular time intervals using different imaging modalities, and subjects are also tested for their motor and cognitive fitness, as different drug treatments are explored to slow the disease. Effective correlation of patterns in pathology with drug treatments requires access to large amounts of integrated data, which is usually not available to any one given laboratory.<br\/>MS-Analyze addresses both of these challenges by combining data collection, data fusion, data analysis, and secure data sharing. The proposed work will generate new methods of representing and managing heterogeneous data streams. It will provide new mechanisms for data sharing and research collaboration, fast pattern discovery and a testbed for developing standards for sharing sensitive information. MS is a good application to demonstrate the system because it offers rich data that challenge the system development.<br\/>Broader Impact: Aside from solving the immediate need for data, this project also has a strong educational and training component as it is built also to train users in dealing with complex data as in MS.","title":"ITR:A System for Data Integration and Pattern Discovery in Multimodal, Spatio-temporal Data: Lesion Analysis and Data Sharing","awardID":"0312629","effectiveDate":"2003-07-15","expirationDate":"2007-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}}],"PIcoPI":["486250",217881,217882,"229551"],"PO":["563751"]},"82012":{"abstract":"CCR 0306382<br\/>PI: Rajeev Alur <br\/>University of Pennsylvania<br\/><br\/>Abstract:<br\/>With recent advances in algorithms for state-space traversal and in techniques for automatic abstraction of source code, model checking has emerged as a key tool for analyzing and debugging software systems. This proposal is centered around the role of games in modeling and analysis of software systems. Games are useful in modeling open systems where the distinction among the choices controlled by different components (for instance, the system and its environment) is made explicit. The first thrust of this research will investigate application of games for component-based design and modular verification. It will explore how games can be used to generate abstractions that capture the most general environment assumptions needed to satisfy<br\/>requirements, and as dynamic types for interfaces. The second thrust will address the challenge of scalability in presence of high computational complexity of the analysis problems. Symbolic techniques and heuristics for solving games with partial information will be developed using satisfiability solvers and packages for manipulating binary decision diagrams. These techniques will be implemented and and applied to case studies in domains such as network protocols, device drivers, and medical devices.","title":"Games For Formal Design And Verification Of Reactive Systems","awardID":"0306382","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["497082"],"PO":["564388"]},"83123":{"abstract":"This project advances the state-of-the-art in distributed collaborative <br\/>computability in the presence of adversity. This is accomplished by <br\/>establishing complexity bounds for fundamental distributed computing <br\/>primitives. The key problems requiring distributed collaboration <br\/>include: performing a common set of tasks in a distributed setting, <br\/>modifying shared memory in a parallel setting, distributed <br\/>collaborative scheduling, collective coin-flipping and leader <br\/>election, and algorithms for gossip and consensus in message-passing <br\/>settings. This research is pursued along two complementary directions:<br\/>(1) distributed computability in abstract information models, and<br\/>(2) distributed algorithmics in specific models of computation.<br\/>Information models are tools that model information in distributed <br\/>systems. Information models capture essential features of wide classes <br\/>of low-level computing models: by proving strong bounds in select <br\/>information models, this research extracts new facts about distributed <br\/>computation in extant low-level models and expands the understanding <br\/>of the essential ingredients of distributed computation. Information <br\/>models facilitate reasoning about distributed algorithms in a fashion <br\/>insulated from the idiosyncrasies of particular low-level models, e.g., <br\/>shared-memory or message-passing models under various assumptions about <br\/>synchrony. The second research direction supports the information model <br\/>research by exploring fundamental properties and intrinsic limitations <br\/>of distributed computing environments from an algorithmic point of view.<br\/>This research considers models of computation focusing explicitly on<br\/>the means of communication used by multiple collaborating processors. <br\/>When studying failures or asynchrony, each of the models is augmented <br\/>with an adversary that interferes with the communication. The goal is <br\/>to develop algorithms that are efficient with respect to a composite <br\/>complexity measure simultaneously reflecting several standard complexity <br\/>measures (e.g., time, rounds, communication). Together, these approaches <br\/>address the problem of distributed algorithm design and analysis by <br\/>treating high-level information flow separately from the underlying <br\/>algorithmic building blocks.<br\/><br\/>Broad impact:<br\/>This project, as a whole, demonstrates the feasibility of a new approach <br\/>to the problem of modeling distributed computation. In this \"information <br\/>model\" approach, one trades problem generality for model independence; <br\/>that is, by focusing on highly specific assumptions about information <br\/>flow (which restrict the family of computational problems captured by <br\/>the model) one obtains results relevant to a wide class of low-level <br\/>computing models. Such a framework is quite appealing for the study of<br\/>distributed computing which, unlike uniprocessor computing, has<br\/>suffered from steadfast disagreement about the validity of extant<br\/>low-level models.<br\/><br\/>The proposed research involves several well-prepared graduate<br\/>students. The project, while addressing issues of interest to<br\/>the entire distributed computing community, is an opportunity for<br\/>these students to apply tools from applied mathematics to problems in<br\/>computer science, become expert with extant low-level computing<br\/>models, and engage in original research in the foundations of<br\/>distributed computation.","title":"Collaborative Research: Distributed Collaborative Computing and Adversity","awardID":"0310503","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["450972"],"PO":["499399"]},"82067":{"abstract":"The notion of prototyping is pervasive in engineering design; one investigates the viability of a new idea by constructing a single implementation, a prototype. Whether the problem is designing an integrated circuit transceiver, a micromachining-based thumbnail-sized chemical agent detector, or an aircraft, the cost and time required to<br\/>construct prototypes is high enough to discourage comprehensive design exploration. It is possible to dramatically reduce the need for physical prototypes by substituting computer models, a process referred to as computational prototyping. The promise of using computational prototyping is that the ease of testing alternative designs will allow designers to examine more radical, and possibility much more efficient, design alternatives. The challenge of computational prototyping is in developing accurate modeling algorithms and techniques which are both flexible and fast enough to allow designers to examine a wide range of design alternatives.<br\/><br\/>For complicated systems, which may have millions of interacting components, computational prototyping using direct numerical simulation is too slow for designers to use in design exploration. Instead, it is necessary to exploit hierarchy in the computational prototype, and most hierarchical computer verification and optimization tools rely on manually generated high-level models for function blocks in the design. This ``by-hand'' process is time-consuming and error-prone, and interferes with rapid deployment of new technology. For this reason, there is strong interest in developing techniques which automatically generate accurate high-level models from more detailed numerical simulation.<br\/><br\/>Over the past decade, substantial effort has been devoted to finding automatic strategies for extracting high-level models from linear interconnect and packaging. This effort was successful in a very practical sense, commercial computer-aided companies now provide users with a wide range of very sophisticated techniques for extracting<br\/>high-level models of interconnect. Chip designers are no longer required to be signal integrity experts. In addition, the research also substantially deepened the understanding of the general problem of model reduction. And it is from this only recently achieved<br\/>vantage point that we now think we can start to tackle the next two problems: generating {\\it parameterized} reduced-order models for use in hierarchical optimization, and automatically reducing the nonlinear systems associated with micromachined devices or analog subsystems. <br\/>We originally proposed to investigate both the problems of nonlinear model<br\/>reduction and parameterized model reduction. With the reduced budget,<br\/>we will investigate only the nonlinear model reduction problem, to <br\/>automatically generate low-order models of micromachined devices and<br\/>analog subsystems. We will be examining strategies involving nonlinear<br\/>generalizations of balanced realizations, and combining such strategies<br\/>with trajectory piecewise linearizations to generate accurate subsystem<br\/>models.","title":"Parameterized Model Reduction Techniques for Simulation and Optimization of Mixed-Signal Systems","awardID":"0306588","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["292331","384455",213994],"PO":["562984"]},"78514":{"abstract":"The goal of this research is to expand our knowledge of communication theory, information theory, and signal processing, and to make an immediate impact in practical wireless communication systems and networks. The research builds upon the general theory and code design strategy for space-time coding. The mathematical methods, algorithms, and concepts developed in this research is applicable and initiates new activities in many different areas of communications and applied mathematics. These areas include modem design, equalization, beam-forming, linear algebra, and so<br\/>on. Of course, the immediate impacts are in increasing the data rates of wireless communication systems as well as advancing our understanding of the behavior of multiple-input multiple-output<br\/>channels.<br\/><br\/>Currently, there is a fundamental theoretical limit which prohibits us from designing full-rate orthogonal space-time block codes for more than two transmit antennas. Also, the available codes have not achieved the best performance claimed by information theory, so there exists a gap between what is possible, and the state of codes today. We seek to eliminate this gap by designing new classes of codes that provide full diversity, high rate, small complexity, high performance and mathematically interesting structures.","title":"CAREER: Coding for multiple-input multiple-output channels","awardID":"0238042","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["518462"],"PO":["348215"]},"83630":{"abstract":"Noiseless Data Compression is a key information technology used in innumerable<br\/>data storage and transmission applications ranging from computer operating systems<br\/>to modems to lossy compression standards. Although the state-of-the-art has<br\/>reached a certain level of maturity, with data compression algorithms that reach<br\/>the fundamental information theoretic limits with linear computational complexity,<br\/>they suffer from several shortcomings when used in packetized noisy channels.<br\/>For this reason, no payload data compression is currently implemented in third-generation<br\/>standards for high-speed wireless data transmission.<br\/><br\/>In this project, we explore an alternative avenue to the conventional approach<br\/>which seeks to design and analyze noiseless data compressors that are suitable for use<br\/>in packetized data transmission through noisy channels while retaining the favorable<br\/>properties of existing algorithms in terms of complexity and elimination of redundancy.<br\/>The new approach is based on the use of modern capacity-approaching<br\/>error-correcting encoding and decoding algorithms<br\/>(such as low density parity check codes and belief propagation, respectively)<br\/>in a novel way that capitalizes on the recent discovery of a reversible<br\/>transformation (block-sorting transform), which<br\/>previous research by the PI and his collaborators has shown to transfer<br\/>essentially all the memory redundancy present in ergodic<br\/>discrete information sources to redundancy in the individual symbol outcomes.<br\/>One of the most exciting applications of the new class of algorithms<br\/>is the problem of joint data compression\/transmission. While Shannon's<br\/>separation principle establishes no loss in asymptotic performance<br\/>when compression and transmission are performed separately, it has long been<br\/>expected that, in the nonasymptotic regime, gains may accrue by joint design.<br\/>However, this promise has not yet been realized as existing schemes that take into<br\/>account the source statistics at the decoder can only cope with very simplistic models.","title":"ITR: Noiseless Data Compression Based on Error Correcting Codes","awardID":"0312879","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["550344"],"PO":["348215"]},"83531":{"abstract":"This project's goal is to develop a synthetic talking face. Humans developed sophisticated abilities to perceive and integrate auditory and visual (AV) speech information long before they were required to read printed text presented by computers. Seeing as well as hearing speech reduces the cognitive workload and improves comprehension over only hearing the talker. To realize the advantages of AV speech for human-computer interactions requires synthesizing visual speech, thereby providing an unlimited supply of visual speech images without having to pre-record data. The approach here is to drive optical speech synthesis with speech acoustics. Computational methods obtain models of the transformation from acoustics to optics. The method capitalizes on the speech production coarticulatory information captured by diphones to produce naturalistic visual speech images. The method is applied directly to natural acoustic speech features to obtain coordination between acoustic and optical signals. The synthesized visual speech is based on a texture-mapped wire frame model. A natural speech corpus to base the synthesis is being obtained via simultaneously recorded 3-D optical, audio, and video data. Synthesis development is guided by human perceptual testing. The DVD archived corpus will be disseminated. <br\/><br\/>The project will lead to expanded access to information and improvement in obtaining knowledge by diverse groups of individuals, for example: children still acquiring literacy skills; adults with inadequate literacy; individuals who are using a second language; and individuals with hearing losses who rely on audiovisual speech. Results will be disseminated broadly through professional outlets. Graduate and undergraduate students will participate.","title":"ITR-Collaborative Research: Development and Evaluation of a Hybrid Concatenative\/Rule-Based Visual Speech Synthesis System","awardID":"0312434","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["416490",217778,217779],"PO":["565227"]},"82442":{"abstract":"Title:<br\/><br\/>Tying Low-level and High-level Planners together with Cellular Decomposit<br\/>ions<br\/><br\/>Abstract:<br\/><br\/>One of the grand challenges in planning is tying together high-level<br\/>global planners with low-level local planners, often called behaviors<br\/>or control laws. Global planners provide provable guarantees but are<br\/>often too abstract to respect the details and dynamics afforded by<br\/>local planners, which in turn lack provable guarantees. The proposed<br\/>work aims to get the best of both worlds: inhereting the provable<br\/>completeness of global planners while respecting the local details and<br\/>dynamics with local planners. The real challenge is: how does one tie<br\/>together the high-level and low-level aspects of a planner. The<br\/>proposed work takes a topological approach towards addressing this<br\/>problem, addressing three focused problems -- planning in narrow<br\/>corridors with probabilistic methods, defining hybrid control laws for<br\/>navigating and mapping tasks, and sensor-based exploration of highly<br\/>articulated bodies. The broad impact of this work lie in path planning<br\/>for non-Euclidean configuration spaces and developing control laws for<br\/>those spaces, as well as further develop a LEGO robotics lab module.","title":"Tying Together Low-Level and High-level Planners with Cellular Decompositions","awardID":"0308097","effectiveDate":"2003-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6857","name":"DIGITAL LIBRARIES AND ARCHIVES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["553216"],"PO":["565227"]},"82453":{"abstract":"Robotics and Human Augmentation Program<br\/><br\/>ABSTRACT<br\/><br\/><br\/>Proposal #: 308157<br\/>Title: Interactive Multisensory Modeling of Physical Objects<br\/>PI: Pai, Dinesh<br\/>Rutgers Univ New Brunswick<br\/><br\/><br\/>We will investigate techniques for easy and interactive construction of high quality multisensory computer models of real, physical objects. By multisensory models we mean models that are capable of supporting multisensory interaction, supporting not only simulation of the appearance of an object but also its physical response, including associated forces and sounds. Because human interaction with the physical world is inherently multisensory, people will be able to effortlessly combine information from vision, hearing, and touch to manipulate such computer models.<br\/><br\/>Specifically, we propose to develop an integrated environment with excellent conditions for interactive modeling by humans. We will explore new techniques to make the modeling task extremely easy, by providing rapid feedback about the state of the model, developing novel sensing and display systems, and developing software tools to plan and generalize measurements. We will use these techniques to interactively create multisensory models of contact.","title":"Interactive Multisensory Modeling of Physical Objects","awardID":"0308157","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["455506"],"PO":["317663"]},"83322":{"abstract":"A computer program is written in a high-level *programming*<br\/> *language*, but to be run, it must be translated into a *machine*<br\/> *language*. Each kind of machine---desktop computer, cell phone,<br\/> handheld computer---has a different machine language and needs a<br\/> different translator. At present, each translator must be written by<br\/> an expert who specializes *both* in translation *and* in machine<br\/> language. This project will develop methods by which a translator can<br\/> be written without such double expertise. An expert who knows only<br\/> about the machine will write a *declarative* description that says<br\/> what the machine does. The project's software will analyze this<br\/> description and mechanically produce components to be used by an<br\/> expert who knows only about translation. Because this approach relies<br\/> crucially on the description's being correct, the project will also<br\/> develop techniques for mechanically checking that the description is<br\/> consistent with real hardware. The descriptions and the analysis<br\/> thereof will enable the machine expert's work to be reused in many<br\/> different translators (and other applications) and the translation<br\/> expert's work to be readily applied to many different machines.<br\/> Translators for new machines will be developed more quickly, more<br\/> reliably, and with less effort than at present.","title":"Using Declarative Machine Descriptions in a Retargetable Optimizing Compiler","awardID":"0311482","effectiveDate":"2003-07-01","expirationDate":"2007-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["386750"],"PO":["565272"]},"82035":{"abstract":"This research is on efficient algorithms for: (i) normalization, i.e., simplification of expressions, using rules (called a rewrite system), and (ii) fundamental properties of rewrite systems. Algorithms that use analysis for normalization with static rules are being developed. Incremental algorithms for normalization with dynamic rules are also being investigated. Extensions of efficient tabling algorithms are being studied. Practical performance of algorithms is being evaluated in the Laboratory for Rapid Rewriting test bed developed at University of Houston. Efficient algorithms and lower bounds are being studied for several fundamental properties, including uniqueness of normal forms, confluence, and the word problem, for decidable subclasses. Tight relationships among these problems are being studied using the concept of resource-bounded reductions. <br\/><br\/>Normalization is a fundamental operation found in virtually all symbolic computation and computer algebra systems. Applications include functional and equational logic programming, data type specification, formal verification, automated deduction, code generation, and type inferencing. This research is expected to yield: (i) enhanced efficiency and power of symbolic computing and computer algebra systems, (ii) better understanding of inherent complexities of the involved operations, and (iii) new results, techniques and insights into fundamental properties of rewrite systems. Broader impacts include: training of undergraduate and graduate students including underrepresented minorities, new course materials based on this research, and dissemination of software developed for extensions and use.","title":"Algorithmic and Foundational Aspects of Rewriting","awardID":"0306475","effectiveDate":"2003-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["550310"],"PO":["565157"]},"77723":{"abstract":"Grid computing, or using several potentially heterogeneous computers to execute an application, is becoming increasingly popular. A grid poses significant software challenges for parallel computing; in particular, fundamental parallel computing problems are made more difficult. This project focuses on the data distribution problem, which is to distribute data to processors to minimize application completion time. We will re-examine the data distribution problem on one specific instance of a grid: a local area network of heterogeneous computers. Processors in such a cluster can have different relative CPU speeds, different amounts of available physical memory, and different effective I\/O latencies. In addition, we will support the increasing number of out-of-core programs on these clusters.<br\/><br\/>We will address this data distribution problem through the design and implementation of what we call Heterogeneous Cluster MPI (HC-MPI), which will be constructed as extensions and analysis built into MPI. The extensions will (1) assist users in writing adaptable programs and (2) allow HC-MPI to collect enough program information to determine automatically an effective data distribution. The expected impact of this work is to allow computational scientists to execute parallel programs efficiently on a variety of heterogeneous clusters.","title":"SOFTWARE: Heterogeneous Cluster MPI: A System for Out-Of-Core, Heterogeneous Data Distribution","awardID":"0234285","effectiveDate":"2003-07-01","expirationDate":"2006-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["517411"],"PO":["565272"]},"78603":{"abstract":"Mobile code can be defined as executable content that is transferred to a<br\/>remote environment and executed there automatically. Attacks from malicious<br\/>mobile code can be detected by analyzing the information associated with the<br\/>execution of code and identifying malicious behavior. This analysis process is<br\/>called intrusion detection while the process of collecting the necessary<br\/>information is called auditing.<br\/><br\/>Unfortunately, most systems that support code mobility provide no auditing<br\/>mechanisms or are able to produce only incomplete information about the<br\/>activity of mobile code.<br\/><br\/>To overcome these problems, a multi-level approach to malicious mobile code<br\/>detection is proposed. The approach relies on the instrumentation of the<br\/>different components of the mobile code execution architecture to gather<br\/>complete information about the actions of the mobile code. The events<br\/>collected at different abstraction levels are used as input to multi-stream<br\/>intrusion detection analysis. The intrusion detection process uses both fusion<br\/>and correlation techniques to detect attacks and perform proactive response<br\/>procedures that limit the impact of an attack. In particular, the research<br\/>focuses on the containment of the spread of worm applications.<br\/><br\/>The results of this research will be used both to retrofit existing systems<br\/>and to secure future applications. In particular, in the near future mobile<br\/>code will become a fundamental mechanism for the upgrade and management of<br\/>mobile devices, as IP connectivity is brought to the millions of cellular<br\/>phones in use today. The use of multi-level intrusion detection will provide<br\/>techniques to protect both the infrastructure and the user terminals against<br\/>malicious mobile code.","title":"CAREER: A Multi-Level Approach to Malicious Mobile Code Detection","awardID":"0238492","effectiveDate":"2003-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2802","name":"TRUSTED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["535035"],"PO":["529429"]},"86017":{"abstract":"This is an interdisciplinary project of theoretical research on design of quantum and biomolecular information processing systems. Specifically, the rate at which increases in the size\/complexity of an information processing system can lead to improvements in robustness against environmental perturbations and systematic implementation errors are being investigated, starting with utilization of model reduction methods from control theory to develop general methods for scrutinizing the robustness of large but finite quantum and biomolecular systems and proceeding to detailed analyses of the efficiency of various known error-correction schemes. The overall goal is to be able to elucidate fundamental limits on the degree of redundancy required to achieve a given level of robustness. An integral part of this research is development of a new course on multi-scale design of information processing systems.","title":"Complexity and Robustness in Quantum and Biomolecular Information Processing Systems","awardID":"0323542","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["490256","544518"],"PO":["521045"]},"82432":{"abstract":"Robotics and Human Augmentation Program<br\/><br\/>ABSTRACT<br\/><br\/><br\/>Proposal #0308056<br\/>Title: Compliant Frame Modular Mobile Robotic Systems<br\/>PI: Minor, Mark<br\/>University of Utah<br\/><br\/><br\/><br\/>Compliant framed wheeled modular mobile robots are the subject of this research. The system consists of rigid differentially steered axle modules coupled by flexible frame modules that provide compliant roll, pitch, and yaw for suspension and highly controllable steering without added hardware. Modularity provides a structure that is easily scaled and reconfigured for a variety of applications oriented around dissimilar configurations. Preliminary studies focusing on a two-axle scout configuration have derived curvature based planning algorithms optimizing energy usage and control authority, developed a modular model structure for dynamic motion control, and established sensor hardware and fusion algorithms for improved postural evaluation. More complex configurations, however, introduce complicated nonholonomic constraints and additional challenges for motion planning and dynamic stabilization. Hence, this research investigates scalable motion planning algorithms and dynamic controllers that provide functionality to modular mobile robotic systems. These algorithms must operate within a distributed modular environment where emphasis is placed on low-level stabilization and planning algorithms, which also requires further modularization of sensor fusion algorithms.<br\/><br\/>Intellectual merit of this research lies in the fact that it examines a novel, and previously unexplored, methods of realizing scalable mobile robotic systems. The potential of this system to expand the service of mobile robotics in space exploration, military service, and even agriculture, mining, and forestry, are tremendous. Broader impacts will be realized within the robotics community, the educational processes at the University of Utah, and engineering outreach activities within the state of Utah. Impact on robotics research will be realized in motion planning, dynamic control, sensor instrumentation, and group behavior. Laboratory experiments and design examples will be derived for the undergraduate mechatronics curriculum, and graduate control courses will benefit from practical examples and experimental platforms. Outreach activities where students interact with mobile robots and learn more about underlying technology have already been developed, and further advances will continue to be made.","title":"Compliant Frame Modular Mobile Robotic Systems","awardID":"0308056","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":["508539"],"PO":["335186"]},"82454":{"abstract":"Proposal #: 308185<br\/>Title: Complex Reflectance, Texture and Shape: Methods and Representations for Object Modeling<br\/>PI: Belhumeur, Peter<br\/>Columbia University<br\/><br\/>This research is to investigate techniques for modeling objects with complex reflectance. Our aim is to develop methods for recovering both shape and reflectance properties of natural objects -- objects that due to their inherent complexity have proven elusive to traditional shape reconstruction techniques such as binocular stereo, structure from motion, photometric stereo, and even laser range finders. Understanding the interplay between shape and reflectance is critical to object modeling and is in many ways central to the field of computer vision. Yet this promising avenue of research has been overlooked in the past, either due to acquisition and storage limitations or, perhaps, due to a lack of appropriate techniques. Recently, the PI's have introduced and prototyped two novel methods for shape reconstruction and reflectance recovery that<br\/>essentially exploit two neglected physical principles. These two methods, named Helmholtz stereopsis and light field reconstruction, are able to recover surface shape regardless of the object's reflectance (BRDF), and they offer numerous advantages over conventional reconstruction techniques. The proposed research includes new shape reconstruction techniques, methods for constructing object models including their shape and BRDF on a point-by-point basis, and applications of these models to image-based rendering. In addition, these reconstruction techniques will serve as tools for a broad study of BRDF's in general as well as 3-D textures whose appearance varies with viewpoint and lighting. <br\/><br\/>Unlike current shape estimation techniques, the proposed methods will produce object models with both geometric and BRDF data. This new capability in object modeling technology will be beneficial in areas such as industrial design, architecture, entertainment, robotics, and medicine. Estimation of the reflectance function across the reconstructed objects, surface normals, and shape, leads directly to new methods for image-based modeling and rendering for computer graphics applications. These modeling techniques will be used to study reflectance properties of 3-D textures and natural materials (including human skin), and this will help to produce more effective generative appearance models that can be used in security applications (e.g., face recognition), human computer interaction applications, entertainment, learning through visual simulation, communications, etc. In addition to the applications of the resulting research and its dissemination through peer-reviewed publications, funding will be used to support the training of two Ph.D. students over three years.","title":"Complex Reflectance, Texture and Shape: Methods and Representations for Object Modeling","awardID":"0308185","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["426630","519250"],"PO":["317663"]},"83312":{"abstract":"This project explores newly developed graph theoretic models and their connections to problems of molecular biology. One of the fundamental challenges in molecular biology research is to construct physical maps of a genome. In order to study a long contiguous segment of DNA, physical mapping starts by cutting DNA into relatively small fragments, called clones, at certain specific locations on the genome. The goal of physical mapping is to arrange the segments as intervals along a line (the linear chain of DNA), so that their pairwise intersections match the experimental data. To develop algorithms to construct maps, a tagged probe interval graph (TPIG) model has been introduced, which arises when some of the clones are radioactively labeled. The TPIG model is a refinement of the probe interval graph (PIG) model introduced for the DNA physical mapping. <br\/><br\/>Improvements upon the existing algorithms are sought to get linear time recognization algorithms for both PIGs and TPIGs that have predefined partition of probes and nonprobes. For the unpartitioned version, the complexity of whether or not the recognization problem is polynomial time solvable is studied. Heuristic algorithms that are robust in the presence of error will also be developed. Finally, probabilistic models are introduced and, using these models as a theoretical framework, average case analyses for selected algorithms will be investigated.<br\/><br\/>The project will also promote teaching, training, and learning. By showing students how mathematical modeling and algorithm analysis has been successfully applied to the field of biology, the PI intends to attract more students to enjoy mathematical thinking, and graph theory modeling for problems they encounter, and to appreciate the powerful and interesting aspect that mathematics and theoretical computer science can add to their work. This is also a major contribution of the PI to the new interdisciplinary bioinfomatics program that is underway at Drexel University.","title":"Physical Mapping: Models, Complexities, and Algorithms","awardID":"0311413","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[217197],"PO":["499399"]},"81387":{"abstract":"Proposal No: 303789<br\/>Title: NER: NanoCAD - Computer-Aided Design Algorithms and Tools for<br\/>Nanotechnologies<br\/><br\/>There has been a huge surge of research activity in recent years in the area<br\/>of nanoelectronic device design and fabrication. Such ``nanodevices'' <br\/>include solid-state quantum-effect devices and molecular devices. <br\/>In terms of work at the logic and architecture levels, quantum cellular <br\/>automata (QCA) based on quantum dot cells, and resonant tunelling diode <br\/>(RTD) based implementations seem ahead of others, and chips based on <br\/>these devices are likely to be feasible within this decade. While working <br\/>devices and logic gates, and even some small circuits, have been <br\/>demonstrated for various nanotechnologies, no logic synthesis methodology <br\/>has evolved yet for automatically synthesizing large logic circuits in these <br\/>technologies. The aim of this proposal is to develop automatic logic <br\/>synthesis algorithms and implement them as software tools to bridge this <br\/>gap. In this exploratory phase of research, we will target QCA and RTD <br\/>based logic circuits.<br\/><br\/>Specifically, we will develop a general multi-level logic synthesis<br\/>algorithm and software tool using threshold gates and majority gates<br\/>as primitive gates. We will derive bit-level pipelined versions <br\/>of different arithmetic units to exploit the concept of nanopipelining. <br\/>We will also derive physical synthesis rules for QCA to ensure its correct<br\/>functionality. <br\/><br\/>The software tools developed in this research will be made available on the<br\/>world-wide web for wide dissemination. The material will be included in <br\/>a new graduate-level course on Computer-Aided Design for Emerging <br\/>Nanotechnologies. Undergraduates will also be involved in this research. <br\/>Princeton encourages applications from women and minority students through <br\/>special fellowships. Such students will be specially welcome to join this <br\/>research.","title":"NER: NanoCAD - Computer-Aided Design Algorithms and Tools for Nanotechnologies","awardID":"0303789","effectiveDate":"2003-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["550002"],"PO":["562984"]},"83367":{"abstract":"Phase-Directed Architecture Optimization and Simulation<br\/><br\/><br\/><br\/>A program's execution typically exhibits time-varying behavior, where architecture resources needed to obtain peak performance for the application can vary significantly over time. We have found that this time-varying behavior has a repetitive pattern for most applications, which allows a program's execution to be broken up into phases, and we can then tailor a program's execution to each phase.<br\/><br\/>This proposal concentrates on developing efficient run-time architectures to capture and predict phase information. We will examine creating phase tracking and prediction architectures for power and energy architecture optimizations as well as to perform hardware code specialization based on phases for trace cache optimizations. Energy consumption can potentially be saved by dynamically re-partitioning many components in a processor (e.g., caches, issue and decode width, branch predictors) on an application specific basis, or selectively using different implementations of these components based upon the phase behavior seen in an application. In addition, trace cache compiler optimizations will be performed for different phases to exploit phase-based behavior.<br\/><br\/>For broader impact, our phase-classification algorithms will allow researchers to efficiently identify, classify, and predict phase-based behavior in programs and use this information to guide their phase-based hardware, compiler, and operating system research.","title":"Phase-Directed Architecture Optimization and Simulation","awardID":"0311710","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["293079"],"PO":["325495"]},"83257":{"abstract":"This project will explore the use of neural learning in micro architectural predictors such as branch predictors. Neural branch predictors replace commonly used counter-based techniques with neural learning, providing better predictive capabilities. Special emphasis will be placed on reducing the latency of neural predictors, which hinders their contribution to overall performance. We expect the following results: (1) improved accuracy for neural predictors, (2) decreased impact of neural predictor latency on performance, (3) a deeper under understanding of the properties of the branch prediction problem that are exploited by neural predictors, and (4) new digital circuits for implementing neural predictors in micro architectures.<br\/><br\/>Microprocessors predict the near-term behavior of a program so that work on future instructions may begin early, reducing the amount of time the program takes to run. These predictions must be highly accurate. Borrowing concepts from neuroscience, we will explore the use of artificial neurons to replace the predictors used today. Artificial neurons have been used as predictors in other domains, and preliminary results show that they work well for predicting program behavior. We will improve these results by exploring ways to make the neurons work faster, more accurately, and with a larger scope, thus improving overall performance of computers.","title":"Improving Microarchitectural Performance with Neural Predictors","awardID":"0311091","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["554979"],"PO":["325495"]},"83632":{"abstract":"A highly secure information network can be build using quantum cryptography by distributing the data encryption keys using single photons. Such a network would be immune to any technological advances in computing technology as the key is encrypted on a quantum physical level. This project focuses on a novel dynamically reconfigurable multi-user quantum key distribution (QKD) system using an optical fiber Sagnac Interferometer. The project will include an experimental demonstration of a four user QKD system that allows multiple quantum encryption links to be established in the same physical optical network. Theoretical studies will focus on the security of the Sagnac Interferometer based multi-user QKD network and its immunity to malicious eavesdropping. In addition, the multi-user QKD network in this project will provide a physical platform that allows the system to simulate the effects of eavesdropping. <br\/>.","title":"ITR-Reconfigurable multi-user quantum key distribution using optical fiber sagnac interferometer","awardID":"0312890","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["559204","545256"],"PO":["565223"]},"81696":{"abstract":"Despite the general belief that research in dense linear algebra libraries has been exhausted, history shows that as new architectural features appear in high-performance computers used for scientific applications, the need for the renewed investigation of widely used dense linear algebra packages resurfaces. This has been the case when vector supercomputers first appeared in the 1970s, when microprocessor based workstations appeared in the 1980s, and when distributed memory parallel architectures appeared in the 1990s. Now, with the emergence of multi-level memories in both sequential and parallel architectures, a redesign is once again warranted, as the performance attained by the current generation of dense linear algebra packages does not match that of the best optimizations of algorithms for individual operations. A closely related concern is that the existing libraries do not always have the functionality nor the performance required by the scientific computing community.<br\/><br\/>The fundamental problem with the traditional approach to developing a new dense linear algebra library from a previous library is that it has been inherently evolutionary. There has been a heavy emphasis on maximal code-reuse in the belief that the ``correctness'' (established largely through exhaustive testing) of the previous library is then inherited by the new library, thus reducing the effort required to produce that new library. Unfortunately, there are identifiable reasons why the evolutionary approach has failed in the past and is doomed to failure in the future. The fundamental premise behind this proposed project is that a revolutionary approach must be developed if the repeated investment of effort is to be avoided.<br\/><br\/>Recent research has uncovered a systematic approach to the derivation of provably correct dense linear algebra algorithms via the application of classic derivation techniques from computer science. The practical solutions that may now be within reach include the (partially) automatic development (derivation, implementation, and cost and stability analysis) of high-performance dense and banded linear algebra libraries. This is in contrast to the traditional<br\/>approaches for implementation of such libraries for which the development, debugging, and maintenance are tedious and error-prone processes because of their complexity. The proposed work promises to deliver libraries that require little or no maintenance through the systematic and direct translation of systematically derived, provably correct algorithms to an imperative programming language.<br\/><br\/>The proposed work will lay the foundation for such automated systems by concentrating on developing the systematic approaches mentioned above, without yet venturing into automation. In addition, prototype libraries, coded using the latest software engineering techniques, will be developed to demonstrate the potential of the approaches. In particular, the ability to overcome the apparent cost of the abstractions that drive the systematic approaches by using C++ techniques like template meta-programming and expression templates will be central to the study. Success will be measured by the degree to which the systematic approaches will enable automation, by the new algorithms that will be uncovered using the methodology, and by the<br\/>performance that can be demonstrated (on sequential and parallel architectures) by the resulting prototype libraries. Automated methods currently used by other projects are not deemed to be competitive, in terms of performance and flexibility, with the proposed libraries.","title":"ALG: ICollaborative Research: A Systematic Approach to the Derivation, Representation, Analysis, and Correctness of Dense and Banded Linear Algebra Algorithms for HPC Architectures","awardID":"0305163","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["556801"],"PO":["565272"]},"83302":{"abstract":"This project advances the state-of-the-art in distributed collaborative <br\/>computability in the presence of adversity. This is accomplished by <br\/>establishing complexity bounds for fundamental distributed computing <br\/>primitives. The key problems requiring distributed collaboration <br\/>include: performing a common set of tasks in a distributed setting, <br\/>modifying shared memory in a parallel setting, distributed <br\/>collaborative scheduling, collective coin-flipping and leader <br\/>election, and algorithms for gossip and consensus in message-passing <br\/>settings. This research is pursued along two complementary directions:<br\/>(1) distributed computability in abstract information models, and<br\/>(2) distributed algorithmics in specific models of computation.<br\/>Information models are tools that model information in distributed <br\/>systems. Information models capture essential features of wide classes <br\/>of low-level computing models: by proving strong bounds in select <br\/>information models, this research extracts new facts about distributed <br\/>computation in extant low-level models and expands the understanding <br\/>of the essential ingredients of distributed computation. Information <br\/>models facilitate reasoning about distributed algorithms in a fashion <br\/>insulated from the idiosyncrasies of particular low-level models, e.g., <br\/>shared-memory or message-passing models under various assumptions about <br\/>synchrony. The second research direction supports the information model <br\/>research by exploring fundamental properties and intrinsic limitations <br\/>of distributed computing environments from an algorithmic point of view.<br\/>This research considers models of computation focusing explicitly on<br\/>the means of communication used by multiple collaborating processors. <br\/>When studying failures or asynchrony, each of the models is augmented <br\/>with an adversary that interferes with the communication. The goal is <br\/>to develop algorithms that are efficient with respect to a composite <br\/>complexity measure simultaneously reflecting several standard complexity <br\/>measures (e.g., time, rounds, communication). Together, these approaches <br\/>address the problem of distributed algorithm design and analysis by <br\/>treating high-level information flow separately from the underlying <br\/>algorithmic building blocks.<br\/><br\/>Broad impact:<br\/>This project, as a whole, demonstrates the feasibility of a new approach <br\/>to the problem of modeling distributed computation. In this \"information <br\/>model\" approach, one trades problem generality for model independence; <br\/>that is, by focusing on highly specific assumptions about information <br\/>flow (which restrict the family of computational problems captured by <br\/>the model) one obtains results relevant to a wide class of low-level <br\/>computing models. Such a framework is quite appealing for the study of<br\/>distributed computing which, unlike uniprocessor computing, has<br\/>suffered from steadfast disagreement about the validity of extant<br\/>low-level models.<br\/><br\/>The proposed research involves several well-prepared graduate<br\/>students. The project, while addressing issues of interest to<br\/>the entire distributed computing community, is an opportunity for<br\/>these students to apply tools from applied mathematics to problems in<br\/>computer science, become expert with extant low-level computing<br\/>models, and engage in original research in the foundations of<br\/>distributed computation.","title":"Collaborative Research: Distributed Collaborative Computing and Adversity","awardID":"0311368","effectiveDate":"2003-07-15","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["451177","486281"],"PO":["499399"]},"83335":{"abstract":"With increasing data processing needs, many applications belonging<br\/>to areas such as communication networks, computational biology,<br\/>e-commerce, etc. require the processing of very large data sets under<br\/>time and space limitations. This need makes it necessary to seek<br\/>new, more efficient algorithmic models involving resource restrictions.<br\/>The main objective of this project is to explore the tradeoffs between<br\/>limiting the computing resources (time, space, access to input) available<br\/>to an algorithm and the resulting loss in accuracy and effectiveness.<br\/>By testing quantitative properties as well as qualitative ones, the link<br\/>between the efficiency\/effectiveness of property testing and those of<br\/>approximation algorithms can be investigated. To this end, this project<br\/>involves devising sublinear algorithms for checking combinatorial<br\/>properties of various types of large data objects such as sequences,<br\/>graphs, and sets. This task can be achieved through intelligent random<br\/>sampling of the data, minimizing the number of bits of the input that<br\/>the algorithm needs to access through either reusing samples or choosing<br\/>samples carefully so that each accessed bit yields as much information<br\/>as possible about the input.<br\/><br\/>The broader impact of this project is in improving the understanding<br\/>of resource-bounded computation on different types of large data. The<br\/>new meta-techniques developed are expected to apply to a variety of<br\/>problems in different application areas which now see the processing of<br\/>input as a major bottleneck. For students of computer science, these<br\/>techniques bring a new point of view to understanding the possibilities<br\/>and limitations of computation as well as an understanding of data-related<br\/>limitations and bottlenecks of certain types of applications that need to<br\/>be addressed via unconventional models.","title":"Sublinear Property Testing of Large Collections of Data Objects","awardID":"0311548","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[217270,"545094"],"PO":["499399"]},"86756":{"abstract":"Principal Investigator: James S Byrnes<br\/>Proposal Number: DMS- 0327088<br\/>Institution: private, Newport RI<br\/><br\/>Title: NATO Advanced Study Institute on Computational Noncommutative Algebra and <br\/> Applications<br\/>Abstract<br\/><br\/>One of the most important applications of geometric algebras to geometry is its use for the representation of groups of Euclidean and Minkowski rotations. This aspect and its direct relation to robotics and vision will be discussed by several of the Principal Lecturers at the Advanced Study Institute conference. Group theory, beginning with the work of Burnside, Frobenius and Schur, has been influenced by even more general problems. As a result, general group actions have provided the setting for powerful methods within group theory and for the use of groups in applications to physics, chemistry, molecular biology, and signal processing. These aspects, too, will be covered in detail by many of the Principal Lecturers.<br\/><br\/>The fusion of algebra, analysis and geometry, and their application to real world problems, have been dominant themes underlying mathematics for over a century. Geometric algebras, introduced and classified by Clifford in the late 19th century, have played a prominent role in this effort, as seen in the mathematical work of Cartan, Brauer, Weyl, Chevelley, Atiyah, and Bott, and in applications to physics in the work of Pauli, Dirac and others. The topics of the Advanced Study Institute conference, geometric algebras and computational group harmonic analysis, have emerged as key tools for resolving the ever expanding conceptual and computational demands on signal and image processing as it rapidly grows in importance. Signal and image processing is used in remote sensing, micro and unmanned vehicles, biological processing and neural and quantum computing. The ASI will bring together world leaders from both academia and industry with extensive multidisciplinary backgrounds for an interactive forum; they will initiate new efforts and intensify existing efforts towards a unified computational framework for the advancement of a broad range of applications. The forum will provide opportunities for young scientists and engineers to learn more about problem areas and about the crucial role played by new mathematical insights from recognized experts in this vital and growing area of both pure and applied science. The team-authored proceedings, to be written by the lecturers, will offer these insights to those unable to attend. A key feature of both the ASI and the proceedings will be the presentation of problems and applications that will shape the twenty-first century computational technology base.","title":"NATO Advanced Study Institute on Computational Noncommutative Algebra and Applications; July 6-19, 2003; Tuscany, Italy","awardID":"0327088","effectiveDate":"2003-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1264","name":"ALGEBRA,NUMBER THEORY,AND COM"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[227004],"PO":["565280"]},"86019":{"abstract":"EIA-032355<br\/>Yaoyun Shi<br\/>University of Michigan Ann Arbor<br\/><br\/>New Directions in Quantum Computation and Communication<br\/><br\/>In recent years, it has been demonstrated that quantum information behaves fundamentally different from classical information, and, it appears that computers based on exact quantum mechanical principles can be dramatically more powerful than those currently deployed. This project investigates several fundamental issues in a wide range of aspects of quantum information science. The topics include searching for fast quantum algorithms, systematic classical simulations of quantum circuits, quantum intractability, restricted quantum computations, and quantum communication protocols, among others. The techniques employed are drawn from many branches of mathematics such as probability theory, combinatorics, matrix analysis, and group theory, as well as from the rich subject of classical complexity theory.<br\/><br\/>As one of the most exciting scientific developments in recent years, quantum information science promises far-reaching impacts on the efficiency and the security of information processing. This project could significantly advance our understanding on the inherent power and limitations of quantum computation and quantum information.","title":"New Directions in Quantum Computation and Communication","awardID":"0323555","effectiveDate":"2003-07-01","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["549690"],"PO":["521045"]},"81993":{"abstract":"As analog ICs have rapidly evolved from the relatively low complexity of<br\/>the early days to the high sophistication of today, the need for more advanced behavioral modeling techniques has become increasingly urgent. The objective of this project <br\/>is to investigate a wavelet based nonlinear companding method for the behavioral modeling of analog circuits, and further develops a simulator based on the obtained behavioral models for the analysis and design of PLL circuit, which is one of the most important circuits in today's RF and high-speed VLSI systems. A nonlinear companding method is proposed to regulate the singularity of the wavelets such that a \"smooth\" fitting can be achieved between the wavelets and the concerned circuit functions. Such an approach will even the model error distribution and significantly improves the simulation efficiency at the system level. <br\/><br\/>This project opens a new research direction for the wavelet theory and application. The investigated nonlinear companding approach will make wavelet a more effective basis function for engineering application. It will lead to new method for analog circuit behavioral modeling and greatly improve the efficiency of today's CAD tools for the analysis of mixed-signal systems. Particularly, it will improve the efficiency of the design and analysis of PLL circuit which is one of the most challenge and timing consuming tasks in high performance RF and analog system design.","title":"Wavelet Based Nonlinear Companding Method for Analog Circuit Behavioral Modeling","awardID":"0306298","effectiveDate":"2003-07-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["485314"],"PO":["562984"]},"83501":{"abstract":"This project investigates feasibility of transmitter optimization for rapidly time varying fading channels encountered in wideband mobile radio communication systems. Adaptive transmitter optimization methods have the potential to significantly improve performance in wireless channels by adjusting the transmitted signal-to-fading conditions. However, rapid channel variation makes application of these closed-loop techniques difficult in wireless systems. Previous investigations demonstrate that it is not sufficient to utilize the outdated Channel State Information (CSI) in adaptive transmission algorithms when vehicle speeds are significant. Prediction of the channel coefficients several tens-to-hundreds of symbols ahead is essential to realize these methods in practice. A novel adaptive, long-range fading channel prediction algorithm (LRP) was previously developed by the PI and her collaborators for narrowband wireless channels. In this research, the long range fading prediction capability is extended to frequency selective fading channels with antenna arrays. These methods are utilized in adaptive modulation and coding techniques for multicarrier and multiple antenna systems and in novel precoding methods and joint transmitter\/receiver optimization techniques for Multiple Input Multiple Output (MIMO) wideband mobile radio systems. This research contributes to the development and realization of adaptive transmission and precoding methods that are essential in reliable high rate wireless communication. Moreover, collaboration with industry benefits graduate students involved in the project. Broader participation of underrepresented groups is enhanced since the PI and the graduate students are females. The infrastructure for research and education is enriched by incorporating the methods investigated in this research in senior and class projects and by integrating with wireless communications activities at NC State. <br\/>The first component of this project focuses on utilization of prediction methods in interference cancellation for the MIMO channels that arise in multicarrier, multiple antennae and the downlink of multiuser Direct Sequence Code Division Multiple Access (DS\/CDMA) channels. While the sources of interference depend on the application, these systems can be described with a single underlying MIMO model. Novel Tomlison-Harashima precoding methods are developed for this model. These methods are expected to improve significantly on previously proposed linear precoding techniques. Moreover, they distribute computational resources between the transmitter and the receiver as dictated by the application. For the downlink of the DS\/CDMA channel, novel low complexity linear techniques that have the potential to significantly simplify precoding at the base station and detection at the mobile station are investigated. The proposed interference cancellation techniques are combined with adaptive modulation and coding. Since these precoding and adaptive transmission methods depend on accurate CSI in the transmitter, reliable LRP and the underlying prediction error statistics are essential in the design and the performance analysis of the proposed methods. In the second component, joint prediction for multiple subcarriers in multicarrier systems and antenna array elements in multiple antenna channels is investigated. Novel linear and nonlinear approaches to long-range prediction are explored. A statistical model of the prediction error is developed and utilized in the design of reliable adaptive modulation and coding methods. Flexible prediction methods that trade spectral efficiency for complexity and feedback requirements are explored.","title":"ITR: Adaptive Signaling and MIMO Precoding for Rapidly Time-Varying Fading Channels","awardID":"0312294","effectiveDate":"2003-07-15","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["451742"],"PO":["348215"]},"82423":{"abstract":"Mobile, location-aware devices raise the potential for fundamentally new information services. However, this potential has not been realized. This is due in part to the absence of a firm conceptual and empirical<br\/>foundation. This proposal explores one basis for constructing the requisite foundation: socially defined places. How places (such as schools, offices, or theaters) shape behavior has been explored in environmental<br\/>psychology and architecture, but the notion of 'place' has not been operationalized for use in interactive systems. This project takes on that task. The key hypotheses are that (a) people's information and<br\/>communication needs are relative to place types, and (b) making 'place' a first-class computational object will increase the effectiveness and usability of location-based systems. This project will test the hypotheses<br\/>through a combination of ethnographic studies, development of novel algorithms and interfaces, and laboratory and field studies, leading to the following results: 1) additional empirical knowledge about the concept of place and its role in organizing people's activities; 2) a conceptual framework and guidelines useful to designers of location-based systems; 3) a general infrastructure for place-centered community information sharing systems; and 4) field studies and laboratory evaluations that demonstrate the utility<br\/>and acceptability of such systems.","title":"Collaborative Research: Mark This! - Operationalizing the Notion of \"Place\" For Interactive Community Systems","awardID":"0308018","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["538322"],"PO":["564456"]},"81466":{"abstract":"Proposal No: 304143<br\/>Title: NER: A Novel Nanobiosensor Architecture<br\/><br\/>Abstract<br\/><br\/> We propose to develop a new nanobiosensor architecture based on chemical deposition of gold nanowires into a commercially available polymer membrane, followed by chemical mechanical planarization (CMP), a processing step widely employed in the semiconductor industry for producing extremely smooth surfaces. A smooth surface is necessary to allow the use of specific chemistries to attach enzymes and other proteins to the biosensor surface. This surface can be patterned using the tip of an atomic force microscope (AFM), allowing different proteins to be attached in different areas. The ultimate goal is a nanowire array seamlessly embedded into a polycarbonate membrane that can be patterned to allow parallel detection of different molecules.<br\/>This new nanobiosensor architecture will be validated by immobilizing glucose oxidase onto the gold nanowires. The detection limit of this glucose biosensor will then be determined by measuring the electrochemical current in solutions of varying glucose concentration. In addition, this biosensor will be tested using more advanced electrochemical methods that can also measure the surface capacitance. This nanobiosensor architecture has a number of advantages relative to other electrochemical biosensors. These include the capability for single molecule detection and for miniaturization, biocompatibility, and compatibility with many different detection methods. <br\/> The impact of this research is potentially quite broad, with the possible development of a new generation of gold\/polycarbonate biosensors, including sensors for monitoring living organisms. This could lead to significant advances in the fields of clinical diagnostics, medical implants, environmental monitoring, homeland security, and the food industry. The results of this research will be broadly disseminated, including the Internet, using the PI's multimedia authoring expertise, which was developed through NSF-sponsored curriculum development projects.","title":"NER: A Novel Nanobiosensor Architecture","awardID":"0304143","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1676","name":"NANOSCALE:  EXPLORATORY RSRCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["559829","551560"],"PO":["562984"]},"82005":{"abstract":"The growth in the market for wireless communications devices has reached dramatic proportions in the past decade. This development has spurred rapid advances in RF integrated circuits technology. Competing requirements on the performance of RF circuits pose significant technical challenges at all levels, from architectural design to integrated circuit fabrication. In order to meet these challenges, significant advances in related computer-aided design (CAD) technology are also needed. The availability of innovative CAD tools that cover the entire design process is critical to the successful design of leading-edge RF systems. <br\/><br\/>An important limiting factor affecting the performance of RF components is electrical noise, a phenomenon intrinsic to all electronic devices. The mechanisms responsible for the generation of electrical noise inside semiconductor transistors are poorly understood, and even less so in the case of devices whose size is in the nanoscale range. For this reason, currently it is very difficult, if not impossible, to assess the impact of noise on the performance of circuits built using nanoscale technology. <br\/><br\/>The goal of this research project is the development of computer-aided software to simulate the mechanisms responsible for the generation of electrical noise in nanoscale semiconductor devices. The simulations will be used to develop improved and more accurate mathematical models of those mechanisms, which will be validated against data from experimental measurements available in the literature. These models will make it possible to realize aggressive designs of nanotechnology-based RF systems. <br\/><br\/>Commercial use of the scientific results of this project will be facilitated by the Interconnect Focus Center, a research center established by the Semiconductor Industry Association at Georgia Tech, whose goal is to sustain the technology growth of the semiconductor industry by placing an emphasis on significant long-term research. Integration of research and education will be pursued through graduate and undergraduate courses and seminars and through the Georgia Tech Regional Engineering Program, a distance-learning program established in collaboration with other Georgia universities to offer innovative engineering education and to serve as catalyst for economic development.","title":"Noise Simulation in Nanoscale Semiconductor Devices","awardID":"0306343","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["470917"],"PO":["562984"]},"82126":{"abstract":"New digital technologies have figured critically in the process of deciding the future of Lower Manhattan after September 11th, not only supplying the infrastructure for soliciting public input but also opening new channels of communication between citizens, designers, advocacy groups, and decision-makers. This research builds on ethnographic work on organizational responses to September 11 and on the early stages of public dialogue about the redevelopment of the World Trade Center site. One component of the current research is a comparative longitudinal study of participation in three public forums held to discuss the future of Lower Manhattan, forums that relied on face-to-face, exclusively online, and mixed technologies of deliberation. The project's second component focuses on the online dialogues themselves. A discursive analysis of a sample of the twenty-six online discussion groups that accompanied the face-to-face forum will identify the conditions that facilitate or obstruct group discussions to approximate the equality, reflexivity, reciprocity, and openness that scholars have seen as hallmarks of authentically democratic deliberation.<br\/><br\/>The third and fourth components of the research turn to the process by which multi-vocal public dialogues are translated into \"public concerns\" and then into design plans. By compiling an archive of all the websites devoted to Lower Manhattan redevelopment issues and tracking changes in the form and structure of the websites over time, the project will examine how old and new advocacy groups are adapting to a political landscape in which new deliberative technologies may be challenging traditional mechanisms of citizen participation. In the fourth component of the research, the focus is on communication between citizens, on one hand, and architects and public officials on the other. Analysis of the website database will chart whether and how architects and urban planners are capitalizing on new digital technologies to involve residents more directly in design. Interviews with representatives of the agencies charged with overseeing the development process will examine whether and how they are relying on internet-mediated representations of the public in their decision-making-and with what effect.","title":"DG: Policy Made Public: Technologies of Deliberation and Representation in Rebuilding Lower Manhattan","awardID":"0306868","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}}],"PIcoPI":["525526","483355"],"PO":["371077"]},"83699":{"abstract":"Abstract:<br\/><br\/>Various types of ad hoc wireless networks have been the subject of<br\/>much recent interest. For our purposes, these networks can be<br\/>characterized as a set of nodes that<br\/>communicate with each other over wireless channels using little or no<br\/>fixed<br\/>infrastructure. In such a network, to reach its<br\/>destination, a message may need to be forwarded over several<br\/>intermediate links. Examples of this type network include mobile<br\/>ad hoc networks (MANET's), sensor<br\/>networks, and various hybrid ad hoc\/cellular architectures. Such<br\/>networks<br\/>have the advantages of being rapidly deployable and requiring little or<br\/>no<br\/>fixed infrastructure. However, efficiently utilizing this type of<br\/>network is<br\/>a challenging task due, in part, to the interference between nodes, the<br\/>time-varying nature of the communication channels, the energy<br\/>limitations of untethered nodes, and the<br\/>lack of centralized control via, for example, base stations.<br\/><br\/>This project is part of ongoing research with the goal of<br\/>furthering understanding of basic performance trade-offs for ad<br\/>hoc networks. This work takes a cross layer view and<br\/>considers both physical layer limitations as well as higher layer<br\/>performance metrics such as packet throughput. Techniques from<br\/>queueing theory, optimization, and information theory will be<br\/>utilized in this work. In contrast with our previous work on problems of this type in<br\/>single-hop wireless settings, or, that is, models more appropriate<br\/>for cases such as cellular networks, where all communication is to<br\/>or from a single point, in this project, we are focusing on multi-hop<br\/>ad hoc networks, with multiple transmitters and multiple<br\/>receivers. In this environment, several new issues complicate the<br\/>analysis. These include the routing and forwarding of traffic<br\/>within the network and the scheduling of transmission and<br\/>reception by each node. Specific problems that are being addressed<br\/>include<br\/>characterizing network stability and identifying adaptive resource<br\/>allocation policies which maximize packet throughput.","title":"Collaborative Research: ITR: Fundamental Performance Trade-offs for Ad Hoc Networks","awardID":"0313329","effectiveDate":"2003-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["560218"],"PO":["348215"]},"83226":{"abstract":"Discrete location theory is concerned with combinatorial optimization<br\/>problems in which spatial considerations play a prominent role. For<br\/>example, one basic discrete location problem asks for a specified<br\/>number of points in a given metric space to be marked so that the sum,<br\/>over all points, of the distance to the nearest marked point is<br\/>minimized. Peer-to-peer computing is an emerging discipline concerned<br\/>with the seamless interconnection of dynamic collections of nodes for<br\/>the purpose of sharing computational resources such as bandwidth,<br\/>storage, and CPU. A good first-order model of peer-to-peer networks<br\/>assumes that the nodes reside in a metric space, and that the cost of<br\/>communication between a pair of nodes is given by the corresponding<br\/>distance in the underlying metric space. Because the internode<br\/>distances in peer-to-peer networks tend to vary significantly, spatial<br\/>considerations play a prominent role in peer-to-peer computing. This<br\/>project addresses a number of basic questions in discrete location<br\/>theory, emphasizing questions that arise in applications to<br\/>peer-to-peer computing.<br\/><br\/>Discrete location theory is not a new subject, having been studied<br\/>within the operations research community for decades. Many of the<br\/>basic problems in this area are NP-hard, so practitioners have<br\/>generally resorted to heuristics in order to search for optimal or<br\/>near-optimal solutions. Unfortunately, with few exceptions, the<br\/>solutions returned by these heuristics can be arbitrarily far from<br\/>optimal. In recent years, there has been an explosion of interest in<br\/>the design and analysis of provably good approximation algorithms for<br\/>NP-hard optimization problems. This research has shed considerable<br\/>light on the approximability of a number of basic discrete location<br\/>problems, for example, establishing that the optimal solution to the<br\/>facility location problem can be approximated to within a factor of<br\/>1.52 in polynomial time, but that achieving a factor of 1.463 would<br\/>violate a standard hardness assumption. Given such theoretical<br\/>advances, it is natural to ask whether these provably good<br\/>approximation algorithms are preferable to existing heuristics in<br\/>practice. For the most part this is not the case, since the running<br\/>times of these approximation algorithms, while polynomial, tend to be<br\/>considerably higher than those of existing heuristics. For the<br\/>discrete location problems addressed in this project, the primary<br\/>objective of this project is to develop constant-factor approximation<br\/>algorithms that are comparable to (or improve upon) existing<br\/>heuristics in terms of running time and ease of implementation.<br\/><br\/>Peer-to-peer computing is a young field with obvious potential, but<br\/>significant technical challenges remain to be addressed before this<br\/>potential can be realized. More scalable than client-server,<br\/>peer-to-peer enables a wide-range of large-scale applications that<br\/>cannot be supported by existing server farms. Furthermore,<br\/>peer-to-peer computing holds the promise of delivering<br\/>supercomputer-like performance to the average user. But before such<br\/>lofty goals can be achieved, existing peer-to-peer systems need to be<br\/>significantly refined and extended in order to comprehensively address<br\/>issues related to locality, concurrency, fault tolerance,<br\/>self-stabilization, heterogeneity, cost sharing, and security. This<br\/>project investigates novel approaches to the design of practical and<br\/>provably efficient peer-to-peer infrastructure, emphasizing<br\/>locality-related concerns.","title":"Discrete Location Theory and Its Application to Peer-to-Peer Computing","awardID":"0310970","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["518067"],"PO":["499399"]},"82016":{"abstract":"0306399<br\/>Yanhong A. Liu<br\/>SUNY @ Stony Brook<br\/><br\/>Many computation problems, including program analysis and model checking problems in particular, are most clearly and easily specified using relational rules. Yet, developing and implementing efficient<br\/>algorithms for these problems is a nontrivial, recurring task. This project proposes to develop a unified method for transforming rule-based specifications into efficient algorithms and characterize the specifications and the transformations to provide time and space guarantees for the derived algorithms. The project will focus on rule-based specifications for program analysis and model checking problems and develop fully automatic methods for the transformations and the time and space analysis in this domain.<br\/><br\/>The development will use a general transformational method that makes computation proceed in an iterative and incremental fashion, analogous to integration by differentiation in calculus. The method will also<br\/>exploit sophisticated structures for storing and accessing complex data. The project also proposes to implement these methods, apply them to existing and new analysis problems, and evaluate them by<br\/>comparing automatically generated implementations with other algorithms and implementations. The research results will enable faster and better development and implementations of computer software<br\/>for solving practical analysis problems. The transformational approach will help assure the correctness and efficiency of the implementations.","title":"From Rules to Analysis Algorithms with Time and Space Guarantees","awardID":"0306399","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["552012"],"PO":["564388"]},"83589":{"abstract":"This project develops an integrated Bayesian framework for vision based control of Unmanned Aerial Vehicles (UAVs) through fundamental research in 1) model-based nonlinear state and parameter estimation, 2) intelligent adaptive control, and 3) image processing. We specifically address how real-time video data can be processed with ground-based sensors (and on-board avionics) to extract spatial and situational information (e.g., vehicle state and model parameters). Using only stationary video cameras, information from the sequence of images are integrated with an adaptive controller that transmits actuator commands directly to the UAV. Our research infrastructure consist of an X-Cell-60 R\/C helicopter with custom avionics, video cameras on the ground, and a PC ground-station to perform all necessary processing<br\/><br\/>A key aspect is to go beyond traditional vision based motion estimation and tracking, utilizing new approaches to recursive Bayesian estimation allowing full coupling with the control system. Heuristically, this involves the propagation of probabilistic density estimates for the state (vehicle position, attitude, and velocities) and model parameters (mass, moments of inertia, aerodynamic forces, etc.). The vision components models the ``image likelihood'' and describes the probability of observing the image given the current state. The estimation combines the vision measurements with the dynamic vehicle model in a recursive filtering procedure using a Sigma-Point Filter (SPF) framework. SPF methods are a recent development in machine learning, and are shown to be far superior to standard EKF based estimation approaches. <br\/><br\/>The intellectual merit of the research contributes to both the individual component areas as well as the integrated whole. The integration of the different components in the proposed manner represents an interdisciplinary new approach, providing new research opportunities and applications in integrated sensing, information processing, and control. Beyond basic research, the broader impact to technology includes the obvious commercial and military applications that can be studied in this controlled environment (e.g. visually assisted vertical take-off and landing for ship board helicopters, or agile maneuvering through urban environments). The core technologies can also be extended to other information technology areas from image tracking and detection, to control of complex biological systems.","title":"Collaborative Research: ITR: A Robotics-Based Computational Environment to Simulate the Human Hand","awardID":"0312693","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["518026"],"PO":["335186"]},"83237":{"abstract":"0311014<br\/>Javier Garcia-Frias<br\/>U of Delaware<br\/><br\/><br\/>This research involves the study of communications systems consisting of multiple sources transmitting correlated information. The final objective is to exploit the correlation among the sources to reduce<br\/>the total transmitted energy. The techniques developed in this research may have a broad impact in the area of sensor networks, making it possible to develop sensors requiring less transmitted power. For example, a possible applications could be the development of less-power-invasive biosensors.<br\/><br\/>In order to reduce the total transmitted energy, the research will focus on the application of turbo-like codes, and specifically of linear codes with low-density generator matrix to i) achieve compression rates close to theoretical limits for a wide range of multi-terminal correlated sources (discrete or continuous, with and without memory) and ii) perform joint source-channel coding for a wide range of multi-terminal correlated sources and different channel environments. The proposed system is characterized by its simplicity at the encoder site. This simplicity facilitates the implementation of the proposed scheme in practical<br\/>applications. Moreover, the encoding process is completely independent for each source, and it does not require knowledge about the correlation model or source statistics, which is of great interest in practical<br\/>applications such as sensor networks and image\/video compression. In order to achieve a performance close to the theoretical limits for source and joint source-channel coding, the statistics of the sources<br\/>are estimated and exploited at the decoder in an adaptive fashion.","title":"Turbo Like Codes for Distributed Source and Joint Source-Channel Coding of Correlated Sources","awardID":"0311014","effectiveDate":"2003-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["409074"],"PO":["564898"]},"81950":{"abstract":"ABSTRACT<br\/><br\/>Information Technology Workforce (ITWF) - FY03<br\/><br\/>Proposal ID: EIA-0306156<br\/>Investigator: Alberto Rodriguez, Randy Yerrick and Cathy Zozakeiwiz<br\/>Institution: UC, San Diego<br\/>Title: Improving the Participation and Achievement of Students in Diverse Schools by Enhancing Teacher Professional Development in Science and Learning Technologies <br\/><br\/> <br\/>The University of California, San Diego has been funded to conduct a 3-year long professional development and research project that is based in a diverse school context. A cohort of elementary students will be selected and followed from fourth grade through sixth grade. Each year these students will be placed in classrooms with teachers who are participating in the project. The transformation of the students' knowledge of science and learning technologies, and the teachers' abilities and confidence in the use of learning technologies to teach for understanding, will be studied for 3 full years. In addition, each year a cohort of pre-service teachers will be recruited to student teach in the participating teachers' classrooms. This design aims to provide teachers with multiple opportunities for practicing the skills and content knowledge modeled during the science methods courses, and during the professional development institutes, in diverse school contexts. The following research questions will be explored:<br\/><br\/>1. In what ways does an inquiry-based, sociocultural constructivist, and multicultural orientation to teaching enhance experienced and pre-service teachers' abilities to use learning technologies with diverse students in the upper elementary classroom?<br\/>2. In what ways does an inquiry-based, sociocultural constructivist, and multicultural orientation to teaching using learning technologies enhance diverse students' attitudes toward and academic performance in science","title":"ITWF: Improving the Participation and Achievement of Students in Diverse Schools by Enhancing Teacher Professional Development in Science and Learning Technologies","awardID":"0306156","effectiveDate":"2003-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":["376804",213720,"452636"],"PO":["564181"]},"83524":{"abstract":"Luo<br\/> The imvestigator studies conditions under which certain<br\/>convex cones can be represented or approximated by linear images<br\/>of the positive semidefinite matrix cone. These conditions are<br\/>the essential tools for the exact finite reformulation of<br\/>semi-infinite constraints arising from robust optimization. The<br\/>project focusses on cones of nonnegative mappings and<br\/>applications to robust optimization, analysis of interior point<br\/>methods for adaptive filtering and robust beamforming, and<br\/>efficient optimization methods for the design and analysis of<br\/>multi-user communication systems and for decoding and detection<br\/>in multi-input-multi-output wireless communication channels. The<br\/>theoretical issues are all strongly motivated by the need to<br\/>develop efficient optimization algorithms for problems in signal<br\/>processing and digital communication systems.<br\/> The project aims at developing new computational techniques<br\/>to solve problems in the information and communication technology<br\/>area. The latter area has, over the last thirty years, been<br\/>relying mostly on traditional numerical optimization methods,<br\/>which can run into difficulty when the problem size becomes large<br\/>or when data rate is high. In this project, the investigator<br\/>studies ways to use state-of-the-art optimization techniques and<br\/>computational tools to resolve some well-known computational<br\/>bottlenecks from the information and communication technology<br\/>area.","title":"Advanced Optimization Methodologies for Signal Processing and Communication","awardID":"0312416","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0504","name":"Division of MICROELECTRONIC INFOR PROCESS","abbr":"MIP"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["517430"],"PO":["565027"]},"82446":{"abstract":"When people communicate, they systematically employ a diverse set of nonverbal cues, and highlight the intended interpretation of their utterances. In face-to-face conversation, expressive movements of the brows and head are particularly pervasive. This project studies these facial conversational signals from the dual perspectives of human communication and computer animation, with the goal of realizing them effectively in spoken dialogue interfaces modeled on face-to-face conversation. The research strategy is to categorize and document facial conversational signals, to model their meaning and interpretation, to implement computational systems that generate and animate these signals, and to evaluate the contribution these systems can make to users' understanding of agents.<br\/><br\/>This specific project fits into a broader program that uses animated agents to enhance the flexibility and accessibility of interfaces, by improving the robustness of dialogue systems, increasing communication bandwidth between users and computers, and supporting more collaborative styles of interaction. The results of this project, including new software, data and training materials, promise to lower the barriers to entry to this research area, and add to its momentum.","title":"Making Discourse Visible: Realizing Conversational Facial Displays in Interactive Agents","awardID":"0308121","effectiveDate":"2003-07-15","expirationDate":"2007-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7274","name":"HUMAN LANGUAGE & COMMUNICATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":["474118","355043"],"PO":["565215"]},"83546":{"abstract":"This project develops transcribing software to make scientific and technical literature freely and almost instantaneously available to braille readers through fully-automated translation of both MathML and LaTeX to Nemeth braille. The PI's software will be the first braille transcribing utility capable of simultaneous and automated processing of mathematical formulas, scientific text, spatial arrangements and XML-described formats. MathML is an emerging low-level standard markup language for archiving mathematical content, and for communicating and processing mathematics by machines. LaTeX is a high-level language for writing mathematical documents. Scientific word processors can export documents in LaTeX, and many authors also use it directly with text editors. A prototype, which transforms MathML to Nemeth braille mathematics, was built on top of the PI's TeX4ht, a powerful tool for automatic conversion of LaTeX documents to MathML mathematics plus XML text. This two-layer approach avoided the need to readdress numerous difficulties, already resolved in TeX4ht, which had plagued earlier attempts to convert legacy LaTeX to Braille, and also answers the call of the National Federation of the Blind to develop tools for direct translation from MathML into Nemeth Braille. The subject areas of machine translation of MathML code to other formats and of automatic generation of Nemeth braille from XML-tagged source have received little attention. The work will identify problem areas in both fields, provide solutions with respect to the braille transcribing system, and identify significant topics for further investigation.<br\/><br\/>Broader Impacts: At the current going rate, it takes over six months to translate a single textbook at a cost of $5,000 and more. The PI's software will be highly portable across numerous platforms, and will be disseminated free of charge as open source in the public domain, as is already the case for LaTeX and TeX4ht. The ultimate outcome of the project should be increased productivity for the large number of people who must rely on Braille to read, which will in turn lead to increased integration of braille readers into technical professions.","title":"ITR: Automatic Translation of Scientific Literature to Braille","awardID":"0312487","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":[217813],"PO":["565227"]},"83249":{"abstract":"Abstract<br\/>0311055<br\/>Tong, Lang<br\/>Cornell University - Endow<br\/><br\/>This research aims to develop novel random access techniques for two emerging wireless applications: high data-rate wireless LAN for hot-spot access and information extraction in large-scale and low-power sensor<br\/>networks. For both applications, conventional medium access control that assumes simplistic channel model is one of the major performance bottlenecks<br\/>The investigators explore new cross-layer design strategies by exploiting tight interactions between signal processing at the physical layer and the medium access control sublayer. The research has two major themes. The first involves developing optimal medium access strategies that capitalize the multi-packet reception capabilities of the physical layer and provide capacity achieving random access. Signal processing techniques are developed to maximize the stable throughput of users. Rate allocations, quality of service requests, and pricing structures are designed based on the characterization of the capacity and stability region of optimal random access protocols. The second theme focuses on low power and energy<br\/>efficient random access for large-scale sensor networks. The cross-layer design approaches take the form of utilizing channel state information at the physical layer. Distributed, scalable random access protocols with<br\/>low complexity are investigated for th e sensor reach-back problem. Novel signal processing techniques at the sensor node and the mobile access point are developed for energy efficient medium access.","title":"Signal Processing for Random Access: A Cross Layer Approach","awardID":"0311055","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["531874"],"PO":["564898"]},"82600":{"abstract":"DMS-0308840<br\/>Tibor Beke<br\/><br\/>This is a CARGO incubation award made under solicitation <br\/>http:\/\/www.nsf.gov\/pubs\/2002\/nsf02155\/nsf02155.htm.<br\/>The main motivation of this project is computational cartography. The<br\/>underlying mathematical observation is that symbolic map layers are<br\/>isotopy invariants. Transcribing the topological information that resides<br\/>in a map amounts, in practice, to specifying an isotopy class of nested,<br\/>labelled planar graphs. The deliverable of this project is a portable<br\/>specification language for doing that. While the case of two-dimensional<br\/>planar configurations is essentially solved, its extension to surfaces of<br\/>higher genus is of interest even as fundamental research. Its importance<br\/>is also highlighted by its connection to the description of solid bodies,<br\/>such as those encountered in computer-aided design.<br\/><br\/>Present-day Geographic Information Systems tend to treat maps as<br\/>superimposed layers of graphic images. But a map also contains<br\/>information that is topological, meaning, metric-independent. Examples of<br\/>this are road or other infrastructure networks, where one is only<br\/>interested in the connectivity between certain nodes, and rough<br\/>qualitative features of the lines connecting them. The storage, indexing<br\/>and transmission of this symbolic topological information may be both more<br\/>important and easier than that of the entire graphical image. The authors<br\/>outline an algorithm for schematic rendering, and a method of attacking<br\/>the map isomorphism problem, where by map one may mean both actual images<br\/>and maps encoded by their proposed scheme. Both of those ideas, however,<br\/>are in need of experimental verification.","title":"Symbolic Topology in Two and Three Dimensions","awardID":"0308840","effectiveDate":"2003-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1260","name":"INFRASTRUCTURE PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[215359],"PO":["313600"]},"81995":{"abstract":"CCR-0306303<br\/>TITLE: Supporting Change in Evolving Software Systems through an Adaptive Learning Approach to the Dynamic Generation of Traceability Links<br\/>PI: Jane Huang<br\/><br\/>This research will investigate the dynamic generation of traceability links to support the impact analysis of entirely new requirements. Without such support, the introduction of new requirements into an implemented software system can lead to unanticipated conflicts with existing functionality and can adversely impact non-functional requirements such as performance, security, safety, and reliability. The research will define a methodology known as Integrated Dynamic Link Generation (IDLG) for dynamically generating traceability links between new requirements and an existing set of software engineering artifacts. IDLG will provide a dynamic environment in which link generation rules are learned and refined throughout the lifetime of the software system. The research will develop relevant algorithms and methodologies, and will deliver a fully functioning prototype tool with interfaces to industrial strength software engineering tools.","title":"Supporting Change in Evolving Software System through an Adaptive Learning Approach to the Dynamic Generation of Traceability Links","awardID":"0306303","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["318748","550521"],"PO":["564388"]},"83700":{"abstract":"The primary objective of this research is to test the hypothesis that information in a document, called jargon, can help to identify the background opinions or beliefs of the author. The first phase of the project tests whether an author's opinion can be identified for a text that explicitly addresses an issue about which the author has an opinion. The second attempts to identify the author's background beliefs for an article that deals with some topic in the general domain to which those beliefs are related. The final phase attempts to identify the author's opinion or background system of beliefs for articles that do not directly deal with an issue or topic in the general domain related to the opinion or system of beliefs. <br\/><br\/>The intellectual merit of the proposal lies in developing more rigorously the linguistic notion of jargon, broadening it to include not only technical terms in domains of activities, but also ideological terms in issue-oriented domains and identifying that ideological jargon in texts that are somewhat removed from the original issue-oriented or activity domain. Although there exist methods for categorizing texts based on information directly available from the text itself (e.g., is it a fact or an opinion, what is the topic), this research goes beyond this to ascertain whether there are properties of the author that are not immediately accessible from the text itself that indicate possible views and beliefs of the author. The immediate impact of the proposed research is the development of a set of tools for identifying jargons and attempting to determine possible attitudes of the authors of texts on the basis of these jargons. If the results are positive, the approach may enable sophisticated text retrieval or web search procedures that allow the search to be constrained by author attitude and belief.","title":"ITR: Using Jargon to Identify Author's Perspective","awardID":"0313338","effectiveDate":"2003-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V242","name":"CIA-MACHINE TRANSLATION(MT)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V603","name":"CIA-HLC PROGRAM"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["226683","355341","355342"],"PO":["491702"]},"81786":{"abstract":"While there are several acceptable algebraic multigrid algorithms for symmetric positive definite problems (or ones with an M-matrix), there is very little known that works, much less works robustly, for general nonsymmetric, indefinite problems. In fact, none of the three basic principles for developing algebraic multigrid methods apply except under special circumstances. For nonsymmetric, indefinite problems we will develop principles, algorithms, and serial and parallel codes that work well with applications when little or no, some, or a lot of application information is provided to make multiscale and multilevel solvers robust. Both symbolic and numeric tools will be developed to make prototyping new algorithms relatively painless. Memory caches will be exploited portably with little tuning necessary from the users. Two diverse applications will motivate the new algorithms and software: ocean and flame modeling. The results should have impact on any area of science or engineering in which nonsymmetric,<br\/>indefinite problems result.<br\/><br\/>At the same time, material from the project can be integrated into a first course in computational sciences (UKy CS521), which is taken by seniors and graduate students. Minority students already are half of the PI's research group (half are female, one is of color). Almost all of the UKy CS grad students are minority students. Dissemination will be through web sites including http:\/\/www.mgnet.org and Douglas' home page http:\/\/www.ccs.uky.edu\/~douglas.","title":"ALGORITHMS: Multiscale, Multicolor, Multigrid-Like Solvers for High Performance Technical Computing","awardID":"0305466","effectiveDate":"2003-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["495104"],"PO":["565272"]},"83612":{"abstract":"ABSTRACT<br\/><br\/><br\/>Proposal #0312802<br\/>Title: ITR: Towards Organic Computing in Computer Vision and Robotics <br\/>PI: Schaal, Stefan<br\/>U. of Southern California<br\/><br\/><br\/>This project focuses on developing a methodology to advance a self-organizing computing paradigm in flexible robotics and vision in natural environments. These application domains require systems of tremendous complexity, as also indicated by the fact that a large part of our brains, with a processing capacity orders of magnitude larger than any technical system, is devoted to vision and control of behavior. It is doubtful whether systems of the required complexity can be designed by current computing methodologies. It is proceeded with the realization that radically new design principles will have to be developed, and are seeing this project as part of a broader effort to establish the new computing paradigm Organic Computing (OC). OC aspires to understand and emulate information processing in living systems, a form of information processing that does not seem to follow traditional algorithmic control, but rather mechanisms of evolution, adaptation, goal-oriented self-organization and learning.<br\/>The study will perform towards this goal by a series of theoretical and experimental studies that will demonstrate the principles of OC and how it can replace traditional programming in classical problems of computer vision and robotic control. <br\/>On the intellectual side, the project will contribute considerably to robust large-scale intelligent and autonomous systems in the future, and a better scientific understanding of the functional principles that are at the basis of autonomous vision, motor control, and visual-motor coordination, also with a view towards interdisciplinary exchange with the neuroscience. The broader impact will be to profoundly transform the style in which large software systems are developed and to open computer systems to the direct creative influence of the non-technical user. The significance of our particular sample applications will be progress with advanced sensing, perception and actuation systems in unstructured environments, as a basis for HCI systems and also for the emerging field of neuro-prosthetics and rehabilitation engineering as well as robotics and autonomous vehicle control.","title":"ITR\/CISE: Towards Organic Computing in Computer Vision and Robotics","awardID":"0312802","effectiveDate":"2003-07-01","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["492859",217983],"PO":["335186"]},"83657":{"abstract":"Abstract<br\/>0313056<br\/>Swindlehurst, Arnold<br\/>Brigham Young University<br\/><br\/>Current audiovisual recording, communication, and playback provides a single, two-dimensional perspective<br\/>of the world as it varies through time. Humans live in a \\four-dimensional\" world, in which they move about<br\/>(in 3-D space over time) at their own volition to experience the world from any perspective. Advances in<br\/>sensor, computer, and networking technologies now make possible new systems that employ multiple cam-<br\/>eras and microphones together with sophisticated processing algorithms to deliver unprecedented immersive<br\/>recording and viewing capabilities. Sufficient sensing, networking, and computing power to practically ad-<br\/>dress this vision already exists; the critical gap in achieving it is the lack of the necessary signal processing<br\/>theory and algorithms.<br\/>This research will develop new signal processing techniques for reconstruction of the audio and visual<br\/>recording at an arbitrary location in space and time from multiple acoustic and video sensors, by extending<br\/>recent research in adaptive beamforming, multisensor signal processing of non-stationary signals, and fun-<br\/>damental new advances in multi-dimensional signal representation. Practical four-dimensional audiovisual<br\/>recording, transmission, and playback, or \\remote reality\", will be demonstrated with low-cost, conven-<br\/>tional sensors attached to networked computers, thus confirming the practicality of the proposed methods<br\/>and applications.","title":"ITR: Multi-user, Multi-antenna Networks: Achieving High Capacity in a Mutual Interference Environment","awardID":"0313056","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["486561","545777","364780","557396"],"PO":["564898"]},"83679":{"abstract":"Wireless networks are being successfully deployed at an exponential rate in public, enterprise and scientific networking environments. Such networks support roaming and dynamic environments, are easier and often cheaper to deploy than their wired counterparts and offer flexible configurations that can simply be modified. Several efforts are currently under way to develop high-speed wireless technology for wireless personal area networks (WPANs), wireless local area networks (WLANs) and storage area networks (SANs). This project is studying the theoretical underpinnings as well as the implementation challenges associated with two wireless high-speed networking technologies based on ultrawideband (UWB) communications. The first system is suitable for very high-speed networking applications where processing power consumption is less of an issue, such as storage area networks and home networks. The second system is an excellent match for power constrained consumer electronic devices in a WPAN, such as the scenarios envisioned by the IEEE 802.15.3a standardization effort in which the project team is actively participating. The project is making progress towards a demonstration of the latter technology in a wireless personal area network video-streaming scenario.<br\/><br\/>In 2002, the FCC released the 3.1 GHz to 10.6 GHz band for UWB communications operating under part 15 of the FCC regulations. UWB communication systems use signals with a fractional bandwidth that is larger than 25% of the center frequency, or more than 1.5 GHz. Prior research established that UWB communication systems offer several potential advantages, including higher reliability, lower power consumption and ability to provide fine ranging information. Most importantly, they support overlaid communications, a key to addressing spectral congestion.<br\/><br\/>The project recently produced a unified view of UWB schemes. This<br\/>Unified view has led to novel UWB schemes with enhanced spectral efficiency. It has also simplified the design and generation of UWB pulse trains using broadband signal generation techniques. The resulting generalized UWB schemes rely on well-known baseband modulation schemes with existing economical chip level implementations, and adaptive agile spreading approaches that detect and avoid narrowband systems. They inherit the advantages of both UWB communications and those of the underlying baseband modulation scheme. The project pays particular attention to implementation issues, focusing on trade-offs between cost and complexity of<br\/>implementation, and spectral efficiency and maximal achievable bit rates. For example, it examines novel one bit novel analog-to-digital and digital-to-analog conversion structures that are well matched to the baseband modulation schemes that the project focuses on, and the high bit rates envisioned in the applications of generalized UWB. These structures are finding applicability beyond the ongoing work.","title":"ITR: Generalized Ultrawideband for High Speed Networking","awardID":"0313224","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["515560","313628"],"PO":["348215"]},"78619":{"abstract":"Proposal Title: PECASE: Management of Immersive Sensor Data Streams<br\/>Institution: University of Southern California<br\/><br\/>The goal of this project is to design and develop a system to enable the acquisition, storage, querying and analysis of immersive sensor data streams, termed immersidata, in the wavelet transformed domain. Immersive sensors are the user interfaces of the future and it is absolutely critical to facilitate the manipulation and analysis of their generated data sets. Management of immersidata is challenging because they are: (1) multi-dimensional, (2) spatio-temporal, (3) continuous data streams, and (4) noisy. In addition, they are produced with a high rate and hence their real-time query processing is a challenging task. The use of wavelets alleviates these challenges collectively. Furthermore, the resulting system enables many other applications dealing with large multidimensional data streams that require approximate and\/or progressive query support. This project is benefiting from the collaboration of a multidisciplinary team of scientists in the areas of computer vision, graphics, signal processing and human factor studies in the PI's home institution. The career development plan will train the database graduate students in continuous mathematics and linear algebra, and provide them with the opportunity to acquire hands-on experiences in utilizing sophisticated math techniques for sensor data analysis purposes. In addition, undergraduate and underrepresented students will become familiar with modern sensory\/immersive interfaces and exercise the concepts of forming mining queries on multidimensional data streams. The main benefits to the society would be through specific application developments, which would enable more natural interaction methods with software systems especially for disabled and elderly users. The project results will be disseminated via Internet (http:\/\/infolab.usc.edu) and other channels in order to increase the impact.<br\/><br\/><br\/>This project was originally funded as a CAREER award, and was converted to a Presidential Early Career Award for Engineers and Scientists (PECASE) award in September 2004.","title":"PECASE: Management of Immersive Sensor Data Streams","awardID":"0238560","effectiveDate":"2003-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["550740"],"PO":["469867"]},"78509":{"abstract":"EIA-0243796<br\/>Darko Stefanovic<br\/>University of New Mexico<br\/><br\/>Deoxyribozyme-deoxyribozyme Logic, a new computational substrate<br\/><br\/>Deoxyribozymes (nucleic acid enzymes) are the basis for a family of molecular-scale elementary logic gates which have input and output signals of the same kind, as concentrations of oligonucleotides, and which operate in solution. Chemical-kinetic properties of these gates are being studied through collaborative experiments, analytical modelling, and computer simulation. Dynamic properties of larger circuits composed of elementary gates and capable of autonomously carrying out complex Boolean functions (decision-making) as well as other computational tasks are being studied, including the reliability of communication between gates, in order to characterize the functional behavior and performance of such circuits. Analysis and synthesis tools tailored to deoxyribozyme-based logic are being developed.<br\/><br\/><br\/>Through this work, the state of knowledge about large-scale chemical kinetics modelling, oligonucleotide library design and other combinatorial problems is being advanced, and interdisciplinary collaborations between computer science and biochemistry and nanotechnology are being established. Through involvement in research and through curriculum development, undergraduate and graduate students are being trained in the emerging area of bioinformatics. Finally, foundations are being laid for diagnostic and therapeutic applications in which individual cells can be analyzed according to Boolean criteria, so that diseased cells can be reliably distinguished from healthy ones and specifically treated.","title":"CAREER: Deoxyribozyme-Deoxyribozyme Logic, a New Computational Substrate","awardID":"0238027","effectiveDate":"2003-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["550110"],"PO":["565223"]},"81985":{"abstract":"0305949\/0306269 <br\/>Matthias Felleisen (Northeastern University)<br\/>Shriram Krishnamurthi (Brown University)<br\/><br\/>Over the past few years, the Web has evolved from a static medium into a dynamic one. Many Web sites now enter into interactive dialogs with consumers. Unfortunately, the existing technology for building<br\/>interactive Web services is inherently flawed. As a result, many Web programs are broken and cannot keep up with consumers' actions in Web browsers. <br\/><br\/>The investigators propose to investigate the foundations of Web services. Specifically, they will develop models of interactions on the Web; they will use the models to study common errors in interactive Web<br\/>programs; and they intend to develop protection mechanisms against such errors. <br\/><br\/>The proposers also intend to translate their theoretical designs into working prototypes. They intend to use their existing server and Web programming infrastructure to implement type systems and run-time checks<br\/>for CGI scripting (`a la Perl) and servlet programming (`a la Java). They will port some existing Web applications (including conference management, workshop registration) to this new software infrastructure and will thus test the practical validity of their efforts. Consequently, they expect to improve the quality of an increasingly critical medium of communication and commerce.","title":"Collaborative Research: Robust Interactive Web Services","awardID":"0306269","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["475166"],"PO":["564388"]},"81754":{"abstract":"0305387<br\/>Susan Horwitz<br\/>University of Wisconsin-Madison<br\/><br\/>Writing correct, secure software is very difficult. Languages like C that have weak type systems exacerbate the problem by making it easy for programmers to introduce errors and potential security holes in<br\/>their code.<br\/><br\/>The goal of this project is the design, implementation, and evaluation of dynamic error-detection and security-enforcement tools for C programs. Existing dynamic error-detection tools are limited by poor<br\/>coverage: they can only detect erroneous behaviors that actually occur during a given program execution. That limitation will be addressed by the use of innovative new dynamic techniques for increasing both<br\/>\"data coverage\" (finding errors that could occur given different input values) and \"path\" coverage (finding errors that could occur if a different path were followed through the program). The security-enforcement tool will provide protection against a wide range of attacks, with low overhead, without requiring modifications to existing source code, and without requiring the programmer to give up control over data representations or memory management.","title":"Dynamic techniques for finding errors and preventing security violations","awardID":"0305387","effectiveDate":"2003-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["331999"],"PO":["564388"]},"91324":{"abstract":"This project will create and apply algorithms and software tools for on-line simulations that continuously (1) assimilate sensor data from dynamic physical processes, and (2) generate optimal strategies for their control. A number of critical industrial, scientific, and societal problems stand to benefit from this research such as aerodynamics, energy, geophysics, infrastructure, manufacturing, medicine, chemical process and environmental applications; two of these will be the focus of the current research. In these and many other cases, the underlying models have become capable of sufficient fidelity to yield meaningful predictions, provided unknown parameters (typically initial\/boundary conditions, material coefficients, sources, or geometry) can be estimated appropriately using observational data.<br\/><br\/>The critical step is the solution of a large-scale nonlinear optimization problem that is constrained by the simulation equations, typically PDEs or their reduced order models. A data assimilation phase will seek to minimize the mismatch between sensor data and model-based predictions by adjusting unknown parameters of the PDE simulation, and the optimal control phase will find an optimal control strategy based on the updated model.<br\/><br\/>Despite advances in hardware, networks, parallel PDE solvers, large-scale optimization algorithms, and real-time ODE optimization, significant algorithmic and software challenges must be overcome before the ultimate goal of real-time PDE data assimilation and optimal control can be realized. Needed are fundamentally new PDE optimization algorithms that must: (1) run sufficiently quickly to permit decision-making at time scales of interest; (2) scale to the large numbers of variables and constraints that characterize PDE optimization and processors that characterize high-end systems; (3) adjust to different solution accuracy requirements; (4) target time-dependent objectives and constraints; (5) tolerate incomplete, uncertain, or errant data; (6) be capable of bootstrapping current solutions; (7) yield meaningful results when terminated prematurely; and (8) be robust in the face of ill-posedness.<br\/><br\/>To create, apply, and disseminate the enabling technologies for real-time PDE data assimilation and optimal control, the project will: (1) Develop algorithms and tools for real-time data assimilation and optimal control that meet the above specifications for a class of important applications. (2) Implement and publicly distribute these algorithms within an object-oriented framework that incorporates problem structure, interfaces easily with high performance PDE solver libraries fosters applicability of our tools to a broad range of real-time data assimilation and optimal control problems, and enables extension of the algorithms without interfering with applications. (3) Apply these algorithms and tools to two critical environmental and industrial problems: modeling and control of chemical vapor deposition (CVD) reactors and of wildland firespread. (4) Interact and work with other user communities to ensure that the algorithms and software we produce are useful across a broad range of applications.","title":"ITR\/AP COLLABORATIVE RESEARCH: Real Time Optimization for Data Assimilation and Control of Large Scale Dynamic Simulations","awardID":"0352334","effectiveDate":"2003-07-27","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["369299"],"PO":["381214"]},"81897":{"abstract":"0305949\/0306269 <br\/><br\/>Matthias Felleisen (Northeastern University)<br\/>Shriram Krishnamurthi (Brown University)<br\/><br\/>Over the past few years, the Web has evolved from a static medium into a dynamic one. Many Web sites now enter into interactive dialogs with consumers. Unfortunately, the existing technology for building<br\/>interactive Web services is inherently flawed. As a result, many Web programs are broken and cannot keep up with consumers' actions in Web browsers. <br\/><br\/>The investigators propose to investigate the foundations of Web services. Specifically, they will develop models of interactions on the Web; they will use the models to study common errors in interactive Web<br\/>programs; and they intend to develop protection mechanisms against such errors. <br\/><br\/>The proposers also intend to translate their theoretical designs into working prototypes. They intend to use their existing server and Web programming infrastructure to implement type systems and run-time checks<br\/>for CGI scripting (`a la Perl) and servlet programming (`a la Java). They will port some existing Web applications (including conference management, workshop registration) to this new software infrastructure and will thus test the practical validity of their efforts. Consequently, they expect to improve the quality of an increasingly critical medium of communication and commerce.","title":"Collaborative Research: Robust Interactive Web Services","awardID":"0305949","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["519555"],"PO":["564388"]},"83889":{"abstract":"0314015<br\/>Arzberger<br\/><br\/><br\/>This is a three-year proposal submitted by Dr. Peter Arzberger, University of California at San Diego to expand the Pacific Rim Application and Grid Middleware Assembly (PRAGMA). This assembly currently includes founding members from nine economies and14 high-performance computing institutes in the Pacific Rim region. This is a renewal of the first PRAGMA project, which was a seed project awarded jointly by the East Asia and the Pacific Program and the Partnerships for Advanced Computational Infrastructure Program (PACI) for the period of March 1, 2002- February 28, 2004. The purposes of the renewal proposal are to provide the following activities to enhance grid applications and to develop grid middleware in the Pacific Rim region: (1) participation in face-to-face meetings, (2) coordination and expansion of cooperative activities to include more institutes and countries, (3) supporting the development of grid infrastructure and middleware in the region, and (4) building collaborations via focused grid and application infrastructure and demonstration projects. This proposal is jointly supported by the following four NSF units: the Office of International Science and Engineering, Division of Advanced Computational Infrastructure and Research, Division of Advanced Networking and Research, and Division of Biological Infrastructure.","title":"Pacific Rim Application and Grid Middleware Assembly","awardID":"0314015","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"5977","name":"AMERICAS PROGRAM"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"5978","name":"EAST ASIA AND PACIFIC PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"4066","name":"PART FOR ADVANCED COMP INFRA"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4089","name":"NETWORK CENTRIC MIDDLEWARE SVC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"1165","name":"ADVANCES IN BIO INFORMATICS"}}],"PIcoPI":["547710","559498"],"PO":["549324"]},"83207":{"abstract":"The intellectual content of the project is concerned with the study of computational<br\/>complexity, which aims at an understanding of which tasks can be effciently performed<br\/>by computers, and which ultimately cannot. There is currently an enormous<br\/>gap between the very restricted computations in which the ultimate limitations can<br\/>be analysed using existing mathematics, and the rich variety of tasks, such as the<br\/>search problems that are NP-complete, where the limitations are currently only conjectured.<br\/>This project is concerned with a new approach to these problems based on implicitly<br\/>exponential models. This approach is inspired by quantum mechanics, which<br\/>describes the transitions in physical systems in terms of well understood mathematics,<br\/>namely linear algebra, but in order to do so works in high dimensions. In this<br\/>project the model of computation is also exponential dimensional, but it needs to be<br\/>simulatable by classical computers effciently in polynomial time. In analogy with<br\/>quantum mechanics, complex computations are to be expressed in terms of well understood<br\/>mathematics, namely linear and polynomial algebra, but at the cost of high dimensions.<br\/>The questions being investigated center on whether those high dimensional<br\/>representations that can express NP-complete and #NP-complete search problems<br\/>are within the class that can be simulated in polynomial time. The approach <br\/>can be viewed as a new branch of algebraic complexity theory that is inspired <br\/>by quantum mechanics and quantum computation.<br\/><br\/>With regard to broader impacts the societal impact of the research will depend on<br\/>its outcome. Since the main thrust is a new view towards efficient algorithms<br\/>for search problems in general, a positive outcome may redefine what is computed<br\/>in practice across a wide range of applications. Whatever the outcome the research <br\/>will be carried out in an educational environment that includes an expanding group <br\/>of diverse students, faculty, postdoctoral fellows and visitors with interests in <br\/>the broad research area of computational complexity.","title":"An Algebraic Approach to Computational Complexity","awardID":"0310882","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["438597"],"PO":["499399"]},"83328":{"abstract":"This project aims at solving a number of algorithmic and architectural<br\/>problems that will enable tabled logic programming engines, such as XSB<br\/>(http:\/\/xsb.sourceforge.net), to become viable platforms for supporting the<br\/>emerging infrastructure for the Semantic Web. The project will address the<br\/>following issues:<br\/><br\/>1. Design of algorithms for solving a long-standing problem of<br\/> inconsistency between tabling and update operations.<br\/>2. Designing an architecture for supporting explanation-based reasoning<br\/> at the engine level.<br\/><br\/>Developing low-overhead algorithms for allowing tabled query answers to be<br\/>correctly updated when changes occur to the underlying predicates is of<br\/>critical importance in applications that require tabling and at the same<br\/>time frequently modify the data underlying those tables (such as workflow<br\/>schedulers, verification algorithms, database queries, CASE tools).<br\/>Explanation based reasoning is important for debugging and verification.<br\/><br\/>The project will develop a new technology, which will allow tabling logic<br\/>engines to provide inference services for the Semantic Web. Logic<br\/>inference engines are expected to become a crucial part of the Semantic Web<br\/>infrastructure. Once in place, the Semantic Web will enable a host of<br\/>applications in information retrieval and Web services, which will become<br\/>inseparable part of people's daily life.","title":"A Deductive Engine for the Semantic Web","awardID":"0311512","effectiveDate":"2003-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["560897","518366","552012","477226"],"PO":["565272"]},"81920":{"abstract":"CCR-0306046<br\/>Functional Hybrid Modeling<br\/>Henrik Nilsson, Paul Hudak, and John Peterson<br\/>Yale University<br\/><br\/>This project integrates functional programming with non-causal hybrid modeling into a powerful, strongly typed, fully declarative modeling and simulation language, combining the strengths of each paradigm. The work on Functional Reactive Programming has demonstrated how causal modeling in a functional setting allows highly dynamic hybrid systems to be described. However, non-causal modeling is instrumental in making large-scale modeling manageable, and the associated symbolic and numerical methods are essential for efficient and numerically sound simulation.<br\/><br\/>The goal is a language that provides modeling capabilities beyond the current state of the art while allowing efficient simulation, and is semantically rigorous to facilitate modeling, reasoning about models, and catching certain modeling errors. Central research areas include the semantics of integrated functional hybrid modeling; reconciling the implementation techniques of the hybrid simulation world with those of modern declarative languages and dynamic code generation; and a type system tailored to physical modeling incorporating recent ideas from type theory to enhance the safety and robustness of the models.<br\/> <br\/>The project will facilitate designing and understanding complex systems in the real world by providing users with a highly expressive modeling language that is suitable for programming a broad range of modeling and simulation applications.","title":"Functional Hybrid Modeling","awardID":"0306046","effectiveDate":"2003-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["541889","541946","226134"],"PO":["564388"]},"81986":{"abstract":"0306270\/0306286<br\/>Collaborative: Exploiting Component Contracts <br\/>Robert Bruce Findler\/Matthew Flatt<br\/><br\/>Despite the central role of testing in the development process, current programming languages and environments provide scarcely any support for testing. Most testing research has focused on generating<br\/>test inputs and test oracles from program specifications, but program specifications are difficult to produce and maintain. Consequently, language and environment designers remain unmotivated to support<br\/>specifications, and opportunities to support testing are lost.<br\/><br\/>Contracts offer a route around the specification problem. Contracts are a form of lightweight specification that work with the base programming language, i.e., there is no need for the programmer to learn a special logic for writing specifications. Contracts are monitored as the program executes, which means that the programmer is forced to maintain contracts with the code. Since contracts are essentially a generalization of assert statements, experience suggests that programmers will use them.<br\/><br\/>The investigators focus on contracts as a specification language for<br\/>automatic test suite generation. To validate their investigations,<br\/>they will extend the DrScheme programming environment to support unit<br\/>testing at component boundaries. In addition, they will use the new<br\/>testing infrastructure to test DrScheme itself.","title":"Collaborative: Exploiting component contracts for static analysis and testing","awardID":"0306270","effectiveDate":"2003-07-01","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["518591"],"PO":["564388"]},"83713":{"abstract":"ABSTRACT<br\/>0313392<br\/>Mitra, Urbashi<br\/>U of Southern California<br\/><br\/>With the emerging ubiquity of wireless communications, we continue to seek to provide high fidelity transmission of heterogenous data on demand. To fulfill this objective, significant improvements must be made over current wireless systems. The principal aim of the proposed research is to extend the state of the art of modulation and associated receiver designs for multiple-input\/multiple-output wireless systems via the generalization of such channels. Spurred by the substantial capacity advantages promised by theoretical analysis for the use of multiple transmit and receive antenna systems, significant attention has been devoted to the design and analysis of spacetime coding and modulation schemes. Experimental transmit diversity systems have offered further hope that the gap between existing systems and theory can be bridged. The proposed research shall encompass commercial wireless systems such as cellular communication networks as well as more speculative systems such as ad hoc sensor networks. Abstractly, these two networks represent extremes in terms of network parameter information and control. A cellular communication system can be characterized as having a simple hierarchy with a basestation as central controller, modest power constraints, highly reliable users, and modest density of users per cell, and possessor of near-complete parameter information of all active users. In contrast, a sensor network will likely have a nested hierarchy of clusterheads communicating with a small set of nodes, severe power limitations, unreliable nodes, and dense deployments. We shall consider three main research foci: collaborative communication for sensor networks; transmit diversity schemes for interference channels; and the coupling of space-time methods<br\/>with multimedia systems.","title":"ITR: From Sensor Networks to Multimedia Systems: New Views on MIMO Channels","awardID":"0313392","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["515828"],"PO":["564898"]},"83724":{"abstract":"This the first International Conference Applied Computer Algebra (ACA), held in Raleigh North Carolina, July 28-31, 2003. In the past several years there has been a dramatic increase in the applications of computer algebra (CA) in engineering, science and education. The goal of this conference is to facilitate this important progress through an international meeting that emphasizes applications of computer algebra and interdisciplinary interaction between computer algebra researchers and serious users from other disciplines. The ACA conference complements nicely the already well-established conferences, such as ISSAC, which are essentially the intradisciplinary forum for computer algebra researchers on fundamental theories (rather than applications and inter-disciplinary interaction). The conference is organized as a collection of twelve mini workshops (sessions) organized by prominent researchers and focused on their own application areas including education, engineering, physical sciences, economics and life\/medical sciences. <br\/><br\/>This conference will have the broad impact on two communities: the huge user community and the research community. The user community will be able to learn about the state of the art theories and algorithms that are proven to be applicable. The research community will be able to learn about important problems\/challenges facing the serious users from other disciplines, inspiring fresh new theoretical research and algorithm design.","title":"International Conference on Applied Computer Algebra","awardID":"0313458","effectiveDate":"2003-07-15","expirationDate":"2003-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["517853","550491","485433"],"PO":["321058"]},"83614":{"abstract":"This project's goal is to develop a synthetic talking face. Humans developed sophisticated abilities to perceive and integrate auditory and visual (AV) speech information long before they were required to read printed text presented by computers. Seeing as well as hearing speech reduces the cognitive workload and improves comprehension over only hearing the talker. To realize the advantages of AV speech for human-computer interactions requires synthesizing visual speech, thereby providing an unlimited supply of visual speech images without having to pre-record data. The approach here is to drive optical speech synthesis with speech acoustics. Computational methods obtain models of the transformation from acoustics to optics. The method capitalizes on the speech production coarticulatory information captured by diphones to produce naturalistic visual speech images. The method is applied directly to natural acoustic speech features to obtain coordination between acoustic and optical signals. The synthesized visual speech is based on a texture-mapped wire frame model. A natural speech corpus to base the synthesis is being obtained via simultaneously recorded 3-D optical, audio, and video data. Synthesis development is guided by human perceptual testing. The DVD archived corpus will be disseminated. <br\/><br\/>The project will lead to expanded access to information and improvement in obtaining knowledge by diverse groups of individuals, for example: children still acquiring literacy skills; adults with inadequate literacy; individuals who are using a second language; and individuals with hearing losses who rely on audiovisual speech. Results will be disseminated broadly through professional outlets. Graduate and undergraduate students will participate.","title":"ITR-Collaborative Research: Development and Evaluation of a Hybrid Concatenative\/Rule-Based Visual Speech Synthesis System","awardID":"0312810","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["558131"],"PO":["565227"]},"83648":{"abstract":"Sign languages are complex, abstract linguistic systems, with their own grammars, and their \"articulation\" involve not just the hands, but the face, shoulder, and arms. In this project the PI and his team will push the state of the art in scalable automated American Sign Language (ASL) recognition formalisms. Presently, there are methods to recognize isolated signs and, to some extent, continuous signs in short sentences from a single signer, mostly using special equipment such as data gloves or magnetic markers or from visual input against plain background and special clothing. The PI seeks to achieve substantial advances in five areas:: (i) recognition against varied backgrounds and different clothing, (ii) use of non-manual aspects such as facial expression and head movement, (iii) recognition across signers, (iv) design of robust, scalable formalisms, and (v) development of large ASL data corpus that exercise variates such as viewpoint, background, time, and signer, to help benchmark progress. To these ends, the PI will (i) develop robust manual and non-manual (face) feature sets for ASL, (ii) construct formalisms to learn sign models from example sentences, (iii) investigate if elemental forms of signs (signemes) can be learned, (iv) employ Bayesian network based indexing schemes to limit the combinatorics of recognition, and (v) explore techniques to incorporate grammar and syntax information into the recognition process.<br\/><br\/>Broader Impacts: With the gradual shift to speech based I\/O devices for human computer interaction, there is great danger that people who rely on sign languages for communication will be deprived access to state of the art technology unless there significant advances in automated recognition of sign language are achieved. Such advances will also enhance the quality of life of persons with disabilities, by facilitating interaction with the general populace in public situations, such as airports and grocery stores. The PI will identify at least one deaf graduate student or a student with communication disorder to participate in the project, to ensure the outcome is relevant and appropriate to the intended user community. The large ASL data corpus collected in this project will be distributed aggressively; this effort will continue to be supported beyond the project end date.","title":"ITR: Fundamental Issues in Automated American Sign Language Recognition","awardID":"0312993","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7496","name":"COLLABORATIVE SYSTEMS"}}],"PIcoPI":["522400",218080],"PO":["565227"]},"82449":{"abstract":"This project will develop effective algorithms for metric temporal domains. In many real world domains, planning involves handling actions with durations that produce and consume discrete as well as continuous resources, and goals with deadlines. The plans themselves may differ in multiple quality dimensions, including completion time, cost and execution flexibility. Previous approaches for synthesizing plans in such \"metric temporal\" domains have been plagued by poor scale-up potential, which inhibited the adaptation of the automated planning technology to such domains. In metric temporal domains, goals have deadlines, actions have temporal durations, and their preconditions and effects occur over arbitrary intervals during action durations, and actions can produce as well as consume discrete or continuous resources. Planning in many real-world situations, including logistics, supply chain as well as space autonomy, has these characteristics. Despite its potential applicability, adaptation of planning technology in these domains has been inhibited by the poor performance of the previous metric temporal planners.<br\/><br\/>The overall aim of this proposed research is to build on the advances in our understanding of the classical plan synthesis to develop scalable approaches to metric temporal planning domains. The contributions of the project will include: (1) development of domain-independent heuristics based on planning graphs that are sensitive to multiple quality metrics of metric temporal plans (2) a suite of techniques based on constraint satisfaction problem encodings for improving the temporal quality of a given plan and (3) development of two planning algorithms with complementary tradeoffs, one searching in the space of position constrained plans, and the other searching in the space of order (precedence) constrained plans, and (4) a comprehensive investigation of the tradeoffs offered by position constrained and order constrained planners. The contributions of this research will be evaluated using the benchmark domains being developed in the planning community. <br\/><br\/>The results of this work will improve the suitability of computerized planning systems for a number of practical application areas. They will also be incorporated into a textbook on foundations of automated planning and scheduling and into an undergraduate level course on automated planning and scheduling.","title":"Scalable Multi-Objective Planning for Metric Temporal Domains: Heuristics, Algorithms and Tradeoffs","awardID":"0308139","effectiveDate":"2003-07-01","expirationDate":"2006-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["490305"],"PO":["491702"]},"83307":{"abstract":"High Data Rate Wireless Networks: A Power Efficiency Perspective<br\/><br\/>B. Aazhang and A. Sabharwal<br\/><br\/>Department of Electrical and Computer Engineering, Rice University<br\/><br\/><br\/>In the last five years, there has been a cultural shift from wired landlocked connectivity to pervasive wireless information access. Most emerging mobile devices are now equipped with some form of embedded wireless radio. The expectations of high data rates and increased battery longevity have put tremendous pressure on all aspects of wireless system design. To meet the challenges of next generation wireless systems, there is a need for fundamentally new understanding in cross-layer design as well as new methods which exploit all available dimensions for optimizations. The following agenda outlines the key elements of the research framework we are pursuing to <br\/>achieve increased power efficiency while simultaneously increasing system wide throughput.<br\/><br\/>1. Fairness, delay and power efficiency: Traditionally network protocols address throughput fairness<br\/>among users and delay requirement of applications while communication algorithms address location dependent reliability, transceiver power, and channel access. This project develops a framework to systematically analyze and design power efficient access and scheduling protocols. Using short time scale channel variations along with source burstiness, the minimal power opportunistic scheduler will meet delay requirements while maintaining a system wide statistical fairness.<br\/><br\/>2. Scalable space-time codes: To achieve the gains from packet and flow scheduling, it is critical that the physical layer coding be able to serve the scheduled data rates. At the same time to achieve high levels of spectral and power efficiency, it is important that the coding methods exploit as much information available regarding the channel at the transmitter. Thus, the design space targeted by this research project covers a wide range of mobile speeds and channel conditions. In particular, space-time codes are designed that have either imprecise or no channel information at the receiver with no information at the transmitter, covering high to medium mobile speed communications. And for low speed mobiles, the project addresses the fundamental issues in the design of efficient feedback channels for space-time coding methods.<br\/><br\/>3. Integration into wireless research platform: The strong theoretical innovations for low power<br\/>design are tested on the rapid prototyping platform for multiple antenna systems at Rice University. The end-to-end system design and development is stress tested, and the power and spectral <br\/>efficiency of the overall system is quantified.<br\/>The ongoing research paves the way to understand the fundamental relationships among spectral efficiency, algorithm complexity, and power consumption to increase the utility of wireless capable mobile devices and to directly impact their emerging market. The broader impact of the project on education is ensured by engaging a large number of undergraduate and graduate students in projects in our laboratory.","title":"High Data Rate Wireless Networks: A Power Efficiency Perspective","awardID":"0311398","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["542042","548311"],"PO":["348215"]},"81800":{"abstract":"Numerous physical phenomena in nature involve the motion of interfaces separating two or more regions. The level-set method was a PDE-based approach designed to capture the interface motion through the evolution of a signed distance function. This exploratory project aims to further extend the level-set method to unstructured adaptive Cartesian grids. <br\/><br\/>We plan to address the following research task: Development of a level-set solver supporting the adaptive Cartesian grids, and various data structures including both tree and list-based data structures. This solver will be coupled to an adaptive Cartesian grid adaptor for interface based grid adaptations to improve the interface resolution and decrease the numerical errors near the interface.<br\/><br\/>A set of software tools will be developed and made available that can be used to both train students with basic interface dynamics, and to conduct research in challenging engineering and scientific disciplines.","title":"ALGORITHMS: Parallel Level Set Method on Adaptive Cartesian Grids for Multi-Scale Interfacial Flow Computation","awardID":"0305504","effectiveDate":"2003-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["511485"],"PO":["565272"]},"81734":{"abstract":"0305318<br\/>Timothy Davis<br\/>Clemson University<br\/>$329,999<br\/>TECHNI: A New Approach to the B.A. Degree in Computer Science<br\/><br\/>This project, which has the potential of having long-term and wide spread impact on national curricula in computing, develops a B.A. degree in computer science that incorporates research results from Digital Production Arts and Computer Graphics into all required computing courses in the curriculum. Instruction in these (new) courses is oriented toward large-scale problem solving. The starting point for the project is the articulation of key concepts that underlie the proposed curriculum including: a machine model (imperative programming, machine capabilities, machine limits); a connection model (networks for communication and distributed processing); software design (the object-oriented paradigm, large-scale development, testing); windowing and operating systems (resource management, protection levels, security); data structures and performance (performance measurement, bottleneck identification, work-flow management); and, cross-platform computing (PDAs, embedded systems, cross-compilation, external device control). The second phase of the curriculum design involves the identification of research results from the graphics and special effects domains that are amenable to mapping, at the undergraduate level, directly into the identified key concepts. Particularly unique in this B.A. in C. S. is the magnitude and origin of the problems to be integrated and the broad impact of the approach across an entire curriculum. The benefits to the students are not only from the problem-solving orientation of the instruction but also the students' exposure to the vitality of real research problems. It is the proposers' supposition that a more balanced educational experience, like the one involved in the B.A.C.S. degree program, may be of substantial benefit to the students and to society.","title":"TECHNI: A New Approach to the B.A. Degree in Computer Science","awardID":"0305318","effectiveDate":"2003-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}}],"PIcoPI":["490673","343491","485200","408667","255445"],"PO":["551712"]}}