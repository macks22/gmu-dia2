{"110540":{"abstract":"CCF-0541049<br\/>PI: Mary Jean Harrold<br\/>GA Tech<br\/><br\/>TITLE: Automatic Fault Localization Using Statistics and Visualization: An Empirical Research Program<br\/><br\/>Software errors significantly impact software productivity and quality. One of the most expensive tasks required to reduce the number of faults is debugging, and locating faults is the most difficult and time-consuming component of debugging. <br\/><br\/>Existing automated debugging techniques have limitations that prevent them from scaling to industrial systems, and there are few reports comparing existing techniques. This project will investigate fault-localization techniques that can be used in practice through several activities. First, the research will develop improved techniques that can more quickly guide the user to the faulty regions of the system and exploit programmer knowledge and guidance. Second, the research will design and conduct controlled experiments and case studies to evaluate cost-benefits of new and improved techniques, and to understand the factors contributing to those costs-benefits. Third, the research will assemble an infrastructure <br\/>of programs, faulty versions, and test suites for future fault-localization approaches. <br\/>Broader Impacts. The ubiquity of software and the high cost of faults, makes improvements in debugging techniques extremely important. This work will provide practical fault-localization techniques, along with empirical data on those techniques and empirical methodologies that will be made available to other researchers for use in future fault-localization research.","title":"Automatic Fault Localization Using Statistics and Visualization: An Empirical Research Program","awardID":"0541049","effectiveDate":"2006-04-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["530729"],"PO":["564388"]},"110452":{"abstract":"Background<br\/><br\/>Rapid increases in computer performance, according to Moores law, have enabled software architects to<br\/>design software of ever increasing functionality and complexity. Consequently, consumers expect software<br\/>that grows in functionality and can process larger data sets with every new release (e.g., huge spreadsheets<br\/>or streaming high-quality video). Unfortunately, since Moores law will likely not hold in the near future,<br\/>owing to physical limitations such as wire delays and heat dissipation, hardware speedups will not keep<br\/>pace with the demands of software. Thus, to meet the performance requirements of new software, one must<br\/>look to software techniques. <br\/><br\/>The proposed work will explore a new approach, algorithmic optimizations in<br\/>virtual machines, to provide the performance for next generations of software.<br\/><br\/>Intellectual Merit<br\/><br\/>-A system for marking implementations of algorithms, data structures, and their uses. A VM that<br\/>knows about this system can exploit the information for connecting uses with suitable implementations.<br\/>A VM that does not know about this system still executes the code correctly but without its<br\/>performance benefits.<br\/>-An implementation within a Java virtual machine that exploits the information for improving performance.<br\/>This implementation support includes modifications to the garbage collector (for migrating<br\/>data from one representation to another), compiler (for inlining away the calls), and scheduler (for<br\/>scheduling profile collection threads with lower priority).<br\/>-A collection of libraries and applications marked to exploit algorithmic optimizations. This will form<br\/>part of the evaluation for the proposed work.<br\/><br\/>Broader Impact<br\/><br\/>The infrastructure developed as part of this research will be used in undergraduate<br\/>algorithms classes to give students deep knowledge about the strengths and limitations of <br\/>commonly used algorithms and data structures. This research will provides the performance <br\/>necessary for meeting the needs of future software systems.","title":"ST-CRTS: Collaborative Research: Algorithmic Optimizations in Dynamic Programming Environments","awardID":"0540600","effectiveDate":"2006-04-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["550110"],"PO":["565272"]},"110595":{"abstract":"ABSTRACT<br\/><br\/>0541035\/0540948\/0541263<br\/>PI (lead\/UMass): Stephen F. Siegel<br\/>co-PI (UMass): George Avrunin<br\/>PI (UNL): Matthew Dwyer<br\/>PI (UChicago): Andrew Siegel<br\/><br\/>Collaborative Research: Finite-State Verification for High-Performance Computing<br\/><br\/><br\/>High performance computation (HPC) has revolutionized a wide range of scientific and engineering endeavors, including climate modeling, computational fluid dynamics, molecular engineering, and the design of<br\/>buildings, airplanes, and cars. Parallel computers, some of which contain over one hundred thousand processors, make this computation possible. Unfortunately, programming these machines is notoriously<br\/>difficult: parallel programs are extremely complex, difficult to debug, and can behave in unpredictable ways. As scientific applications increase in scale and complexity, the effort required to develop them is growing at an alarming rate, and there is an emerging consensus that new development methods are required.<br\/><br\/>The goal of this project is to develop a body of finite-state verification (FSV) techniques that will allow HPC developers to root out bugs and confirm that their programs meet specified requirements. FSV techniques involve the application of algorithmic methods to a formal model of the program being analyzed. While these techniques have been used successfully in some other programming domains, a number of challenges must be overcome before they can be successfully applied to HPC software. The ultimate result will be an increase in productivity of HPC developers and an increased confidence in the correctness of their programs.","title":"Collaborative Research: Finite-State Verification for High-Performance Computing","awardID":"0541263","effectiveDate":"2006-04-15","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["561782"],"PO":["564388"]},"119649":{"abstract":"Hurricane Katrina disrupted many social networks. Over 1.5 million people evacuated New Orleans. Companies, schools, and families were geographically separated after the storm, and they needed to rediscover the locations of their members. A \"burst\" of new data sources, generated on the web by disaster relief organizations, news outlets, employers, and hurricane survivors, aided those affected by the storm in searching for members of their social groups. This project investigates methods to automate tracking the geographical movements of social networks using the variety of information sources available on the web. As more time passes from Hurricane Katrina, these sources become increasingly inaccessible; therefore, the first step is to quickly harvest these Katrina-related blogs and forums, survivor databases and websites. Next, this project explores new link analysis techniques that utilize these new heterogeneous data sources to track the geographical movement of the network members, given information about the social networks prior to the disruption. These new techniques combine previous work in social network analysis, based on the study of the patterns of connections between members or subgraphs in a social networks, and link analysis methods, which determine whether a connection or link exist between members. <br\/>The project will result in a range of broader impacts. The project will aid in harvesting and archiving Hurricane Katrina-related highly perishable data that will provide a historical record of the disaster. The resulting information resource will allow researchers to access an extensive dataset to develop new research in social networks and disaster recovery. The new link analysis developed in this project will assist in automating the social network reconnection task for Hurricane Katrina victims and make us better prepared to handle future disasters. The project Web site (http:\/\/www.eecs.tulane.edu\/tejada\/katrina) provides access to more information about the research and the results.","title":"SGER: WebTrack - Learning Geographical Movements of Social Networks through the Web","awardID":"0628792","effectiveDate":"2006-04-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7496","name":"COLLABORATIVE SYSTEMS"}}],"PIcoPI":["557889"],"PO":["563751"]},"110530":{"abstract":"0541005<br\/>Milor, Linda<br\/>Georgia Tech Research Corp<br\/><br\/>Collaborative Research: Hierarchical Testing and Yield Enhancement of High End Integrated RF Systems<br\/><br\/>A wide variety of applications, including home electronics, personal devices, local area networks, bioelectronic monitoring devices, etc., rely on wireless communications. The trend for increased wireless communications forces the use of higher frequency bands (15-60GHz) to avoid interference. Although it is very challenging to design systems that operate in the 15-60GHz frequency range, testing of such systems is even more challenging, as the test equipment must at least achieve equal performance. The result is that the cost of test of RF systems often exceeds 50% of project cost. <br\/>Even if the cost of testing is absorbed into product cost, two trends require a radically new approach to test. First, an increasing component of system cost and source of system performance loss is associated with packaging. The industry is exploring the use of new packaging methodologies involving multiple bare dies. The use of such packaging methodologies requires manufacturers to guarantee chip performance based on wafer probe tests. Second, progressively shorter product cycles require yield enhancement work based on wafer probe data, so that the process of bringing a prototype to production does not exceed the market window. Unfortunately, testing of RF components at wafer probe is currently not possible, due to the high probe capacitances that prevent high frequency measurements.<br\/>This project addresses these needs by developing new approaches to design-for-test and design-for-yield-enhancement for RF systems for use at wafer probe. It will provide a framework for both of these tasks that enables improved data collection through on-chip sensing structures and improved loop-back-based system test methods, combined with algorithmic methods to enable fast and accurate fault isolation.","title":"Collaborative Research: Hierarchical Testing and Yield Enhancement of High End Integrated RF Systems","awardID":"0541005","effectiveDate":"2006-04-15","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["485916"],"PO":["562984"]},"111443":{"abstract":"Intellectual Merit of the Proposed Activity. The proposed research is creative and original because it addresses two important challenges in wireless communications, namely energy eciency and channel uncertainty, in combination and oers to establish the important tradeos when energy eciency is the ultimate goal. The expected outcomes of the proposed research and educational activities include an understanding of the impact upon the ultimate performance limits, such as channel capacity, of operating under channel uncertainty; proposals of novel energy ecient signaling schemes using the fundamental bandwidth-power tradeo as a benchmark; an understanding of the eect of cooperation and cross-layer designs on the energy-eciency; creation of excitement in the discipline among undergraduate and graduate students using the attractive wireless communication ideas and using this excitement to have the students participate in the advancement of knowledge by actively engaging in research.<br\/><br\/>Broader Impacts. Integration of this research project with education will be accomplished through<br\/>revised and newly introduced courses, involving undergraduates in the PI's research program, and the<br\/>secondary school outreach. The participation of underrepresented groups will be achieved by the PI's<br\/>eorts to recruit female undergraduate and graduate students, and activities in the Nebraska MESA<br\/>Program, which target Nebraska's minority and underrepresented youth. Broad dissemination of results<br\/>will be accomplished by publishing the results of this research project in high quality, peer-reviewed<br\/>journals and international conferences and creating a web site for the PI's research group. It is ex-<br\/>pected that the outcomes of the research will improve wireless systems to achieve ecient and robust<br\/>performance even under severe physical impediments, leading to the realization of the wireless vision<br\/>of ubiquitous communications. Especially the energy eciency analysis is expected to have an impact<br\/>upon the design of next generation wireless networks, including sensor networks that have various ap-<br\/>plication areas in health, the military, and the home. Along with these research outcomes, society will<br\/>also benefit from the outreach programs proposed as part of the educational plan of the project.","title":"CAREER: Energy-Efficient Wireless Communications Under Channel Uncertainty: Fundamental Limits and Tradeoffs","awardID":"0546384","effectiveDate":"2006-04-15","expirationDate":"2011-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["512215"],"PO":["432103"]},"110596":{"abstract":"Energy conservation has become a major challenge in system design. Most existing techniques focus on individual layers: devices, circuits, architecture, or software. The proposed research conserves energy through a collaborative and coherent approach across different layers. In the hardware layer, special registers report cumulative energy consumption of different hardware modules. These registers are called energy counters, similar to performance counters in modern processors. Operating systems use the information from energy counters and follow a set of accounting rules to determine the amount of energy consumed by each process. These processes can adjust their behavior to use energy efficiently. For portable systems powered by batteries, some processes may off-load computation to a remote server that is grid-powered. A programming environment will be developed to assist programmers in improving energy efficiency. This proposal has broader impacts on improving the energy efficiency of electronic systems to provide services for longer periods of time for mobile users.","title":"CPA: Cross-Layer Energy Management by Architectures, Operating Systems, and Application Programs","awardID":"0541267","effectiveDate":"2006-04-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["540778"],"PO":["550859"]},"118945":{"abstract":"This is funding to partially support a workshop of about 19 participants, invited mainly on the basis of their expertise in computer and information science or the social sciences (with a focus on work organization and practice) or mindfulness practices and the contemplative arts, to explore problems as a consequence of the manner in which new information technologies have over the past several decades transformed the way people work. E-mail, cell phones, and the World Wide Web, among other innovations, have made it possible to conduct work at vast distances and at all hours of the day and night. But with these remarkable opportunities has also come a range of unintended consequences. The easy availability of vast amounts of information has led to a pervasive sense of overload. The presence of multiple communication devices and information sources has fostered an interrupted style of working that often threatens concentration. And the possibility of communicating and acting quickly has encouraged a speedup in work practices and expectations that feels unsustainable and at times counterproductive. But doing more faster doesn't necessarily mean doing it better. There is growing concern that today's dominant mode of working, which features multi-tasking across multiple information sources and devices, is degrading the quality of work results, as well as workers' satisfaction in the process. And there is mounting evidence that work stress is contributing to a range of health problems, including heart attack, stroke, anxiety, and depression. What can be done to reduce work stress, and to increase the quality of work and the satisfaction in its accomplishment? This workshop will focus on developing a research agenda that can lead to the development of technologies that engage human attention in calm and informative ways, and the creation of workplaces that encourage and support mindful, productive, and healthful work. Questions to be addressed will include: How can computer systems (both hardware and software) be designed to encourage relaxed attention? How might computer screens and interfaces be redesigned to take account of people's fuller humanity, including their embodiment? What might more fully-embodied office practices and procedures look like? How might workspace architecture, furniture, sound and light be redesigned to encourage mindful presence? This workshop constitutes a follow-on to the Workshop on Information, Silence, and Sanctuary which was organized by the PI in Seattle in May, 2004 with funding from the MacArthur Foundation and NSF. Additional funding\/support for the current event is anticipated from the MacArthur Foundation, the University of Washington, and the Library of Congress.<br\/><br\/>Broader Impacts: The PI's goal is to raise public awareness of the workshop topics and to stimulate broad discussion of them. To these ends, he will write a report summarizing the workshop presentations and its major findings, and will make it available online in addition to publishing it in a reputable vehicles of dissemination. The workshop will take place over five days in the Washington DC area in mid-March, 2006. At the conclusion of the workshop, a public program at the Library of Congress to which the press will be invited will include a presentation on the topic of the workshop and a panel discussion among some of the participants.","title":"Workshop on Mindful Work and Technology","awardID":"0624439","effectiveDate":"2006-04-01","expirationDate":"2007-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":["427132"],"PO":["565227"]},"110520":{"abstract":"ABSTRACT<br\/><br\/>0541035\/0540948\/0541263<br\/>PI (lead\/UMass): Stephen F. Siegel<br\/>co-PI (UMass): George Avrunin<br\/>PI (UNL): Matthew Dwyer<br\/>PI (UChicago): Andrew Siegel<br\/><br\/>Collaborative Research: Finite-State Verification for High-Performance Computing<br\/><br\/><br\/>High performance computation (HPC) has revolutionized a wide range of scientific and engineering endeavors, including climate modeling, computational fluid dynamics, molecular engineering, and the design of<br\/>buildings, airplanes, and cars. Parallel computers, some of which contain over one hundred thousand processors, make this computation possible. Unfortunately, programming these machines is notoriously<br\/>difficult: parallel programs are extremely complex, difficult to debug, and can behave in unpredictable ways. As scientific applications increase in scale and complexity, the effort required to develop them is growing at an alarming rate, and there is an emerging consensus that new development methods are required.<br\/><br\/>The goal of this project is to develop a body of finite-state verification (FSV) techniques that will allow HPC developers to root out bugs and confirm that their programs meet specified requirements. FSV techniques involve the application of algorithmic methods to a formal model of the program being analyzed. While these techniques have been used successfully in some other programming domains, a number of challenges must be overcome before they can be successfully applied to HPC software. The ultimate result will be an increase in productivity of HPC developers and an increased confidence in the correctness of their programs.","title":"Collaborative Research: Finite-State Verification for High-Performance","awardID":"0540948","effectiveDate":"2006-04-15","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":[292815],"PO":["564388"]},"110597":{"abstract":"Network processors are designed as application specific processors to perform the packet processing at very high line speeds. Network processors are geared towards achieving both fast processing speed and allowing flexibility of programming. Achieving high processing speed as well as providing enough programmability has led to the peculiar architectural design of network processors which is clocked at very high frequencies to match the line speeds. The architecture of network processors considers many special properties for packet processing, including multiple threads, multi-processing, special functional units, dual bank register files, simplified ISA and simplified pipeline, etc. The architectural peculiarities of network processors raise new challenges for compiler design. It is particularly important for the compiler to generate optimized code that could make best use of the architecture features without imposing a heavy burden on the programmer. In addition, unique needs of network processing applications (such as real time response needs etc.) tend to add more burden on the programmers and compiler writers. This could serve as a barrier to the wide scale deployment of applications on these processors. Newer packet processing requirements for intrusion analysis, QoS, traffic analysis etc. in fact demand more and more functionalities to be fulfilled during routing and therefore such barriers must be removed. In this work, it is proposed to develop compiler optimizations for Intel IXP 1200\/2400\/2800 series. It is believed that the proposed work will solve major problems that pose major barriers currently: how to infuse the application behavior into optimization decisions without jeopardizing the programmability and how to perform aggressive optimizations to deploy these applications in a system wide setting that work at high line speeds. The optimizations will be extensively tested in a real system on real system wide applications.","title":"Compiler Optimizations for Network Processors","awardID":"0541273","effectiveDate":"2006-04-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["550988"],"PO":["550859"]},"118935":{"abstract":"Creation of a Goal-Oriented, Human-Machine Spoken Dialog Corpus<br\/><br\/>Annotated data sets are a necessity for data-driven speech and language processing approaches. Many of the speech and natural language processing tasks such as automatic speech recognition, question answering, machine translation, part-of-speech tagging, parsing, named entity extraction, and semantic role labeling have benefited significantly from shared tasks for benchmarking of algorithms and comparison of results on shared data sets. The goal of this project is to create a goal-oriented, mixed-initiative, naturally spoken human-machine spoken dialog system for conference services and publicize the spoken dialogs collected from this system for research purposes. The users can call a phone number and learn about the conference<br\/>paper submission, program, venue, visa requirements, accommodation options and costs, etc.<br\/><br\/>We have an iterative approach, where the SDS is first deployed for the IEEE SLT workshop, to be held in December 2006, and all the components can be improved using the data collected from this deployment. Further data can be collected using the improved system for other conference\/workshops.<br\/><br\/>Given that data-driven approaches are getting more popular for many speech and language processing applications, we believe that such a corpus annotated with system prompts, user utterance transcriptions, user intentions, overall task success, etc., would be a useful resource for dialog management, spoken language understanding, automatic speech recognition and other related tasks. These annotations can also be extended with user emotion tags, disfluencies, syntactic and semantic parses, etc. in the future.","title":"SGER: Creation of a Goal-Oriented, Human-Machine Spoken Dialog Corpus","awardID":"0624389","effectiveDate":"2006-04-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7274","name":"HUMAN LANGUAGE & COMMUNICATION"}}],"PIcoPI":["434792"],"PO":["565215"]},"110631":{"abstract":"Award Abstract<br\/>0541447<br\/>Tim Sheard<br\/>Portland State University<br\/><br\/>Mitigating Human Error in Programs Through Combined Language\/Reasoning Systems.<br\/><br\/>The theory and practice of combined programming\/reasoning tools is investigated. The strategy is to enable programmers to define and reason about their programs, cast in terms of properties they defined themselves, all from within the programming language itself. The system developed will reason directly about programs (not models) using enhanced type systems to capture the properties of direct interest to the programmer. The project will develop a sound theory of combined programming\/reasoning systems, and apply that theory by extending and refining the existing Omega language.<br\/>The system will have four important characteristics that separate it from competing approaches. (1) Each property defined by the programmer has semantic meaning within the programming language independent of its role as a logical entity. (2) The systems separates values from types to maintain a familiar programming style. (3) Management of the constraints is performed inside the language using the well understood mechanism of constrained types. And, (4) The system partitions constraint management into static and dynamic parts, allowing the user to choose when constraints can be discharged at compile-time or at run-time.<br\/>The broader impacts of combined programming\/reasoning tools is to enable programmers to construct higher quality software. The reasoning capabilities allow an efficient division of labor:<br\/>Experts design software by specifying its properties, and competent programmers fill in the details. The reasoning facilities check that the constructed software actually contains the desired properties.","title":"Mitigating human error in programs through combined language\/reasoning systems","awardID":"0541447","effectiveDate":"2006-04-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["551061"],"PO":["564388"]},"112754":{"abstract":"Duke University's Institute for Genome Sciences & Policy (IGSP) offers undergraduates research involving Explorations in the Genome Sciences. The interdisciplinary program involves IGSP faculty from the Departments of Biology, Cell Biology, Biochemistry, Molecular Genetics & Microbiology, Biomedical Engineering, Statistics, and Biostatistics & Bioinformatics. The IGSP-wide emphasis on challenges represented by the Genome Revolution unifies faculty research interests and will create common themes for the individual student projects. Aside from an intensive research effort, student activities over the 10-week summer program include: (1) one-day orientation; (2) genome research ethics retreat; (3) weekly genome sciences research seminars; (4) weekly genome science career forums; (5) weekly student research discussion sessions; and (6) end of summer research forum for students to present their research results and conclusions. Facilities at Duke and the IGSP will provide students the necessary tools with which to engage and conceptualize their individual project within the developing area of genome sciences research. Undergraduates will be drawn from a national and diverse pool. Applications from women and under-represented minority students are encouraged. The integrated research experience will enable students from a broad range of disciplines to disseminate knowledge and appreciation of the interdisciplinary nature of genome sciences research. More information is available at http:\/\/www.genome.duke.edu\/edexchange\/programs\/summerfellowships, or by contacting Dr. Huntington Willard at hunt.willard@duke.edu or Dr. Gregory Wray at gwray@duke.edu.","title":"REU Site: Explorations in the Genome Sciences","awardID":"0552605","effectiveDate":"2006-04-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"1139","name":"RSCH EXPER FOR UNDERGRAD SITES"}}],"PIcoPI":["460959",298441],"PO":["545311"]},"117418":{"abstract":"The future advent of ultra-high-resolution display technology has the potential to open up powerful new possibilities for information visualization. In the past, data quantity and complexity have grown much more rapidly than display capability, placing severe constraints on visualization design and forcing information visualization research to focus on a paradigm of highly aggregated overviews with details of only a small number of data items. An impending 100x increase in display capability has the potential to enable the simultaneous multi-scale visualization of vast and detailed information spaces. But current uses of high-resolution displays for visualization are rudimentary, with researchers focusing primarily on the computational aspects of graphics rendering. To take full advantage of the new possibilities, fundamentally different approaches to information visualization will have to be developed. In this project the PI will explore, through rigorous scientific experimentation and comparative analysis, the HCI aspects of interactive information visualization on ultra-high-resolution displays, with a view to both truly enabling the high-resolution revolution, and to determining if there should indeed be a revolution at all. Previously awarded NSF research infrastructure funding has enabled the PI to construct one of the highest resolution display facilities in the world. Taking advantage of that platform, the PI will collaborate with life scientists who work in an information-rich area of the bioinformatics domain in which there is a desperate need for improved exploration of massive data, along two major thrusts. Basic research will identify and quantify the fundamental capabilities, benefits, and value of high-resolution visualization, by exploring the task and parameter space of high-resolution displays to identify optimal configurations and evaluate human abilities for perceiving large quantities of high-resolution, multi-scale information. Applied research will design and evaluate new high-resolution interactive information visualization techniques, to identify the most effective approaches for exploiting the increased display space. An important project outcome will be baseline scientific data describing the effect of various display issues such as resolution, display size, bezels, and information display techniques on perception, cognition, user performance, and user behavior. Another outcome will be fundamental new techniques for high-resolution information visualizations that are demonstrably more scalable than traditional low-resolution counterparts.<br\/><br\/>Broader Impacts: This project will spur development of a new research area at the intersection of HCI, massive data visualization, and high-resolution display technology. Through development of advanced bioinformatics tools, it will directly impact cutting-edge research in the life sciences. In the longer term, it will guide future industry development of ultra-high-resolution form factors and advanced visualization tools for massive data, which will result in fundamental new data analysis capabilities across a broad range of application domains such as the sciences, command and control, infrastructure management, and the intelligence community.","title":"SGER: Design and Evaluation of Scalable Information Visualizations with High-Resolution Displays","awardID":"0617975","effectiveDate":"2006-04-15","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":["518252"],"PO":["565227"]},"110610":{"abstract":"Clusters have emerged as the most effective solution to design high-performance servers, which are increasingly being deployed in supporting a wide variety of web-based services. Along with high and predictable performance, optimization of energy consumption in these servers has become a serious concern due to their high power budgets. In addition, the critical nature of many Internet-based services mandate that these systems should be robust to attacks from the Internet, because numerous security loopholes of cluster servers come to the forefront. Therefore, design and analysis of high-performance, energy-efficient, and secure clusters is crucial for the next-generation cluster systems not only from academic and industrial standpoints, but from socio-economic and environmental standpoints. Although some initial investigation on cluster energy consumption and security has appeared recently, an in-depth design and analysis of a cluster interconnect considering the three parameters above have not been undertaken. On the other hand, such an investigation is extremely challenging because there are numerous controllable factors across many dimensions and frequently these factors are conflicting. Therefore, the proposed research attempts to address three issues in a ground-up fashion starting from the basic cluster components to the entire system. It is planned to design all components to be plugged into the simulation testbed to assess their impacts on performance.","title":"Collaborative Research: Design and Analysis of High-Performance, Energy-Efficient, and Secure Clusters","awardID":"0541360","effectiveDate":"2006-04-15","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["390805"],"PO":["562984"]},"120312":{"abstract":"0306486 <br\/>Kim B. Bruce <br\/>Williams College<br\/><br\/>RUI: Modules and Parallel Specialization of Object Types <br\/><br\/><br\/>The goal of this research is to improve the design of statically-typed object-oriented programming languages, especially with regard to issues of programming in the medium to large scale. Research will focus on: <br\/><br\/>1. Module systems for object-oriented languages: Modules provide support for organizing code, abstraction barriers to hide information, and support for separate compilation. However, current module systems lack the flexibility to capture a number of common ways in which to design programs in object-oriented languages. <br\/>2. Simultaneous modification of mutually recursive types and classes: This research will develop ways to support incremental modification of mutually recursive types while preserving type safety and important <br\/>relations between types and classes. <br\/>3. Typed intermediate languages and virtual machines for object-oriented languages: Run-time systems like the target virtual machines for Java and C# lack support for advanced object-oriented languages and module systems. Exploring compilation techniques and virtual machines to support these features is necessary to ensure their eventual adoption. This research will continue using LOOM as a test bed for further investigations into the best way to support features of object-oriented languages, while also investigating how to use the best of these ideas to enhance real languages like Java and its successors.","title":"RUI: Modules and Parallel Specialization of Object Types","awardID":"0632458","effectiveDate":"2006-04-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":[319445],"PO":["564388"]},"110633":{"abstract":"This project is premised on the belief that a fundamentally different approach is necessary for designing and implementing multimedia compression, protection and transmission algorithms and systems for resource-constrained networked devices. The main idea is that instead of considering multimedia algorithms as a given and adapting the implementations reactively to match the requirements of these algorithms, proactive joint optimization of the algorithms, their parameters, and the implementation task partitioning is performed. The system is therefore able to maintain operation at a point in rate-distortion-complexity space in accordance with a constellation of factors and constraints including distortion tolerance, power, platform architecture characteristics, network behavior, and critically, data attributes. A global optimization framework and associated algorithms that consider the data-specific impact on the system factors listed above, allowing cost functions describing these tradeoffs to be formally described and then optimized as appropriate to different devices, networks and applications will be developed. Specifically: 1) complexity description methods and models, 2) data-aware channel decoding methods that combine feedback from the decoder and system knowledge to adapt the processing, 3) joint source-channel coding methods that exploit error concealment in combination with knowledge of the time- and content-varying sensitivity of multimedia data to errors, and 4) optimized resource management. The broader impact of the project is in the area of educating next generation students and industry partners in optimized resource-aware system design. Developing formal methods, algorithms and models will lead to improved multimedia performance over existing resource-constrained systems and also provide valuable insights into the design of next generation multimedia compression algorithms that should be complexity scalable, as well as system designs that should be multimedia aware.","title":"Complexity Optimization Strategies for Adaptive Multimedia Receivers","awardID":"0541453","effectiveDate":"2006-04-15","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["558132"],"PO":["550859"]},"110534":{"abstract":"Abstract<br\/>0541021<br\/>John C. Reynolds<br\/>Carnegie Mellon University<br\/><br\/>Reasoning about Shared Structure and Concurrency<br\/><br\/>The specification and verification of computer programs is investigated, along with the semantics needed to insure the soundness of verification. Of specific interest are:<br\/><br\/>Separation Logic, which treats programs employing shared mutable data structures or shared-variable concurrency. The goal is to extend the logic to high-level languages using safe type systems and automatic<br\/>storage reclamation, and also to machine-level languages permitting pointers to code to be embedded within data structures.<br\/><br\/>Grainless Semantics, which treats shared-variable concurrency without imposing any default level of atomic operations, by regarding race conditions (i.e., simultaneous access to the same storage by concurrent<br\/>processes) as catastrophic events. The goal is to simplify the understanding of programs by avoiding useless distinctions between programs with unacceptable behavior.<br\/><br\/>The intellectual merit of this research is that it will substantially increase the domain of discourse of separation logic, and facilitate soundness arguments for this and other logics for shared-variable<br\/>concurrency.<br\/><br\/>The broader impact is that it will become easier to avoid errors in an important class of useful but difficult computer programs. Eventually, it should be possible to automate proof-checking in the logic so that<br\/>programs in this class can be accompanied by machine-checkable proofs of their correctness.","title":"Reasoning about Data Structures, Concurrency, and Resources","awardID":"0541021","effectiveDate":"2006-04-15","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["511108"],"PO":["564388"]},"111403":{"abstract":"Networking has become an integral part of modern computer systems, so the architecture of the network subsystem is a critical design consideration. In the past, the performance of the network subsystem has kept up with network bandwidth growth because of the exponential rate of improvement in processor performance. However, to counteract the increasing complexity and wire-limited nature of modern uniprocessors, chip multiprocessing and simultaneous multithreading have emerged as dominant microprocessor architectures. These architectures sacrifice uniprocessor performance growth in favor of increased parallelism, so current network subsystems that rely on single-thread performance will be unable to saturate the network in future systems. This project will develop new network subsystem architectures to efficiently utilize next generation microprocessors and maximize network performance. This research will improve networking parallelism and scalability by considering all levels of the network subsystem, including the operating system's network stack, network device drivers, the I\/O system, and network interface hardware. This will result in a restructuring of the hardware and software interfaces within the network subsystem to provide mechanisms for parallel communication and collaborative processing between microprocessors and network interfaces. This project will also be used to expose students to system-level networking issues in both undergraduate and graduate computer systems and architecture courses.","title":"CAREER: Parallel, Scalable, and Efficient I\/O for Network Servers","awardID":"0546140","effectiveDate":"2006-04-01","expirationDate":"2012-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["551027"],"PO":["559883"]},"112536":{"abstract":"This project, developing a series of sensor networking testbeds in New Mexico, focuses on the use of wireless sensor networks in the following applications:<br\/><br\/>-Tracking, controlling, and behavioral monitoring of livestock on rangeland, <br\/>-Micro-monitoring of weather and climate on an ecological research site, <br\/>-Protecting contextual privacy of distributed sensing tasks, <br\/>-Developing component-based middleware engineering for embedded sensor nodes and gateways, <br\/>-Creating an integrated sensor net design environment and testbed, and <br\/>-Developing a real-time collaborative virtual environment for smart office design and distance learning.<br\/><br\/>Establishing infrastructure, core capabilities, and expertise in Wireless Sensor Networks (WSN) and Distributed Sensor Information Technology, the project services three specific research projects:<br\/><br\/>-Privacy protection issues in distributed sensing,<br\/>-Component-based middleware engineering for WSN, and<br\/>-Integrated sensor net design environment and testbed.<br\/><br\/>The equipment requested includes different types of embedded processor-radio modules (motes), compatible sensor data acquisition modules, serial and Ethernet programming and interface boards, and embedded specialized gateway computers. <br\/><br\/>Broader Impact: Newly developed courses on sensor networks service students at many levels, strengthening the education at this minority serving university. Furthermore, the research projects offer benefit to society directly and indirectly. Improved methods for integrating distance learning with main-campus learning attracts and engages more students in higher education; improved understanding of rangeland usage and livestock behavior helps improve land utilization and food production for society; and improved understanding of sensor network privacy security issues and of software engineering for sensors helps further the deployability mission-critical areas.","title":"CRI: Infrastructure for Networked Sensor Information Technology","awardID":"0551734","effectiveDate":"2006-04-01","expirationDate":"2011-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["525154",297892,297893,"557313","557313","448648"],"PO":["561889"]},"118806":{"abstract":"This project addresses one of the vital problems in natural language processing -- the quality of linguistic annotation of electronic natural language corpora. The purpose of this project is to explore a new method for automatically detecting and correcting errors in annotated corpora, to extend its applicability and improve its precision and recall for different types of linguistic annotation, and to investigate its relation to fraud\/anomaly detection approaches developed outside of computational linguistics.<br\/><br\/>More specifically, the project examines and extends the variation n-gram method for detecting annotation errors by exploring its applicability to dependency annotation, increasing the recall of the method through a generalization of what constitutes comparable contexts for different types of annotation, adding an error correction stage, and researching and evaluating the effect of annotation errors and their correction on the use of corpus annotation for human language technology. The project includes an exploration of the potential broader impact beyond language technology, which is significant given that the error detection methodology developed by the project is in principle applicable to all collections of data which encode judgments or classifications of repeated data subunits. <br\/><br\/>The success of data-driven approaches and stochastic modeling which are widely used now in computational linguistic research and applications is rooted in the availability of linguistically annotated electronic natural language corpora. However, despite the central role that annotated corpora play for training and testing human language technology, the question of how errors in the annotation of corpora can be detected and corrected has received only little attention. This project is the first systematic attempt to remedy the situation by developing automatic methods to improve the linguistic annotation quality. The implemented error detection and correction algorithms will be made freely available, and the theoretical results will be made accessible through publications at leading international computational linguistics conferences.","title":"Exploration of Error Detection and Correction in Corpus Annotation","awardID":"0623837","effectiveDate":"2006-04-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7274","name":"HUMAN LANGUAGE & COMMUNICATION"}}],"PIcoPI":[314900,314901],"PO":["565215"]},"116408":{"abstract":"The Association for Computational Linguistics (ACL) and the <br\/>International Committee on Computational Linguistics (ICCL) are two <br\/>primary international organizations in the field of natural language <br\/>processing and language engineering. For over forty years, the two <br\/>organizations have sponsored conferences to facilitate the exchange of <br\/>research ideas. This year, the two organizations will co-host a joint <br\/>conference from July 17th to July 21st, 2006, in Sydney, Australia. It <br\/>is the major international conference for the field. This award <br\/>subsidizes the expenses for students selected to participate in the <br\/>Student Research Workshop associated with the main COLING-ACL <br\/>conference. The students-only workshop allows participants to present <br\/>their research and receive feedback from a panel of established <br\/>researchers in the field. The forum provides students the opportunities <br\/>to gain exposures to external perspectives on their work at a critical <br\/>time in their professional development. The intimate format encourages <br\/>students to begin building rapport with established researchers. The <br\/>Student Research Workshop contributes to the maintenance and development <br\/>of a skilled and diverse computational natural language workforce, <br\/>helping to produce a pool of future researchers for Natural Language <br\/>Engineering. The workshop encourages a spirit of collaborative research, <br\/>and builds a supportive environment for a new generation of <br\/>computational linguists.","title":"Student Research Workshop in Computational Linguistics, at the COLING-ACL 2006 Conference","awardID":"0612690","effectiveDate":"2006-04-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7274","name":"HUMAN LANGUAGE & COMMUNICATION"}}],"PIcoPI":["357107"],"PO":["565215"]},"110634":{"abstract":"This proposal addresses the fundamental challenges in todays on-chip thermal sensing and control problems. The thermal sensing technique in even the most recent processors predominantly relies on thermal diodes or sensors. Both suffer from the fixed-location problem as on-chip hot spots migrate at run time. This project aims to develop novel techniques for spectrum and high precision temperature sensing to mitigate this problem. The proposed solutions can lead to more efficient and effective on-line thermal management. The intellectual merit of the project includes the development of a fast, lean, and accurate software thermal sensor for online temperature tracking. The software adopts a highly efficient and fine-tuned numerical method to calculate temperatures at a fine granularity both temporally and spatially. Experimentation will demonstrate its superior accuracy and speed, and hence the great potential of becoming a truly competitive remedy for chip temperature sensing.","title":"Fast Software Thermal Sensing and Control for Efficient Dynamic Thermal Management","awardID":"0541456","effectiveDate":"2006-04-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["364777","535204","508314"],"PO":["325495"]},"113912":{"abstract":"Title: SGER: Exploratory Investigation of Hierarchical Image Classification and Database Indexing<br\/>PI: Jianping Fan<br\/><br\/>Abstract<br\/><br\/>As high-resolution digital cameras become more affordable and widespread, high-quality digital images become ever more available and useful. There is an urgent need to support more effective image retrieval over large-scale image archives. In this Small Grants for Experimental Research (SGER) project, a novel framework for image content representation will be developed based on using salient objects to characterize middle-level image semantics. For salient object detection, automatic image segmentation will be integrated with image region classification. Also, hierarchical image classification will be incorporated to enable concept-oriented image database indexing and retrieval through exploring the underlying relationships among different semantic image concepts. Hierarchical finite mixture models will be presented for multi-level semantic image concept modeling, image classification and database indexing, and an adaptive Expectation Maximization (EM) algorithm will be developed for model selection and parameter estimation. Finally, a novel framework for statistical image modeling will be developed to enable partial image matching in the procedures for classification and retrieval. The research will lead to useful tools that could significantly impact computer vision, image database management, digital library or museum management, and image data mining for a wide range of applications. The project will provide a good environment for interdisciplinary education in information technology that bridges the gap between traditional areas of computer vision, databases, visual information retrieval, and machine learning to benefit students of all levels.<br\/><br\/>Project web page: http:\/\/www.cs.uncc.edu\/~jfan\/project3.html","title":"SGER: Exploratory Investigation of Hierarchical Image Classification and Database Indexing","awardID":"0601542","effectiveDate":"2006-04-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["475621","536619"],"PO":["317663"]},"110557":{"abstract":"Application-specific instruction set processors (ASIPs) promise to offer a good tradeoff between flexibility and performance, and have been adopted widely. However, most of the research work has focused on improving performance specifically, taking a flawed assumption that low execution time always leads to energy savings. Energy-optimized ASIP synthesis and associated problems have escaped thorough scrutiny. The objective of the proposed work is to develop a new framework for energy-efficient ASIP synthesis, where a fast and accurate energy evaluation tool will be exploited to aid design space exploration, various energy optimization techniques at different levels will be applied throughout the design flow, and multiple facets of system design will be re-examined from the energy perspective. First, a hybrid energy estimation model for configurable and extensible processors in the early design cycle will be designed. The energy macro-model will capture not only the extensibility provided by additional custom hardware components through a structural macro-model, but also the configurability of the baseline processor through an instruction-level macro-model, including register file size, and pipeline issue width, etc. A set of energy-optimization techniques will be utilized during ASIP design flow systematically. Focus will be on multi-level application-specific optimization techniques, ranging from data-level storage size adaptation, to task-level parallelism extraction, up to system-level spurious switching activity suppression. Multiple facets of the system design to achieve best energy efficiency will be investigated. The proposed work will be a general study of several core technologies to enable the design, spanning the fields of compilation techniques, design space exploration, customized processor architecture generation, and high-level synthesis methodologies and tools. The ultimate goal is to expand the paradigm of ASIP synthesis along an important but not well-investigated dimension of energy efficiency. The framework and tools developed will provide an ideal mechanism through which students can interact with tangible examples of computer architecture, design hierarchy, hardware-software co-design, compiler concepts - it will allow them to rapidly prototype systems, experiment with new ideas, and thereby build intuition about embedded processor and application design.","title":"A Multi-level\/multi-faceted Framework for Energy-efficient Application-Specific Instruction Set Processor Synthesis","awardID":"0541102","effectiveDate":"2006-04-01","expirationDate":"2011-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["557273"],"PO":["559883"]},"111415":{"abstract":"Ben Zhao<br\/>University of California, Santa Barbara<br\/>0546216<br\/>Abstract<br\/><br\/>The future of Internet-scale applications relies on securely connecting<br\/>distributed resources such as corporate data centers, video-conferencing<br\/>sites, and distributed web caches. <br\/>Unfortunately, today's Internet is extremely inhospitable for distributed<br\/>applications. New viruses and worms are discovered on a daily basis, and<br\/>constantly threaten to compromise end hosts by exploiting software and<br\/>network vulnerabilities. Meanwhile, administrators<br\/>are in a constant race to secure machines before they are compromised.<br\/><br\/>The objective of this project is to investigate the design and deployment of<br\/>a self-healing network infrastructure that allows distributed applications to<br\/>survive in this environment by maintaining availability while resisting<br\/>attacks from compromised internal hosts and external attackers. The project<br\/>focuses on both proactively avoiding interactions with suspicious network<br\/>entities, and reactively detecting and recovering from internal and external<br\/>attacks. <br\/><br\/>Compared to end-host or protocol based approaches, our approach is unique in<br\/>trying to maintain long-term application availability despite the assumption<br\/>that end hosts are insecure and will be continuously attacked and<br\/>compromised. The PI is building the Gaia infrastructure on top of a structured<br\/>peer-to-peer overlay and leverage overlay routing for our scalable<br\/>communication primitive.","title":"CAREER: GAIA: A Self-organizing, Self-healing Network Infrastructure","awardID":"0546216","effectiveDate":"2006-04-01","expirationDate":"2011-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["560388"],"PO":["529429"]},"110579":{"abstract":"ABSTRACT<br\/>0541192<br\/>Wang, Li-Chung<br\/>U of Cal Santa Barbara<br\/><br\/>Project Title: Statistical Tools and Methodologies for Timing Validation and Silicon Debug<br\/><br\/>With the advances to nanometer manufacturing technologies, the gap between chip design and chip manufacturing has become wider due to increased parametric variations. This gap causes the reduction in percentage of good parts coming out of an advanced manufacturing line, resulting in significant loss of manufacturing resources. Bridging the gap demands novel tools and methodologies in design validation and silicon debug. This project proposes a novel validation and debug framework that facilitates the automatic information flow between manufacturing testing and design simulation. Integrated with the planned educational activities, the research proposes a coherent software framework consists of three components: (1) pattern-based statistical timing analysis, (2) silicon emulation, (3) test data mining. The primary goal of pattern-based statistical timing analysis is to enable robust test development and silicon chip debug. It is a simulation tool that analyzes the sensitivity of test results with respect to model assumptions and process variations. To better understand the gap between design and manufacturing, a careful study on how different parametric variations interact to impact chip performance is required. To enable this study, the PI proposes the development of a silicon emulator that exposes the complexity of performance variations in a simulation environment. In the third component, novel data mining and statistical learning techniques will be developed to extract useful design information from test data. Novel debug and validation methodologies that incorporate the test simulation tool and test data mining techniques will be developed to facilitate the convergence between design and manufacturing. <br\/><br\/>This research complements other research efforts in the design automation and test areas. While the research in statistical static timing analysis aims to provide new tools to better cope with variations in reaching timing closure, the proposed framework aims to resolve the issues after the timing closure. While the research in delay testing aims to enhance the effectiveness of screening frequency-dependent manufacturing defects, the proposed framework aims to effectively diagnose the discrepancies between design behavior and silicon chip behavior. This research introduces novel applications of data mining and statistical learning techniques. While most data mining techniques focus on model predictability and accuracy, the proposed techniques will focus on model interpretability and diagnosis. These new techniques can inspire the data mining community to develop novel approaches to be applied in design automation and manufacturing test applications. Technologies developed through the project will be transferred to the leading semiconductor companies. These technologies will improve their design and test methodologies and continue the push of manufacturing technologies to the cutting edge.","title":"Statistical Tools and Methodologies for Timing Validation and Silicon Debug","awardID":"0541192","effectiveDate":"2006-04-15","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["535169"],"PO":["562984"]},"110547":{"abstract":"Nanotechnology has emerged in recent years as a promising technological platform for new computational structures. Substantial research has been carried out into hardware based on nanotubes and nanowires: structures which have feature sizes of just a few nanometers. <br\/><br\/>The principal objective of the project is to develop the foundations for nanoscale computational structures, circuits and architectures, that will take advantage of the unique properties of nanoscale devices. In particular, the defect density of nanodevices is expected to be much greater than that of conventional microelectronic devices. Fault-tolerance techniques to allow circuits containing faults to still continue to function will be developed. Fault tolerance always requires introducing redundancy; fault-tolerant design involves making tradeoffs between the overhead imposed by such redundancy and the available gain in reliability. <br\/><br\/>Design tools for synthesizing computer architectures based on nanotechnology will be developed. Such tools will take into consideration realistic parameters and physical constraints that affect the placement of nanocircuits, and will allow the designer to study architectural tradeoffs in nanotechnology.","title":"Exploring Design Approaches and Fault Tolerance in Nano Streaming Processors","awardID":"0541066","effectiveDate":"2006-04-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["553614","553615","408968"],"PO":["550859"]},"110527":{"abstract":"Background<br\/><br\/>Rapid increases in computer performance, according to Moores law, have enabled software architects to<br\/>design software of ever increasing functionality and complexity. Consequently, consumers expect software<br\/>that grows in functionality and can process larger data sets with every new release (e.g., huge spreadsheets<br\/>or streaming high-quality video). Unfortunately, since Moores law will likely not hold in the near future,<br\/>owing to physical limitations such as wire delays and heat dissipation, hardware speedups will not keep<br\/>pace with the demands of software. Thus, to meet the performance requirements of new software, one must<br\/>look to software techniques. <br\/><br\/>The proposed work will explore a new approach, algorithmic optimizations in<br\/>virtual machines, to provide the performance for next generations of software.<br\/><br\/>Intellectual Merit<br\/><br\/>-A system for marking implementations of algorithms, data structures, and their uses. A VM that<br\/>knows about this system can exploit the information for connecting uses with suitable implementations.<br\/>A VM that does not know about this system still executes the code correctly but without its<br\/>performance benefits.<br\/>-An implementation within a Java virtual machine that exploits the information for improving performance.<br\/>This implementation support includes modifications to the garbage collector (for migrating<br\/>data from one representation to another), compiler (for inlining away the calls), and scheduler (for<br\/>scheduling profile collection threads with lower priority).<br\/>-A collection of libraries and applications marked to exploit algorithmic optimizations. This will form<br\/>part of the evaluation for the proposed work.<br\/><br\/>Broader Impact<br\/><br\/>The infrastructure developed as part of this research will be used in undergraduate<br\/>algorithms classes to give students deep knowledge about the strengths and limitations of <br\/>commonly used algorithms and data structures. This research will provides the performance <br\/>necessary for meeting the needs of future software systems.","title":"ST-CRTS: Collaborative Research: Algorithmic Optimizations in Dynamic Programming Environments","awardID":"0540997","effectiveDate":"2006-04-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["461768"],"PO":["565272"]},"110627":{"abstract":"Abstract<br\/>0541417<br\/>Alexander Sawchuk<br\/>Los Angeles, CA<br\/><br\/>Investigation of Reliability - Constrained On-Chip Networks<br\/><br\/>Among the many challenges computer architects will face over the next decade and beyond is the growing demand for reliable on-chip communication between system microarchitecture functional domains. Continued increases in scaling and integration of transistor and wiring resources are allowing more system functions to be implemented on chip, but also more circuit defects and variability. Recent trends toward partitioning the system microarchitecture into multiple on-chip compute domains in the form of functional unit blocks, tiles and processor cores mitigate chipcrossing delays and facilitate chip survivability. That is, it helps to prevent system performance and cost from being encumbered by deep submicron technology scaling. With these developments, support for low latency, high throughput, and fault tolerant communication is becoming more and more critical within the on-chip network used to interconnect the compute domains. Much recent research is directed toward the design of on-chip networks to meet certain cost\/performance goals<br\/>(chip area, latency and throughput), but very little architecture research explores on-chip network<br\/>reliability issues specific to the problem of hard faults, which is recognized as a growing problem.<br\/>In this research, we investigate reliability challenges and techniques for on-chip networks that will meet manufacturing yield and chip reliability targets as technology scales into the deep submicron regime. The goal is to understand the problem more fully and to develop on-chip network techniques for efficient resource and reliability management, fault isolation, dynamic reconfiguration and fault recovery to allow fault-stricken microarchitectures partitioned across a chip to have increased usability and prolonged life. We endeavor to increase understanding of chip failure mechanisms (their causes and impact); appropriately model them as related specifically to on-chip networks; develop approaches and techniques that will allow on-chip networks (in cooperation with techniques for other components of the chip microarchitecture) to be resilient to<br\/>hard faults; evaluate and assess the benefit of the proposed techniques under expected workloads and common-case operational conditions; and, furthermore, understand the tradeoffs in using the proposed fault-resilient on-chip network techniques that is, identify those situations in which various techniques can be most usefully applied given the existence of other possible constraints. The Intellectual Merit of this research is substantial. The research is timely as it addresses an important issue that will only worsen with continuing advancements in technology scaling. The research will culminate with key contributions made in (1) increasing our understanding of the fundamental design, process, and operational mechanisms most responsible for on-chip interconnect failures and (2) producing original and promising techniques for increasing on-chip interconnect reliability and chip reliability as a whole. Beyond the specific results produced by the models and simulation environments we will develop through this project, these tool artifacts will likely have a profound impact on future research infrastructure and education for years to come. They<br\/>will be invaluable assets to researchers, students, and practitioners for understanding, developing,<br\/>evaluating, and trading-off alternative reliability techniques as demanded by advanced technologies and systems. The tools we develop will be made publicly available and are expected to have widespread use. The results of this research will also be widely disseminated through publications. The Broader Impact of this research is significant and far-reaching. This research can have a profound impact on the success of near-future nanoscale technologies (molecular, quantum, etc.) used to implement integrated circuits beyond the CMOS era as ICs implemented in these technologies are expected to have substantially more hard faults (orders of magnitude) than CMOS ICs. Reliability techniques such as the ones that will be derived from this research will be critical to systems implemented in these technologies as well as those implemented in future deep submicron technology. In the nearer term, many of the ideas coming from this research may be transferrable to system-level networks, where form-factor constraints often are not as rigid as they are on-chip.","title":"Investigation of Reliability-Constrained On-Chip Networks","awardID":"0541417","effectiveDate":"2006-04-15","expirationDate":"2012-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["325495",293075],"PO":["559883"]},"110619":{"abstract":"Clusters have emerged as the most effective solution to design high-performance servers, which are increasingly being deployed in supporting a wide variety of web-based services. Along with high and predictable performance, optimization of energy consumption in these servers has become a serious concern due to their high power budgets. In addition, the critical nature of many Internet-based services mandate that these systems should be robust to attacks from the Internet, because numerous security loopholes of cluster servers come to the forefront. Therefore, design and analysis of high-performance, energy-efficient, and secure clusters is crucial for the next-generation cluster systems not only from academic and industrial standpoints, but from socio-economic and environmental standpoints. Although some initial investigation on cluster energy consumption and security has appeared recently, an in-depth design and analysis of a cluster interconnect considering the three parameters above have not been undertaken. On the other hand, such an investigation is extremely challenging because there are numerous controllable factors across many dimensions and frequently these factors are conflicting. Therefore, the proposed research attempts to address three issues in a ground-up fashion starting from the basic cluster components to the entire system. It is planned to design all components to be plugged into the simulation testbed to assess their impacts on performance.","title":"Collaborative Research: Design and Analysis of High-Performance, Energy-Efficient, and Secure Clusters","awardID":"0541384","effectiveDate":"2006-04-15","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":[293053],"PO":["562984"]},"119560":{"abstract":"The East Coast Computer Algebra Day, ECCAD 2006, meeting, will be held at Drexel University in Philadelphia on Saturday May 6, 2006. The meeting is the thirteenth of the series of ECCAD meetings that have been held annually since 1994 at a variety of US East Coast locations. We estimate a turnout of about 80 participants from the Computer Algebra research community, doing either theoretical research or important applications in science and engineering. There will be three very distinguished invited speakers: Prof. Herbert Wilf of the University of Pennsylvania, Dr. Daniel W. Lozier from the National Institute of Standards and Technology, USA, and Prof. Masakazu Suzuki, of Kyushu University, Japan. Participants will come mainly from the eastern half of North America and especially from the East Coast of the United States. In addition to the invited talks there will be contributed poster and software demonstration sessions, covering all aspects of computer algebra: theoretical research, software and applications.","title":"East Coast Computer Algebra Day 2006","awardID":"0628174","effectiveDate":"2006-04-15","expirationDate":"2007-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[317120,"555302",317122],"PO":["321058"]},"119032":{"abstract":"This is a conference-related grant to support the travel, subsistence and registration expenses of approximately 16 student participants in a trading agent competition, being held in Hakodate, Japan, plus providing computational resources to enable educational experiences. The Trading Agent Competition (TAC) is an international forum designed to promote and encourage high-quality research about trading agents. TAC provides a platform for researchers to evaluate programmed trading techniques by competing with agents from other design groups in a simulated market scenario. TAC tournaments have been held annually since 2000, and have attracted participants from institutions in dozens of countries around the world. Involvement of American students will not only advance their individual careers in computer science, but will also contribute to the nation's future science and technology workforce. This activity contributes to curriculum development by developing and demonstrating methods for involving students in the design and competitive use of artificial agents.<br\/><br\/>Entries in the Trading Agent Competition are software programs designed to trade in electronic markets. They are called \"agents\" because these programs operate autonomously in the market: sending bids, requesting quotes, accepting offers, and generally negotiating deals according to market rules. Although the agent's activity is ultimately determined by its programmers, the trading behavior is itself fully automated: humans do not intervene while the negotiation is in progress. Trading agents face a most challenging task. To play the market effectively, an agent must make real-time decisions in an uncertain and fast-changing environment, taking into account the actions of other agents that are doing the same. Capable agents rapidly assimilate market information from many sources, forecast future events, optimize complex offers and resource allocations, anticipate strategic interactions, and learn from experience. Successful trading agents adopt and extend state-of-the-art techniques from artificial intelligence, operations research, statistics, and other relevant fields. <br\/><br\/>The annual trading agent competitions were initiated to promote research and education in the technology underlying trading agents. At the annual competition, the developers of techniques in trading strategy evaluate these ideas and communicate their results in a public forum for the benefit of the broader research community. The educational function of TAC is manifest by the significant role of students on almost all teams entering the competition. Many universities use TAC as an exercise for teaching about electronic commerce and artificial intelligence techniques. TAC is a useful aid in the classroom because it is by nature a hands-on (laboratory-like) experience for computer scientists. Research and education in trading agents promises to improve the state of art and practice in their development, and ultimately lead to more effective electronic markets. Equally important, increasing public knowledge in this area promotes understanding of the behavior of autonomous software agents as such systems become more prevalent in commerce and other domains.","title":"The Trading Agent Competition","awardID":"0624886","effectiveDate":"2006-04-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}}],"PIcoPI":["517963"],"PO":["564456"]},"109430":{"abstract":"News stories or Web pages can contain a great deal of reused information. Different authors may each present different versions of a story or event based on the same sources, and the facts of an event may get recapitulated or restated each time it is presented. Sometimes such presentations have little in common with each other; at other times one may be a copy of the other with minor edits. Given a topic of interest, then, a sufficiently extensive archive could be used to identify when particular ideas or statements originated and to check their validity. The goal of this project is to develop techniques to identify alternative versions of the same information in order to reconstruct how information \"flows\" between documents.<br\/><br\/>The project involves the investigation of a range of approaches to detecting reuse at the level of sentences, passages and documents. The research is evaluated using a range of corpora, such as news, Web crawls, and blogs, in order to explore the dimensions of reuse and information flow in different situations. <br\/><br\/>The research and its outcomes will have a significant impact on the design of tools that can be used to validate and assess information that comes from sources of differing reliability. Such a tool would be valuable in many applications in education, scientific research, and national security. The results of the research will be published in papers, will be accessible via the project Web site (http:\/\/ciir.cs.umass.edu\/research\/textreuse.html) and source code will be distributed through the popular Lemur toolkit (http:\/\/www.lemurproject.org\/).","title":"Text Reuse and Information Flow","awardID":"0534383","effectiveDate":"2006-04-15","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["507668"],"PO":["563751"]},"119000":{"abstract":"The PI believes research in human-computer interaction is on the cusp of solidifying a paradigm of embodied cognition that emphasizes the role of bodily reality in the creation of meaning, which constitutes the essence of our thought and experience. When we speak, our heads, eyes, bodies, arms, hands, and face are brought into the service of communication. There is substantial evidence that communication does not exist for us separately from its embodied roots. The cognitive and perceptual resources of attention (both visual and conceptual), working memory, long-term memory, temporal sequencing, language processes, touch, internal physiological experience, and proprioception must be considered together to understand the range of meanings a behavior has to the actor and receiver in a situation. The PI recently submitted a proposal entitled \"CRI: Interfaces for the Embodied Mind\" by means of which he was able to secure funds for the acquisition of equipment necessary to exploring ideas such as those outlined above, but not for the exciting applications he envisages in collaboration with partner institutions. The supplemental funding provided by this SGER will enable the PI to explore the implications of embodied interaction technology to communication in the domain of the liberal arts, specifically through participant and travel funding to support collaborative projects: (a) with Prof. Cassandra Newby-Alexander and others in Norfolk State University's Department of History who are investigating the Underground Railroad network in the four large urban slave centers of Norfolk, Portsmouth, Richmond, and Petersburg; and (b) with Prof. Pinckney Benedict of Hollins University's English Department on a pilot project in which two or more creative writing students develop an embodied adaptation of James Joyce's classic short story \"Araby\" which is rich in concrete environmental cues and written in a dreamlike narrative style that lends itself gracefully to nonlinear storytelling. Each project will involve two faculty and two student members from the partner organization, along with students and faculty from Virginia Tech.<br\/><br\/>Broader Impacts: Norfolk State University is an HBCU, and Hollins is a women's university. This project will afford an opportunity to these partner institutions for their students and faculty to participate in the broader federally-funded research enterprise, and will at the same time expose students at Virginia Tech to students with diverse backgrounds and perspectives that will improve their ability to engage realistic HCI problems. To these ends the PI will develop a collaborative research outreach model based on cross-institutional faculty teams. He will define research projects that span multiple institutions and provide a scaffold for outreach partner institution students to work with their professors during the summers on the Virginia Tech campus. Partner institution students will continue their research upon returning to their home institution, thereby transferring their experiences to home faculty and students while keeping their connection with the faculty and students they befriended over the summer. Electronic communications and periodic cross-visits by the faculty team will keep the research vital. An annual workshop at Virginia Tech will expose partner institution students and faculty to the broader research and stimulate greater synergy.","title":"Embodied Communication: Vivid Interaction with History and Literature","awardID":"0624701","effectiveDate":"2006-04-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}}],"PIcoPI":["486496","563439","394686"],"PO":["565227"]},"118233":{"abstract":"This is a small grant for exploratory research to gather data on how the global communication network functioned around Hurricane Katrina, in terms of its use by various social groups. This timely research will provide insights into how emergency warning messages can be propagated through the social network. It will start by building an understanding of the nature of the communication network in New Orleans and the Gulf Coast, including connections with global communication networks, and the nature of the social group structure that overlay this communication network. The basic research questions to be addressed are: building models of how information flows in a communication network and taking into account the \"trustability\" of the information.<br\/><br\/>The enormity of the tragedy following Hurricane Katrina indicates a failure in an apparently otherwise well functioning global communication network, wired and wireless. Despite the fact that two to three days before Hurricane Katrina hit, the level of danger was known and broadcast, a large fraction of the population and the local administration did not trust the forecasts enough to take action. Technology by itself cannot ensure that the data are processed into information and delivered in a timely fashion in a form that is meaningful to the user and of value in emergency response decision-making. A significant component of how information flows through the society is intertwined with the social network dynamics of the society.<br\/><br\/>Individuals will not act upon information unless they consider the information trustworthy. This fact takes on special significance during a time of technological transition, as traditional mass media like newspapers and network television are supplemented or supplanted by online news and other computer-enabled communication media. The theoretical approach of this research postulates that there must be multiple trustworthy paths from the source of the information to the individual before some action is taken. This idea is used to build new measures of the quality of a social communication network relating to how well it will perform in conveying emergency warning messages. In turn, these measures can provide specific recommendations regarding how the network could be improved.","title":"Social Communication Networks for Early Warning in Disasters","awardID":"0621303","effectiveDate":"2006-04-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7496","name":"COLLABORATIVE SYSTEMS"}}],"PIcoPI":["548125","320859","489824"],"PO":["564456"]},"116451":{"abstract":"This research investigates how information and communication technologies played a role in the Hurricane Katrina disaster for disadvantaged populations. Research on the effects of disasters consistently shows that women and victims of lower socioeconomic status are more vulnerable to the adverse effects of forced relocation. In what ways did information and communication technologies enable or fail to enable significant help for these vulnerable groups? <br\/><br\/>Recent natural disasters forced thousands of people to relocate involuntarily and damaged or destroyed many communities. Information and communication technologies appear to have played a significant role in helping victims cope with the aftermath of the disasters. Informal reports suggest that evacuees, and people who helped evacuees, used the Internet to find family and friends, to search for updates on the state of their neighborhoods, to search for housing and jobs, and to exchange needed services, goods, and monetary aid. Many of the informal reports also suggest that great strides still need to be made if technology is to be used effectively in disasters. For instance, a flood of poor quality information, such as misspellings of people's names, made searching quickly for family and friends through people-locator sites difficult or impossible. This research focuses on how technology may have affected the exchange of help and support after an involuntary relocation in the wake of this natural disaster and in the search for family and friends lost in evacuations. <br\/><br\/>Adjustment to effects of major disasters and involuntary relocation can take many months or even years. In order to obtain information about technology use immediately after the disaster and to assess rates of adjustment post-disaster, this project will conduct two rounds of retrospective interviews and a self-report survey over the course of 4-6 months. Because this research is focused on the effectiveness of help, the study participants are residents of Baton Rouge and New Orleans who were affected by Hurricane Katrina, and on-the-ground volunteers who have continued to work in temporary accommodations there. This research examines the coping mechanisms that displaced individuals employed to deal with the aftermath of the hurricane and the technologies they found most useful. It also examines how volunteers used technology and their ability to help those hurricane victims who did not have direct access to the Internet or cellular phones. The researchers will study whether volunteer-run support, people-locator and in-kind donation websites were able to reach populations most needing support. <br\/><br\/>The results will simultaneously be of interest to computer scientists interested in innovative uses of technology that worked and did not work in this disaster and to social scientists concerned with the processes underlying social support and disaster coping. This research would also be of interest to policy makers who need hard information about the role played by information and communication technology in the disaster and where investments need to be made to alleviate the effects of future disasters. The intellectual merit of this work is that it examines use of information and communication technologies in understudied populations during a unique disaster event. The broader impact of this work is to provide important insights for policy makers and technology developers about the types of information and communication technologies that are most useful, and would be most useful. The findings will lead to recommendations for the design of new technologies and services that would be of most use to displaced persons in a disaster situation, especially those who are most disadvantaged.","title":"Adapting to Evacuation: Using Information Technology for Social Support","awardID":"0612870","effectiveDate":"2006-04-01","expirationDate":"2008-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7496","name":"COLLABORATIVE SYSTEMS"}}],"PIcoPI":["519226","560995"],"PO":["564456"]},"119510":{"abstract":"This NSF award provides support to enable U.S. participation in ISORC 2006. ISORC, the 9th IEEE International Symposium on Object and component-oriented Real-time distributed Computing, will be held on April 24-26, 2006, in Gyeongju, Korea. The purpose of the award is to provide travel assistance for graduate students and academic researchers from American universities who are accepted as speakers in this international conference, thereby increasing US participation and extending interaction.","title":"CSR-EHS: Workshop Student Support for ISORC 2006","awardID":"0627936","effectiveDate":"2006-04-15","expirationDate":"2008-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[316962],"PO":["561889"]},"109555":{"abstract":"The goal of this project is advance the state-of-the-art in image retrieval by explicitly addressing two issues: the semantic gap and the scalability. The semantic gap is related to the retrieval effectiveness, e.g., being able to use natural language to describe the images of interest, rather than having to specify low-level features, such as color histograms. The scalability refers to both the diversity in image content and quality and the number of images in the database. The specific approach consists of using probabilistic models and novel co-clustering techniques to explicitly exploit the synergy between multiple modalities typically co-existing in many real world image retrieval applications. Broader impact includes a series of innovative community outreach activities. The results of this research project will be incorporated into the development of image search engines and K-12 learning tools in two non-profit education and service organizations. The specific scenarios include a museum and a botanical garden, with a direct benefit to the communities and the society these organizations provide services to. Knowledge dissemination includes providing learning and research experience to students involved in the project. In addition, the organizations' technical personnel will provide expertise to develop useful services to the communities and society, will help in validating the results and also benefit from involvement in the project. The project Web site (http:\/\/www.fortune.binghamton.edu\/ir.htm) will be used to disseminate further information and results.","title":"Exploiting Multimodel Synergy for Large Scale and Diverse Image Retrieval in Digital Archives","awardID":"0535162","effectiveDate":"2006-04-15","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["451438"],"PO":["563751"]},"117443":{"abstract":"The proposal requests support for a workshop being held in conjunction with the 12th IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS 2006), in San Jose, California, April 4-7, 2006. The workshop explores research challenges that have subtle yet critical relationships in four major research areas: real-time computing, embedded computing, cyber security and networking. Broader impacts are expected through the outreach to students, in an extremely important area for national economic competitiveness.","title":"A workshop on next generation of real-time and embedded computing for cybersecurity, and networking","awardID":"0618109","effectiveDate":"2006-04-15","expirationDate":"2007-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["368857"],"PO":["561889"]},"109424":{"abstract":"This project extends search engine indexing and retrieval models<br\/>developed initially for structured (e.g., XML) documents to provide<br\/>convenient support for capabilities required by question answering,<br\/>computer-assisted language learning, and other human language<br\/>technology (HLT) applications. These new capabilities are required<br\/>because it is becoming common for HLT applications to use search<br\/>engines to find information in large text databases. HLT applications<br\/>typically use extensive text annotations to reduce the mismatch<br\/>between the word-based representations convenient for text retrieval<br\/>and the concept-based representations convenient for reasoning. They<br\/>also can describe very specific requirements that retrieved text<br\/>passages must satisfy. This use of text search is very different than<br\/>ad-hoc, interactive search, and current search engines do not support<br\/>it well. This project extends probabilistic indexing and retrieval<br\/>models to support multiple, detailed, overlapping text annotations,<br\/>hierarchical text annotations, annotations with associated confidence<br\/>values, and similar capabilities. By explicitly recognizing HLT<br\/>applications as first-class users, the proposed research broadens the<br\/>research community's view of text search well beyond the simple<br\/>queries, text representations, and retrieval models that characterize<br\/>interactive, ad-hoc search today. It may also lead to improved<br\/>interactive search by providing a medium in which search interfaces<br\/>can express constraints and preferences derived from sophisticated<br\/>user and task models. Research results are disseminated in technical<br\/>papers, and as part of the open-source Lemur Toolkit. The project Web<br\/>site (http:\/\/www.cs.cmu.edu\/~callan\/Projects\/IIS-0534345\/) provides<br\/>additional information.","title":"Search Engines Support for HLT Applications","awardID":"0534345","effectiveDate":"2006-04-15","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}}],"PIcoPI":["541869"],"PO":["427499"]},"119588":{"abstract":"We request support for a workshop on Edge Computing using New Commodity Architectures. The workshop will be held at UNC Chapel Hill in May 2006. It will bring together leading researchers and developers from computer architecture, computer graphics, compilers, database and data streaming, high performance computing and GPGPU. We also expect significant participation from industry and federal agencies. We request travel support for invited speakers and graduate students.","title":"Conference Support for Edge Computing Workshop","awardID":"0628359","effectiveDate":"2006-04-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["543535","559716"],"PO":["532791"]},"112471":{"abstract":"This project, developing research devices for levitation of surfaces, pursues research projects in control and human-computer interaction (HCI), additionally enabling the physical simulation of virtual reality. The infrastructure services research topics in multivariable nonlinear control, electromagnetic modeling and analysis, haptic interaction, and general physical HCI. Two testbeds will be built: A<br\/><br\/>-Haptic device with Lorentz force actuation and 25 mm and 45 degree motion ranges for a handle for human interaction, and<br\/>-Planar device for levitation of a flat platform by electromagnetic repulsion, capable of lifting a 100 kg human and with a 25 mm vertical range and arbitrarily large horizontal range.<br\/><br\/>Due to the increasingly large memories and high computational speeds currently available in embedded processor control systems, the researchers compensate variations in magnetic levitation devices over large ranges of motion with sophisticated multivariable nonlinear control methods. Applications include medical training simulations, sensitive force-reflecting teleoperation, vehicle simulations, physical rehabilitation, vibration isolation, and the study of human haptic perception and task performance.<br\/><br\/>Broader Impact: The use of magnetics, non-linear control and finite-element simulation provide a novel combination for the development of a new class of devices. The new haptic devices might support new kinds of haptic studies, inspire others, and enable a beginning researcher to set up a laboratory and train students in different areas.","title":"CRI: Infrastructure Acquisition: Magnetic Levitation Systems for Human Interaction","awardID":"0551515","effectiveDate":"2006-04-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[297673,"390888",297675],"PO":["565227"]},"117652":{"abstract":"ABSTRACT<br\/>0618957<br\/>Leon Osterweil<br\/><br\/>University of Massachusetts Amherst<br\/><br\/>This provides travel grants to U.S. students to attend the 2006 International Conference on Software Engineering (ICSE 2006) Doctoral Symposium, to be held in May 2006 in Shanghai, China. Selected students who are presenting at the Doctoral Symposium will have an opportunity to publicly discuss their research goals, methods, and results at an early stage in their research, and to receive feedback from an distinguished international panel of experts. Attending the ICSE event is an opportunity to start becoming part of the software engineering community and to have an international research experience.","title":"Student Travel Support for ICSE 2006 Doctoral Symposium","awardID":"0618957","effectiveDate":"2006-04-01","expirationDate":"2007-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["536737"],"PO":["564388"]},"110591":{"abstract":"This research explores new technologies and system architectures for supporting high-resolution distributed computer displays. This problem is fundamental to the future growth and development of the computer <br\/>industry and the fields of computer science and engineering. Our approach is to raise the semantic level of computer-display communication in an effort to increase the intelligence of computer displays. This innovation will reduce system bandwidth and, thus, provide greater scalability. By providing local intelligence in displays it will be possible to decouple the notion of a data display from the specific computer to which it is attached. This decoupling of displayed information from computing resources has broad implications that span all the way from personal hand-held devices to public large-format displays.<br\/><br\/>The investigators approach provides seamless access to any display from any device. It also supports multiple simultaneous connections to a single display. This is achieved via the efficient transport of display <br\/>primitives, and by using effective higher-level protocols. Developing higher-level semantics for display interfaces improves bandwidth utilization and, therefore, supports larger higher-resolution displays. <br\/>The combination of higher-level interfaces and local intelligence improves interaction with the ability to adapt the presentation of a content stream to the specific display platform. Intelligent displays <br\/>will tailor their presentations to low-resolution mobile devices as well as wall-sized boardroom displays. The research includes three initiatives: 1) To investigate and develop new intelligent display <br\/>architectures capable of overcoming the limitations of traditional frame-buffer architectures. 2) To develop higher-level and more bandwidth-efficient protocols for communicating data to and from visual <br\/>displays. 3) To develop new synchronization and context-switching capabilities necessary to virtualize displays so that they might be used as distributed resources.","title":"Tera-Pixels - Next Generation Display Architectures","awardID":"0541242","effectiveDate":"2006-04-15","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["371221",292988],"PO":["532791"]},"109404":{"abstract":"Social networks can be represented as time-evolving graphs where the nodes are the members\/entities of the social network and the edges represent connections\/relationships between the nodes. This project tries to answer questions such as: How does a \"normal\" social network look like? How will it evolve over time? How can we spot \"abnormal\" interactions (e.g., spam), in a time-evolving e-mail graph? The approach developed in this project is to look for time-evolution \"laws\", and to design fast, scalable data mining tools for real graphs with millions and billions of nodes. The approach consist of two efforts: (1) discovery of patterns that hold when graphs evolve over time and (2) tools to analyze, visualize and mine such graphs to discover anomalies. The resulting tools will have a broad applicability. They will be vital for mining and outlier detection in numerous settings, such as money-laundering rings, mis-configured routers on the Internet, suspicious user accesses to database records, surprising protein-protein interactions in a gene regulatory network, and many applications involving large-scale evolving social networks. The project Web site (http:\/\/www.cs.cmu.edu\/~christos\/PROJECTS\/GRAPH-MINING\/) provides additional information and will be used for results dissemination.","title":"Finding Patterns and Anomalies in Large Time-Evolving Graphs","awardID":"0534205","effectiveDate":"2006-04-15","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}}],"PIcoPI":["548220"],"PO":["563751"]},"112066":{"abstract":"This project, testing and measuring for the fabrication of a microgripper development facility, contributes in the fabrication by resolving some difficulties encountered which involve<br\/><br\/>-Cutting the Ionic Polymer Metal Composite (IPMC) as small as desired using the current mechanical methods; therefore, a laser cutting system,<br\/>-Characterizing the performance of IPMC; hence, system identification techniques that measure true characteristics; measuring the response to develop a pole\/zero model for the plant (microgripper actuator) and feedback (microgripper position sensor), and<br\/>-Using load cells for accurate performance measurements. <br\/><br\/>The project examines the mechanical and electrical characteristics of the micro-sized IPMC; thus, the microgripper laboratory contributes to nano\/micro manipulation. Although these characteristics have been studied in macro-scale, its micro-scale application has not yet been fully investigated. The infrastructure allows the development of the state-of-the-art microgripper system based on a new material patented at UNM and services projects involving blood cell property measurements, carbon nanotubes, fluid and flow visualization.<br\/><br\/>Broader Impact: The facility serves as a recruitment tool for graduate school in pertinent areas in this minority serving institution. These students in turn motivate underrepresented students to follow a science\/engineering career. Hence, the equipment creates an obvious link to engineering and may influence students to pursue careers in the field.","title":"CRI: Microgripper Laboratory Development","awardID":"0549563","effectiveDate":"2006-04-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["449805"],"PO":["557609"]},"112496":{"abstract":"This project, developing a Unified Linguistic Annotation (ULA) that integrates in one framework different layers of annotation (e.g., semantics, discourse, temporal, opinions), provides a large word corpus with balanced and annotated data. The effort involves the integration of several existing resources, including PropBank, NomBank, TimeBank, Penn Discourse Treebank, and coreference and opinion annotations. The work addresses the lack of progress in automatically producing semantic representations, a current major obstacle for natural language processing. The project aims at<br\/><br\/>-Achieving an international consensus on a meta-specification framework allowing individual annotations to cohabit with one another (consistency) and specification components from different schemas to refer to merged information (integration);<br\/>-Improving techniques for producing high performing systems for the reliable types of semantic annotation;<br\/>-Producing a stable and language-independent methodology for the process of unified linguistic annotation, complete with widely accessible and broadly applicable tools and guidelines;<br\/>-Validating (with workshops) generality and robustness of ULA by incorporating additional annotation schemas, new genres, and additional languages; and<br\/>-Actively promoting dissemination of the techniques embodied in the ULA, as well as the resulting annotated corpora throughout the community, for use and further evaluation.<br\/><br\/>The incorporation of automatic taggers into natural language processing (NLP) systems is expected to improve performance in question answering, information extraction, and machine translation, among others, by moving NLP to a new level of richer, deeper processing. The annotated data provides training material for automatic taggers and a wealth of data for further corpus linguistic studies. This project should bring us one step closer to understanding the nature of meaning.<br\/><br\/>Broader Impact: The project offers the opportunity to produce an abstract representation of a vast amount of text, and therefore a vast amount of knowledge, which may be key to turning knowledge representation in general into a more tractable problem. The activity enhances infrastructure for research and education by providing a resource that could lead to major advances in robust, broad coverage semantic processing. The workshops provide tremendous learning opportunities for the students.","title":"CRI - Towards a Comprehensive Linguistic Annotation of Language","awardID":"0551615","effectiveDate":"2006-04-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"7731","name":"OTHER GLOBAL LEARNING & TRNING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["511542","562587","409250","311362","429881"],"PO":["565215"]},"105852":{"abstract":"The goal of this research project is to develop methods for uncovering information hidden in large linked real-world networks. The approach is based on spectral analysis techniques that were previously developed for sparse graphs, and are extended to work on complex networks in order to defuse the \"majority rule\" where the stronger areas of the network structure dominate the overall structure. The outcome is that the network analysis will not be dominated by the stronger nodes obscuring weaker nodes that are structurally significant in the network. The project also develops methods for tracking changes in such graphs in order to discover new clusters, e.g., novel research communities. Clusters that occur in most of these networks are the natural communities, and provide a basic unit of analysis for tracking over time. Finally, the network analysis and modeling methods are extended to heterogeneous graphs that include multiple link types. The work combines the development of formal models with the analysis of empirical data. The research project provides an excellent educational platform, introducing undergraduate and graduate students to statistical techniques and algorithmic methods. The theoretical results will find applications in a broad range of areas, including Web searching, citation analysis, data mining and information discovery, social networks analysis, or power grid management. The project's Web site (http:\/\/www.cs.cornell.edu\/jeh\/linked.htm) will provide access to the project's results and resulting software.","title":"The Analysis and Modeling of Large Linked Networks","awardID":"0514429","effectiveDate":"2006-04-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7496","name":"COLLABORATIVE SYSTEMS"}}],"PIcoPI":["382681","560669"],"PO":["563751"]},"118942":{"abstract":"This NSF award supports two linked workshops, held in Washington, DC, March 14-15 and March 16-17, 2006. The first workshop, entitled. Beyond SCADA: Networked Embedded Control Systems initiates an effort to identify key technologies that must be developed for not only reducing the vulnerabilities of existing SCADA systems but also planning the next generation of distributed network embedded control systems for our National infrastructures. This workshop is planned with the guidance and support of the interagency High Confidence Software and Systems Coordinating Group. <br\/><br\/>The context for this workshop is the ubiquitous use of information and communication technologies that has pervaded other infrastructures, rendering them more intelligent, increasingly interconnected, complex, interdependent, and therefore more vulnerable. They are global and geographically distributed beyond any jurisdictional or governmental boundary. Todays critical national and large-scale industrial systems depend on ICT characteristic of an aging infrastructure. They exhibit rudimentary control and coordination automation, are poorly secured, and operations often are driven to hazardous safety and security practice due to the cost of adoption for needed new (and vulnerable) technologies such as wireless networking.<br\/><br\/>The United States and Europe share common concerns for renewing and protecting large infrastructures such as power grids, transportation systems, telecommunication infrastructure, health-care systems, and safety-critical manufacturing systems (e.g., chemical manufacturing). Following the US HCSS meeting is the second workshop, a joint US-EU workshop for collaborative research on similar themes, titled Large ICT-based Infrastructures and Interdependencies: Control, Safety, Security and Dependa","title":"Beyond SCADA: Network Embedded Control Systems","awardID":"0624431","effectiveDate":"2006-04-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"T366","name":"NSA-SOFTWARE VERIFICATION & HI"}}],"PIcoPI":["526900"],"PO":["561889"]}}