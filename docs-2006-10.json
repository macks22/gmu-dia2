{"119427":{"abstract":"Sushil Jadodia<br\/>George Mason University<br\/>0627493<br\/>Panel: P060969<br\/>Collaborative Research: Intrusion Detection Techniques for Voice Over IP<br\/><br\/>Recently Voice over IP (VoIP) is experiencing a phenomenal growth.<br\/>Being a time sensitive service, VoIP is even more susceptible to<br\/>malicious attacks than regular Internet services. Moreover, VoIP uses<br\/>multiple protocols for call control and data delivery, making it<br\/>vulnerable to various attacks at different protocol layers. The<br\/>already-known security solutions for data networks fall short of<br\/>defending VoIP applications because of the differences of cross protocol<br\/>interactions and the way in which the different handshakes effect the<br\/>distributed service elements and consequently the end user quality of<br\/>service (QoS) of VoIP. This project seeks to develop a series of<br\/>intrusion detection techniques suitable for VoIP and experimentally<br\/>verify their commercial viability. Overall objective is to detect known<br\/>and unknown attacks in an accurate and timely manner, without incurring<br\/>a noticeable delay in call setup times.<br\/><br\/>The research plan consists of two major components:<br\/>Protocol-state-machine (PSM) based mechanism for detecting known<br\/>attacks, and Hellinger-distance based (HD) mechanism for detecting<br\/>unknown attacks. The incorporation of the communication between<br\/>protocol state machines is particularly suited for intrusion detection<br\/>in VoIP, because call control and media delivery protocols are<br\/>synchronized by exchanging synchronization messages for critical events<br\/>throughout the established sessions. The core of HD detection scheme is<br\/>based on using the Hellinger distance to measure the deviation from<br\/>normal network protocol behaviors. To have the detection mechanism<br\/>insensitive to site and traffic pattern, a dynamic self-regulating<br\/>threshold is used, thus making the detection mechanism robust, widely<br\/>applicable, and easier to deploy. Research results of the project will<br\/>be broadly disseminated through publication, web pages, and technology<br\/>transfer to industry.","title":"CT-ISG: Collaborative Research: Intrusion Detection Techniques for Voice over IP","awardID":"0627493","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["540637",316739],"PO":["529429"]},"123871":{"abstract":"CT-ISG: The VoComp University Voting Systems Competition<br\/><br\/>This project will organize and hold a university competition, called VoComp, for students to design, implement, demonstrate, and evaluate voting systems.<br\/><br\/>Motivation. The world needs better voting systems. Competitions stimulate innovation and involvement. Currently, there is no student voting competition.<br\/><br\/>Proposed Research. The student voting competition will be run in four phases. In Phase 0 (October), student teams will register and submit proposals, and judges will be announced. In Phase 1 (January), each team will post an initial voting system design. In Phase 2 (by May), each team will conduct a realistic election on a university campus either a real or mock student election. In Phase 3 (June), any team completing Phase 2 will be invited to travel to the finals where each team will participate in a mock election, make an academic-style presentations about their system, and optionally give critical evaluations of other systems. A variety of prizes will be awarded.<br\/><br\/>Intellectual Merit. Students will design, implement, and demonstrate voting systems that provide election integrity, voter privacy, transparency, accuracy of vote capture, and user friendliness, and accessibility to the disabled. Students will present their designs, and optionally, critique competing ones and propose practical metrics for evaluating voting system performance.<br\/><br\/>Broader Impact. Vocomp will help advance voting technology, involve students in information security and voting system research, foster interactions among students and researchers, support courses on voting technology, and increase participation and awareness in the crucial civic process of voting. All designs, source code, and educational materials will be publicly shared.","title":"SGER: CT-ISG: The VoComp University Voting Competition","awardID":"0650498","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["560875"],"PO":["521752"]},"123893":{"abstract":"The proposed research is to develop an integrated micro-electromechanical systems<br\/>(MEMS)-based miniature microphone system for directional sensing of sound based on a<br\/>biological model. The design follows that used by the Ormia ochracea fly. This fly employs a<br\/>unique coupled mechanical bar system operating at the critical damping to extract the direction<br\/>of sound. The Ormia ochracea fly has ears separated by only about 500um yet has remarkable<br\/>sensitivity to direction of sound. The separation of the fly's ears is too small for it to sense the<br\/>direction of a sound source if it uses traditional human techniques such as the difference in<br\/>arrival times or the amplitudes of sound at each ear. The fly's unique ear structure helps to<br\/>magnify the difference in amplitude as well as phase of the sound wave.<br\/>The proposed MEMS device uses two flapping wings which are textured to achieve critical<br\/>damping to mimic the fly's ear system. An initial finite element simulation shows the proposed<br\/>design replicates the directional sound response of the fly's biological system. The<br\/>displacement of the membrane due to typical sound pressure is on the scale of nanometers and<br\/>requires a highly sensitive position measurement approach to determine the amplitude. Such a<br\/>position sensitivity can be achieved by optical means using two diffraction gratings at the edges<br\/>of the wings. A diffractive interferometer using fixed and moving gratings can provide subnanometer<br\/>displacement sensitivity. By placing the gratings at the outer edges of the<br\/>membrane, a maximum displacement can be achieved. This translates into angular resolution<br\/>of the direction of sound to about 1-2 degrees. The proposed tasks include the design,<br\/>fabrication and characterization of the sensor performance.<br\/>Intellectual merit: The proposed research provides an opportunity to combine the advances in<br\/>MEMS technology with unique functionalities of biological systems to develop a novel sound<br\/>sensor. The findings of the research will enable us to understand how the fly's hearing system<br\/>achieves such a remarkable directional sensitivity and to fabricate an electromechanical<br\/>equivalents. A set of such sensors can be used for pinpointing explosions by monitoring the<br\/>direction of sound which can be deployed using micro air vehicles. In addition, a network of<br\/>these sensors can be used for unattended movement monitoring. Other applications of such<br\/>sensors include use in low noise hearing aids. The principal investigators and research staff<br\/>have extensive experience in MEMS design, fabrication and testing. The Naval Postgraduate<br\/>School has state-of-the-art MEMS design facilities as well as an anechoic chamber with<br\/>experienced staff for acoustic testing.<br\/>Broader impact: This research will accomplish two goals. First, it will significantly expand<br\/>current knowledge on the inner workings of the Ormia ochracea fly's unique hearing system and<br\/>on the design and fabrication of electromechanical equivalent for applications involving<br\/>pinpointing sources of sound. Second, the graduate students participating in this investigation<br\/>will gain valuable knowledge and experience in this multidisciplinary area of research. The<br\/>Naval Postgraduate School also provides research opportunities for promising local high school<br\/>students. Current plans are to include high school students in the research, where possible, in<br\/>order to enhance their skills in mathematics and science and to encourage them to pursue<br\/>mathematics, science, and engineering in higher education. The findings of the research will be<br\/>published in scholarly journals and conferences.","title":"SGER: MEMS-Based Miniature Microphone for Directional Sound Sensing","awardID":"0650585","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T489","name":"DIA-MASINT CONSORTIUM PROJECT"}}],"PIcoPI":[328783,328784],"PO":["565136"]},"125510":{"abstract":"ABSTRACT<br\/>NSF-0534276<br\/><br\/>Pottenger, William<br\/><br\/>The burgeoning amount of textual data in distributed sources combined with the obstacles involved in creating and maintaining central repositories motivates the need for effective distributed information extraction and mining techniques. Different kinds of records on a given individual may exist in different databases - a type of data fragmentation. Even with standards, however, the ability to integrate schemas automatically is an open research issue. A related issue is the fact that current Association Rule Mining (ARM) algorithms for mining distributed data are capable of mining data (whether vertically or horizontally fragmented) only when the global schema across all databases is known. In the case of information extracted from distributed textual data, no preexisting global schema is available. This is due to the fact that the entities extracted vary between documents - new input text can contain previously unseen entities. As a result, a fixed global schema cannot be assumed and existing algorithms cannot be employed.<br\/><br\/>This effort describes a distributed higher-order text mining framework that requires neither the knowledge of the global schema nor schema integration as a precursor to mining rules. The framework, termed D-HOTM, extracts entities and discovers rules based on higher-order associations between entities in records linked by a common key. The entity extraction is based on information extraction rules learned using a semi-supervised active learning algorithm previously developed. The rules learned are applied to automatically extract entities from textual data that describe, for example, criminal modus operandi. The entities extracted are stored in local relational databases, which are mined using the D-HOTM distributed association rule mining algorithm.<br\/><br\/>The broader impacts of thework lie in the collaboration with local law enforcement and healthcare providers for deploying live test beds that enable problem solving by mining reports and identificaiton of physician best practices. Pre-college internships are provided for students as well as support for graduate students.","title":"Collaborative Knowledge Discovery in Digital Government Data Using Distributed Higher-Order Text Mining","awardID":"0703698","effectiveDate":"2006-10-01","expirationDate":"2009-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["451739"],"PO":["563751"]},"123156":{"abstract":"The Internet is a critical infrastructure underpinning international social and economic<br\/>development. In considering a new infrastructure such as GENI, the Global Environment for Network Investigations, reflection and dialogue are vital on the Internet as a driver of innovation leading to economic growth and social well-being, on the security and privacy functions that can be embedded into the design of future networks to factor in privacy and personal choice considerations, as well as on the evolution process of networking infrastructures. The joint NSF\/OECD Workshop \"Social and Economic Factors Shaping the Future of the Internet\" will bring together researchers, policy-makers, leading academics, private sector organizations, and civil society representatives to discuss the Internet's role as a critical infrastructure underpinning international economic prosperity, scientific innovations and social activity. More specifically, workshop participants will seek to draw lessons from the applications and usages associated with the evolution of the current Internet, which is evolving into a broadband network of networks with increasingly wireless access, to identify both the features that have been critical to the Internet's success, as well as where and why the Internet community has not been able to solve some of the issues the Internet has encountered in a scalable way. A guiding theme for the workshop will be the necessary virtuous dynamics of economics, responsibility and trust that help to ensure a balance between economic, social and technological tensions, as well as the process of transferring results from research experimentation into a real-world scenario.","title":"Joint NSF\/OECD Workshop: Social and Economic Factors Shaping the Future of the Internet, January 29-31, 2007","awardID":"0647166","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"024F","name":"GENI CONCEPT\/DEVELOPMENT"}}],"PIcoPI":["499471"],"PO":["565136"]},"120781":{"abstract":"Lead proposal<br\/>NUMBER: 0635008<br\/>INSTITUTION: Ohio State University Research Foundation<br\/>PRINCIPAL INVESTIGATORS: Dey, Tamal K. & Wenger, R. S.<br\/>Collaborative Proposal<br\/>NUMBER: 0635366<br\/>INSTITUTION: University of Illinois<br\/>PRINCIPAL INVESTIGATORS: Ramos, Edgar<br\/><br\/>PROJECT TITLE: Collaborative Research: Non-smoothness in Meshing and Reconstruction<br\/><br\/>The problems of meshing and reconstruction are pervasive in science and engineering where geometric domains need to be digitally represented, analyzed, inspected, or prototyped. Although the input forms to these two problems differ, both have a similar goal in producing a triangular representation of a geometry. Considerable theoretical advances have been recently made in designing and implementing provable algorithms for both. However, these algorithms assume some form of smoothness of the input domain. As a result, current solutions are not broad enough to handle many of the geometric domains which arise in scientific studies and engineering applications. Designing machine parts in automotive industry, creating virtual environments with buildings, simulating cracks and shocks in scientific studies are a few examples with non-smoothness as a basic impediment. This project studies the difficulty of non-smoothness in meshing and reconstruction by designing sound algorithms and by developing robust software based on these algorithms.<br\/><br\/>Meshing produces a triangulation of an explicitly specified geometric domain whereas reconstruction does the same with a point sample. Most of the provable reconstruction algorithms exploit the differential structure of a smooth surface whereas meshing algorithms, though allowing polyhedral domains, restrict the input angles not to be small. The result of these restrictions is that non-smooth domains such as piecewise smooth surfaces and non-manifolds cannot be handled with full generality. The goal of this project is to broaden the class of input geometry for which models can be computed with assurance of accuracy. The research in this project uses concepts from various mathematical disciplines such as differential geometry, differential topology, and non-smooth analysis and also tools from areas of theoretical computer science such as computational geometry, computational topology, and numerical optimization. Graduate students supported by the project develop skills in theoretical computer science, most notably in computational geometry and topology and also in writing robust, efficient and user-friendly software.","title":"Collaborative Research: Non-smoothness in Meshing and Reconstruction","awardID":"0635008","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[320924,"549998"],"PO":["565157"]},"120792":{"abstract":"Planar graphs are graphs that can be drawn on the plane with no crossings. Such graphs naturally arise in application areas such as image processing, logistics in road maps, and VLSI. In addressing fundamental optimization problems on graphs, if the input graphs can assumed to be planar, algorithms can be used that are faster and\/or more accurate than algorithms that do not require this assumption.<br\/>This research involves the design and analysis of such planarity-exploiting algorithms for fundamental optimization problems. Advances in this area could lead to improvements in methods for solving problems in road maps such as routing, network design, and facility location, and for solving problems in image processing such as segmentation and tracking.<br\/><br\/>The research consists of two thrusts. The first aims to discover polynomial-time approximation schemes for NP-hard graph problems such as Steiner tree, multiterminal cut, metric labeling, and uncapacitated<br\/>facility location. The approach is to find edge deletions and contractions that approximately preserve the objective while reducing the tree-width of the graph to a small number, then solving the problem optimally in the low-tree-width graph, then lifting the solution to the original graph. The second thrust aims to discover simple and efficient algorithms for some fundamental polynomial-time problems such as flow problems. The approach involves specifying a pivot rule for which the number of iterations of network simplex is small. The algorithm \"sweeps\" through the graph, analogous to the way a sweep-line algorithm solves a problem in the plane.","title":"Exploiting Planarity in Optimization Algorithms","awardID":"0635089","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["438410"],"PO":["562944"]},"120771":{"abstract":"This project intends to focus on several aspects of this development: self-improving algorithms; online data reconstruction; sublinear algorithms; dimension reduction; low en-tropy data structures, nonuniformly priced computation; and data analysis. The scope of the proposed research is intentionally wide-ranging. Indeed, it is the PI's belief, backed by his preliminary investigations, that progress on these topics will rely on the emergence of common threads and broadly applicable methods.<br\/>The intellectual merit of this proposal is to lay down the foundations for a systematic study of data-powered algorithms. The eort will involve theoretical investigations coupled with extensive computer experimentation. The principal beneciaries of data-powered algorithms come from engineering and the natural sciences, and this is where the broader impact of this proposal will be sought; specically, via the PI's ongoing research collaborations with biologists and physicists and, on the educational front, his joint effort with them to develop a year-long algorithms-based course for quantitative scientists.","title":"Data-Powered Algorithms","awardID":"0634958","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["450650"],"PO":["562944"]},"119319":{"abstract":"In upcoming years, continual advances in chip design, power consumption, and battery technology will lead to wide deployment of software defined radios (SDRs). SDRs take advantage of programmable hardware modules to dynamically modify the functionality of various radio subsystems. This flexibility allows them to find and adaptively switch between several network protocols. A hybrid wireless network is constructed when SDRs form end-to-end multihop communication links that traverse multiple network standards. The key task for these nodes is to efficiently utilize available network interfaces at each hop to move data from source to destination. This research studies and develops algorithms to help SDRs find, identify, and select appropriate interfaces over which to route diverse traffic streams while efficiently allocating radio resources. The goal is to develop foundational results in these areas so that a broader investigation of hybrid wireless networks can then be pursued.<br\/><br\/>This research will have impact on both commercial and military service providers who wish to extend network capacity and coverage by interconnecting heterogeneous, overlaid wireless systems. The PIs aim to demonstrate broader impact of hybrid wireless networks by offering short workshops that educate rural Pennsylvania communities on its benefits and by designing such a network for Susquehanna County, a rural community in Northern Pennsylvania. Research results will be disseminated and incorporated into wireless communications and networking courses at the respective institutions.","title":"Collaborative Research: NeTS-ProWIN: Multi-Tier Hybrid Wireless Networks","awardID":"0626905","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["550356"],"PO":["434241"]},"116569":{"abstract":"Understanding an adaptive and learning computing system such as the brain requires an understanding of the computations performed by its basic components individual neurons. At the single neuron level, the dendritic tree provides the means for interpreting spatiotemporal activity patterns of synaptic input. However, no comprehensive framework exists for translating dendritic features into a functional architecture that can be related directly to behavior. Our goal is to develop rules for how dendritic structure and synapse distributions follow functional architecture principles that relate directly to a neuron's individual behavioral requirements. In holometabolous insects, like the moth Manduca sexta, the structure, physiology and function of individually identifiable neurons are modified in parallel during metamorphosis, allowing for studies that directly relate structure and behavioral function. This remodeling has been described in particular detail for the motoneuron MN5, which transforms from a tonically firing larval crawling motoneuron into a phasically firing adult flight motoneuron. In this collaborative research effort, Dr. Sharon Crook and Dr. Carsten Duch will combine experimental and computational approaches to investigate the function of the structural and synaptic remodeling of MN5 for its changing behavioral role. We will also test Cajal's Neuron Doctrine, which envisions the neuron as the smallest functional entity, by examining the computational and functional independence of dendritic sub-domains. Another important aspect of our modeling studies is that we will investigate the roles of excitatory\/inhibitory input synapse ratios and distributions in single neuron computation. Excitatory\/inhibitory imbalances are known to contribute to several neurological diseases. The use of Manduca provides a unique model system where one neuron has completely different geometries, each of which is associated with dramatically different firing outputs. The results will contribute significantly to our understanding of single neuron computation, which is an important step towards understanding neural networks.","title":"Behaviorally Relevant Neuronal Modification during Postembryonic Development","awardID":"0613404","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7334","name":"MATHEMATICAL BIOLOGY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}}],"PIcoPI":["407679"],"PO":["564318"]},"127009":{"abstract":"The theoretical and experimental research efforts in this project focus on haptics interaction and visual servoing of deformable objects through shared control. In particular, novel haptics modeling of interaction with deformable objects will be explored, using overlaying local nonlinear modeling of elasto-dynamic properties of objects; new visual servoing algorithms will be formulated for tracking deformable objects using position and image based control techniques; new robot control algorithms will be developed for safe interaction with a pacient's organs using haptics and vision. The project's education component focuses on developing new courses, involving students in course development, and involving undergraduate students in research. The PI will also participate in education of the under-represented high school students.","title":"CAREER: Minimally Invasive Surgery Using Haptics and Vision","awardID":"0711038","effectiveDate":"2006-10-01","expirationDate":"2008-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["379044"],"PO":["403839"]},"120761":{"abstract":"Abstract<br\/>--------<br\/><br\/>Problems in many areas where the input data is a matrix have been tackled by Spectral Methods which use low-rank approximations to the input matrix. In the important application area of Clustering, only known proofs that the methods succeed have to make the unrealistic assumption that all entries of the matrix are<br\/>statistically independent. This research removes this important impediment to the wider use of the method by developing the theory and algorithms that work under limited independence, a much more realistic assumption. To illustrate, in one example - document-term matrices - this research assumes only that the<br\/>documents are statistically independent, not the terms in each document.<br\/><br\/>The research also generalize the methods developed for limited independence to deal with matrix-valued random variables which are of interest in several areas and to date have no substantial work on them. The PI and supported students will extend the applications of spectral-like methods to tensors - multi-dimensional arrays. It is expected that the research here will be one of the key bridges between theory and practice in this<br\/>area, thus leading to a broad transfer of theoretical developments into practice in time.","title":"Spectral Methods: Algorithms and Applications","awardID":"0634904","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[320878,"483530"],"PO":["550329"]},"120783":{"abstract":"Due to technology scaling and the increased susceptibility of ultra deep submicron (UDSM) circuitry to uncertainties originating from noise (reduced noise margins) and soft errors (induced by atmospheric neutrons), it will become necessary to design error detection and correction capability into future logic designs for reliable computation. In the past, data encoding techniques have mostly been used for error detection and correction in wired and wireless communications channels. In order to enable reliable computing in nanoscale technologies of the future, not only data but also computation performed on the data will need to be encoded for real-time error detection and correction (i.e. redundant computations will need to be performed). It is expected that error rates of combinational logic in scaled technologies will escalate by 9 orders of magnitude from 1992 to 2011, when it will equal the error rate of \"unprotected\" memory elements. Currently, coding techniques are used to design reliable memory banks and enable reliable memory access, but their use in on-chip signal processing has been limited. One of the key barriers to widespread use of coding techniques for reliable on-chip computing is the cost of data and circuit redundancy necessary to implement a coding technique with logic error detection and correction capabilities for single and multiple bit-errors. While error detection is accomplished relatively easily across the majority of known algorithm-based and communication systems coding techniques, error correction is a harder problem and can require significant computation for exact error correction. This renders real-time correction without loss of significant throughput difficult, if not impossible, to achieve and is especially true for the majority of DSP applications that involve matrix-vector multiplications and are the core subject of this proposal. In this context, it is important to point out that in future scaled technologies with high error rates, rapid error correction with least impact on throughput will be a critical technology enabling factor. Without this capability, technology scaling itself may grind to a halt due to gross loss of circuit and system level performance.<br\/>This research focuses primarily on coding and probabilistic correction techniques for on-chip linear and non-linear digital signal processing computations that allow near-exact correction to be performed with minimal impact on circuit performance and power consumption in systems composed of unreliable components that generate intermittent errors on their output lines at much higher error rates than can be handled by exact error correction techniques without significant loss of performance The increased error rate is assumed to be driven by very aggressive technology scaling issues. The errors are due to reduced noise margins in UDSM circuitry, power\/ground bounce and radiation-induced effects or due to permanent failures (stuck-at-0\/stuck-at-1) on internal signal lines that are excited intermittently by real-time stimulus. In addition, the proposed techniques can be applied to special classes of analog circuits as well (filters, amplifiers, etc). . It is expected that the proposed research will open up new avenues for addressing the challenges of performing reliable computation with unreliable hardware (high error rates), a problem that is expected to dominate the field of reliable computing in the coming decade.","title":"Efficient Probabilistic Correction of Noise-Induced Errors in Encoded Signal Processing Algorithms","awardID":"0635016","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["550643"],"PO":["564898"]},"110630":{"abstract":"Prop ID: CCF-0541444 <br\/>PI: Somenzi, Fabio <br\/>Institution: University of Colorado at Boulder <br\/>Title: Decision Procedures for Large Scale Model Checking <br\/> <br\/> <br\/>ABSTRACT<br\/>The major extant hurdle to a more widespread use of formal methods like model checking in systems design is the limited capacity of the algorithms. While most key problems in formal verification are theoretically intractable, the last decade has witnessed algorithmic improvements that have greatly extended the realm of what can be done both rigorously and automatically. Continued increase in capabilities is a priority for most large electronic design organizations.<br\/><br\/>Recent advances in formal verification technology have been particularly remarkable in abstraction refinement and in the procedures based on propositional satisfiability. These advances have been made possible by clever uses of decision procedures and even by the cooperation of different approaches. However, little has been done in leveraging the strengths of different approaches via a deeper integration. Current techniques often fail on problems that require a combination of strengths from different approaches, rather than the choice of a suitable approach from a toolbox. The aim of this proposal is to pursue such integration and to explore the benefits that come from realizing that the separation of abstraction-based model checking and decision procedures is an artificial one. The anticipated result is an effective strategy for large scale model checking that represents a significant leap in capacity.<br\/><br\/>The strength and, at the same time, weakness of satisfiability (SAT) solvers is their ability to forget. While fixpoint computations accumulate sets of states whose representations often become unwieldy, a SAT solver can, in principle, avoid saving any information about the search except for the decision stack. The price of forgetfulness is repetition, and even though modern SAT procedures record conflict clauses, one encounters problems where such procedures flounder because of their inability to represent the relevant information about subproblems already solved. The proposed research will address this issue in the context of abstraction-based model checking. A significant fallout for other applications of satisfiability and for the general problem of combinatorial search is also expected.","title":"Decision Procedures for Large Scale Model Checking","awardID":"0541444","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["518587"],"PO":["562984"]},"120794":{"abstract":"Geometric retrieval and range searching are fundamental computational problems. A large set of points is given in multi-dimensional space, and the problem is to preprocess these points so that it is possible to count or report the number of points lying inside a given query range. This problem has wide ranging applications in science in areas such as knowledge discovery, pattern recognition, and data compression. Although this is well studied, its computational complexity is quite high. Preliminary work by the investigator and his collaborators has shown that approximation can result in considerably faster running times, but there are still many issues that remain to be understood. The goal of this research is to systematically study the computational complexity of approximate range searching.<br\/><br\/>This research will study how the computational complexity of range searching depends on various elements of the problem's formulation, including geometric characteristics of the range space (such as smoothness and sharpness) and properties of the semigroup (such as integrality and idempotence). The investigators will study range searching through the development of efficient algorithms and data structures, derivation of lower bounds, and exploration of space-time trade-offs. The intellectual merit of this research his research stems from a deepening understanding of the computational complexity of exact and approximate range searching and the discovery of new, more efficient computational solutions. The broader impacts include the production of new efficient software systems for approximate range searching, which will serve to advance the state of the art in applications areas, and the production of instructional materials on range searching, which will be made available over the web.","title":"Approximation Algorithms for Geometric Retrieval","awardID":"0635099","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["486206"],"PO":["565157"]},"118968":{"abstract":"The Third International Conference on Broadband Communications, Networks, and Systems (BroadNets 2006), will be held in San Jose, California from October 1-5, 2006. BroadNets 2006 is an international conference focusing on broadband communications, networks, and systems and covers the entire gamut of next generation networks, communications systems, applications and services. The conference is co-sponsored by IEEE Communications Society (www.comsoc.org) and Create-Net (www.create-net.it), a research institute based in Italy. The conference consists of three symposia, the Optical Communications, Networks and Systems Symposium, the Wireless Communications, Networks and Systems Symposium, and the General Symposium as well as several workshops. This award assists up to twenty (20) United States-based graduate students to attend the conference. Participation by graduate students in conferences such as BroadNets provides valuable experience for the students, enabling them to interact with senior researchers, as well as exposing them to a broader range of research topics. <br\/><br\/>The intellectual merit of the proposed activity consists of the exchange of information between the supported graduate students and other researchers attending BroadNets. Opportunities for the intellectual exchange of information will be provided in technical sessions, panels, tutorials, workshops, and keynote talks. By attending such events, students will be able to gain valuable insight into various topics and the state-of-the-art in the field of broadband networking from both an academic as well as industry perspective. BroadNets conference with three symposia and several workshops covering emerging trends in broadband networking, is expected to generate a significant amount of interest and intellectual discussion. <br\/><br\/>The broader impact of the proposed activity consists primarily of the valuable training and learning experience provided to the supported students. By presenting their work in technical sessions, students will gain valuable experience in educating others about their own research. The students will also benefit from their interaction with peers and leading researchers from other institutions, and they will be able to develop their own professional networks. Such experiences will enable the graduate students to prepare for their own careers in research and education.","title":"Student Travel Support for BroadNets 2006 Conference","awardID":"0624574","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["564860"],"PO":["565090"]},"120773":{"abstract":"In the proposed research, highly reliable memories made of unreliable components will be developed<br\/>and characterized in terms of their complexity and ability to retain the stored information. The main challenge is that in nano-scale systems both the storage elements and logic gates are faulty. It is in contrast to the state-ofthe- art systems where only the memory elements are considered unreliable while error correction encoders and decoders are assumed to be made of reliable logic gates.<br\/>The set of problems that will be addressed in this research can be condensed into the following question:<br\/>given n memory cells and m universal logic gates which fail following a known random mechanism, what is the optimal memory architecture which stores the maximum number of information bits for the longest period of time with arbitrary low probability of error? This complex problem can be divided and reformulated in many ways, but, interestingly, even some of the most fundamental questions related to this problem are still unanswered. The most important question is related to the following two fundamentally different approaches in fault-tolerant memories: (i) To improve reliability, the logic gate resources may be invested into a von Neumann multiplexing scheme. In this way, one can build highly redundant reliable networks that simulate the function of universal logic gates, and then use such better gates to build an error correction encoder and decoder. (ii)<br\/>Alternatively, the logic gate resources may be invested into building a more powerful error correcting code (i.e.,<br\/>decoder) capable of handling both memory elements as well as logic gates errors. Which of these two<br\/>approaches is optimal for a given failure mechanism? On a broad scale, is it better to deal with a reliability issue on a device or on a system level?<br\/>Intellectual Merit:<br\/>The unique feature of the nano-systems that both the storage elements and logic gates are unreliable makes the problem of ensuring fault-tolerance theoretically very important, because the process of error correction is not error-free as assumed in classical information theory. Making error correcting codes stronger and transmitters and receivers more complex will not necessarily improve the performance of a system. It is likely that for a given failure mechanism, there is a trade off between receiver complexity and its performance.<br\/>Our approach to developing fault-tolerant memory architectures is based on a method developed by<br\/>Taylor and refined by Kuznetsov. Taylor and Kuznetsov (TK) showed that memory systems have nonzero<br\/>computational (storage) capacity, i.e. the redundancy necessary to ensure reliability grows asymptotically<br\/>linearly with the memory size. Two fundamental open problems that will be addressed in this research are<br\/>determining storage capacity of nano-scale memories and the development of capacity approaching fault-tolerant architectures. The equivalence of the restoration phase in the TK method and faulty Gallager-B algorithm (as explained in Project Description), will enable us to tackle these and other important problems in reliable storage on unreliable media using the large body of knowledge in codes on graphs and iterative decoding gained in the past decade.<br\/>Broader Impact:<br\/>This program will contribute significantly to the evolution of data storage technologies and the information<br\/>infrastructure in the United States of America and abroad. Another important aspect is the establishment of a<br\/>tight interdisciplinary integration of knowledge in coding and signal processing, and nano-scale devices and<br\/>subsystems into programs benefiting undergraduate and graduate students at the University of Arizona and the<br\/>industrial technical research community.","title":"Error Correction Systems for Nano-Scale Fault-Tolerant Memories","awardID":"0634969","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["548044"],"PO":["432103"]},"120784":{"abstract":"This project is directed toward developing specialized logics for certain applications in computer science. The objective is to develop more efficient and effective modes of reasoning in particular instances by exploiting the special mathematical structure of the application.<br\/>One would like to provide natural logical rules that match the problem at hand, thereby streamlining the reasoning process and making it more amenable to automation.<br\/>Within this general context, five specific research projects are proposed, involving (1) Kleene algebra and Kleene algebra with tests; (2) binary relation, trace, and language models; (3) stochastic processes, probabilistic programs, and dynamical systems; (4) basic category theory; and (5) monads and monad composition.<br\/>In each of these projects, new modes of reasoning or logical principles can be developed that take advantage of the particular mathematics of the problem to enhance the reasoning process. In some cases, automated tools based on these principles can be developed. For example, in (3), existing techniques for stochastic processes typically require heavy use of mathematical analysis. However, for certain arguments, one can give algebraic or logical proof principles that encapsulate the necessary analysis and allow reasoning to take place<br\/>at a more abstract algebraic level. In (4), a new deductive system for arguments in basic category theory is proposed that will allow much of the process to be automated. Finally, in (5), one can give a new high-level approach to reasoning about monads and other categorical structures based on string rewriting.<br\/>2 Broader Impacts Resulting from the Proposed Activity<br\/>The new intellectual tools proposed here can help shortcut complicated arguments and significantly increase reliability of complex systems. As the complexity of systems that we deal with increases from day to day, such tools are sorely needed. For example, monads have recently become very popular in functional and logic programming. They have been shown to provide a means to combine modules or extend functionality of programming languages or data structures with new features such as continuations, state, and concurrency. Monad composition proofs are notoriously involved, however. With the new techniques proposed<br\/>in (5), researchers in those areas will find it easier to reason about monads and monad composition.<br\/>The project will fully integrate research and education and provide significant opportunities<br\/>for both undergraduate and graduate students for involvement in research and independent<br\/>study.<br\/>The project will enhance the infrastructure for research and education through the KATML system and software to be developed for basic category-theoretic reasoning. Both of these systems will promote intellectual rigor and provide students with a tool for understanding formal systems through experimentation. The KAT-ML system has already been used successfully in the undergraduate course on automata and computability at Cornell for working with regular expressions, and has proved to be quite popular.","title":"Specialized Logics for Applications in Computer Science","awardID":"0635028","effectiveDate":"2006-10-15","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[320931],"PO":["565157"]},"123743":{"abstract":"This award supports a workshop to identify research directions relevant to the Division of Information and Intelligent Systems (IIS) and NSF's Global Environment for Networking Innovations (GENI). GENI is a critical emerging arena of technical innovation that can benefit from existing and projected IIS-related research, and that can create new needs and opportunities for IIS-related investigations. The workshop will fill a critical niche in examining and surfacing many areas of joint concern and opportunity between networking research and such areas as information integration, intelligent systems, and human-computer interaction. It will seek the most intellectually rewarding and productive directions in these areas, reviewing, analyzing, and finding ways to extend current research by IIS-supported communities to incorporate and support the goals and new possibilities of the GENI program. The broader impacts of this work include potentially greater effectiveness, breadth of impact, and cross-community integration of the results of the GENI initiative itself; potentially more usable, manageable, societally-productive, cost-effective and efficient technologies for network design and application; and potentially wider application of novel networking technologies and capabilities for large-scale, distributed, and intelligent information discovery, management, and analysis tools and theories.","title":"IIS-GENI Workshop","awardID":"0649864","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"024F","name":"GENI CONCEPT\/DEVELOPMENT"}}],"PIcoPI":["503214"],"PO":["564456"]},"110576":{"abstract":"Depiction and Perception of Shape in Line Drawings Doug DeCarlo, Manish Singh and Matthew Stone <br\/>Rutgers University<br\/><br\/>Abstract<br\/><br\/>Because they can focus on what is essential about a scene, line drawings are a fundamental way of conveying shape that are often preferred over realistic representations,. This research couples the development of computer-based methods to synthesize line drawings with experiments on human observers that reveal how line drawings are perceived and interpreted. The ultimate goal is to produce depictions of 3D shape that are easier to understand. By improving line drawings, this research improves a source of communicative imagery that could become ubiquitous---appearing in better maps, more flexible and customizable technical and medical illustration, more broadly accessible scientific visualization, and many other applications. At the same time, new interactive techniques promise to make computer representation of 3D objects accessible to a wider range of users for a wider range of tasks and on a wider range of platforms.<br\/><br\/>This research involves three interrelated thrusts: (1) Computational studies are used to characterize the ambiguity inherent in interpreting line drawings and to develop new line styles that aim to reduce this ambiguity; (2) Experimental studies that probe the visual perception of 3D shape are used to discover what shape information human viewers gather from line drawings; and (3) The investigators are developing applications that exploit a perceptual model derived from the experimental studies in order to guide the placement of lines in drawings, and to guide the interpretation of hand-drawn sketches for interactive model construction.","title":"Depiction and Perception of Shape in Line Drawings","awardID":"0541185","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["474118","355043",292951],"PO":["532791"]},"123787":{"abstract":"SGER: CogNet (Cognitive Complete Knowledge Network) System<br\/><br\/>Award 0650048<br\/><br\/>Ramesh Rao<br\/><br\/><br\/>The Cognitive Complete Knowledge Network (CogNet) system combines advances in storage, network connectivity, and device capability, with advances in analytical approaches to make next generation wireless systems refine their performance throughout the lifetime of their<br\/>deployment. The CogNet architecture exploits the availability of low cost \"some time some where\" communications to asynchronously and automatically gather large amounts of user data by enlisting users, devices, and even the whole network as probes. The information gathered<br\/>is derived from the radio state, network state, environment state, and user profiles, and is spatio-temporally tagged and archived in a distributed repository and made available to the community of participants. A user in a particular spatio-temporal region could learn<br\/>about typical usage, others' experiences, network conditions, and protocol parameters by querying this repository and tune or adapt itself to achieve the desired quality of service.<br\/><br\/>We use stochastic analysis, algorithm development, performance analysis and optimization, and prototype deployment to identify globally relevant internal state data from the vast amounts generated within any mobile device; manage data transfer and archiving; design learning models and algorithms to fit the large and diverse collected dataset; develop cross-layer, cross-system adaptations to use information obtained from the collective experience; and analyze and learn the network traffic and network parameters to identify appropriate service compositioning parameters to guarantee user experience.","title":"SGER: The CogNet (Cognitive Complete Knowledge Network) System","awardID":"0650048","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[328469],"PO":["565090"]},"123215":{"abstract":"This SGER award supports a study that will contribute to a new Cyber-Physical Systems (CPS) vision being developed in NSF's Computer and Network Systems division. The goal of CPS research is new science and engineering methodologies that integrate the interacting cyber and physical elements of future engineered systems. CPS seeks new design principles and systems technology that can routinely yield high-confidence physical and engineered systems. This study explores a central aspect of CPS: the extent to which the mismatch between the core abstractions of computation and properties of physical processes impedes progress. In the physical world, the passage of time is inexorable and concurrency is intrinsic. Neither of these properties is present in today's computing and networking abstractions. The study will consider these and other central issues in the foundations of CPS.","title":"CSR-SGER: Cyber-Physical Systems - Are Computing Foundations Adequate?","awardID":"0647591","effectiveDate":"2006-10-01","expirationDate":"2008-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["71498"],"PO":["561889"]},"120840":{"abstract":"The project addresses some of the currently most important basic research challenges in error<br\/>control coding theory. The bulk of current theory on efficient decoding methods is asymptotic in nature, and thus does not yield useful predictions for the shorter and intermediate blocklengths needed for delaysensitive applications (e.g., high-speed communications, streaming and peer-to-peer networks). A closely related goal is to gain a deeper understanding the so-called \"error floor\" behavior exhibited by certain classes of low-density parity check (LDPC) codes. These error floors seriously limit their usefulness for very low bit error rate (BER) applications (e.g., high-speed communications, data storage). A current major bottleneck is the lack of systematic and reliable methods to explore this deep BER regime, and we propose to address this challenge through a combination of combinatorial and geometric analysis, hardware-based emulation,<br\/>and fast stochastic simulation.<br\/><br\/>Intellectual merit: Lack of finite-length analysis and the existence of error floors is a key bottleneck slowing down the deployment of high performance codes in a range of very important applications. Addressing this challenge requires a range of deep and fundamental scientific questions, drawing on ideas from polyhedral combinatorics, graph theory, dynamical systems and probability theory. Hardware implementation of message passing algorithms for high performance applications is in itself challenging. The research uses the development of high performance iterative decoders as a design driver for developing a novel emulation-simulation based design approach. This paradigm is novel and poses many fundamental challenges,<br\/>including the development of fast simulation techniques for gathering error statistics, stochastic<br\/>adaptive algorithms that exploit error statistics in order to design better codes, and investigation of systematic techniques for efficiently mapping code designs onto a field-programmable gate array (FPGA) platform.<br\/><br\/>Broader impact: Message-passing algorithms on graphs play a fundamental role across of a broad spectrum of scientific and engineering disciplines. The range of applications covers areas as diverse as communication systems, image processing, bioinformatics, statistical physics, and natural language processing, among many others. Consequently, our research, with its explicit goal of gaining a deeper understanding of message passing algorithms, has the potential for broad impact and dissemination across a variety of areas. In addition, our project has a significant educational and outreach component. Graduate students at Berkeley will be trained in both algorithmic and VLSI design techniques while participating in this research. Students coming our of this program will have a unique skill set that spans both theory and implementation,<br\/>thereby constituting an invaluable addition to the nation's technical workforce. We are also very active in promoting undergraduate research and in facilitating research experiences for students from historically underrepresented communities. The overall thrust of this project has many well defined subprojects, which makes it very well-suited to such outreach activity at the undergraduate level.<br\/>1","title":"Theory and Methodology for the Design and Evaluation of High-Performance LDPC Codes","awardID":"0635372","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["560233","560234","542073"],"PO":["564898"]},"120741":{"abstract":"Project Abstract: Randomness Extraction and Applications<br\/>Randomness is extremely useful in computation. In practice, however, it is expensive or impossible<br\/>to get truly random numbers. so scientists use \"pseudorandom generators.\" However, these sometimes fail, so scientists using them to run simulations may get the wrong answers without knowing it. This research focuses on randomness extractors: efficient algorithms which extract high-quality randomness from a low-quality random source. Such extractors have proven useful not only for their stated goal, but for areas as diverse as cryptography, coding theory, and network constructions.<br\/><br\/>More specifically, this research involves finding randomness extractors for as general a class of random sources as possible. Sources to be investigated include independent sources, small space sources, affine sources, as well as new sources. It is impossible to construct randomness extractors for completely general sources; however, it is possible with the addition of a small random seed. This research includes improving the length of the seed and the output to close to their optimal values.<br\/>Finally, this research further develops several application areas: pseudorandomness, cryptography,<br\/>hardness of approximation, and random selection.","title":"Randomness Extraction and Applications","awardID":"0634811","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["518438"],"PO":["565157"]},"120763":{"abstract":"Fourier techniques in cryptography and coding<br\/>Daniele Micciancio (UCSD)<br\/>August 31, 2006<br\/>Abstract<br\/>Digital computers and communication networks are routinely used in a growing number of security sensitive applications, like on-line shopping, on-line banking, etc. Cryptographic primitives (i.e., the basic operations performed by computers to protect their data) play a fundamental role in securing the digital world, so our confidence in their security is paramount. Unfortunately, for the sake of efficiency (i.e., fast execution by computers), many cryptographic primitives used in practice are not supported by mathematical proofs of security. This research investigates the design and analysis of cryptographic primitives that are both very efficient and provably secure in a rigorous mathematical sense. The project builds on mathematical techniques and problems (mostly from the areas of Fourier analysis and point lattices) that are interesting in a broader perspective, beyond security, with potential applications to other areas of mathematics and engineering.<br\/>There is a wide and discomfortable gap between the current state of the art in practical crypto- graphic design and theoretical cryptography. Ad-hoc design methods offer cryptographic primitives whose efficiency is unmatched by theoretical constructions, but at the price of loosing every security guarantee. This research addresses this gap by investigating constructions (of hash functions and other cryptographic primitives) that are both efficient and provably secure. The investigators consider computational problems mostly from the areas of point lattices, coding theory and algebraic number theory. Efficiency is achieved considering problems with special structure (e.g., cyclic lattices), and Fourier techniques (whose development is an integral part of the project) both as algorithmic design and security analysis tools.","title":"Fourier Techniques in Cryptography and Coding","awardID":"0634909","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["486529"],"PO":["565157"]},"120785":{"abstract":"In large and complex communication networks, architectural decisions regarding functionality allocation are extremely important. The time is ripe for building a scientific foundation for network architectures, both to capitalize on unique clean-slate design opportunities (such as GENI and MANET) and to guide the evolution from existing network architectures to new ones. Such a foundation can lead to highly efficient, robust, and scalable protocols that could have a significant impact on the communications industry.<br\/><br\/>The recent successes of understanding protocols as optimizers and layering as mathematical decompositions offer a promising starting point for such an analytic foundation one that is conceptually unifying, mathematically rigorous, and practically relevant. However, there is still much work to be done in developing an analytic foundation for network architectures. This research focuses on three main thrusts: <br\/><br\/>Alternative architectural choices: Past mathematical results have focused on one architecture derived from a particular decomposition. There is in fact a wide range of alternative decompositions that result in different scalability, convergence, and complexity tradeoffs. This research systematically explores architectural choices using appropriate decompositions.<br\/><br\/>Stochastic network dynamics: This research develops new architectural designs taking into account stochastic (rather than deterministic) network dynamics, which are critical in modeling real systems and in developing high-performance network architectures. <br\/><br\/>Non-convexity and robustness: Non-convexity persists in real networks, which could lead to instability, poor performance, and impractical computational complexity. Nonetheless, most past results have been derived only for the convex case. This research explores architectural choices that are robust to non-convexity.","title":"Collaborative Research: Towards An Analytic Foundation for Network Architectures","awardID":"0635034","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["531675","561963"],"PO":["564898"]},"110522":{"abstract":"This research aims to create techniques for enhancing the reliability of software systems -- a problem that is hugely valuable in today's computerized society. The goal is to develop improved techniques for<br\/>(i) verifying properties of a program's behavior, and (ii) finding potential bugs and security vulnerabilities. The project will develop static-analysis techniques, which obtain information about the possible states that a program passes through during execution (but without running the program on specific inputs). Instead, all possible inputs are considered, and all possible reachable states are explored.<br\/>The trick to making this feasible is to run the program on descriptors that represent multiple states.<br\/><br\/>The project will extend the Three-Valued Logic Analyzer (TVLA), a tool for analyzing programs that allocate and deallocate memory and destructively update pointers. These actions are essential in most modern programming languages, but are extremely difficult to analyze.<br\/>TVLA uses finite three-valued logical structures to model the possibly infinite set of states that such programs can reach. The goals of this research are (i) to develop methods for allowing TVLA to combine analyses of sub-programs, which would allow the creation of reusable summaries of library functions; (ii) to develop symbolic methods, such as decision procedures for logic fragments, that interpret three-valued models as precisely as possible; and (iii) to apply these techniques to analyze low-level assembly code.","title":"Collaborative Research: Advanced Static-Analysis Techniques for Ensuring Reliable Software","awardID":"0540955","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["401760"],"PO":["564388"]},"110533":{"abstract":"This research aims to create techniques for enhancing the reliability of software systems -- a problem that is hugely valuable in today's computerized society. The goal is to develop improved techniques for<br\/>(i) verifying properties of a program's behavior, and (ii) finding potential bugs and security vulnerabilities. The project will develop static-analysis techniques, which obtain information about the possible states that a program passes through during execution (but without running the program on specific inputs). Instead, all possible inputs are considered, and all possible reachable states are explored.<br\/>The trick to making this feasible is to run the program on descriptors that represent multiple states.<br\/><br\/>The project will extend the Three-Valued Logic Analyzer (TVLA), a tool for analyzing programs that allocate and deallocate memory and destructively update pointers. These actions are essential in most modern programming languages, but are extremely difficult to analyze.<br\/>TVLA uses finite three-valued logical structures to model the possibly infinite set of states that such programs can reach. The goals of this research are (i) to develop methods for allowing TVLA to combine analyses of sub-programs, which would allow the creation of reusable summaries of library functions; (ii) to develop symbolic methods, such as decision procedures for logic fragments, that interpret three-valued models as precisely as possible; and (iii) to apply these techniques to analyze low-level assembly code.","title":"Collaborative Research: Advanced Static-Analysis Techniques for Ensuring Reliable Software","awardID":"0541018","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["485266"],"PO":["564388"]},"123777":{"abstract":"ABSTRACT <br\/>SGER: Compiler-driven Adaptive Execution<br\/>The goal of this project is to study dynamically adapting compiler-translated MPI applications in a distributed system. In previous work we have created a system for dynamic adaptation and tuning of computational applications. The current study includes, deploying a translator that converts OpenMP parallel applications directly into MPImessage passing programs, and investigating new methods to enable dynamic, adaptive optimization and tuning in diverse architectures. The architecture space explored includes<br\/>homogeneous multiprocessors, clusters, and heterogeneous systems, such as computational Grids.<br\/>The project includes: selecting compilation parameters in the OpenMP-to-MPI translator,<br\/>developed previously, that are amenable to dynamic adaptation, and turn these parameters into compiler options; adaptively optimize these parameters with a perfomance tuning system; and study the potential for dynamic adaptation through analytic and measured evaluation of the overheads and gains of performing adaptation during the program execution. An important part of the evaluation will be to test the methods pursued on applications that exhibit potential for such adaptation. As part of the project a workshop on Compiler-driven Adaptive Execution Systems will be organized, to convene researchers active in the field of dynamic and adaptive Compilation, and address a broad range of topics from success stories of compiler-driven adaptive execution to the potential of such methods to the enabling technology.","title":"SGER: Compiler-Driven Adaptive Execution","awardID":"0650016","effectiveDate":"2006-10-01","expirationDate":"2008-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["558595"],"PO":["551712"]},"123799":{"abstract":"SGER: Strategies for Increasing Stability of Self-Assembling DNA Nanostructures<br\/>PI: Thomas H. LaBean<br\/>DNA nanostructure self-assembly is being developed as a \"bottom-up\" approach for the<br\/>fabrication of desired patterns and structures with feature sizes smaller than achievable with conventional<br\/>lithography techniques. The exploratory research proposed here aims to increase the stability of DNA<br\/>nanostructures by incorporating isoG\/isoC base pairs within and between DNA building blocks, by<br\/>facilitating ligation between abutting oligonucleotides, and by exploring interstrand chemical crosslinking.<br\/>The long-term goal of the proposed project is to develop methods for stabilizing DNA and<br\/>biomolecular nanostructures to increase production yields, improve the halflife, and diversify the physical<br\/>and chemical conditions under which the structures are useable. In addition, it has been noted, that<br\/>attachment of nanomaterials, for example metallic nanocrystals or large proteins, to the component DNA<br\/>strands prior to assembly into lattices can result in assembly difficulties and decreased size of the<br\/>resulting superstructures. Stabilized lattices may help to overcome these issues.<br\/>Summary of Proposed Research Tasks: 1). Develop DNA self-assembly materials and techniques<br\/>for formation of larger addressable arrays and lattices. 2). Incorporate stabilizing strategies, including<br\/>isoG\/isoC and ligation into DNA tile lattice designs. Perform nanoscale structural characterization. 3).<br\/>Investigate interstrand cross-linking via chemical bridges within DNA building blocks. 4). Test the<br\/>stabilized nanostructures as templates for metal deposition and scaffolds for immobilization of<br\/>conducting, semi-conducting, and dielectric nanomaterials.<br\/>The proposed project fulfills the goals of the CCF Division and of the Emerging Models and<br\/>Technologies Program by advancing fundamental capabilities in computer science and engineering using<br\/>advances and insights from areas as diverse as biological systems and molecular engineering. The<br\/>proposed research innovations enable fundamentally different ways of approaching the fabrication of<br\/>computing and communications hardware. The intellectual merit of the proposed activities is high<br\/>because the project brings together cutting-edge research in DNA self-assembly with new synthetic<br\/>basepair options and chemical cross-linking procedures, and from the fact that the PIs lab is uniquely<br\/>qualified and poised to execute the proposed study at this time. There is a high likelihood that the research<br\/>tasks will be completed as described and very high scientific and technological payoffs when they<br\/>succeed. Broader impacts of the proposed work include the possibility of stabile DNA nanostructures for<br\/>application not only to computing but also to biomedical uses, as well as the certainty of establishing<br\/>research training opportunities for the next generation of creative experts in bioinspired design, molecular<br\/>engineering, self-assembly, and nanostructure templated chemistry.","title":"SGER: Strategies for Increasing Stability of Self-Assembling DNA Nanostructures","awardID":"0650083","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["531216"],"PO":["565223"]},"120830":{"abstract":"The design of efficient, and low complexity, coding and decoding algorithms is a critical component in<br\/>the optimization of wireless and wire-line networks. Recent years have witnessed the emergence of new<br\/>powerful paradigms, e.g., belief propagation, as well as the revival of classical approaches, e.g., sequential<br\/>decoding, for the design of decoding algorithms. This proposal seeks a deeper theoretical foundation for the<br\/>design and analysis of efficient decoding algorithms, along with the associated code ensembles, by building<br\/>on this recent progress.<br\/>The overarching goal of the proposed research is to unify under one framework the class of algorithms<br\/>that exploit the power of local constraints, e.g., the linear programming or belief propagation decoders and<br\/>generalization thereof, with the class of algorithms that optimize a global cost function, e.g., tree search<br\/>decoders. The proposed research benefits from our preliminary results that establish interesting connections<br\/>between different decoding schemes (as detailed in the sequel). Our results are expected to elucidate the<br\/>properties of known algorithms and inspire the design of more powerful decoders. Our investigations<br\/>will start with the traditional point-to-point scenario where the complexity of the model ranges from the<br\/>simplified binary erasure channel to the challenging multi-path fading channel. In the latter case, our<br\/>work will target a joint detection and decoding approach and consider, explicitly, the associated signal<br\/>processing tasks, e.g., the preprocessing stage which will be defined rigorously in the sequel.<br\/>Our research will then proceed to investigate three distinct categories of multi-user channels. The<br\/>first thrust will focus on the outage-limited multiple access channel where efficient decoding algorithms<br\/>that achieve the optimal diversity-multiplexing tradeoff will be pursued. The same line of work will also<br\/>seek code ensembles and decoding algorithms that achieve the capacity of the compound multiple access<br\/>channel (CMAC), one of the few examples where the celebrated successive cancellation decoder fails to<br\/>achieve capacity. The CMAC model captures the essential characteristics of delay-limited multiple access<br\/>channels with limited transmitter channel state information. The study of the broadcast channel (i.e.,<br\/>downlink) is our second research thrust where low complexity and high performance dirty paper codes<br\/>and rateless codes will be constructed. Within this context, classical works on tree search source coding<br\/>will be revisited in order to construct low complexity vector quantization stage for the dirty-paper coding<br\/>scheme. The third research thrust will be devoted to cooperative channels. Here, our efforts will seek<br\/>to characterize the diversity-multiplexing tradeoff of the cooperative interference channel along with the<br\/>associated optimal coding and decoding algorithms. This work will build on the intimate relationship<br\/>between the CMAC and the interference channel. Finally, inspired by our recent work in the area, novel<br\/>approaches for cooperative lattice coding and decoding algorithms will be unveiled.<br\/>The intellectual merit of the proposed research lies in the fundamental nature of the posed questions<br\/>and the expected contributions to information and coding theories. New bridges between information<br\/>theory, from one side, and optimization and algebraic number theories, from the other side, will be built.<br\/>Furthermore, analytical tools, as well as efficient numerical methods, that aid in the design and analysis<br\/>of decoding algorithms will be pursued.<br\/>The PIs, with collective expertise in information theory, coding theory, wireless communications, and<br\/>signal processing, form a qualified team for undertaking the research plan. This project builds on recent<br\/>results from the PIs' ongoing collaboration. The proposed theoretical research is expected to have a broad<br\/>impact on the information theory and signal processing communities. Furthermore, the anticipated results<br\/>will contribute to the design of efficient encoding\/decoding algorithms which percolate through many<br\/>applications (please refer to the support letters from Texas Instruments and ST Microelectronics). On the<br\/>educational front, graduate students at the two participating institutions will be heavily involved in the<br\/>research activities.","title":"Collaborative Research: Towards a Unified Framework for Decoding Algorithms: Unveiling the Hidden Connections","awardID":"0635326","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["508212"],"PO":["432103"]},"120731":{"abstract":"Title: Collaborative Research: Cognitive Ad Hoc Networks: Capacity Optimization Through Local Adaptation<br\/><br\/>0635003, Weber<br\/>0634979, Andrews<br\/>0634763, Jindal<br\/><br\/>Due to the unpredictability of the environment in which unplanned (ad hoc) wireless networks will operate, an appealing approach is to allow the network to dynamically adapt to the perceived conditions. We define such ad hoc networks as cognitive. A framework is developed for understanding the benefits of local adaptation, by breaking adaptive techniques into the four major degrees of freedom available to the designer: time, frequency, code, and space. The aim is to address the following two questions. First, what are the fundamental limits on information flow through unplanned networks; in particular, how valuable is localized information and coordination in seeking to achieve this limit? Second, what are the relative values of adaptation in time, space, frequency, and code in terms of information flow; in particular, how does the network designer identify which degree of freedom is most valuable in a variety of networking scenarios?<br\/><br\/>In this research, information theory and stochastic geometry are connected through a novel metric for ad hoc network capacity, termed the transmission capacity. This metric captures the maximum spatial intensity of transmissions subject to a specified outage probability. While related to other popular capacity metrics, notably the transport capacity, the transmission capacity is unique in its allowance of explicit and accurate characterization of capacity for any conceivable communication scheme or transmission environment. The transmission capacity is an indispensable unifying metric for this analysis since i) it allows closed-form results, ii) does not require any global coordination or optimization, iii) accurately models the interference environment of an ad hoc network. These innovations will allow a basis for more efficient wireless network design.","title":"Collaborative Research: Cognitive Ad Hoc Networks: Capacity Optimization Through Local Adaptation","awardID":"0634763","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["357689"],"PO":["432103"]},"120775":{"abstract":"Title: Collaborative Research: Cognitive Ad Hoc Networks: Capacity Optimization Through Local Adaptation<br\/><br\/>0635003, Weber<br\/>0634979, Andrews<br\/>0634763, Jindal<br\/><br\/>Due to the unpredictability of the environment in which unplanned (ad hoc) wireless networks will operate, an appealing approach is to allow the network to dynamically adapt to the perceived conditions. We define such ad hoc networks as cognitive. A framework is developed for understanding the benefits of local adaptation, by breaking adaptive techniques into the four major degrees of freedom available to the designer: time, frequency, code, and space. The aim is to address the following two questions. First, what are the fundamental limits on information flow through unplanned networks; in particular, how valuable is localized information and coordination in seeking to achieve this limit? Second, what are the relative values of adaptation in time, space, frequency, and code in terms of information flow; in particular, how does the network designer identify which degree of freedom is most valuable in a variety of networking scenarios?<br\/><br\/>In this research, information theory and stochastic geometry are connected through a novel metric for ad hoc network capacity, termed the transmission capacity. This metric captures the maximum spatial intensity of transmissions subject to a specified outage probability. While related to other popular capacity metrics, notably the transport capacity, the transmission capacity is unique in its allowance of explicit and accurate characterization of capacity for any conceivable communication scheme or transmission environment. The transmission capacity is an indispensable unifying metric for this analysis since i) it allows closed-form results, ii) does not require any global coordination or optimization, iii) accurately models the interference environment of an ad hoc network. These innovations will allow a basis for more efficient wireless network design.","title":"Collaborative Research: Cognitive Ad Hoc Networks: Capacity Optimization Through Local Adaptation","awardID":"0634979","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["522873"],"PO":["432103"]},"110567":{"abstract":"Abstract:<br\/>Contemporary software development emphasizes components with clearly specified APIs. In current practice, while a typical software component has a precise<br\/>(static) interface in terms of methods it supports, the information about the correct sequencing of method calls is missing. Behavioral interfaces can capture such temporal constraints, and can play a critical role in documentation, maintenance, testing, and compositional analysis. This proposal is centered around specification and automatic extraction of such interfaces.<br\/>Research will be pursued to develop formal notions of behavioral interfaces for Java classes and web services; algorithms for automatic synthesis of interfaces using abstraction, learning, and symbolic model checking; application of interfaces for compositional software model checking via assume-guarantee reasoning; and experimentation and evaluation on Java2SDK library classes.","title":"Behavioral Interfaces for Software Components","awardID":"0541149","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["497082"],"PO":["564388"]},"122954":{"abstract":"Although digitization of health information and IT applications in health care have received significant focus in recent years (mainly from government officials interested in improving care while achieving more cost efficiencies), the concerns of patients and their needs, as they relate to information access, have not received a similar level of attention. There is a gap in expectation with regard to the role of health information systems between the general public, who are increasingly becoming more computer- and Web savvy, and health IT professionals. A deeper consideration of the gap is expected to help clarify the challenges and point to potential means for developing effective information systems that deliver health information.<br\/><br\/>The main goal of this national workshop is to focus on key challenges involved in delivering high quality digital health information directly to citizens and health care providers. The challenges cover many areas encompassing access, interpretation, decision-making, security\/privacy, evaluation, and economic issues. A goal of the workshop is to create a forum where multi-disciplinary perspectives are supported to develop a scope for research in this area, identify critical research topics, and establish associated knowledge resources that may be helpful.","title":"NSF Frontiers in Health Information Delivery Workshop","awardID":"0646421","effectiveDate":"2006-10-01","expirationDate":"2007-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":["486098","486226"],"PO":["565136"]},"123746":{"abstract":"Minimizing the energy cost and improving thermal<br\/>performance of datacenters are among the key issues towards optimizing computing<br\/>resources and maximally exploiting a datacenter's computation capability. <br\/>In general,<br\/>there is no automated feedback control loop between the physical environment of a data<br\/>center, and the datacenter's management software\/system software scheduler for making<br\/>thermal and power-efficient resource management decisions. The energy cost and thermal<br\/>distribution of a datacenter depends on the multi-scale properties of a large number of<br\/>multi-modality system variables, such as cooling capacity, hardware thermal<br\/>characteristics, and workload profiles. An integrated, system-oriented approach towards<br\/>this problem could provide several significant benefits.<br\/>Thiswork, will investigate approachers for a unique merger between physical infrastructure and the resource<br\/>management functions of the cluster operating system to take a holistic view of data center<br\/>management, and make global (at the level of a datacenter), power-aware, and<br\/>thermal-aware job scheduling decisions. The project seeks to develop a comprehensive solution<br\/>for on-line, dynamic and automated thermal management and control framework for the<br\/>power-limited datacenters. The methods include an integrated two-pronged approach for incorporating fast thermal evaluation, using an abstract heat model, in the resource management<br\/>functions of the datacenters; namely: new thermal aware scheduling techniques will be developed<br\/>by making traditional scheduling techniques cognizant of thermal performance of their<br\/>scheduling decisions by taking into account the spatio-temporal thermal implications of<br\/>job placements. This entails making scheduling decisions by taking into account not only<br\/>job characteristics but also physical placement of computational node, the thermal<br\/>characteristics of heterogeneous systems, and thermodynamics. These thermal-aware<br\/>scheduling techniques will be comprehensively evaluated using new performance metrics<br\/>such as utilization per operating cost.","title":"SGER--CSR\/SMA: Thermal Aware Dynamic Resource Management for Datacenters","awardID":"0649868","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["448647","535238","559502"],"PO":["551712"]},"127915":{"abstract":"Independent component analysis (ICA) has emerged as an attractive analysis tool for discovering hidden factors in observed data and has been successfully applied for data analysis in a wide array of applications such as biomedicine, communications, finance, and remote sensing. In a good number of these application domains, the data are typically complex valued. This is also the case in biomedical image analysis where ICA has been recognized as a promising tool for studying the brain function. Most biomedical image analysis techniques, however, use only the magnitude information and discard the phase, resulting in an unnecessary loss of information. Moreover, most brain imaging studies collect multiple data types where each existing modality for imaging the brain reports upon a limited domain and provides complementary information. Thus processing of imaging data in its native, complex form and by utilizing multiple modality images promises significant advances in our understanding of the brain function. We propose to develop a class of complex ICA algorithms, in particular for analysis of biomedical imaging data and demonstrate the power of joint data analysis as well as performing the analysis on the complete set of data, i.e., by utilizing both the magnitude and the phase information. We focus upon three image types, functional magnetic resonance imaging (fMRI), structural MRI (sMRI) and diffusion tensor imaging (DTI). These three imaging data provide complementary information about brain connectivity, and all can benefit from the incorporation of a complex-valued data processing approach.<br\/><br\/>The broad impact of the proposed work lies in its potential to substantially impact science and information technology as well as in its educational features. Study of human brain connectivity is a very challenging and rich problem. The ICA-based fusion approach as well as the use of imaging data in its native, complex form, we believe is the key for achieving significant advances in the field. Successful demonstration of our approach for medical imaging data will also benefit other areas of science and technology where data from multiple sources and\/or data in complex form need to be jointly analyzed for inferences. A significant broader impact of our proposal is to stimulate research at the interface between medical imaging and information processing by making the tools for the study of brain connectivity widely available through a toolbox and a medical imaging database.","title":"Collaborative Research: SEI: Independent Component Analysis of Complex-Valued Brain Imaging Data","awardID":"0715022","effectiveDate":"2006-10-09","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":["486018"],"PO":["565136"]},"116607":{"abstract":"Software evolution, through maintenance or updates, requires the greatest effort in the software industry's product life-cycle. This effort is increasing with the industry's desire to be more responsive to customer needs. Mass customization, and related theories, provides guidance for producing high quality, low cost, mass-produced craft products. The design theories, however, provide no guidance for post-purchase customization, as required of software evolution. This project contributes to a Science of Design by addressing how to keep a software application's behavior in line with the requirements. The Requirements Monitoring (ReqMon) approach taken in this research provides guidance on when and what aspects of software should be evolved by dynamically monitoring individual user goals.<br\/><br\/>Assistive technologies (AT), which support individuals with disabilities such as cognitive impairments (CI), are in the vanguard of those that require individually personalized software. Monitoring individual user goals during their use of AT may mitigate the commonplace abandonment of AT, by providing clinicians and designers the necessary and timely feedback for continuous adaptation and redesign of software-intensive AT. A new, personalized design science is needed to improve the design, use, function and stability of AT systems to assist the large and growing population of individuals that required AT to fully participation in society.<br\/><br\/>This research is developing user goal monitoring tools and theory in support of dynamically personalizing AT specifically, and software evolution more generally. This research will offer two contributions. First, the monitoring method provides an approximate solution to understanding the dynamically changing needs of software users. Second, the mass-personalized software development (MPSD) theory provides a framework for analyzing continuously personalized designs in mass markets. MPSD suggests strategies for post-purchase customization of digital products.","title":"SoD-TEAM: Monitoring in Support of Design Science Principles","awardID":"0613698","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7372","name":"ITR-SCIENCE OF DESIGN"}}],"PIcoPI":["549985"],"PO":["564388"]},"120755":{"abstract":"In large and complex communication networks, architectural decisions regarding functionality allocation are extremely important. The time is ripe for building a scientific foundation for network architectures, both to capitalize on unique clean-slate design opportunities (such as GENI and MANET) and to guide the evolution from existing network architectures to new ones. Such a foundation can lead to highly efficient, robust, and scalable protocols that could have a significant impact on the communications industry.<br\/><br\/>The recent successes of understanding protocols as optimizers and layering as mathematical decompositions offer a promising starting point for such an analytic foundation one that is conceptually unifying, mathematically rigorous, and practically relevant. However, there is still much work to be done in developing an analytic foundation for network architectures. This research focuses on three main thrusts: <br\/><br\/>Alternative architectural choices: Past mathematical results have focused on one architecture derived from a particular decomposition. There is in fact a wide range of alternative decompositions that result in different scalability, convergence, and complexity tradeoffs. This research systematically explores architectural choices using appropriate decompositions.<br\/><br\/>Stochastic network dynamics: This research develops new architectural designs taking into account stochastic (rather than deterministic) network dynamics, which are critical in modeling real systems and in developing high-performance network architectures. <br\/><br\/>Non-convexity and robustness: Non-convexity persists in real networks, which could lead to instability, poor performance, and impractical computational complexity. Nonetheless, most past results have been derived only for the convex case. This research explores architectural choices that are robust to non-convexity.","title":"Collaborative Research: Towards an Analytic Foundation for Network Architectures","awardID":"0634891","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["509442"],"PO":["564898"]},"120766":{"abstract":"A basic assumption in classical communications systems is that the transmitter and receiver are synchronized: for each transmitted symbol, one receives one channel output sample. This assumption has been valid under high signal-to-noise ratio regimes, where timing errors can generally be tracked nearly perfectly. With the new coding techniques of the last decade, leading to much lower signal-to-noise ratios, this assumption is no longer valid; synchronization is an increasingly important issue in many applications. It is likely to become a bottleneck in real systems in the years to come, as practical near-capacity codes for basic channels become standard under low signal-to-noise regimes.<br\/>Despite this, channels with synchronization errors, including even the simplest random deletion channel, represent a remarkably under-studied area in information theory. While there are families of practical near-capacity codes for basic channels with errors, such as the binary symmetric channel and the additive white Gaussian noise (AWGN) channel, there is not even a known provable expression for the capacity of the deletion channel, much less more difficult variations of channels with synchronization. This represents a fundamental gap in modern information theory. This research involves advancing the state of the art in coding for channels with synchronization errors, both by expanding on the information-theoretical foundations and by designing new codes and coding\/decoding techniques.<br\/>i","title":"Towards a Basic Understanding of Channels with Synchronization Errors","awardID":"0634923","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["560092"],"PO":["432103"]},"120799":{"abstract":"ABSTRACT<br\/>0635119<br\/>Northeastern University<br\/>Rajaraman, Rajmohan<br\/><br\/>How blissful is ignorance? The role of obliviousness in network<br\/>optimization <br\/><br\/>The next-generation Internet will be orders of magnitude bigger in scale, connecting together nodes that are very different in their capabilities and requirements. Traditional algorithm design has tended to focus on problems with complete information over a homogeneous platform. However, the dramatic increase in scale and heterogeneity means that nodes can neither hope to obtain nor store and process full information. Therefore, it is important today to focus on algorithms and protocols that can operate obliviously, i.e.,<br\/>with limited knowledge. This research develops formal frameworks for quantifying the associated tradeoffs in oblivious network optimization and delivers infrastructure-class algorithms that can secure the<br\/>foundations of the Internet of tomorrow. This project also trains students in the design of advanced network infrastructure and incorporates the research into the curriculum for algorithms and networking.<br\/><br\/>The focus of this research is three-fold. The first component concerns oblivious algorithms for network design through universally approximate solutions that simultaneously approximate the optimal over<br\/>all possible inputs. The investigators study the TSP, Steiner tree, and other fundamental optimization problems within this framework, and also apply these ideas to protocols for data acquisition in sensor<br\/>networks. The second component of this project is the design of fault-oblivious algorithms for load balancing and scheduling that can offer performance guarantees for arbitrary unknown and unpredictable<br\/>fault patterns. The third component of this research is confluent routing, which is a source-oblivious approach to routing that increases efficiency while maintaining scalability.","title":"How Blissful is Ignorance? The Role of Obliviousness in Network Optimization","awardID":"0635119","effectiveDate":"2006-10-01","expirationDate":"2009-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["518070","516944"],"PO":["432103"]},"120568":{"abstract":"The need to estimate signals from their deformed versions arises in a wide range of engineering problems. Such deformations may be due to inherent physical effects or due to the measurement process. As an example consider two images of a single object taken from different viewpoints and distances. These images are deformed versions of each other. Any attempt to compare these images requires transforming one or both so that corresponding elements in the images appear at the same coordinates. In the one-dimensional case consider the case of a signal measured in the presence of (possibly time-varying) delay and Doppler which causes a distortion of the time axis and a corresponding deformation of the signal shape. The difficulty of these problems is their non-linear and high-dimensional nature.<br\/><br\/>This research provides a fundamental mathematical solution to a class of signal deformation problems. The solution involves a certain parametric model of the family of possible transformations (or rather, their inverses). Using this model the original problem is transformed into an equivalent problem expressed in the form of a linear system of equations in the low dimensional space. In other words, the method converts a computationally prohibitive estimation problem into a set of linear equations in the unknown parameters of the deformation model. In the case of affine transforms an exact closed form solution of the deformation parameters is obtained. In the more general case of homeomorphic deformations the solution is approximate, but high accuracy is achievable. The focus of the research is on evaluating the performance of this approach in the presence of noise and other uncertainties, and the optimal design of certain basis functions used by the algorithm. This research has applications to a wide range of image and video processing systems, in particular to medical imaging systems, and video\/image based security systems.<br\/><br\/><br\/>%%%%%%%%%%%%%%%%%%%%%%%%%%<br\/>> 3. A level of effort statement.<br\/><br\/>At the recommended level of support, the PI will make every effort to address all of the issues related to the basic operation of the proposed technique as described in the proposal, with emphasis on its performance in the presence of noise and various uncertainties, and the optimal design of the basis functions. Because of the greatly reduced budget and period of performance the specific applications of the technique will not be addressed in any detail.","title":"Multidimensional Signal Processing in the Presence of Homeomorphic Deformations","awardID":"0633775","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["345777"],"PO":["564898"]},"123725":{"abstract":"This award is for a workshop on the topic of \"New Research Directions in High Confidence Software Platforms for Cyber Physical Systems\" to be held in Alexandria, Virginia, on November 30-December 1, 2006. This one of several workshops in a series of activities that focus on high-confidence and certifiable software and systems. These workshops are sponsored in cooperation with the High Confidence Software and Systems (HCSS) Coordinating Group (CG) under the NSTC Networking and Information Technology Research and Development (NITRD) Subcommittee.<br\/><br\/>The purpose of the workshop is to provide an open, working forum for leaders and visionaries from industry, government, research laboratories, and academia, who are concerned with systems software technology platforms for real-time embedded sensing and control systems. The workshop will address challenges that are not resolved by current real-time embedded systems software: real-time operating systems (RTOS), middleware (MW), and virtual machines (VM). The workshop will assess the inadequacy of the current real-time technology base to support future complex physical and engineered systems. The workshop also will assess research needs and develop a research roadmap for achieve fundamentally new approaches and cyber technology to support a new generation of cyber-physical systems.","title":"New Research Directions in High Confidence Software Platforms for Cyber Physical Systems Workshop","awardID":"0649761","effectiveDate":"2006-10-15","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["534923"],"PO":["561889"]},"125628":{"abstract":"Current computational models for deformable tissue simulation in interactive surgical training applications make enormous sacrifices in physical accuracy to achieve real-time performance. This is particularly true of challenging real-time, haptic force-feedback rendering of tool-tissue interactions, where simulations must be ideally performed at near kilohertz rates. Furthermore, even if real-time demands are met, such as by faster processors in coming decades, we must still address our fundamental lack of accurate computational models for (a) force response of realistic, variable, nonlinear, soft tissues, and (b) many nontrivial physical processes associated with realistic tool-tissue interaction that are central to surgical intervention, such as, needle insertion, for example. Consequently, the scientific community can help surgical simulation practitioners improve patient safety by providing both accurate and real-time computer models of soft tissues interactions.<br\/><br\/>INTELLECTUAL MERITS: At the very fundamental level, this project seeks to bridge the apparent gap between efficient computer simulation algorithms, and the realistic surgical tissues and interactions they seek to mimic. Specifically, we propose a multi-disciplinary research program on reality-based measurement and computer simulation that leverages our unique strengths to address three key areas:<br\/>1. Reality-based modeling: Accurate, realistic mathematical models that describe nonlinear soft-tissue response and calibrated interaction models for performing needle insertion. We will develop a range of experimental apparatuses for measuring soft-tissue responses during needle insertion. All measurements will be accompanied by a rigorous empirical modeling process to arrive at accurate parametric tissue and interaction models.<br\/>2. Data-driven, real-time, simulation algorithms: Given accurate mathematical models of needle-tissue interaction, very efficient simulation algorithms will be devised for real-time haptics and graphics display. Novel computer models based on data-driven, pre-computed, reduced-coordinate, deformation algorithms will be used to accelerate accurate nonlinear deformation, and needle-tissue contact interactions. To support adaptation, and allow runtime modification, we will combine hierarchical domain decomposition using fast reduced domain models, with direct and data-driven simulation models.<br\/>3. Closing the loop: Quantitative evaluation of experimental and computer simulation models will help refine and validate our reality-based modeling processes throughout the project. In our final year, we will also develop a simulator-based training system for sample needle insertion tasks.<br\/><br\/>BROADER IMPACTS: The proposed research will make fundamental advances in our ability to simulate and reason about soft-tissue interactions accurately, and will lead to several exciting scientific and clinical possibilities. Scientifically, we will be able to develop accurate and reality-based soft-tissue models based on actual experimental trials, which have wide application in medicine. Optimized numerical algorithms can then build on these accurate nonlinear material and contact interaction models for real-time graphical and haptic force-feedback display of soft tissues. Clinically, this will allow a more widespread use of surgical simulators for resident training (for both minimally invasive and direct procedures), whereby residents will be able to experience more realistic soft-tissue interaction response in surgical tasks. Advancement in this area will also open avenues for modeling any other organ or soft-tissue for which training is desired, after the core reality-based simulation issues are resolved.","title":"CompBio: Reality-based Data-driven Computer Models for Surgical Simulation","awardID":"0704138","effectiveDate":"2006-10-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["379044"],"PO":["565223"]},"120833":{"abstract":"ABSTRACT<br\/>0635339<br\/>Ilya Dumer<br\/>U of Cal Riverside<br\/><br\/>Collaborative Research: Digital Fingerprinting: Information Theoretic Analysis<br\/>and Code Design<br\/><br\/>The problem of fingerprinting for copyright protection has come to the forefront of research in information<br\/>and coding theory, motivated by varied applications such as digital fingerprinting of software, images, audio<br\/>signals as well as multimedia fingerprinting for video-on-demand and related applications. Copies of data<br\/>distributed to the users of the system are protected by digital fingerprints,i.e., short strings of bits designed<br\/>individually for each user of the system in such a way that individual users cannot redistribute their contents<br\/>without being traced by the distributor of the system. The tracing becomes substantially more difficult when<br\/>coalitions of users cooperate to create a pirate copy of the data, attempting to obfuscate their fingerprints. The<br\/>prime objective of this research is to use information-theoretic and coding methods to establish the maximum<br\/>number of users of a system with reliably fingerprinted data and to construct new, large-size fingerprinting<br\/>schemes.<br\/>This research considers two main models of the fingerprinting system, one in which the pirates first detect<br\/>the positions of the fingerprint by comparing their copies of the data, and then form a fingerprint for the unregistered copy, and the other in which they are allowed to alter their data files as long as the unregistered copy created satisfies some distortion constraints as compared to their original copies. Representing fingeprinting systems as multiple-access channels, either fixed or time-varying, this research establishes the capacity limits of fingerprinting under these two models. New fingerprinting codes are constructed equipped with efficient identification\/tracing algorithms and related performance bounds. Finally, this research investigates the properties and designs new families of combinatorial coding schemes such as separating and frameproof codes, codes with the identifying parent property, and traceability codes.","title":"Collaborative Research: Digital Fingerprinting: Information Theoretic Analysis and Coding Design","awardID":"0635339","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["478371"],"PO":["432103"]},"120844":{"abstract":"This project studies the emerging field of network coding in several new directions. Network coding offers the promise of improved performance over conventional network routing techniques, by allowing network nodes to mathematically combine information prior to retransmission. Over the next decade, it has the potential to become a pervasive technology that could radically change the way information is communicated. In particular, network coding principles can significantly impact the next-generation wireless, ad hoc, and sensor networks, in terms of both energy efficiency and throughput.<br\/><br\/>While most of the existing network coding theory assumes error-free links, in practice these links are usually noisy. In fact, error-correction coding for such noisy links is sub-optimal when it is separated from network coding --- to maximize the throughput of a network, channel and network coding must be<br\/>combined. Thus, one of the main research topics studied in this project is joint network and channel coding. The project focuses on the following subtopics: (i) Reverse concatenation of network and channel coders (ii) Joint network\/channel coding at the node level (ii) Global network\/channel coding at the network level The second main research topic is broadcast-mode network coding. This concerns applications in ad hoc wireless networks. Subtopics include: (i) Linear network coding (ii) Network capacity (iii) Iterative design<br\/>of broadcast-mode network codes.","title":"Network and Channel Coding Theory and Practice","awardID":"0635385","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["517054","485936"],"PO":["432103"]},"120789":{"abstract":"Wireless networks, sensor networks in particular, are vulnerable to intrusion and attack. Even the strongest encryption and authentication techniques are not adequate to provide security and trust. Because signals propagate in shared media, the very acts of transmission reveal crucial aspects of networking. More challenging for wireless sensor networks is that sensors are not physically secure; An adversary can then gain full control of a fraction of the sensors and program them to attack from within.<br\/><br\/>This research investigates the theory and practical techniques of secure wireless and sensor networking. Statistical signal processing and machine learning techniques are developed to extract networking information. Countermeasures against traffic analysis are developed using network coding and randomized mixing techniques. Tradeoffs between stable throughput and secrecy (with delay constraints) are characterized theoretically. <br\/><br\/>For sensor networks performing signal processing tasks, optimal attacking strategies using Byzantine sensors are investigated. Countermeasures are developed using robust and universal detection and estimation techniques. Collaborative sensor networking strategies are also investigated to counter Byzantine attacks.","title":"A Statistical Signal Processing Framework for Secure Wireless and Sensor Networking","awardID":"0635070","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["531874"],"PO":["564898"]},"120800":{"abstract":"Abstract for NSF Grant 0635121 - Approximation algorithms for discrete stochastic and deterministic optimization problems<br\/>David B. Shmoys<br\/><br\/><br\/>Intellectual Merit - For most applications in which optimization algorithms are employed in industry, the input data is actually not known. This might be because the data is based on measurements, which are estimates due to noise, or because only a forecast of future data is available. Stochastic optimization models incorporate this uncertainty as part of the input, and are harder to solve than their deterministic counterparts. This research investigates algorithms that compute provably near-optimal solutions for <br\/>stochastic optimization problems; in contrast, traditional approaches mostly show convergence results without guarantees to efficiently produce near-optimal solutions. Broader Impact - Finding good approaches to gain new efficiencies in logistical planning is important for the overall US economy. This research develops new algorithmic techniques to do this. By studying simplified models, this research devises algorithmic paradigms that can then be applied in more concrete applications, thereby providing industry <br\/>with better solutions. Furthermore, it is important that the US workforce has sufficient expertise to meet the technological challenges of the coming century, and by integrating this research with the training of both graduate and undergraduate students, this work helps to meet this need in maintaining the economic competitiveness of the US.<br\/><br\/>This research addresses a number of specific discrete stochastic optimization problems, focusing primarily on problems from logistics: routing, scheduling, inventory management and network design. This research focuses on a \"black box\" that specifies the probabilistic input by means of polynomial independent <br\/>samples from the underlying distribution; this is important when one has access to historical data. This work studies the stochastic TSP, max cut, and single-machine scheduling problems; 2-stage with recourse models of the survivable network design and maximum job selection problems, and multistage <br\/>stochastic models from inventory control, options pricing, and AdWords bidding. This research also studies the deterministic asymmetric TSP, bin-packing, and capacitated facility-location problems.","title":"Approximation algorithms for discrete stochastic and deterministic optimization problems","awardID":"0635121","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["562312"],"PO":["562944"]},"120757":{"abstract":"In large and complex communication networks, architectural decisions regarding functionality allocation are extremely important. The time is ripe for building a scientific foundation for network architectures, both to capitalize on unique clean-slate design opportunities (such as GENI and MANET) and to guide the evolution from existing network architectures to new ones. Such a foundation can lead to highly efficient, robust, and scalable protocols that could have a significant impact on the communications industry.<br\/><br\/>The recent successes of understanding protocols as optimizers and layering as mathematical decompositions offer a promising starting point for such an analytic foundation one that is conceptually unifying, mathematically rigorous, and practically relevant. However, there is still much work to be done in developing an analytic foundation for network architectures. This research focuses on three main thrusts: <br\/><br\/>Alternative architectural choices: Past mathematical results have focused on one architecture derived from a particular decomposition. There is in fact a wide range of alternative decompositions that result in different scalability, convergence, and complexity tradeoffs. This research systematically explores architectural choices using appropriate decompositions.<br\/><br\/>Stochastic network dynamics: This research develops new architectural designs taking into account stochastic (rather than deterministic) network dynamics, which are critical in modeling real systems and in developing high-performance network architectures. <br\/><br\/>Non-convexity and robustness: Non-convexity persists in real networks, which could lead to instability, poor performance, and impractical computational complexity. Nonetheless, most past results have been derived only for the convex case. This research explores architectural choices that are robust to non-convexity.","title":"Collaborative Research: Towards An Analytic Foundation for Network Architectures","awardID":"0634898","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["560222"],"PO":["432103"]},"120768":{"abstract":"The research. The research topics in this proposal are: 1) Fundamentals for the wiretap channel:<br\/>error control codes for reliability and security, 2) Opportunistic wireless physical-layer security, 3) Highly<br\/>reliable and secure distributed data storage. In all areas the focus is not only on theory but on practical<br\/>algorithms to achieve the very high levels of security and reliability promised by information theory.<br\/>The PI and collaborations: About half of the work in this proposal will be done with collaborators<br\/>in Europe connected to the Georgia Tech Lorraine (GTL) campus in Metz, France where the PI was<br\/>formerly Director of Research and currently Deputy Director. In March 2006, the PI and his colleagues, in<br\/>partnership with the French Centre National de Recherche Scientifique (CNRS), established a joint research<br\/>center dedicated to international collaborations in secure networks and advanced materials, the first such<br\/>center of its kind in France. This center hosts fifteen people and ten projects from four institutions. The<br\/>wireless portion of this proposal will be done in collaboration with colleagues at the University of Porto (J.<br\/>Barros) and Cambridge University (M. Rodrigues). The international component of our project creates<br\/>opportunities and challenges. For example, the landscape for recruiting and educating graduate students<br\/>today is characterized by i) decreases in the students applying to US universities, particularly from China<br\/>and India, and ii) a lack of US students getting meaningful engineering educational and research experience<br\/>outside the US. We believe this project, coupled with the GTL campus in France, can address some of<br\/>these challenges and help create future engineers, educators and researchers with a more global perspective.<br\/>The broader impact of this project has two dimensions: i) global connections with a pathway to<br\/>commercialization of new wireless security concepts; and ii) new models for training graduate students<br\/>across national boundaries. The students and faculty would participate in new and long-lasting international<br\/>research and educational activities. These students will have a much more global perspective, have the<br\/>opportunity to interact on a long-term basis with European students and faculty, and the project would<br\/>serve to identify (for companies, universities and government agencies) outstanding people with a global<br\/>perspective. This model addresses the changing landscape and needs of student and faculty in the US and<br\/>abroad.<br\/>The intellectual merit is in new approaches to physical layer communications, from both theoretical<br\/>and practical perspectives. The notion of a highly secure and reliable physical layer has the potential to<br\/>significantly change how people think of the physical layer. The error control codes in this work will have<br\/>the dual roles of reliability and security, adding a new dimension to a subject that has been studied<br\/>extensively for more than 60 years. We believe the research presented here will result in changes in how<br\/>classical communication systems are perceived and used, and is the next natural step in higher<br\/>performance, physical layer communication systems.","title":"Physical Layer Security: Error Control Coding for Information Theoretic Security in Wireless and Beyond","awardID":"0634952","effectiveDate":"2006-10-01","expirationDate":"2010-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["386026"],"PO":["432103"]},"120779":{"abstract":"Title: Collaborative Research: Cognitive Ad Hoc Networks: Capacity Optimization Through Local Adaptation<br\/><br\/>0635003, Weber<br\/>0634979, Andrews<br\/>0634763, Jindal<br\/><br\/>Due to the unpredictability of the environment in which unplanned (ad hoc) wireless networks will operate, an appealing approach is to allow the network to dynamically adapt to the perceived conditions. We define such ad hoc networks as cognitive. A framework is developed for understanding the benefits of local adaptation, by breaking adaptive techniques into the four major degrees of freedom available to the designer: time, frequency, code, and space. The aim is to address the following two questions. First, what are the fundamental limits on information flow through unplanned networks; in particular, how valuable is localized information and coordination in seeking to achieve this limit? Second, what are the relative values of adaptation in time, space, frequency, and code in terms of information flow; in particular, how does the network designer identify which degree of freedom is most valuable in a variety of networking scenarios?<br\/><br\/>In this research, information theory and stochastic geometry are connected through a novel metric for ad hoc network capacity, termed the transmission capacity. This metric captures the maximum spatial intensity of transmissions subject to a specified outage probability. While related to other popular capacity metrics, notably the transport capacity, the transmission capacity is unique in its allowance of explicit and accurate characterization of capacity for any conceivable communication scheme or transmission environment. The transmission capacity is an indispensable unifying metric for this analysis since i) it allows closed-form results, ii) does not require any global coordination or optimization, iii) accurately models the interference environment of an ad hoc network. These innovations will allow a basis for more efficient wireless network design.","title":"Collaborative Research: Cognititve Ad Hoc Networks: Capacity Optimization Through Local Adaptation","awardID":"0635003","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["533238"],"PO":["432103"]},"120801":{"abstract":"The design of efficient, and low complexity, coding and decoding algorithms is a critical component in<br\/>the optimization of wireless and wire-line networks. Recent years have witnessed the emergence of new<br\/>powerful paradigms, e.g., belief propagation, as well as the revival of classical approaches, e.g., sequential<br\/>decoding, for the design of decoding algorithms. This proposal seeks a deeper theoretical foundation for the<br\/>design and analysis of efficient decoding algorithms, along with the associated code ensembles, by building<br\/>on this recent progress.<br\/>The overarching goal of the proposed research is to unify under one framework the class of algorithms<br\/>that exploit the power of local constraints, e.g., the linear programming or belief propagation decoders and<br\/>generalization thereof, with the class of algorithms that optimize a global cost function, e.g., tree search<br\/>decoders. The proposed research benefits from our preliminary results that establish interesting connections<br\/>between different decoding schemes (as detailed in the sequel). Our results are expected to elucidate the<br\/>properties of known algorithms and inspire the design of more powerful decoders. Our investigations<br\/>will start with the traditional point-to-point scenario where the complexity of the model ranges from the<br\/>simplified binary erasure channel to the challenging multi-path fading channel. In the latter case, our<br\/>work will target a joint detection and decoding approach and consider, explicitly, the associated signal<br\/>processing tasks, e.g., the preprocessing stage which will be defined rigorously in the sequel.<br\/>Our research will then proceed to investigate three distinct categories of multi-user channels. The<br\/>first thrust will focus on the outage-limited multiple access channel where efficient decoding algorithms<br\/>that achieve the optimal diversity-multiplexing tradeoff will be pursued. The same line of work will also<br\/>seek code ensembles and decoding algorithms that achieve the capacity of the compound multiple access<br\/>channel (CMAC), one of the few examples where the celebrated successive cancellation decoder fails to<br\/>achieve capacity. The CMAC model captures the essential characteristics of delay-limited multiple access<br\/>channels with limited transmitter channel state information. The study of the broadcast channel (i.e.,<br\/>downlink) is our second research thrust where low complexity and high performance dirty paper codes<br\/>and rateless codes will be constructed. Within this context, classical works on tree search source coding<br\/>will be revisited in order to construct low complexity vector quantization stage for the dirty-paper coding<br\/>scheme. The third research thrust will be devoted to cooperative channels. Here, our efforts will seek<br\/>to characterize the diversity-multiplexing tradeoff of the cooperative interference channel along with the<br\/>associated optimal coding and decoding algorithms. This work will build on the intimate relationship<br\/>between the CMAC and the interference channel. Finally, inspired by our recent work in the area, novel<br\/>approaches for cooperative lattice coding and decoding algorithms will be unveiled.<br\/>The intellectual merit of the proposed research lies in the fundamental nature of the posed questions<br\/>and the expected contributions to information and coding theories. New bridges between information<br\/>theory, from one side, and optimization and algebraic number theories, from the other side, will be built.<br\/>Furthermore, analytical tools, as well as efficient numerical methods, that aid in the design and analysis<br\/>of decoding algorithms will be pursued.<br\/>The PIs, with collective expertise in information theory, coding theory, wireless communications, and<br\/>signal processing, form a qualified team for undertaking the research plan. This project builds on recent<br\/>results from the PIs' ongoing collaboration. The proposed theoretical research is expected to have a broad<br\/>impact on the information theory and signal processing communities. Furthermore, the anticipated results<br\/>will contribute to the design of efficient encoding\/decoding algorithms which percolate through many<br\/>applications (please refer to the support letters from Texas Instruments and ST Microelectronics). On the<br\/>educational front, graduate students at the two participating institutions will be heavily involved in the<br\/>research activities.","title":"Collaborative Research: Towards a Unified Framework for Decoding Algorithms: Unveiling the Hidden Connections","awardID":"0635127","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["347888"],"PO":["564924"]},"120812":{"abstract":"Recent years have seen the advent of sophisticated multiple-input, multiple-output (MIMO) technology for communication networks. Achieving the substantial performance gains promised by such technology requires not only the development of suitable network protocols, but also rethinking aspects of the associated network architecture based on careful consideration of issues that transcend individual layers in the hierarchy. To date, most effort has focused on the physical and medium-access layers, emphasizing coding and multiplexing issues. However, there is an emerging appreciation that the issues at the higher networking layers are at least as important and equally rich, and that the associated broader view of MIMO network design challenge is critical to realizing the performance potential of this technology. The proposed research is aimed at precisely these challenges, and will start from fundamental principles to the develop key insights from which efficient and robust networking protocols and resource management algorithms will follow. With the strong interest in the development of standards such as 802.16 and 802.11n, among many others, such research is especially timely and the potential for practical impact is particularly high.<br\/><br\/>This broader research theme will be approached by investigating several specific questions and issues. Of central importance is the scheduling problem: given a MIMO link for delivering data to a collection of endusers, what are the fundamental tradeoffs between throughput, delay, reliability, fairness, and complexity achievable in such systems? In our preliminary research, we have established that fundamentally new spatiotemporal scheduling protocols can and must be developed, and that the structure of such protocols is strikingly different from those in their non-MIMO counterparts. In the proposed research, we will investigate issues ranging from how users should be selected in any given transmission interval, to how feedback should be exploited, to how training should be accomplished.<br\/><br\/>The research will further pursue the still richer problems when multiple MIMO links of this type are<br\/>used in conjunction with one another, as is the case in any practical network. In such settings, cooperation and relaying become integral aspects of efficient scheduling and resource management, and the research will pursue the development of such algorithms in detail. Throughout our investigation, we will take into account the strong network dynamics, including multipath fading and shadowing, rapid fluctuations in user connectivity and populations, and the bursty nature of traffic in such networks.<br\/><br\/>Distinguishing features of our research include its emphasis on 1) stronger notions of optimality than have been considered to date; 2) its finer grain analysis beyond simple asymptotics; and 3) its tight integration of traditional information-theoretic and communication-theoretic treatments with queuing-theoretic, scheduling-theoretic and algorithm-theoretic treatments in the protocol design and analysis. Indeed, more generally the spectrum of issues to be investigated will require the coordinated application of a wide range of design and analysis tools, making the research highly interdisciplinary in nature. Indeed, these tools will be drawn from information theory, queuing theory, communication theory, graph theory, coding theory, algorithms and complexity theory, and estimation theory and machine learning.<br\/><br\/>Intellectual Merit: The proposed research will significantly advance the fundamental theory and practice of networking in systems exploiting MIMO technology, and to provide a solid foundation for the development of the higher performance networking infrastructure needed to enable a new generation of applications.<br\/><br\/>Broader Impacts: The proposed research is a highly education-centered activity with a focus around the professional and intellectual development of students. Through outreach efforts, the PI's will continue their strong commitment to involving in their research programs students from groups traditionally underrepresented in engineering. The PI's are also strongly committed to the integration of the results of this research into the curriculum, and in particular to new networking subjects under development at their respective institutions that specifically integrate network- and physical-layer design and analysis. Finally, the research<br\/>program will help foster important new interlaboratory research collaboration between two academic institutions for which there is the potential for strong partnership, as well as with additional key industrial partners. In particular, the proposed effort will foster closer research and educational ties between OSU and MIT in the communication and networking area, and rapid transition of the resulting research results to practice through close interaction with the networking laboratories of Hewlett-Packa","title":"Collaborative Research: MIMO Networking: From Principles to Protocols","awardID":"0635191","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["550597"],"PO":["432103"]},"123739":{"abstract":"CCF - 0649824<br\/>Title: SGER - Dynamic hardware adaptation of high performance CMPs for managing thermal hotspots<br\/>PI: Kundu, Sandip<br\/>Inst: University of Massachusetts Amherst<br\/><br\/><br\/>Abstract<br\/><br\/>As the transistor dimensions shrink, more devices are integrated per unit area, driving up power consumption and heat dissipation of a chip on a per unit area basis. A software program executing on a processor can be tailored to create a thermal hot spot on a specific area of a chip. Performance of CMOS circuits depends on temperature. Large temperature differentials caused by hot spots often result in functional failures due to signal race introduced by such changes in temperature. Hot spots also cause long term reliability problems. In this proposal, we are addressing this problem for a multi-core processor by monitoring software execution, measuring the rate of change of temperature as the software executes, predicting potential problems and avoiding them by various intercept methods including voltage and frequency control, suspension, switching thread to a different core etc. The monitoring process executes on the same multi-core processors and is hidden from operating systems or applications. This scheme of monitoring and adapting the hardware","title":"SGER: Dynamic hardware adaptation of high performance CMPs for managing thermal hotspots","awardID":"0649824","effectiveDate":"2006-10-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["509294"],"PO":["562984"]},"120824":{"abstract":"ABSTRACT<br\/>0635271<br\/>Alexander Barg<br\/>U of Maryland College Park<br\/><br\/>Collaborative Research: Digital Fingerprinting: Information Theoretic Analysis<br\/>and Code Design<br\/><br\/>The problem of fingerprinting for copyright protection has come to the forefront of research in information<br\/>and coding theory, motivated by varied applications such as digital fingerprinting of software, images, audio<br\/>signals as well as multimedia fingerprinting for video-on-demand and related applications. Copies of data<br\/>distributed to the users of the system are protected by digital fingerprints,i.e., short strings of bits designed<br\/>individually for each user of the system in such a way that individual users cannot redistribute their contents<br\/>without being traced by the distributor of the system. The tracing becomes substantially more difficult when<br\/>coalitions of users cooperate to create a pirate copy of the data, attempting to obfuscate their fingerprints. The<br\/>prime objective of this research is to use information-theoretic and coding methods to establish the maximum<br\/>number of users of a system with reliably fingerprinted data and to construct new, large-size fingerprinting<br\/>schemes.<br\/>This research considers two main models of the fingerprinting system, one in which the pirates first detect<br\/>the positions of the fingerprint by comparing their copies of the data, and then form a fingerprint for the unregistered copy, and the other in which they are allowed to alter their data files as long as the unregistered copy created satisfies some distortion constraints as compared to their original copies. Representing fingeprinting systems as multiple-access channels, either fixed or time-varying, this research establishes the capacity limits of fingerprinting under these two models. New fingerprinting codes are constructed equipped with efficient<br\/>identification\/tracing algorithms and related performance bounds. Finally, this research investigates the properties<br\/>and designs new families of combinatorial coding schemes such as separating and frameproof codes, codes<br\/>with the identifying parent property, and traceability codes.","title":"Collaborative Research: Digital Fingerprinting: Information-Theoretic Analysis and Code Design","awardID":"0635271","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["561385","518024"],"PO":["432103"]},"121826":{"abstract":"The biological databases provide a rich source of data to serve as training data for statistical and machine learning approaches to text mining; they also provide expert-curated, \"gold standard\" data for evaluation of system performance. The strategy is to focus on problems of importance to working biologists, such as overcoming the curation bottleneck for biological literature, providing better mappings between biological ontologies and text, and giving biologists better access to textual information in both in the literature and in curated databases. This proposal focuses on development of mechanisms to promote progress in text mining to problems of biological significance. The short term focus is to continue work in organizing BioCreAtIvE: Critical Assessment for Information Extraction in Biology. The long term focus is to improve text mining tools to support expert curators in their cost-effective acquisition of information for biological databases, as well as to improve access to biological information via the use of shared semantics (ontologies), with particular focus on interactive tools and extraction of complex relations, such as host-pathogen or ecosystem interactions. The specific tasks proposed here are 1) running the Gene Normalization task for BioCreAtIvE II (to take place in 2006-2007) and analyzing and disseminating the data and results of the BioCreAtIvE II; 2) providing input into the creation of a Roadmap for BioCreAtIvE; 3) defining new evaluation tasks to meet needs of a wider range of biological curators; this will include an evaluation of interactive curation tools, done in conjunction with the RegCreative Jamboree; and methods for the representation and capture of complex biological relations, such as host-pathogen interaction and ecosystem interactions, in conjunction with standards consortia.","title":"Critical Assessment of Information Extraction in Biology","awardID":"0640153","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7602","name":"INFORMATION INTEGRATION"}}],"PIcoPI":["389934"],"PO":["565136"]},"120748":{"abstract":"Abstract<br\/>0634847<br\/>Collaborative Research<br\/>PIs: Andras Farago, The University of Texas at Dallas<br\/> Stefano Basagni, Northeastern University<br\/><br\/>Modeling Networks with Multiple Physical Layers - The Case for Multi-Radio Networks<br\/><br\/>Two major and lasting trends in the networking landscape are the growing importance of wireless networks and the increasing diversity of wireless networking solutions and standards. These trends come hand in hand with the rapidly decreasing cost and shrinking physical size of radio interfaces. It is now technically and economically feasible to put several radio transmitters\/receivers in a single wireless network node. This creates an environment where the network effectively has multiple physical layers. This is expected to become ubiquitous in the future. While the technical possibility of multiple physical layers is already quite clear today, it is much less obvious how can this opportunity be efficiently utilized to gain significant improvement in the network performance. Or, from the practical\/economical point of view, the ultimate question is: will the multiple physical layer (multi-radio) network development lead to sufficient performance improvement that justifies the investment?<br\/><br\/>In this research project the investigators develop and analyze novel mathematical methods that can can quantify the network performance gain that is obtained via multiple physical layers. Specifically, the investigators model the network topology with an edge-labeled multigraph. This model offers surprisingly richer opportunities than the traditional graph model. Using this approach, the investigators study the following main areas: (1) quantifying the multi-radio gain in the network topology; (2) new algorithmic problems at the network layer; (3) new issues in network reliability; and (4) modeling and choosing routes in a mobile environment (5) experimental validation of the results via a testbed built in the project.","title":"Collaborative Research: Modeling Networks with Multiple Physical Layers-The Case for Multi-Radio Networks","awardID":"0634847","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["499280"],"PO":["432103"]},"120759":{"abstract":"Model reduction seeks to replace a large-scale system of differential equations by a system of substantially lower dimension that has response characteristics similar to the original system while requiring far less computational resources. Such large-scale systems often arise through spatial discretization of time dependent PDE systems. For example, in chip manufacturing the physical verification step involves detailed simulation of all constituent components of the chip to verify its behavior. Full simulation is intractable due to computational complexity. A reduced model with guaranteed accuracy is required to complete a reliable simulation in a reasonable period of time. Additional applications of broad impact include: weather prediction, air quality management, micro-electro-mechanical systems, and many others.<br\/><br\/>In general terms, this research is focused on the development, analysis, and implementation of reduction methods for very large problems. Where needed, the work will involve extending the underlying theory of dimension reduction. The primary goal is to provide reliable and efficient dimension reduction methods that preserve structure and system properties with bounds on the approximation error. More specifically, this project is concerned with the investigation of four topics in model reduction. (a) Selection of interpolation points in rational Krylov methods with new applications in reduction methods which (i) preserve dissipativity, and (ii) are optimal in the H2 norm. (b) Provably convergent parameter free large scale Lyapunov solvers with applications to approximate balanced truncation with rigorous error bounds.<br\/>(c) Symmetry preserving principal component analysis for dimension reduction based upon a new symmetry preserving singular value decomposition; and (d) domain decomposition techniques for rapid solution and dimension reduction of large scale systems with application to power grids of VLSI chips and complex building models.","title":"Advanced Projection Techniques for Dimension Reduction of Large Scale Dynamical Systems","awardID":"0634902","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["551029","551030"],"PO":["432103"]},"120716":{"abstract":"This project will attempt to build a full text index to the textual web pages in the historical collections of the Internet Archive. The Internet Archive has taken a snapshot of the web every two months since 1996 and stored it. It now comprises approximately 40 billion web pages, consuming multiple petabytes of storage.<br\/>The resulting index of the project may be the largest and best organized inverted index ever created that is freely available to academic researchers. It will enable social and information scientists to explore altogether new dimensions of contemporary events and practices, while offering information scientists a vital large-scale testing resource in areas such as advanced information retrieval on semistructured collections.","title":"SGER: Exploratory Research: Using the Cuberinfrastructure to build a Full Text Index to the Web","awardID":"0634677","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6857","name":"DIGITAL LIBRARIES AND ARCHIVES"}}],"PIcoPI":["410331"],"PO":["433760"]},"120848":{"abstract":"Fundamental Problems in Classical and Quantum Computation.<br\/><br\/>Abstract: <br\/><br\/>To realize the tremendous computational power of quantum computers there are a number of algorithmic issues that must be addressed. This project studies two major issues. The first is the search for further applications for quantum computers that go beyond the breaking of cryptosystems. i.e. the search for quantum algorithms that provide an exponential speedup over classical algorithms. The second is efficient implementation of fault-tolerant quantum computation. This is an essential step towards the actual experimental realization of quantum computation. The investigators also study the design of classical cryptosystems that are immune to quantum cryptanalysis.o<br\/><br\/>In classical computing, over the last few years there have been breakthrough results on a fundamental algorithmic problem ---approximating graph separators. This project studies both whether the approximation factor can be improved beyond the $\\sqrt{log n}$, whether the running time can be improved beyond $\\tilde{O}(n^{1.5})$ and what insights these new techniques give into the performance of <br\/>practical heuristics such as METIS for this important problem. Finally, the project studies the typical performance of a recently proposed algorithm for the Google AdWords auction.","title":"Fundamental Problems in Classical and Quantum Algorithms","awardID":"0635401","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["402689"],"PO":["565157"]},"120749":{"abstract":"Abstract<br\/>0634848<br\/>Collaborative Research<br\/>PIs: Andras Farago, The University of Texas at Dallas<br\/> Stefano Basagni, Northeastern University<br\/><br\/>Modeling Networks with Multiple Physical Layers - The Case for Multi-Radio Networks<br\/><br\/>Two major and lasting trends in the networking landscape are the growing importance of wireless networks and the increasing diversity of wireless networking solutions and standards. These trends come hand in hand with the rapidly decreasing cost and shrinking physical size of radio interfaces. It is now technically and economically feasible to put several radio transmitters\/receivers in a single wireless network node. This creates an environment where the network effectively has multiple physical layers. This is expected to become ubiquitous in the future. While the technical possibility of multiple physical layers is already quite clear today, it is much less obvious how can this opportunity be efficiently utilized to gain significant improvement in the network performance. Or, from the practical\/economical point of view, the ultimate question is: will the multiple physical layer (multi-radio) network development lead to sufficient performance improvement that justifies the investment?<br\/><br\/>In this research project the investigators develop and analyze novel mathematical methods that can can quantify the network performance gain that is obtained via multiple physical layers. Specifically, the investigators model the network topology with an edge-labeled multigraph. This model offers surprisingly richer opportunities than the traditional graph model. Using this approach, the investigators study the following main areas: (1) quantifying the multi-radio gain in the network topology; (2) new algorithmic problems at the network layer; (3) new issues in network reliability; and (4) modeling and choosing routes in a mobile environment (5) experimental validation of the results via a testbed built in the project.","title":"Collaborative Research: Modeling Networks with Multiple Physical Layers - The Case for Multi-Radio Networks","awardID":"0634848","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["451890"],"PO":["432103"]},"123708":{"abstract":"CCF - 0649691 <br\/>Electronic Information Processing After Scaling: Physical Limits for Technology Assessment<br\/>PI: Anderson, Neal G. <br\/>Inst: University of Massachusetts Amherst<br\/><br\/><br\/>Abstract:<br\/><br\/>As scaling of silicon integrated circuit technology approaches the point of diminishing returns, and proposals for successor technologies proliferate, there will be a growing need for critical assessment of new and unconventional approaches to information processing. New tools will be needed to gauge the ultimate potential of would-be successor technologies to reliably process information at speeds and circuit densities exceeding those that will have been achieved in silicon at the end of scaling. This program aims to develop such tools through extension and application of physical information theory, bringing the power of this theoretical approach to bear on the practical assessment of emerging electronic information processing technologies. Exploration of fundamental limits on signal fidelity and the physical costs of information processing (e.g. in heat dissipation) in technologically relevant settings will receive the highest priority. Success in this program will provide useful approaches for evaluating the information processing capabilities and resource requirements of emerging technologies - including but not limited to charge-based nanoelectronics - and will promote understanding of the physical aspects of information processing and appreciation of their practical consequences.","title":"Electronic Information Processing After Scaling: Physical Limits for Technology Assessment","awardID":"0649691","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["409314"],"PO":["562984"]},"120815":{"abstract":"The emerging areas of Network information theory, distributed signal processing and cooperative communication promise unprecedented capabilities for improving the ways people represent, collect, disseminate and interpret information. They are also revolutionizing the communication and information world by introducing new models, new concepts, new methodologies, and new results. Despite the vast excellent results on the information-theoretic aspects, practical approaches that allow for software or hardware implementation with manageable complexity have been far lagging behind.<br\/><br\/>To bridge the theory-practice gap, this proposal focuses on the algorithmic, schematic and strategic design of efficient and practical coding solutions in a cross-disciplinary context. To utilize channel coding technology in source coding is an interesting, albeit not particularly new, idea, whose efficiency has been exemplified by many recent findings in distributed source coding. Complimentary to the research in that line, here the PI proposes to bring source coding ideas to channel coding by investigating a basic but<br\/>generalizable cooperative communication model where two nodes communicating wirelessly with the assist from a third node. To promote the recently developed notion that \"source coding and channel coding are specialization of network coding\" and that \"network coding is generalization of source coding and channel coding\", the PI further expands the system model to include many communicating and\/or collaborating<br\/>terminals and proposes to exploit source and channel coding ideas in network coding.<br\/><br\/>The proposal starts with motivating the proposed methodologies, followed by a brief discussion of the technical background. After demonstrating the efficiency of her approaches with encouraging preliminary results, the PI proposes and seeks support for<br\/> <br\/>Leveraging useful source coding ideas to develop efficient channel coding solutions: Developing practical and efficient compress-forward user cooperation schemes by means of practical distributed source coding technology at large, and Slepian-Wolf coding and Wyner-Ziv coding in particular; developing guidelines for quantizer design and error control in the new scenario; and establishing significant analytical and algorithmic results.<br\/><br\/> Unifying source and channel coding technology in network coding: Designing practical graph-based network codes that can effectively adapt to the changing network state and topology; unifying source and channel coding and other signal processing technology in network coding design; exploiting joint source, channel and network coding technology in such problems as data gathering, data dissemination and distributed detection; and examining bounded and asymptotic behavior of such communication\/signal-processing systems.<br\/><br\/>Intellectual Merit: The proposed work capitalizes on the PI's extensive experience in coding and cooperative systems, and helps cross-fertilize several important areas in the emerging field of network information theory. The proposal constitutes several refreshing and promising ideas to break the boundary between source coding and channel coding, to exploit the best of each and to incorporate them in network coding. Additionally, through constructive code design in a unified framework, the proposed research allows for the<br\/>development of meaningful analytical results, one that sets more realistic limits and guidelines for practice than pure information-theoretic analysis.<br\/><br\/>Broader impacts: The ideas and results developed in the proposed research have tight relationship with and bear useful implication in a wealth of problems and applications in the emerging area of network information theory, including, for example, multicast and broadcast problems, multiple access channels, coding for memories with defect, sensor networks, multimedia streaming, multi-source data collection and fusion systems and multi-destination data distribution systems. This is particularly important and timely considering that the current knowledge of network information processing is still very incomplete. The proposal also includes a teaching plan that serves as a useful complement to the proposed research activities.","title":"Unifying Source, Channel and Network Coding Technology","awardID":"0635199","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["545094"],"PO":["432103"]},"120805":{"abstract":"Abstract for Proposal No. 0635147:<br\/>Mechanism Design for Profit Maximization<br\/>The Internet, with its varying degrees of collaboration and competition, has become the de facto platform for large scale distributed computing. This has motivated a great deal of research into the design of protocols for resource allocation and electronic commerce among parties with diverse and selfish interests. The investigators continue this line of research, focusing on the study of mechanism design, also known as incentive engineering. A mechanism is a protocol (or algorithm) that is explicitly designed so that rational participants, motivated solely by their self-interest, end up achieving the designer's goals. Research into mechanism design for private value optimization problems is fundamental to the design and effective functioning of systems, such as networks and peer-to-peer systems, systems based on software agents (as studied in artificial intelligence), and systems for data mining and electronic commerce.<br\/>The major intellectual challenges being undertaken include: (1) the design and analysis of new, more effective and efficiently implementable techniques for profit maximization in mechanism design, especially in online and repeated settings; (2) the development of a theory of profit benchmarks; (3) the design and analysis of profit-maximizing pricing schemes; (4) the development and study of alternative solution concepts and appropriate corresponding analysis frameworks; (5) the incorporation of new aspects of utility into the study of mechanism design; (6) the exploration of new design and analysis techniques in repeated games such as the use of low internal regret strategies to achieve approximate correlated equilibria; (7) the development of a theory of reputation; and (8) the theoretical and empirical study of other practical problems with interesting incentive structures, including ad auctions, routing, and backoff in wireless networks.","title":"Mechanism Design for Profit Maximization","awardID":"0635147","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["450784"],"PO":["550329"]},"120816":{"abstract":"In large and complex communication networks, architectural decisions regarding functionality allocation are extremely important. The time is ripe for building a scientific foundation for network architectures, both to capitalize on unique clean-slate design opportunities (such as GENI and MANET) and to guide the evolution from existing network architectures to new ones. Such a foundation can lead to highly efficient, robust, and scalable protocols that could have a significant impact on the communications industry.<br\/><br\/>The recent successes of understanding protocols as optimizers and layering as mathematical decompositions offer a promising starting point for such an analytic foundation one that is conceptually unifying, mathematically rigorous, and practically relevant. However, there is still much work to be done in developing an analytic foundation for network architectures. This research focuses on three main thrusts: <br\/><br\/>Alternative architectural choices: Past mathematical results have focused on one architecture derived from a particular decomposition. There is in fact a wide range of alternative decompositions that result in different scalability, convergence, and complexity tradeoffs. This research systematically explores architectural choices using appropriate decompositions.<br\/><br\/>Stochastic network dynamics: This research develops new architectural designs taking into account stochastic (rather than deterministic) network dynamics, which are critical in modeling real systems and in developing high-performance network architectures. <br\/><br\/>Non-convexity and robustness: Non-convexity persists in real networks, which could lead to instability, poor performance, and impractical computational complexity. Nonetheless, most past results have been derived only for the convex case. This research explores architectural choices that are robust to non-convexity.","title":"Collaborative Research: Towards An Analytic Foundation for Network Architectures","awardID":"0635202","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["548182","548182","382234","382234"],"PO":["564898"]},"120827":{"abstract":"The proposed investigation is a 3 year single investigator proposal involving an academic lead institution (MIT) and participation by personnel from a technology company (IBM).<br\/>The intellectual merit of the proposed research is that it will broaden our foundational understanding of what can be done with code in contrast with black box access to code.<br\/>The broader impact of the proposed research will be through the PI and senior personnel disseminating the knowledge and understanding gained in journal publications, public conference and workshop presentations, and teaching graduate level and undergraduate level courses in theory of computation, cryptography, and network security. Finally, two members of the research team, including the PI, are women.","title":"Program Obfuscation: Foundations and Applications","awardID":"0635297","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":["562010"],"PO":["565157"]},"123809":{"abstract":"A Project Summary<br\/>There are many challenging problems at the interface of computer science and many other<br\/>fields. Recently computer scientists have contributed extensively to areas such as DNA<br\/>sequence assembly, sequence analysis, protein-protein interactions and intra-cellular signal<br\/>communication. However, there are many system-level biology areas which are ripe for<br\/>inter-disciplinary approaches. This proposal explores one such area - the immune system.<br\/>In response to a microbial infection, the immune cells which are highly specific to small<br\/>structural components of the microbe get activated. The process by which this highly parallel<br\/>search gets accomplished will be studied. Another challenging problem is the process by<br\/>which the immune cells develop tolerance. Such a study will be useful in understanding how<br\/>malignant cells become immortalized evading the immune cells.<br\/>Intellectual Merit<br\/>The proposed research will provide a theoretical foundation to very important immunological<br\/>phenomenon. New search algorithms based on spatial and temporal aspects require<br\/>development of new theoretical tools.<br\/>Broader Impacts<br\/>New breakthroughs in cancer treatment will depend upon rapid advances in immunology.<br\/>Even though generic treatments for cancer (radiation treatment for localized forms and<br\/>chemotherapies for metastatic forms) will continue to play dominant roles in cancer treatment,<br\/>more targeted antibody and other immunological modalities will become more widely<br\/>used. These will have potentially fewer side effects compared to generalized chemotherapies.<br\/>A better understanding of the immune responses could result in new approaches for<br\/>coping with spam, phishing and viral attacks of computer systems.<br\/>A-1","title":"Understanding the Immune System Responses","awardID":"0650141","effectiveDate":"2006-10-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[328532],"PO":["409909"]},"120828":{"abstract":"We propose the coordination of many nodes by distributed communication. The idea is to switch attention from exchanging information to setting up cooperative action. This changes network communication into a new problem. For example, suppose one wishes to have each of m nodes perform a different task, where the distribution of tasks is uniform over the m! assignments. Node 1 is specified by nature to do task i, say. How much information must be sent to each of the nodes to achieve this distribution?<br\/><br\/>In general, how much information {Rij}, j = 1, 2..., m, must be conveyed to achieve a specified joint distribution p(y1; y2; :::; ym) at the nodes, given that the values at a certain subset of the nodes is specified? Once the set of all distributions achievable for a given description region {Rij} is found, rate distortion theory becomes a special case. Furthermore, the broadcast channel and all the other multiuser channels in network information theory can be revisited. Here we are interested, not in the number of bits that can be distributed over this network, but rather the set of all coordinated actions that can be achieved.<br\/><br\/>This should have applications in distributed task assignment in a computer, coherent transmission of relay information, cooperative communication, coordination of sensors in a sensor network, and cooperative behavior in a society. What does it take to generate cooperative behavior?<br\/><br\/>Intellectual merit of activity: The proposed work requires investigating the fundamentals of how joint distributions are created under constraints. This should be interesting as well as reasonably fundamental. It will provide guidance to the efficient creation of coordination in networks.<br\/><br\/>Broader impacts resulting from proposed activity: Understanding the generation of joint actions should have impact on communication, coordination of sensor networks, distributed computation, control systems and social networks.","title":"Cooperative Capacity","awardID":"0635318","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[321034],"PO":["432103"]},"120807":{"abstract":"Two-way communication systems, with a transmitter\/receiver at each end, are ubiquitous.<br\/>The bi-directional information flow gives rise to the possibility of using some kind of feedback in order to help combating channel noise and uncertainty.<br\/>It has long been recognized that feedback not only can help increase capacity when the channel has memory, but it may actually lead to decreased complexity and delay in order to meet performance targets. However, the gulf between practice and theory remains wide as far as taking advantage of the capabilities of feedback.<br\/><br\/>This is a research project on both theory-driven design and practice-driven theory of feedback communication systems. Methods of design and analysis are tightly coupled and guided by the teachings of both Shannon theory and modern coding theory.<br\/>Through the synergistic combination of feedback and belief propagation decoding of sparse-graph codes, the projects studies implementable schemes for both reliable transmission and joint source-channel coding that can harness the power of even a modicum of feedback in order to improve delay-rate tradeoffs. To help bridge the gap between theory and practice, a major thrust of the project is on the analysis of technology-independent fundamental limits departing from conventional models and performance measures such as the exponential error rate decrease when transmitting at a fraction of capacity.","title":"Reliable Communication with Feedback: Coding Schemes and Fundamental Limits","awardID":"0635154","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["550344"],"PO":["564898"]},"120818":{"abstract":"Sampling, or the conversion of continuous-domain real-life signals to discrete numbers that can be manipulated by computers, is the essential bridge between the analog and the digital world. Our entire digital information revolution relies on this fundamental process. However, in a wide range of sensing and sampling problems, due to physical constraints or timing requirements only a limited number of measurements can be acquired from the unknown object. A recent breakthrough in mathematics under the name compressed sensing shows that sparse or compressible finite length discrete signals can be recovered from small number of linear, non-adaptive (i.e., universal), and random measurements. This is important, because many signals of interest, including natural images, diagnostic images, videos, speech, music are sparse when represented in an appropriately chosen dictionary. <br\/><br\/><br\/>This research extends the current methods in compressed sensing to other setups that have overwhelming practical significance. These extensions include constrained acquisition, additional statistical prior on sparse signals, and infinite dimensional cases. The specific goals of this project are to: (1) improve signal reconstruction quality; (2) reduce number of measurements required to achieve a specified reconstruction quality; (3) speed up the reconstruction time; and (4) to demonstrate these gains on real applications, and in particular in challenging magnetic resonance imaging applications, including functional imaging of the human brain. These goals are achieved by effectively exploiting additional priors about unknown signals to design optimized acquisition\/sensing schemes that facilitate faster and more accurate reconstruction. The project develops a series of novel algorithms for optimized acquisition design and signal reconstruction. The performance of these methods is theoretically characterized and evaluated in real data and applications.","title":"Practical Compressed Sensing","awardID":"0635234","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["551068","518402"],"PO":["564898"]},"120819":{"abstract":"Recent years have seen the advent of sophisticated multiple-input, multiple-output (MIMO) technology for communication networks. Achieving the substantial performance gains promised by such technology requires not only the development of suitable network protocols, but also rethinking aspects of the associated network architecture based on careful consideration of issues that transcend individual layers in the hierarchy. To date, most effort has focused on the physical and medium-access layers, emphasizing coding and multiplexing issues. However, there is an emerging appreciation that the issues at the higher networking layers are at least as important and equally rich, and that the associated broader view of MIMO network design challenge is critical to realizing the performance potential of this technology. The proposed research is aimed at precisely these challenges, and will start from fundamental principles to the develop key insights from which efficient and robust networking protocols and resource management algorithms will follow. With the strong interest in the development of standards such as 802.16 and 802.11n, among many others, such research is especially timely and the potential for practical impact is particularly high.<br\/><br\/>This broader research theme will be approached by investigating several specific questions and issues. Of central importance is the scheduling problem: given a MIMO link for delivering data to a collection of endusers, what are the fundamental tradeoffs between throughput, delay, reliability, fairness, and complexity achievable in such systems? In our preliminary research, we have established that fundamentally new spatiotemporal scheduling protocols can and must be developed, and that the structure of such protocols is strikingly different from those in their non-MIMO counterparts. In the proposed research, we will investigate issues ranging from how users should be selected in any given transmission interval, to how feedback should be exploited, to how training should be accomplished.<br\/><br\/>The research will further pursue the still richer problems when multiple MIMO links of this type are<br\/>used in conjunction with one another, as is the case in any practical network. In such settings, cooperation and relaying become integral aspects of efficient scheduling and resource management, and the research will pursue the development of such algorithms in detail. Throughout our investigation, we will take into account the strong network dynamics, including multipath fading and shadowing, rapid fluctuations in user connectivity and populations, and the bursty nature of traffic in such networks.<br\/><br\/>Distinguishing features of our research include its emphasis on 1) stronger notions of optimality than have been considered to date; 2) its finer grain analysis beyond simple asymptotics; and 3) its tight integration of traditional information-theoretic and communication-theoretic treatments with queuing-theoretic, scheduling-theoretic and algorithm-theoretic treatments in the protocol design and analysis. Indeed, more generally the spectrum of issues to be investigated will require the coordinated application of a wide range of design and analysis tools, making the research highly interdisciplinary in nature. Indeed, these tools will be drawn from information theory, queuing theory, communication theory, graph theory, coding theory, algorithms and complexity theory, and estimation theory and machine learning.<br\/><br\/>Intellectual Merit: The proposed research will significantly advance the fundamental theory and practice of networking in systems exploiting MIMO technology, and to provide a solid foundation for the development of the higher performance networking infrastructure needed to enable a new generation of applications.<br\/><br\/>Broader Impacts: The proposed research is a highly education-centered activity with a focus around the professional and intellectual development of students. Through outreach efforts, the PI's will continue their strong commitment to involving in their research programs students from groups traditionally underrepresented in engineering. The PI's are also strongly committed to the integration of the results of this research into the curriculum, and in particular to new networking subjects under development at their respective institutions that specifically integrate network- and physical-layer design and analysis. Finally, the research<br\/>program will help foster important new interlaboratory research collaboration between two academic institutions for which there is the potential for strong partnership, as well as with additional key industrial partners. In particular, the proposed effort will foster closer research and educational ties between OSU and MIT in the communication and networking area, and rapid transition of the resulting research results to practice through close interaction with the networking laboratories of Hewlett-Packa","title":"Collaborative Research: MIMO Networking: From Principles to Protocols","awardID":"0635242","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[321011,"506208"],"PO":["432103"]},"119281":{"abstract":"Many compromised computers today generate maltraffic, such as denial-of-service (DoS) attacks, spyware reporting home, unauthorized applications, spam, and worms. Current defenses are becoming increasingly brittle. There are several reasons for this challenge: encryption limits packet content inspecting, aggregation at network edge limits use of filtering and blacklisting due to potential collateral damage, increased traffic volumes allow maltraffic to hide, and applications are often cloaked through layered protocols (SOAP over HTTP or varying port allocation) or active concealment.<br\/><br\/>This proposal applies signal processing and detection theory to network traffic to detect maltraffic in these challenging scenarios. We will use features such as packet timing and frequency, careful design of the measurement and detection systems, and study of inherent behaviors in protocols to address these challenges.<br\/><br\/>Broader Impact: The results of this work will include (a) the development of a systematic methodology for applying signal processing methods to network traffic; (b) the analysis of new signal representation and detection methods specific to maltraffic; and (c) the identification, understanding, and modeling of key identifying features and inherent behaviors of maltraffic and how they are shaped by the network. Our new approaches will yield a deeper understanding of network traffic, and will be tested with traces of real network traffic, resulting in new tools to combat these problems.","title":"NeTs-NBD: Maltraffic Analysis and Detection in Challenging and Aggregate Traffic (MADCAT)","awardID":"0626696","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["515828","451991","377594","559198"],"PO":["565090"]},"118270":{"abstract":"Disk-array performance is the primary bottleneck for data-intensive applications, and significant challenges remain in system modeling, algorithm design, and performance optimization of data-intensive applications. Existing analytical models do not incorporate application characteristics, internal disk behavior, and I\/O interconnection network contention. <br\/>The PIs propose to develop an integrated execution-driven simulation environment that incorporates optimization approach driven by application characterization. Unlike traditional worst-case studies that assume adversarial request sequences and lookahead, the proposed approach captures the predictability of the request sequences of real applications, and is expected to lead to the development of better algorithms for applications with large data sets that depend on secondary storage.<br\/>The proposed environment will be based on a unified and flexible disk-array access model that provides a simple framework for algorithm designers to reason about the complexity of algorithms on disk arrays by incorporating contention on the interconnection network between disks and memory, and consider the complex interactions within a disk access to enhance accuracy over traditional models that assume a flat unit cost per disk block access.","title":"Performance Models and Systems Optimization for Disk-Bound Applications","awardID":"0621457","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":[313394,"550778","517389","518519","518316"],"PO":["565272"]},"118292":{"abstract":"Storage systems for large distributed clusters of computer servers are themselves large and distributed. Their complexity and scale make it hard to ensure that applications using them get good, predictable performance. At the same time, shared access to the system from multiple applications, users, and internal system activities leads to a need for predictable performance.<br\/><br\/><br\/>This research investigates mechanisms for improving storage system performance in large distributed storage systems through mechanisms that integrate the performance aspects of the path that I\/O operations take through the system, from the application interface on the compute server, through the network, to the storate servers. The research focuses on five parts of the I\/O path in a distributed storage system: I\/O scheduling at the storage server, storage server cache management, client-to-server network flow control, client-to-server connection management, and client cache management.","title":"End-to-End Performance Management for Large Distributed Storage","awardID":"0621534","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"H370","name":"DEPT OF ENERGY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"S098","name":"DEPT OF ENERGY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T693","name":"ENERGY-PETASCALE STORAGE FOR H"}}],"PIcoPI":["540846","557751","557748"],"PO":["565272"]},"119282":{"abstract":"Sensor networks today are many wireless islands, each isolated and connected through the Internet to its owner, not easily usable by others. The goal of NeTS-FIND: Sensor-Internet Sharing and Search (SISS) is to allow owners to easily bridge these sensornet islands, sharing their data with anyone on the Internet, and to help guests discover and explore these shared sensor data. This research therefore seeks to meld evolving sensornet and Internet architectures, with new approaches to:<br\/><br\/>a. allow easy integration of new sensornets into the Internet and integration of sensornets bridged by the Internet;<br\/><br\/>b. manage disclosure of sensor data, supporting confidentiality, estimates of data quality, and visualization of trends;<br\/><br\/>c. support sensor discovery, linking and search.<br\/><br\/>The results of this work are expected to include new protocols for sharing sensor data streams, for archiving sensor data on the Internet, and for letting users search, aggregate, and process these streams. The PIs will explore this research by designing new algorithms, evaluating them through simulation and prototype implementation. The PIs expect to specify the protocols and elements that enable this work, enabling future optimization of individual components.<br\/><br\/>Beyond the immediate impact of this research, the goal of this research is to enable the \"citizen scientist\", allowing amateurs to contribute sensor data and process sensor streams.","title":"NeTS-FIND: Sensor-Internet Sharing and Search","awardID":"0626702","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7631","name":"ITR-NeTS"}}],"PIcoPI":["537785","377594",316294],"PO":["565090"]},"119470":{"abstract":"Proposal Number: 0627783<br\/>PI: Giovanni Vigna <br\/>Institution: University of California, Santa Barbara <br\/>Title: Using Structural and Behavioral Models to Detect Malware <br\/><br\/>Abstract<br\/><br\/>In the past few years, malicious software (malware) has evolved and diversified. Simple viruses that attach to existing programs and require action on behalf of a user to execute have evolved into worms that can infect<br\/>thousands of hosts and disrupt entire networks. Rootkits and Trojan horses have evolved from simple programs that mimic legitimate applications for malicious purposes into sophisticated software components that operate at the OS kernel level, making it difficult to detect and eradicate them. In addition, completely new types of malware, such as spyware and adware, have emerged.<br\/><br\/>Unfortunately, the evolution of malware has not been matched by a corresponding evolution in defense tools. Most malware detection tools are syntax-based and use relatively simple pattern matching techniques. These<br\/>techniques can only recognize known malware, and, in addition, they can be easily fooled by sophisticated malware that uses obfuscation and polymorphic techniques.<br\/><br\/>To be able to reliably detect sophisticated malware it is necessary to develop analysis techniques that rely on information other than the syntactic structure of the program. Therefore, this research effort will focus on<br\/>developing novel binary analysis techniques that use structural and behavioral models to detect sophisticated malware. More precisely, the proposed techniques use a composition of static and dynamic analysis to identify code whose behavior is similar to the behavior of malware such as viruses, worms, spyware programs, or rootkits. This approach will allow one to detect previously unseen malware and identify mutations of known malware.","title":"CT-T: Using Structural and Behavioral Models to Detect Malware","awardID":"0627783","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["402618","535035"],"PO":["521752"]},"118360":{"abstract":"With the success of the Human Genome Project, a microarray can now potentially handle the genes in an entire genome scale. A typical microarray data set involves a massive number of genes. A dramatic dimension reduction to a much smaller number of significant genes, responsible for specific conditions, can potentially increase the possibility of further biological study and knowledge regarding the roles of specific genes.<br\/><br\/>Any methodology that can improve our recognition of significant genes among a large number of genes, and often a limited set of available experimental results, could have a significant impact on our understanding of diseased and normal states, and eventually on diagnosis, prognosis, and drug design. The method that we propose to investigate here is intended to provide critical information on the roles of genes where the key component of our approach is subspace-based methods, which have demonstrated great success in numerous pattern recognition tasks including efficient classification, clustering, and fast search.<br\/><br\/>The development of effective computer-based algorithms for gene selection is indispensable since it is virtually impossible to rely solely on biological testing due to the enormous complexity of the problems. What is novel and unique in our proposed research is that we seek to find a mathematically rigorous framework that models gene selection problems, with careful consideration of the significance of the biological characteristics of the problem. Utilizing our knowledge and previous results on feature extraction, and by discovering their mathematical relationship to feature selection, efficient and effective nonparametric methods for gene selection will be designed. An important role will be played by the nonnegative matrix factorization in building a mathematically rigorous bridge between feature extraction and feature selection in our proposed research. In the process, we will also explore novel methods for estimating missing values as a preprocessing stage of gene selection based on the alternating least squares and the structured total least norm formulations. All results obtained, the new algorithms and software developed, as well as the new data sets generated and compiled will be made available to the research community, to teaching faculty, and to both graduate and undergraduate students, using existing Web servers at the Georgia Institute of Technology and University of Texas at Dallas.<br\/><br\/>Intellectual Merit: This research will produce methods that will have a great impact on computational microarray analysis. The gene selection and missing value estimation methods developed in this research allow significant reduction in complexity of biological testing due to the initial reduction of the problem dimension, thus substantially improve detailed study of significant genes. The feature selection and feature extraction algorithms developed in this research will be applicable to many other problems where data sets in high dimensional spaces need to be handled efficiently and effectively, such as text processing, facial recognition, finger print classification, iris recognition. The missing value estimation methods designed in this research can also be utilized in recovering missing data such as in collaborative filtering.<br\/><br\/>Broader Impact: The research will enhance advanced theory of computational biology and bioinformatics. The developed techniques will also have potential applications in database management, medical examination and diagnosis, bio-chemical selection, and biological networks. The graduate student involvement in this research will have numerous future benefits. The discovery and research experience of the students will prepare them for productive careers in academia, research labs, and industry in highly important current research areas in bioinformatics.","title":"CompBio: Collaborative Research: Development of Effective Gene Selection Algorithms for Microarray Data Analysis","awardID":"0621889","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["536648","562362"],"PO":["565223"]},"118393":{"abstract":"In eukaryotic organisms, evolution is mostly a process of vertical descent; in other words, genetic material is inherited from ancestors to descendants. In bacteria, on the other hand, genetic material may also be transferred \"horizontally\" among distantly related organisms. This process, known as horizontal gene transfer (HGT) and which is believed to be ubiquitous among various groups of bacteria, plays a major role in genome diversification as well as other bacteria-related issues, such as antibiotic resistance acquirement. <br\/><br\/>In this project, we propose a set of methodologies to enable efficient and accurate phylogeny-based reconstruction of HGT in bacterial genome. In the phylogeny-based approach, a tree is built for each gene in the genome of a group of bacteria, and the gene tree disagreements are quantified and analyzed to detect HGT. Two major challenges face this approach:<br\/><br\/>1. Gene trees may disagree not only due to HGT, but also due to other processes, such as gene duplication and loss, lineage sorting, etc. Hence, it is imperative to develop a framework that determines the cause of gene tree disagreements before any estimates of HGT events are made.<br\/><br\/>2. Comparing and reconciling gene trees to obtain estimates of HGT events and rates are computationally very hard tasks.<br\/><br\/>To ameliorate these two challenges, we propose to develop a stochastic framework that incorporates population genetics theories with evolutionary events that act among species to classify gene tree disagreements. Further, we will develop mathematical criteria and algorithmic techniques for comparing gene trees, quantifying their disagreements, and inferring the numbers and locations of HGT events. <br\/><br\/>All the models and algorithms will be implemented in a software package, Sequoia, which will be made publicly available through open source mechanisms.","title":"Comp Bio: Collaborative Research: EMT: \"Efficient Techniques for Reconstructing Horizontal Gene Transfer in Bacteria\"","awardID":"0622037","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["541859"],"PO":["565223"]},"118283":{"abstract":"This research explores design and implementation strategies for insulating the performance of high-end computing applications sharing a cluster storage system. In particular, such sharing should not cause unexpected inefficiency. While each application may see lower performance, due to only getting a fraction of the total attention of the I\/O system, none should see less work accomplished than the fraction it receives. Ideally, no I\/O resources should be wasted due to interference between applications, and the I\/O performance achieved by a set of applications should be predictable fractions of their non-sharing performance. Unfortunately, neither is true of most storage systems, complicating administration and penalizing those that share storage infrastructures.<br\/><br\/>Accomplishing the desired insulation and predictability requires cache management, disk layout, disk scheduling, and storage-node selection policies that explicitly avoid interference. This research combines and builds on techniques from database systems (e.g., access pattern shaping and query-specific cache management) and storage\/file systems (e.g., disk scheduling and storage-node selection). Two specific techniques are: (1) Using prefetching and write-back that is aware of the applications associated with data and requests, efficiency-reducing interleaving can be avoided; (2) Partitioning the cache space based on per-workload benefits, determined by recognizing each workload's access pattern, one application's data cannot get an unbounded footprint in the storage server cache.","title":"Performance Insulation and Predictability for Shared Cluster Storage","awardID":"0621499","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["342331","465808","486331"],"PO":["565272"]},"109384":{"abstract":"The goal of this project is to provide new foundations for a more robust decision theory and game theory that, among other things, does not assume that the state and outcome space are known, nor that the game is common knowledge. The proposed framework would allow for the incorporation of unanticipated observations, deal with the fact that different agents can be aware of different features of the world and the game, even to the extent that they might not be aware of all the possible moves that they can make or of which agents are playing the game, allow for the possibility that agents might use different languages to describe a decision problem or game, and allow for the fact that part of learning involves modifying the language (i.e., learning new concepts). The project will use techniques that involve viewing acts as syntactic programs and representing the (lack of) awareness of agents in a formal logic. The broader impact of the proposed work includes making decision theory and game theory more robust, and thus far more applicable in complex environments ranging from exploration of Mars by robots (where there are bound to be unanticipated events) to auctions in large peer-to-peer networks (where the identity of all the players is certainly not commonly known).","title":"Taking Awareness, Language, and Novelty into Account in Decision-Making and Game Theory","awardID":"0534064","effectiveDate":"2006-10-15","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["516488"],"PO":["491702"]},"119340":{"abstract":"Proposal Number: 0627028<br\/>Investigators: Viktor Prasanna (PI), Bhaskar Krishnamachari (Co-PI)<br\/>Institution: University of Southern California<br\/>Title: NeTS-NOSS: A Middleware Framework for Rapid Composition and Deployment of Compute-Intensive Networked Embedded Systems<br\/><br\/><br\/>Abstract<br\/><br\/>Wireless sensor networks (WSN) have the potential to revolutionize data collection and analysis in physical sciences and other fields by allowing intelligent dense monitoring of the environment. The primary focus of WSN research till now has been the design and implementation of the basic sensor node hardware and low-level protocols such as those for localization, time synchronization, medium access, routing, etc. However, the composition and deployment of a complex networked sensing application is still a daunting task for the non-expert end user. <br\/><br\/>This project involves the design and evaluation of reusable middleware functions that provide easy-to-program abstractions of the underlying hardware and network services, allowing rapid composition and deployment of sensor network applications. The topics addressed by our research include the development and evaluation of suitable topological abstractions, task mapping and migration techniques, communication and computational primitives, and realistic performance models to evaluate designs.<br\/><br\/>We expect the middleware techniques developed in this project to significantly enhance the state of the art in the design of sensor networks for complex applications. The abstractions and tools we develop will help establish a paradigm shift from the current dependence on application-specific customized solutions to a generalized automated approach that facilitates rapid design and ease of deployment for a wide range of applications. <br\/><br\/>The outcomes of the research will be disseminated in a timely basis through publications, presentations, and collaborations to the academic research community as well as to industry. The project will also have a significant educational impact, by supporting graduate student research, and providing material for a course on wireless sensor networks taught at USC.","title":"NeTs-NOSS: A Middleware Framework for Rapid Composition and Deployment of Compute-Intensive Networked Embedded Systems","awardID":"0627028","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["563658","531832"],"PO":["564777"]},"129383":{"abstract":"Randomness and geometry play a central role in the discovery of polynomial- time algorithms for fundamental problems. This project develops a set of algorithmic tools to tackle problems on the frontier of research in algorithms.<br\/>The problems explored here are of a basic nature and originate from many areas, including sampling, optimization (both discrete and continuous), machine learning and data mining. Progress on these problems, in addition to its potential practical impact, unravels deep mathematical structure and yields new<br\/>analysis tools. As the yield of algorithms grows rapidly and extends its reach far beyond computer science, such tools play an important role in forming a theory of algorithms. The research results of this project contribute to several courses (with notes available online) and the graduate courses are the basis for<br\/>textbooks to benefit the research community.<br\/>The tools developed by this project are based on randomness and geometry. Three specific approaches are studied | (a) sampling high-dimensional distributions by random walks, (b) convex relaxation of discrete sets and (c) spectral projection. Fundamental problems have been solved by these techniques (yielding effcient algorithms), including volume computation, convex optimization, approximation algorithms for some NP-hard discrete optimization problems and learning mixtures of distributions. The project addresses<br\/>the scope and effciency of these techniques and tackles basic open problems in the process. These include: what functions can be sampled effciently by the random walk approach? what is the complexity of volume computation? is the asymmetric TSP harder than the the symmetric version? what are the limits<br\/>of the spectral method?","title":"Fundamental Algorithms based on Random Sampling, Convex Relaxation, and Spectral Analysis","awardID":"0721503","effectiveDate":"2006-10-01","expirationDate":"2010-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["565103"],"PO":["562944"]},"119351":{"abstract":"NeTS-NBD: An Integrated Approach to Opportunistic Spectrum Access<br\/><br\/>Award 0627090<br\/><br\/>Qing Zhao<br\/><br\/>Built upon a hierarchical access structure with primary and secondary users, opportunistic spectrum access resolves the inefficiency of the current command-and-control model of spectrum regulation while maintaining compatibility with legacy wireless systems. The basic idea is to allow secondary users to exploit instantaneous spectrum availability and communicate non-intrusively to primary users. While conceptually simple, opportunistic spectrum access presents challenges not encountered in conventional networks. Cognitive medium access control coupled with signal processing for identifying and exploiting instantaneous spectrum opportunities is one of the central technological underpinnings. This research develops algorithms and protocols for opportunistic spectrum access under energy and hardware constraints. The main scientific ideas being developed include (i) cognitive sensing and access strategies that learn from observations and offer improved performance over time, (ii) distributed protocol implementations in ad hoc networks without central controllers or dedicated communication\/control channels, (iii) quantitative characterizations of the trade-off among optimality, complexity, and robustness of opportunistic spectrum access strategies. This research contributes to the advance of network science. It provides technical data for the assessment of the potentials of opportunistic spectrum access. Specific research topics arising from this project provide educational and research opportunities for undergraduate and graduate students.","title":"NeTS-NBD: An Integrated Approach to Opportunistic Spectrum Access","awardID":"0627090","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["551133"],"PO":["557315"]},"119462":{"abstract":"The security of national information infrastructures is undermined by constant malicious attacks exploiting vulnerabilities in systems software. Most existing attacks exploit memory-based flaws, such as stack or heap overflows, and format string vulnerabilities. Current defense mechanisms, either network- or host-based, are not sufficient against many advanced attacks such as polymorphic or metamorphic worm exploits. This project is to provide a comprehensive framework for detecting, analyzing, and exterminating such attacks. The PIs take an interdisciplinary approach, combining their expertise in computer architecture, computer and network security, programming languages, compilers, and software engineering to tackle this difficult problem. <br\/>In particular, the PIs propose a layered defense and analysis framework that consists of: (1) an architecture layer for detecting and recovering from unknown attacks; (2) an analysis layer for diagnosing attacks and generating attack signatures; and (3) a testing layer for discovering and fixing unknown software vulnerabilities. The intellectual merit of this project will lie in the advanced techniques developed in this project to defend against unknown, large-scale memory-based attacks. <br\/><br\/>This interdisciplinary project will allow an effective approach to tackle this problem and advance knowledge in each of the requisite disciplines with both systems concepts and advanced programming language and analysis techniques. The broader impact of this project is the potential for a more reliable and secure information systems infrastructure. This will have tremendous economical impact on society because of our growing reliance on information technologies. Research results from this project (such as systems, simulators, and tools) will be widely disseminated so that they can be further evaluated, enhanced, and adopted to benefit other researchers and the industry.","title":"Collaborative Research: CT-T: A Vertical Systems Framework for Effective Defense against Memory-Based Attacks","awardID":"0627749","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["507838","562712"],"PO":["565327"]},"119352":{"abstract":"In order to meet expected requirements for reliability and performance in Future Internet, a large majority of the management functions will be automated and rely on embedded capabilities in the network. The objective of the proposed research is to explore design alternatives in the development of technology that will facilitate critical network management tasks in Future Internet. The research program to achieve these capabilities is focused on exploring design alternatives for network management building blocks that will provide essential low-level functionality and can then be composed in different configurations to enable an Internet Management Plane. Specific management building blocks that are targeted for this work include: 1) ubiquitous instrumentation, 2) protocols for data sharing, 3) protocols for end host signaling, 4) event detection mechanisms, and 5) systems for data organization, summarization and presentation. This research program will be facilitated by the use of large-scale test-bed facilities that enable both in situ measurements and controlled laboratory experiments. The broader impacts of this project are that it provides a foundation for flexible, measurement-based investigation of new network architectures and network management practices. The expected results of this work include documentation of designs and in some cases initial prototypes for each of the building blocks. The project also includes education and outreach activities that will develop course materials on network measurement, configuration and management. These web-based materials, which will be openly available to the community, will emphasize a hands-on approach intended to provide students with practical, relevant experiences.","title":"NeTS-FIND: Design for Manageability in the Next Generation Internet","awardID":"0627102","effectiveDate":"2006-10-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["402290","560207","410016"],"PO":["565090"]},"118440":{"abstract":"Background. One of the greatest contrasts between the biological organisms and human technology lies in how they are constructed. Plants and animals grow from the inside out, often from a single cell to an organism containing billions of cells, each of which is built from molecular components that are manufactured with atomic precision within the cell. In contrast, mankind's greatest engineering marvels, such as airplanes and skyscrapers and computers, are put together from the outside in, with components being manufactured in factories and assembled piece by piece. This distinction is often referred to as \"bottom-up\" vs \"top-down\" assembly in the biological \"bottom-up\" approach, the assembly process is guided by the components themselves, while in the engineering \"top-down\" approach, there is an entity conceptually above the object being built that supervises and guides the manufacturing process. Human engineering has mastered top-down methods to create systems of great complexity (but has not extended them to the atomic and molecular scale) and has exploited bottom-up methods for the synthesis of diverse molecular, polymeric and crystalline structures (but has not created information-rich structures of great complexity). <br\/>Project Goals. Our goal is to demonstrate how bottom-up techniques can create complex atomically-defined structures, as biology does, by embedding information and computational processes within the molecules themselves. In biological development, a program (the genome) uses biochemistry to guide the growth process and determine the ultimate form of the organism. In the parlance of computer science, a system that can be programmed to accomplish any task that can be accomplished is called a \"universal\" system. A universal computer can be programmed to perform any computation, while a universal constructor can be programmed to carry out any construction task. Recent work has theoretically shown that universal molecular self-assembly is possible and has experimentally demonstrated that the approach shows promise, using DNA as a construction material to create functional molecular devices so-called \"DNA nanotechnology\". In this proposal, we aim to bring DNA nanotechnology to the point where universal bottom-up self-assembly can be achieved well enough that immediate technological applications can be demonstrated. <br\/>Specific Aims. We aim to make major advances both in our ability to program complex self-assembly logic and in our ability to interface the DNA structures to chemically-, optically-, and electronically-relevant materials. We will focus on four main goals, which span the range from long-term fundamental work to near-term development: (1) self-assembly of a template for a complex molecular-scale electronic circuit; (2) programming the behavior of molecular walking motors to transport components in nanofabrication tasks; (3) attaching carbon nanotube wires to create small nanoscale electronic circuits; and (4) integrating bottom-up and top-down fabrication by placing and orienting self-assembled components at target locations on silicon wafers with functional electrical contacts. <br\/>Uniquely, the aims of this research require simultaneously development of two novel computing systems: the first, inspired by biological self-assembly and development, operates at the level of molecular machines and biochemistry, and will be programmed to construct the second, composed of carbon nanotubes assembled into nanoscale circuits, which operates at the electrical level like conventional computing devices.<br\/>Broader Impact. An important aspect of this project will be the training of young scientists (undergraduates, graduate students, and postdocs) capable of spanning the interdisciplinary subjects involved in this work.","title":"Nano: Collaborative Research: EMT: Toward Universal Bottom-Up Nanofabrication with DNA","awardID":"0622254","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["549585","558957","480791","549587"],"PO":["565223"]},"118286":{"abstract":"This research project is aimed at understanding and developing microdata storage systems, a technology which is needed for many application ares, including genome processing and radar knowledge formation. Microdata storage systems are designed to perform well for small files (microfiles), as well as for large files (macrofiles).<br\/>Today's filesystems are optimized for reading and writing data in large blocks, but they perform poorly when dealing with large volumes of microdata.<br\/>The research focuses on three promising technologies:<br\/> - Microdata storage structures, such as buffered repository B-trees, which can improve the performance of insertions and range queries of microfiles by orders of magnitude over traditional B-trees, while still preserving high performance on macrofiles.<br\/> - Cache-oblivious data structures, which provide passive self-tuning of the file organization and may actually outperform tuned cache-aware data structures for disk file systems.<br\/> - Virtual-memory-based transactional memory, which allows programmers to implement complex file structures in a straightforward manner, while providing lock-free programming and automatic crash recovery.<br\/>The investigators employ benchmarks, such as the DARPA HPC SSCA#3 benchmark (an I\/O-only version of which they developed), to evaluate the impact of microdata storage systems on high-end computing. The investigators are also developing course materials on microdata storage systems which will be made freely available under the MIT OpenCourseware initiative http:\/\/ocw.mit.edu.","title":"HECURA: Microdata Storage Systems for High-End Computing","awardID":"0621511","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["548187","471720"],"PO":["565272"]},"116582":{"abstract":"Rats have approximately sixty large facial whiskers that serve as exquisitely sensitive tactile sensors. Tactile information from the whiskers is carried to a brain structure called the trigeminal ganglion, and then to structures called the trigeminal nuclei. This pathway is roughly analogous to the pathway that carries information from the human skin to the brain. Professors Hartmann, Memik, and Ogrenci -Memik are using the rat whisker system as a model to study the process by which animals acquire and refine incoming sensory information to construct representations of the world around them. The research iterates between neurophysiological recordings in the trigeminal nuclei and computational modeling of the observed neural responses. Because the nervous system is massively parallel, the models quickly exceed the limits imposed by conventional computers. The models are therefore being implemented with Field Programmable Gate Arrays (FPGAs), which can effectively replicate the immense parallelism of brainstem circuitry. Notably, the models are built to process data from both real and robotic whiskers, and can be used to generate predictions about physiological responses in the real animal. <br\/><br\/>This work has the potential to have an impact in industry as well as academia. From an industrial perspective, it is likely to inspire studies on unsupervised 3D object recognition using non-optical sensors. Such sensing ability may be useful in a variety of specialized environments. For example, search and rescue robots operating with limited or no vision could potentially use hardware and algorithms similar to those developed here. In neurophysiology, the work is shedding light on processing algorithms common across modalities (vision, audition, somatosensation and proprioception). The work also contributes to strongly-interdisciplinary training at the post-doc, graduate and undergraduate levels, as well as to minority student recruiting. The work also complements graduate-level courses developed by the PI and Co-PIs.","title":"Computational and Hardware Models of Active Sensing Behaviors","awardID":"0613568","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["409687","518459","513124"],"PO":["564318"]},"119321":{"abstract":"In upcoming years, continual advances in chip design, power consumption, and battery technology will lead to wide deployment of software defined radios (SDRs). SDRs take advantage of programmable hardware modules to dynamically modify the functionality of various radio subsystems. This flexibility allows them to find and adaptively switch between several network protocols. A hybrid wireless network is constructed when SDRs form end-to-end multihop communication links that traverse multiple network standards. The key task for these nodes is to efficiently utilize available network interfaces at each hop to move data from source to destination. This research studies and develops algorithms to help SDRs find, identify, and select appropriate interfaces over which to route diverse traffic streams while efficiently allocating radio resources. The goal is to develop foundational results in these areas so that a broader investigation of hybrid wireless networks can then be pursued.<br\/><br\/>This research will have impact on both commercial and military service providers who wish to extend network capacity and coverage by interconnecting heterogeneous, overlaid wireless systems. The PIs aim to demonstrate broader impact of hybrid wireless networks by offering short workshops that educate rural Pennsylvania communities on its benefits and by designing such a network for Susquehanna County, a rural community in Northern Pennsylvania. Research results will be disseminated and incorporated into wireless communications and networking courses at the respective institutions.","title":"Collaborative Research: NeTS-ProWIN: Multi-Tier Hybrid Wireless Networks","awardID":"0626914","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["342763"],"PO":["434241"]},"119542":{"abstract":"This project will develop 15 modules for high school students that connect biology, computation, and mathematics with corresponding teacher materials and professional development activities. The project will build on the extensive experience of and be conducted as collaboration among Rutgers University (New Brunswick), the Center for Discrete Mathematics and Theoretical Computer Science (DIMACS), the Consortium for Mathematics and its Applications (COMAP), and Colorado State University. The modules will draw on an approach to biological phenomena as involving information processing, in three illustrative areas conducive to learning at the high school level: Bioinformatics and Computational Biology, Mathematical Methods in Epidemiology, and Mathematical Methods in Ecology. These areas are likely to bring excitement about contemporary interconnections between the biological and mathematical sciences to the high school classrooms, hence increase student motivation to study both subjects. The modules will include self-contained text and problem situations (including web-based interactive materials) that can be used separately in high school mathematics courses or biology courses, as well as in Bio-Math integrated courses. Most modules will include about ten 40-minute class meetings, whereas a few will include 1-2 lessons that can be inserted into the existing curriculum. All modules will be developed within a four-phase process that includes pilot testing by teachers who are also involved in module development, field testing by teachers who received training workshops (at least five schools), evaluation and dissemination. Modules will be made available to high school mathematics\/biology teachers, free of charge, during the first three years of the project (print and electronic formats). Both formative and summative evaluation will be conducted to examine the merit\/impact of the project.","title":"The Bio-Math Connection","awardID":"0628091","effectiveDate":"2006-10-15","expirationDate":"2012-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1629","name":"BE: NON-ANNOUNCEMENT RESEARCH"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1106","name":"Division of HUMAN RESOURCE DEVELOPMENT","abbr":"HRD"},"pgm":{"id":"1260","name":"INFRASTRUCTURE PROGRAM"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7355","name":"INSTRUCTIONAL MATERIALS DEVELP"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":["264120","352753"],"PO":["564840"]},"119025":{"abstract":"PROPOSAL NUMBER: CTS-0624830<br\/>PRINCIPAL INVESTIGATOR: E. LAUGA<br\/>INSTITUTION: MIT<br\/><br\/>LIFE AT THE INTERFACE: BIOLOCOMOTION NEAR BOUNDARIES <br\/><br\/>This grant addresses the locomotion of swimming microorganisms near deformable and rigid interfaces. Most microorganisms spend much of their lives near interfaces such as soft biological surfaces (e.g. blood vessels, muscle tissue and the epithelium), rigid boundaries, and air-water interfaces. These surfaces may be characterized by complex topography, malleability and complex physicochemical characteristics. In addition to impacting the hydrodynamic stresses encountered by a swimming cell, these boundaries may affect the local concentration of chemical species including nutrients, oxygen levels and pH, consequently modifying the behavior of neighboring organisms. The research addresses a number of unsolved scientific problems through a combination of asymptotic analysis, numerical computations and experiments. Despite its prevalence and importance in a variety of ecosystems and medical applications, the dynamics of self-propelled microorganisms near boundaries remains largely unexplored. The investigators address a number of fundamental questions regarding the physical interactions between swimming cells and their environment. The required interdisciplinary work lies at the frontier of Engineering, Mathematical Sciences and Biology and combines theory, experiment and computation: this is a realm where the fundamental problems addressed herein are closely related to many natural phenomena and technological applications. Small scale locomotion at interfaces has important implications in microbiological applications including control and understanding of surface-associated infections (e.g. the most common hospital-acquired infection, catheter-associated bacteriuria), biofilm formation, and biodegradation (e.g. using bacteria at polluted sites to metabolize undesirable compounds). Knowledge gained through this research has the potential to impact biomedical applications and environmental technologies. For example, bacteria in biofims are more difficult to treat with antibiotics costing the U.S. billions of dollars every year in equipment damage, product contamination and medical infections. The societal benefits associated with the ability to understand and eventually suppress this type of film formation are significant. Educational impact includes the development of a new graduate-level course on biolocomotion, jointly taught by all three investigators, and the interdisciplinary training of graduate students jointly supervised by faculty in the Departments of Mathematics and Mechanical Engineering.","title":"Life at the Interface: Biolocomotion Near Boundaries","awardID":"0624830","effectiveDate":"2006-10-01","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"1443","name":"FLUID DYNAMICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"7446","name":"MATH PRIORITY SOLICITATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1266","name":"APPLIED MATHEMATICS"}}],"PIcoPI":["441807","555208","511925"],"PO":["380014"]},"116584":{"abstract":"To determine how long-term memories are encoded in the adult brain remains a central problem of neuroscience. Growing evidence suggests that in addition to fast activity-dependent changes in the strengths of synaptic connections between neurons mediated by LTP\/LTD, structural remodeling of axons, dendrites, and spines can also occur in the mature brain, and may contribute to the consolidation of long-term memories through changes in the cortical wiring diagram on a much slower time scale. Importantly, for both \"weight based\" and \"wiring based\" modes of learning, changes in the morphological and biophysical properties of pyramidal neurons that occur with aging, disease and mental retardation could reduce the capacity of the associated brain areas to store information effectively, leading to memory deficits and\/or loss of cognitive function. The goal of this collaborative project, involving coordinated experimental and computer simulation studies in normal, aged, and Ts65Dn mice, is to flesh out the causal chain that links the morphological and biophysical properties of individual pyramidal neurons to long-term memory function and dysfunction in the adult brain.","title":"Anatomical, Physiological, and Modeling Studies of Memory-Related Neural Form and Function","awardID":"0613583","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"T628","name":"NIH-TOBACCO INF GRID (TO BIG)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"T669","name":"NIH-RES ON BRAIN FUNCTIONS"}}],"PIcoPI":["499592"],"PO":["564318"]},"119400":{"abstract":"Proposal Number: 0627320<br\/>Panel: P060975<br\/>PI: Vern Paxson <br\/>Institution: International Computer Science Institute, University of California, Berkeley <br\/>Title: CT-T: Approaches to Network Defense Proven in Open Scientific Experiments <br\/><br\/>Abstract<br\/><br\/>This effort pursues research in network intrusion detection where the research is tied to large-scale operational settings in an exceptionally strong manner. The central component the work builds upon is the \"Bro\" network intrusion detection system previously developed by the PIs. The PIs participate in Bro's deployment for 24x7 operational cybersecurity monitoring at the Lawrence Berkeley National Laboratory (LBNL), the Berkeley campus of the University of California, and the Technical University of Munich.<br\/><br\/>The theme of the research is to develop advances in technology for security monitoring of network traffic where the approaches are directly grounded in the pragmatics of network security at large institutes. The advances under investigation span a range of themes: (1) developing new ways of detecting attacks (detecting network \"triggers\" used by automated exploit software and by worms; drawing upon LBNL's immense archive of logs of past network traffic to devise robust anomaly detection algorithms; identifying possibly unknown malware by re-executing suspicious flows against a fully instrumented honeypot system); (2) new approaches to protocol analysis (exploiting dynamic analysis of protocols that avoid identification via standard ports; extending an abstract protocol description language for specifying analyzers that are then compiled into C++ classes); (3) integrating new sources of information into analyses (distributed monitors; flow records; honeynets; historic behavior; host-based context); and (4) addressing challenges in monitoring very high-speed, high-volume links (transparent load-balancing and cluster operation; hardware support for filtering, state management, normalization, and enabling intrusion prevention).","title":"CT-T: Approaches to Network Defense Proven in Open Scientific Environments","awardID":"0627320","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["562327","563433","562329"],"PO":["529429"]},"119796":{"abstract":"The AToL (Assembling the Tree of Life ) is a large-scale collaborative research effort sponsored<br\/>by the National Science Foundation to reconstruct the evolutionary origins of all living things. Currently 31<br\/>projects involving 150+ PIs are underway generating novel data including studies of bacteria, microbial eukaryotes, vertebrates, flowering plants and many more. The data being generated by these projects include and are not limited to: (i) Specimens and their provenance including collection information, voucher deposition, etc.; (ii) Phenotypic descriptions and their provenance; (iii) Genotypic descriptions and their provenance; (iv) Interpretation of the primary measurements including homology ; (v) Estimates of phylogenies and methods employed; and (vi) Post-tree analyses such as character evolution hypotheses.<br\/>While the data collection, storage, and dissemination within each projects are well coordinated, there is a critical need to develop the infrastructure to integrate all ATOL data sources, allowing the individual efforts to become multipliers for global hypotheses. Furthermore, as the projects continue to expand and address diverse corners of the Tree of Life, efficient project management will be greatly aided by workflow and data management tools targeted towards the ATOL problem domain. The project will develop new, compact, abstract data models for phylogenetics, leveraging use cases from a broad survey of empirical projects. The integration system will develop novel mappings between different phylogenetic data domains, and allow individual projects to join a network of integrated databases in an incremental manner. The data provenance system, which allows tracking of how each data object was created, will be unique to<br\/>systematics data management. The provenance system will not only allow tracking of what kinds of decisions were made in producing a particular tree or a particular column of a data matrix, but will also allow tracking of alternative data lineages such that, for example, different opinions on character homology might be tracked. The results of the research will be delivered in robust software tools that can be used by the entire evolutionary biology community. The study will develop a community-based formal model of data objects used in systematics, primarily through a continuing set of workshops. This activity will not only develop new data management tools, but will also have the effect of synthesizing disparate views of the phylogenetics research domains. The results of the system will be extensible to other domains of evolutionary biology, thereby contributing to the broader mission of evolutionary synthesis. The project will also provide training for the general systematics community in latest database technologies. Finally, by leveraging existing outreach efforts at the Penn Center for Bioinformatics, the project will link to other biological database efforts in genomics and biomedical sciences, disseminating phylogenetic information to the broad biomedical research community.","title":"Collaborative Research: Core Database Technologies to Enable the Integration of AToL Information","awardID":"0630033","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1691","name":"DYN COUPLED NATURAL-HUMAN"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7602","name":"INFORMATION INTEGRATION"}}],"PIcoPI":["538708","486619","343275"],"PO":["565136"]},"119455":{"abstract":"CT-ER: Security-Resource Trade-offs in Wireless Networks<br\/>0627688<br\/><br\/>Guaranteeing security in wireless networks is challenging because of the unique problems that this environment poses, viz., highly error prone links, high variability in the error rates in these links, often limited bandwidth and finally the limited battery power of the end devices. The traditional solutions to these problems - encryption and error correction compete for the same resources and if not designed well, can work at cross-purposes to each other. Power consumption and bandwidth usage are impacted by both error correction and encryption. Therefore a holistic approach must be taken to address the fundamental trade-offs in wireless security: power consumption, error resilience, security and throughput<br\/><br\/>This project uses concepts from coding theory, cryptography, renewal\/reward theory, stochastic modeling etc to address the fundamental trade-offs. Specifically, this project addresses the design of error correcting encryption systems and, link state and battery power adaptive encryption. The practical aspects of the project include implementing and testing the theoretical results in battery power constrained devices. The theory-implementation loop is closed by using power measurements to derive mathematical models for power consumption and then feeding them back to the theoretical optimization problems. <br\/><br\/>The protocols and algorithms developed in this project will impact resource constrained wireless security and sensor network based applications. Some of the outcomes such as power consumption modeling for encryption based secure networking could have a profound effect on low power wireless technology for secure communications.","title":"CT-ER: Security-Resource Trade-Offs in Wireless Networks","awardID":"0627688","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["409877"],"PO":["529429"]},"119466":{"abstract":"The security of national information infrastructures is undermined by constant malicious attacks exploiting vulnerabilities in systems software. Most existing attacks exploit memory-based flaws, such as stack or heap overflows, and format string vulnerabilities. Current defense mechanisms, either network- or host-based, are not sufficient against many advanced attacks such as polymorphic or metamorphic worm exploits. This project is to provide a comprehensive framework for detecting, analyzing, and exterminating such attacks. The PIs take an interdisciplinary approach, combining their expertise in computer architecture, computer and network security, programming languages, compilers, and software engineering to tackle this difficult problem. <br\/>In particular, the PIs propose a layered defense and analysis framework that consists of: (1) an architecture layer for detecting and recovering from unknown attacks; (2) an analysis layer for diagnosing attacks and generating attack signatures; and (3) a testing layer for discovering and fixing unknown software vulnerabilities. The intellectual merit of this project will lie in the advanced techniques developed in this project to defend against unknown, large-scale memory-based attacks. <br\/><br\/>This interdisciplinary project will allow an effective approach to tackle this problem and advance knowledge in each of the requisite disciplines with both systems concepts and advanced programming language and analysis techniques. The broader impact of this project is the potential for a more reliable and secure information systems infrastructure. This will have tremendous economical impact on society because of our growing reliance on information technologies. Research results from this project (such as systems, simulators, and tools) will be widely disseminated so that they can be further evaluated, enhanced, and adopted to benefit other researchers and the industry.","title":"Collaborative Research: CT-T: A Vertical Systems Framework for Effective Defense against Memory-Based Attacks","awardID":"0627767","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["486140"],"PO":["497499"]},"119026":{"abstract":"The goal of the project is to develop a mathematical theory of sparsity in high-dimensional problems of learning theory. These problems are often formulated as penalized empirical risk minimization with convex loss function and convex complexity penalty and they include, in particular, various versions of regression<br\/>and pattern classification problems. There is a variety of approaches to their solution including many popular machine learning algorithms developed in the recent years (kernel machines, boosting, etc).<br\/>The problems in question are most often very high-dimensional or even infinite dimensional and the existence of sparse solutions has crucial impact on the generalization performance of the methods, especially,<br\/>when the amount of training data is small comparing with the dimensionality of the problem. Building upon some recent progress in understanding of the role of sparsity in Computational Harmonic Analysis, Signal Processing and Nonparametric Statistics as well as upon new mathematical approaches to generalization bounds in learning theory utilizing methods of High Dimensional Probability and Asymptotic Geometric Analysis, the investigators study several basic classes of learning algorithms, including optimal aggregation in regression and classification, ensemble learning and kernel machines learning, in order to show that in this type of empirical risk minimization problems the sparsity of the empirical solution can be explicitly <br\/>related to the sparsity of the true solution. The investigators also study the impact of sparsity on generalization performance and develop learning algorithms that are adaptive to unknown sparsity of the problem.<br\/><br\/>The project is at the very intersection of several important lines of research in Pure Mathematics, Statistics<br\/>and Computer Science (Machine Learning). Proving rigorous mathematical results about sparsity is a very challenging problem and solving this problem would require the development of new mathematical tools that are likely to have impact on other developments in these areas. On the other hand, the role of sparsity is crucial in most important applications of machine learning algorithms in such areas as Brain Imaging and Bioinformatics. For instance, in Brain Imaging, it is of great importance to develop methods of automatic classification of activation patterns in fMRI and of automatic selection of features relevant for a particular classification problem. The classification methods taking into account the degree of sparsity of high dimensional objects are directly related to such applications. The project also includes a number of activities that increase its impact on graduate and undergraduate education and facilitate applications of the methods of high-dimensional statistical learning theory.","title":"MSPA-MCS: Sparsity in High-Dimensional Learning Problems","awardID":"0624841","effectiveDate":"2006-10-15","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["565104","551496"],"PO":["565309"]},"117970":{"abstract":"With the rapid advancement of processor and networking technology, and with the falling price of memory and disks, computing resources of CPU cycles, available bandwidths at different levels of inter- and external connections (memory, I\/O, and Internet), and large capacity of memory and disks are increasingly plentiful to us to build high-end systems. Unfortunately, the improvement of data access latency, particularly, the access latency to disks, has significantly lagged behind. The speed gap between data processing in CPU and data accessing in disks has reached to an intolerable level and will only become worse as time goes by. This bottleneck has seriously hindered the development of high-end computing systems for data-intensive applications that demand fast accesses to a huge amount of data. One solution to address this problem is to build large memory buffers to cache data for reuse by taking advantage of low price and large capacity of DRAM memory, and to prefetch data for predicted future use by taking advantage of high and idle bandwidths of networks.<br\/>This research project will focus on a small buffer caching topic: to develop and test a general clock-based system framework for caching management in a large scope of storage hierarchy for core, distributed and Internet systems. The PI will design and implement a clock-based and unified memory buffer management framework with following unique merits: (1) it does not require any global synchronization, and it is system independent; (2) it will be easily used by any types of buffer management at any level of the storage hierarchy, such as buffer caches for I\/O data, data buffer for large scientific data bases, memory buffers for large data streams, and others; and (3) it will be designed to flexibly adopt and test different types of novel ideas of exploiting data access localities.","title":"Memory Caching and Prefetching to Improve I\/O Performance in High-End Systems","awardID":"0620152","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["551992"],"PO":["565272"]},"116892":{"abstract":"Thermal and power problems impose limits on the performance that single-processor chips can deliver. Multicore architectures (or chip multiprocessors), which include several processors on a single chip, are being widely touted as a way to circumvent this impediment. Several chip manufacturers already have released chips with a small number of processors, and 32-processor systems are predicted within the decade. For software designs to take advantage of the parallelism available in these systems, careful attention must be paid to resource-allocation issues. For throughput-oriented applications, initial work on such issues has begun. However, almost no such work has targeted real-time applications, which require very different resource-allocation methods, as they need performance guarantees. This project investigates synthesis of real-time applications on multicore systems. The focus is soft real-time applications, in which some deadline misses are tolerable, but hard real-time applications, in which deadlines can never be missed, are also of concern.<br\/><br\/>In multicore systems, care must be taken when scheduling and synchronizing tasks in order to avoid thrashing shared on-chip caches. In real-time systems, timing constraints must be ensured as well. This project is developing an allocation framework that addresses both concerns. The thesis is that such a framework should be based upon global real-time scheduling algorithms. Such algorithms are more flexible than the alternative, partitioning approaches. This flexibility yields two advantages. First, global algorithms are better able to use information about cache behavior to influence co-scheduling choices. Second, such algorithms can be immunized from the bin-packing problems that plague partitioning approaches. In real-time systems, such problems can result in the need to place restrictive caps on overall utilization, wasting resources. Preliminary research confirms flexibility of the approach in its ability to ensure timing constraints, while encouraging low miss rates in shared caches. Research issues include how to support dynamic workloads, cache-aware real-time synchronization, and cache-profiling for real-time applications. Operating-system components developed by the project will be made available to the research community.","title":"CSR---EHS: Real-Time Computing on Multicore Platforms","awardID":"0615197","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["31436","518412"],"PO":["561889"]},"116585":{"abstract":"he central active challenge we are constantly addressing in daily life is to correctly assess the intent of others ('What is she trying to do? ...') and the import of sensory events ('What - good or bad - may happen now? ...') based on active perception ('It looks to me like she is trying to ...') and retrieved associations (''And she was the one who ...'). The corresponding problem for cognitive neuroscience is to identify, ideally from non-invasive brain activity recordings, those patterns of distributed brain activity that accompany and support active human cognition and behavior. This problem has two parts: First, -What patterns of distributed brain dynamics follow from, accompany, and predict specific world events and subject behavior? -To fully understand the experience and behavior of subjects in performing a given task, we must take into account both the import of each task event to the subject and the intent of each of behavioral event. These factors cannot be known directly, but they may be accurately guessed or inferred, in many cases, from detailed recordings of subject behavior and from the specific historical context in which each recorded environmental or behavioral event occurs. In the case of electroencephalographic (EEG) and\/or magnetoencephalographic (MEG) signals recorded non-invasively from the human scalp, a second part of the problem remains -Which brain areas generate the identified signal patterns?' <br\/><br\/>The usual approach to analyzing electromagnetic scalp data has been to separate recorded events and behavior into a few simple categories, to average the recorded brain dynamics time locked to each event category, and then to apply physical inverse source estimation methods to scalp maps of peaks in the resulting averages. This project will explore using new machine learning methods, including advanced independent component analysis (ICA) and sparse Bayesian learning (SBL) methods, to jointly model the recorded task event, subject behavior, and brain dynamic data recorded in a complex learning task. The project has two goals: First, to identify patterns in unaveraged EEG and\/or MEG data that reliably accompany subject behavior in specific contexts, and second to determine the exact areas of the subject's cortical mantle that locally synchonize their electromagnetic activities to produce the identified scalp patterns. If successful, the project will enhance the value of noninvasive electromagnetic brain imaging for identifying and measuring, with high temporal and spatial resolution, complex, distributed patterns of locally synchronous cortical activity that support active human cognition.","title":"Multimodal Dynamic Imaging of Human Brain Activity","awardID":"0613595","effectiveDate":"2006-10-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"7704","name":"Science of Learning Activities"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T884","name":"NGIA-COMP NEUOSCIENCE GRANT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}}],"PIcoPI":["496128","499664","520809"],"PO":["564318"]},"119786":{"abstract":"The AToL (Assembling the Tree of Life ) is a large-scale collaborative research effort sponsored<br\/>by the National Science Foundation to reconstruct the evolutionary origins of all living things. Currently 31<br\/>projects involving 150+ PIs are underway generating novel data including studies of bacteria, microbial eukaryotes, vertebrates, flowering plants and many more. The data being generated by these projects include and are not limited to: (i) Specimens and their provenance including collection information, voucher deposition, etc.; (ii) Phenotypic descriptions and their provenance; (iii) Genotypic descriptions and their provenance; (iv) Interpretation of the primary measurements including homology ; (v) Estimates of phylogenies and methods employed; and (vi) Post-tree analyses such as character evolution hypotheses.<br\/>While the data collection, storage, and dissemination within each projects are well coordinated, there is a critical need to develop the infrastructure to integrate all ATOL data sources, allowing the individual efforts to become multipliers for global hypotheses. Furthermore, as the projects continue to expand and address diverse corners of the Tree of Life, efficient project management will be greatly aided by workflow and data management tools targeted towards the ATOL problem domain. The project will develop new, compact, abstract data models for phylogenetics, leveraging use cases from a broad survey of empirical projects. The integration system will develop novel mappings between different phylogenetic data domains, and allow individual projects to join a network of integrated databases in an incremental manner. The data provenance system, which allows tracking of how each data object was created, will be unique to<br\/>systematics data management. The provenance system will not only allow tracking of what kinds of decisions were made in producing a particular tree or a particular column of a data matrix, but will also allow tracking of alternative data lineages such that, for example, different opinions on character homology might be tracked. The results of the research will be delivered in robust software tools that can be used by the entire evolutionary biology community. The study will develop a community-based formal model of data objects used in systematics, primarily through a continuing set of workshops. This activity will not only develop new data management tools, but will also have the effect of synthesizing disparate views of the phylogenetics research domains. The results of the system will be extensible to other domains of evolutionary biology, thereby contributing to the broader mission of evolutionary synthesis. The project will also provide training for the general systematics community in latest database technologies. Finally, by leveraging existing outreach efforts at the Penn Center for Bioinformatics, the project will link to other biological database efforts in genomics and biomedical sciences, disseminating phylogenetic information to the broad biomedical research community.","title":"Collaborative Research: Core Database Technologies to Enable the Integration of AToL Information","awardID":"0629846","effectiveDate":"2006-10-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1629","name":"BE: NON-ANNOUNCEMENT RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7602","name":"INFORMATION INTEGRATION"}}],"PIcoPI":["541878","541876",317758,"558589"],"PO":["565136"]},"123790":{"abstract":"Project Summary<br\/>Self-assembly is the process by which small \"components\" follow simple<br\/>combination rules to form intricate structures. DNA Self-assembly is widely<br\/>believed to be a key tool for nano-technology, nano-robotics, and molecular<br\/>computation. DNA self-assembly in particular, and nano-technology in general,<br\/>offer significant algorithmic challenges. This project will explore two<br\/>exciting directions in this field:<br\/>1. Error correction at the nano scale.<br\/>2. Models and algorithms for molecular machines.<br\/>Intellectual merit: Error correction at the nano scale appears to require<br\/>tools and techniques that are significantly different from those required for<br\/>error correcting codes. For instance, errors and error correction mechanisms<br\/>are bound by thermodynamic laws at the nano scale, as well as by the constraint<br\/>that any \"computation\" required to correct an error must be carried<br\/>out using the same underlying error prone physical mechanism. While specific<br\/>error correction mechanisms are now known in certain cases, a general<br\/>understanding has been elusive.<br\/>Many experimental groups have developed rudimentary (but very promising)<br\/>DNA based molecular machines. Modeling of these machines is in a preliminary<br\/>stage. Exploratory research in modeling these machines is likely to<br\/>lead to interesting and challenging algorithmic questions.<br\/>Broad impact: Error correction at the nano scale will facilitate sophisticated<br\/>tasks such as counting, growing crystals of pre-specified sizes (no<br\/>larger, no smaller), shape recognition etc. with great precision using inherently<br\/>error-prone DNA self-assembly (or other technologies). This would be<br\/>a new engineering primitive, somewhat like the engine and the semiconductor,<br\/>with important immediate as well as unforeseen uses. Also, molecular<br\/>machines may act as sensors, signal carriers, actuators, or drug delivery<br\/>mechanisms. Undoubtedly, much of the hard work in achieving these goals<br\/>will be done (and is being done) by experimentalists. But the PI believes<br\/>that algorithmic techniques will also play an important supporting role.<br\/>The PI will organize an informal reading seminar where students will<br\/>explore this area in depth. The PI hopes that this seminar will provide a<br\/>valuable educational experience. The PI also plans to write an article in a<br\/>book intended for scientifically literate readers who are not experts in this<br\/>field; the article will highlight algorithmic issues at the nano scale.<br\/>1","title":"SGER: Algorithmic Issues at the Nano Scale","awardID":"0650058","effectiveDate":"2006-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["429621"],"PO":["565223"]},"125298":{"abstract":"National Science Foundation<br\/>NETS - Research in Network Technologies and Systems <br\/>CISE\/CNS<br\/><br\/><br\/>ABSTRACT<br\/><br\/>Proposal Number: 0435341<br\/>Principal Investigator: Liu, Qingchong<br\/>Institution: Oakland University<br\/><br\/>Proposal Number: 0435155<br\/>Principal Investigator: Chunming Qiao<br\/>Institution: SUNY at Buffalo<br\/><br\/><br\/>Proposal Title: Collaborative Research: NeTS-NR: Ultra-Broadband Optical Wireless Communication Networks<br\/><br\/><br\/>This collaborative research project undertakes a multidisciplinary approach to optical wireless communications (OWC) networks and focuses on the first\/last mile between the existing fiber-optic backbone and many homes and small of-fice buildings. The project aims at developing novel OWC networking and communications theory and techniques including those at the physical layer that overcome the scintillation (variation in light intensity) caused by the at-mospheric turbulence in OWC networks through sub-carrier modulation and coding, and those at the link and network layers that take into consideration the unique capabilities and constraints of OWC when designing optimal topol-ogy, survivable routing, and innovative dynamic reconfiguration algorithms to mitigate the negative effects of heavy or dense fog, as well as reduce the per link cost. As a result of the project effort, an OWC ring network will be built, running multimedia applications. The success of the project is expected to pro-vide an affordable ultra-broadband first\/last mile access, enable new multime-dia applications to be delivered to residential homes and small office buildings, and serve as a stepping stone to the integration of heterogeneous technologies based on radio frequency (e.g., Wireless Local Area Networks, WLAN, and cellu-lar networks) and fibers. <br\/><br\/>Dr. Admela Jukan<br\/>Program Director, CISE\/CNS<br\/>Aug 5, 2004.","title":"Collaborative Research: NeTS-NR: Ultra-Broadband Optical Wireless Communication Networks","awardID":"0702874","effectiveDate":"2006-10-15","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["533504"],"PO":["402055"]},"119413":{"abstract":"T. J. Jankun-Kelly<br\/>Mississippi State University<br\/>0627407<br\/>Panel P060969<br\/><br\/>Abstract<br\/><br\/><br\/>This research investigates a process for developing empirically validated computer security and digital forensics visualization tools. The amount of security and forensics data is too immense for an analyst to understand directly; this project utilizes computer generated depictions of this information to facilitate the comprehension of the data and to reduce decision time required to act upon the data. In order to validate that the generated visualization are effective---vital for the visualization's products to be used in a court of law---an empirical validation of the developed tools is being performed. <br\/><br\/>Two problems domains are explored by this research: Network security (the securing of Internet traffic) and computer forensics (the process of gathering evidence on digital devices). For each, a thorough domain-analysis is being performed in cooperation with network analysts and law enforcement officials in order to identify their data of interest and required tasks. This domain analysis informs the design of the visualization, which in-turn are being validated via user studies involving the pertinent users.<br\/><br\/>The results of this research will address pressing needs in network security and computer forensics---the data involved in each is too demanding for direct analysis. First, tools with measurable benefits will be provided to the user community. Secondly, the process used to created said tools will inform the development of similar tools in order to promote demonstrably rigorous and effective visualization methods.","title":"CT-ISG: Empirically-Based Visualization for Computer Security and Forensics","awardID":"0627407","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["528454","551043","497247","551799"],"PO":["529429"]},"119776":{"abstract":"The AToL (Assembling the Tree of Life ) is a large-scale collaborative research effort sponsored<br\/>by the National Science Foundation to reconstruct the evolutionary origins of all living things. Currently 31<br\/>projects involving 150+ PIs are underway generating novel data including studies of bacteria, microbial eukaryotes, vertebrates, flowering plants and many more. The data being generated by these projects include and are not limited to: (i) Specimens and their provenance including collection information, voucher deposition, etc.; (ii) Phenotypic descriptions and their provenance; (iii) Genotypic descriptions and their provenance; (iv) Interpretation of the primary measurements including homology ; (v) Estimates of phylogenies and methods employed; and (vi) Post-tree analyses such as character evolution hypotheses.<br\/>While the data collection, storage, and dissemination within each projects are well coordinated, there is a critical need to develop the infrastructure to integrate all ATOL data sources, allowing the individual efforts to become multipliers for global hypotheses. Furthermore, as the projects continue to expand and address diverse corners of the Tree of Life, efficient project management will be greatly aided by workflow and data management tools targeted towards the ATOL problem domain. The project will develop new, compact, abstract data models for phylogenetics, leveraging use cases from a broad survey of empirical projects. The integration system will develop novel mappings between different phylogenetic data domains, and allow individual projects to join a network of integrated databases in an incremental manner. The data provenance system, which allows tracking of how each data object was created, will be unique to<br\/>systematics data management. The provenance system will not only allow tracking of what kinds of decisions were made in producing a particular tree or a particular column of a data matrix, but will also allow tracking of alternative data lineages such that, for example, different opinions on character homology might be tracked. The results of the research will be delivered in robust software tools that can be used by the entire evolutionary biology community. The study will develop a community-based formal model of data objects used in systematics, primarily through a continuing set of workshops. This activity will not only develop new data management tools, but will also have the effect of synthesizing disparate views of the phylogenetics research domains. The results of the system will be extensible to other domains of evolutionary biology, thereby contributing to the broader mission of evolutionary synthesis. The project will also provide training for the general systematics community in latest database technologies. Finally, by leveraging existing outreach efforts at the Penn Center for Bioinformatics, the project will link to other biological database efforts in genomics and biomedical sciences, disseminating phylogenetic information to the broad biomedical research community.","title":"Collaborative Research: Core Database Technologies to enable the Integration of AToL Information","awardID":"0629702","effectiveDate":"2006-10-01","expirationDate":"2008-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7602","name":"INFORMATION INTEGRATION"}}],"PIcoPI":["500379","424851"],"PO":["565136"]},"118346":{"abstract":"With the success of the Human Genome Project, a microarray can now potentially handle the genes in an entire genome scale. A typical microarray data set involves a massive number of genes. A dramatic dimension reduction to a much smaller number of significant genes, responsible for specific conditions, can potentially increase the possibility of further biological study and knowledge regarding the roles of specific genes.<br\/><br\/>Any methodology that can improve our recognition of significant genes among a large number of genes, and often a limited set of available experimental results, could have a significant impact on our understanding of diseased and normal states, and eventually on diagnosis, prognosis, and drug design. The method that we propose to investigate here is intended to provide critical information on the roles of genes where the key component of our approach is subspace-based methods, which have demonstrated great success in numerous pattern recognition tasks including efficient classification, clustering, and fast search.<br\/><br\/>The development of effective computer-based algorithms for gene selection is indispensable since it is virtually impossible to rely solely on biological testing due to the enormous complexity of the problems. What is novel and unique in our proposed research is that we seek to find a mathematically rigorous framework that models gene selection problems, with careful consideration of the significance of the biological characteristics of the problem. Utilizing our knowledge and previous results on feature extraction, and by discovering their mathematical relationship to feature selection, efficient and effective nonparametric methods for gene selection will be designed. An important role will be played by the nonnegative matrix factorization in building a mathematically rigorous bridge between feature extraction and feature selection in our proposed research. In the process, we will also explore novel methods for estimating missing values as a preprocessing stage of gene selection based on the alternating least squares and the structured total least norm formulations. All results obtained, the new algorithms and software developed, as well as the new data sets generated and compiled will be made available to the research community, to teaching faculty, and to both graduate and undergraduate students, using existing Web servers at the Georgia Institute of Technology and University of Texas at Dallas.<br\/><br\/>Intellectual Merit: This research will produce methods that will have a great impact on computational microarray analysis. The gene selection and missing value estimation methods developed in this research allow significant reduction in complexity of biological testing due to the initial reduction of the problem dimension, thus substantially improve detailed study of significant genes. The feature selection and feature extraction algorithms developed in this research will be applicable to many other problems where data sets in high dimensional spaces need to be handled efficiently and effectively, such as text processing, facial recognition, finger print classification, iris recognition. The missing value estimation methods designed in this research can also be utilized in recovering missing data such as in collaborative filtering.<br\/><br\/>Broader Impact: The research will enhance advanced theory of computational biology and bioinformatics. The developed techniques will also have potential applications in database management, medical examination and diagnosis, bio-chemical selection, and biological networks. The graduate student involvement in this research will have numerous future benefits. The discovery and research experience of the students will prepare them for productive careers in academia, research labs, and industry in highly important current research areas in bioinformatics.","title":"CompBio:Collaborative Research: Development of Effective Gene Selection Algorithms for Microarray Data Analysis","awardID":"0621829","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["450698"],"PO":["565223"]},"113594":{"abstract":"New Algebraic Techniques for Constructing Sequences and Arrays with Good Correlation Properties<br\/><br\/><br\/>K.T.Arasu<br\/>Department of Mathematics & Statistics, Wright State University,<br\/>Dayton, OH 45435<br\/><br\/> The Correlation Problem covers a broad fundamental combinatorial problem with deep mathematical content. Specific solutions to the Correlation Problem have practical diverse applications in communications, experimental design, laboratory instrumentation and manufacturing. As technology changes so too will the instances of the Correlation Problem which need solving. This research develops new mathematical techniques for attacking the problem. <br\/> The Correlation Problem is to design sequences or arrays with specified dimensions with entries chosen from a specified finite set so that all non-trivial periodic autocorrelations lie in a prescribed restrictive set. Usually the autocorrelations are computed using a quadratic form. More generally, one may seek several arrays with good autocorrelations and good cross-correlations. The Correlation Problem divides into two questions: When do solutions exist, and if so, what does the solution space look like? <br\/>The sequence design problems that we investigate have a variety of applications in communication engineering. The methods used will be very algebraic and would employ tools from algebra, finite fields, algebraic number theory and representation theory. Calculation of the linear span (p-ranks) of the obtained sequences will use combinatorial tools, in conjunction with the algebraic tools developed here.","title":"New Algebraic Techniques for Constructing Sequences and Arrays with Good Correlation Properties","awardID":"0556191","effectiveDate":"2006-10-01","expirationDate":"2010-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["450820"],"PO":["564898"]},"119447":{"abstract":"Authenticating Reality 0627645<br\/><br\/>Digital photography is a poor way to authenticate reality because<br\/>digital photographs are notoriously easy to manipulate and falsify.<br\/>This same unreliability applies to other digital media: voice<br\/>recordings can be spliced and synthesized and digital manipulation of<br\/>movies is a high art. Nonetheless, it is people's nature to believe<br\/>what they see and hear, and falsified digital recordings are<br\/>frequently used to influence public opinion. Doctored images of<br\/>political figures, scientific and legal evidence, and news and current<br\/>events are commonplace.<br\/><br\/>This project explores new technologies, based on cryptographic<br\/>hardware, advanced digital signature schemes, and computer-vision<br\/>techniques, to enable users to verify the authenticity of digital<br\/>photographs and other media. With tamper-proof cryptographic hardware<br\/>embedded in digital cameras, photographers can produce photos along<br\/>with proof of their authenticity. By employing homomorphic signature<br\/>schemes, photographers and editors can perform necessary scaling and<br\/>cropping on those images while maintaining the proof of authenticity.<br\/>By authenticating other internal state of the camera when the picture<br\/>is taken, we can strengthen existing machine-vision fraud-detection<br\/>algorithms and enable new ones.<br\/><br\/>This project may revolutionize the way society values and uses<br\/>photographic and other digital evidence. The research serves the<br\/>interests of all scientific disciplines --- not just computer security<br\/>--- as well as the disciplines of law, economics, and journalism.","title":"CT: Authenticating Reality","awardID":"0627645","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["510424","533341","508251"],"PO":["497499"]},"119337":{"abstract":"NeTS-NBD: XORs in the Air: Practical Wireless Network Coding<br\/><br\/>Award 0627021<br\/><br\/>Dina Katabi Muriel Medard<br\/><br\/>Wireless networks are indispensable; they provide the means for mobility, city-wide Internet connectivity, distributed sensing, and outdoor computing. Current wireless implementations, however, suffer from severe limitations in throughput and do not scale to dense large networks. This project explores a new architecture that significantly increases the throughput of wireless networks. In addition to forwarding packets, routers mix (i.e., code) packets to increase the information content of each transmission. The new design is rooted in the theory of network coding. Prior work on network coding is primarily theoretical. This project aims to bridge theory with practice; it addresses the common case of unicast traffic, dynamic and potentially bursty flows, and practical issues facing the integration of network coding in the current network stack.<br\/><br\/>If successful, this project can enable high-throughput dense wireless networks. Specifically, the PIs expect the project to produce new protocols that provide large improvements in the throughput of wireless networks, facilitate mobility, allow opportunistic routing without node-coordination, and maintain throughput gains in the presence of malicious wireless nodes. The results will be disseminated to the research community and industry via publications, collaborations, and the educational process.","title":"NeTS-NBD: XORs in the Air: Practical Wireless Network Coding","awardID":"0627021","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[316455,"560189"],"PO":["557315"]},"123781":{"abstract":"Fundamental principles of non-linear single molecular devices as building blocks for future<br\/>computing technologies<br\/>This SGER proposal aims at exploring theoretical foundations for new types of<br\/>nanoelectronic single molecular devices. The major driving force for this effort is the realization<br\/>of unique electronic properties of single organic molecules that can be exploited to implement<br\/>novel elements of electronic circuitry to be used as an alternative for traditional, silicon based<br\/>CMOS technology. The main goal of this proposal is to develop computational tools and apply<br\/>them to perform device modeling and intelligent design of molecular architectures based on firstprinciples<br\/>understanding of electron transport. A series of unique molecules that exhibit specific<br\/>electronic functions such as molecular diodes, transistors and switches will be explored. Charge<br\/>transport through single molecules and molecular arrays will be investigated. The fundamental<br\/>understanding of electrical characteristics of such molecular building blocks will be used to<br\/>explore the intelligent design of molecular architectures. Simulation and prediction of electrical<br\/>behavior of molecular nanoelectronic components based on their atomic and electronic<br\/>properties will be ultimately extended to achieve the virtual integrated prototyping of molecular<br\/>devices optimization of current-voltage characteristics of specific electronic devices by<br\/>choosing the most appropriate chemical composition that has desirable electronic properties.<br\/>The proposed research program addresses the goal of NSF EMT program to explore new<br\/>capabilities for future computing technologies, thus allowing to extend the lifespan of Moore's<br\/>law beyond the lifetime of silicon technology.<br\/>Broader Impact.<br\/>The broader impact of this SGER effort will be achieved by educating science and<br\/>engineering graduates who will be the main driving force for future advances in future<br\/>computing technologies. Students involved in this project will learn a broad range of knowledge<br\/>in molecular engineering and first-principles device modeling. This project will serve to attract<br\/>brilliant minds, to train and promote young talented students with the aim to reinforce the science<br\/>and engineering workforce.<br\/>In addition to educational activities, this synergistic effort in theory\/simulation will advance<br\/>our understanding the fundamental mechanisms of electron transport in molecular nanostructures<br\/>and to establish predictive structure-property relationships between molecular structure and<br\/>electrical device characteristics. This knowledge base is important for practical implementation<br\/>of the next generation of computing hardware based on molecular components aimed at<br\/>achieving lightweight, flexible circuits with low energy consumption and high density<br\/>information storage.","title":"Fundamental principles of non-linear single molecular devices as building blocks for future computing technologies","awardID":"0650028","effectiveDate":"2006-10-01","expirationDate":"2009-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["458060"],"PO":["565223"]},"121174":{"abstract":"Applications for computer simulations include many research areas such as weather prediction, tracking the location and concentrations of contaminants in groundwater, oil recovery, studying disease processes, designing experiments, and developing medications. In these and several other applications, it is desirable to achieve speedup of numerical code. Current work in speeding up numerical simulations has several disadvantages. Considering the various disadvantages of each method, project will develop methods that increases the speed and (1) does not require rewriting an existing algorithm, although could be improved even further by making minor coding modification, (2) does not require algorithms written in traditional languages to be rewritten in other language, (3) executes portions of the code in parallel but does not suffer from the overhead of either a single microprocessor or multi-processor architecture, and (4)does not require time and effort to engineer and implement a special circuit for different types of numerical algorithms. This work proposes to develop such a technology using flowpaths where, starting with a C (or potentially FORTRAN) description of a numerical algorithm, a compiler will generate an executable that can be downloaded and will run on the Power PC embedded in an FPGA with parallel flowpaths to speedup the bottleneck loops in the numerical algorithm automatically. <br\/><br\/>With such a speed-up, some simulations that require real-time execution that can not currently be achieved by a PC will be able to run at a higher speed and achieve a real-time pace. The success of this research will result in future investigation including deriving optimizations for the compiler and resulting circuits, improving numerical schemes for optimal implementation in hardware and enhancing the compiler to support other popular languages.<br\/><br\/>The intellectual merit of this research project from a scientific computational standpoint lies in the discovery of new coding techniques that make optimal use of flowpaths in order to achieve higher simulation speeds. The intellectual merit in hardware design for speedup lies in the unique use of flowpaths for creating special-purpose processors for new and existing numerical code, automatically. This project serves as a novel interdisciplinary approach, combining expertise in scientific computation of numerical algorithms and high-speed embedded systems for significantly increasing the performance of numerical code, with impact both in software as well as in hardware technologies.","title":"SGER: Numerical Speedup Using Flowpaths","awardID":"0636895","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["358378",322011],"PO":["551712"]},"119404":{"abstract":"Haining Wang<br\/>College of William and Mary<br\/>0627340<br\/>Panel: P060969<br\/>Collaborative Research: Intrusion Detection Techniques for Voice Over IP<br\/><br\/>Recently Voice over IP (VoIP) is experiencing a phenomenal growth.<br\/>Being a time sensitive service, VoIP is even more susceptible to<br\/>malicious attacks than regular Internet services. Moreover, VoIP uses<br\/>multiple protocols for call control and data delivery, making it<br\/>vulnerable to various attacks at different protocol layers. The<br\/>already-known security solutions for data networks fall short of<br\/>defending VoIP applications because of the differences of cross protocol<br\/>interactions and the way in which the different handshakes effect the<br\/>distributed service elements and consequently the end user quality of<br\/>service (QoS) of VoIP. This project seeks to develop a series of<br\/>intrusion detection techniques suitable for VoIP and experimentally<br\/>verify their commercial viability. Overall objective is to detect known<br\/>and unknown attacks in an accurate and timely manner, without incurring<br\/>a noticeable delay in call setup times.<br\/><br\/>The research plan consists of two major components:<br\/>Protocol-state-machine (PSM) based mechanism for detecting known<br\/>attacks, and Hellinger-distance based (HD) mechanism for detecting<br\/>unknown attacks. The incorporation of the communication between<br\/>protocol state machines is particularly suited for intrusion detection<br\/>in VoIP, because call control and media delivery protocols are<br\/>synchronized by exchanging synchronization messages for critical events<br\/>throughout the established sessions. The core of HD detection scheme is<br\/>based on using the Hellinger distance to measure the deviation from<br\/>normal network protocol behaviors. To have the detection mechanism<br\/>insensitive to site and traffic pattern, a dynamic self-regulating<br\/>threshold is used, thus making the detection mechanism robust, widely<br\/>applicable, and easier to deploy. Research results of the project will<br\/>be broadly disseminated through publication, web pages, and technology<br\/>transfer to industry.","title":"CT-ISG: Collaborative Research: Intrusion Detection Techniques for Voice over IP","awardID":"0627340","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["409225"],"PO":["529429"]},"120790":{"abstract":"Abstract of project: Efficient algorithms and data structures via geometric realizations<br\/>Assaf Naor<br\/>When one needs to perform an algorithmic task on a large data set it is often the case that the best<br\/>way to fathom the massive amount of information contained in the data is to visualize it geometrically. By<br\/>representing the input as a geometric object one can observe certain helpful features, such as clusters or<br\/>low-dimensional structures within the data, and harness this structural information to solve the algorithmic<br\/>problem at hand. In recent years the geometric approach to the design of algorithms has been greatly enhanced<br\/>by the use of powerful methods from modern mathematics. This research project is at the frontier<br\/>both for mathematics and computer science. Using ideas from analysis and geometry, combined with modern<br\/>algorithmic methods and intuitions, the investigator's study will lead to advances on central algorithmic<br\/>primitives for a wide variety of tasks ranging from combinatorial optimization and machine learning to<br\/>information retrieval and bioinformatics.<br\/>The goal of this research is to embed data into a well understood geometric object (such as Euclidean<br\/>space), so that certain essential features of it are preserved. This makes it possible to harness intuitions and<br\/>methods that were not available before the embedding was performed. The investigator will develop new<br\/>mathematical tools for the study of algorithms for partitioning and clustering large networks, techniques for<br\/>dimensionality reduction, constructions of compact representations of data, and methods for fast similarity<br\/>search and classification. The proposed research will be of interest to computational scientists in a broad<br\/>range of areas. The connection between mathematics and computer science will be deepened in order to<br\/>disseminate powerful new ideas for algorithm design.<br\/>1","title":"Efficient algorithms and data structures via geometric realizations","awardID":"0635078","effectiveDate":"2006-10-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["382698"],"PO":["562944"]},"120570":{"abstract":"The workshop on the future directions in numerical computing and optimization will be held at the Stanford University, California in early April 2007, most likely in the Gates Computer Science Building. We estimate a turnout of at least 80 participants from the numerical computing and optimization communities, doing either theoretical research or important applications in science and engineering. The workshop is expected to bring in a large group of researchers in various stages of their career from the key leaders in the field to the graduate students. This will be accomplished by inviting a number of speakers with national and international visibility and leadership including women researchers. The proposed workshop is strategically proposed to be co-located with the 50th History of Numerical Computing workshop.","title":"Special Meeting: Workshop on Future Direction in Numerical Algorithms and Optimization","awardID":"0633793","effectiveDate":"2006-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[320271,"536648","562362","450698"],"PO":["381214"]},"123870":{"abstract":"ABSTRACT <br\/>The meeting will bring together researchers to discuss the fundamental requirements on DDDAS environments, identify architectural components, structural elements, and mechanisms and policies which are crucial for a large-scale, shared facility commensurate with the requirements and objectives of DDDAS environments. The proposed workshop, will focus and will address more extensively on DDDAS related infrastructure and testbeds. The workshop will also address modalities and approaches of how collaboration with other communities such as distributed systems, software engineering, networking, algorithms and security, on both collaborative research and on experimental infrastructure suitable for large-scale, interdisciplinary experimentations be achieved.<br\/> The 2-day workshop will bring together a community of researchers and students active in the areas of the theme of the meeting. The invitees will be asked to provide position papers, and the meeting conducted in the format of presentations, working groups sessions, and plenary sessions discussing recent advances in DDDAS and other NSF infrastructure efforts. The workshop will produce a report, which will be made available to the broader community.","title":"Workshop on Research Challenges in Dynamic Data Driven Application Systems and Infrastructure","awardID":"0650480","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["533851","194325"],"PO":["565136"]},"116665":{"abstract":"0613997<br\/>Rastislav Bodik<br\/>U of Cal - Berkeley<br\/><br\/>TITLE Programming by Sketching<br\/><br\/>Software is designed and implemented in a layered way: requirements are refined into designs, which are in turn implemented with low-level code.<br\/>The implementation gap is responsible for the lack of a formal correctness connection between the layers: functionality is typically not formally specified and implementations are produced by hand rather than generated. <br\/><br\/>This project proposes to bridge the implementation gap by making it easier both to write specifications and to implement the low-level code. To make the challenging problem manageable, the project focuses on the domain of scientific computing. The chief technical approach is sketching, a new software synthesis approach where the programmer develops a partial implementation --- a sketch --- and a separate specification of the desired functionality. The synthesizer then completes the sketch to behave like the<br\/>specification. <br\/><br\/>This exploratory project will (a) investigate how to extend the applicability of a SAT-based synthesizer to increasingly larger programs; b) develop techniques for synthesizing programs that are currently beyond the power of the SAT synthesizer; and (c) develop a robust sketching synthesizer for a class of scientific programs.","title":"SoD-TEAM: Programming by Sketching","awardID":"0613997","effectiveDate":"2006-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7372","name":"ITR-SCIENCE OF DESIGN"}}],"PIcoPI":["556819","497138"],"PO":["564388"]}}