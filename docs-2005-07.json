{"102179":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Reconfigurable computing infrastructure for high end and embedded computing applications <br\/>Proposal: CNS 0454407<br\/>PI: Prasanna, Viktor K.<br\/>Institution: University of Southern California<br\/> <br\/>The investigators will acquire a reconfigurable computer comprised of general purpose processors, field programmable gate arrays (FPGAs), a common memory, and an interconnect fabric joined under a programming model that works with all the parts. The acquisition of this machine will enable research at a realistic scale on actual reconfigurable machines for performance testing, validation, and applications demonstrations. This infrastructure will be robust enough to implement application \"kernels\" such as (e,g, an LU implementation or n-body simulation) that give realistic scale experimental results. Applications that will be explored include matrix operations, computational genomics, molecular dynamics, density functional theory, and finite element methods. The team will also be able to work on energy efficiency for embedded FPGAs. Broader impacts of this project include the potential impact on reconfigurable systems, use of FPGAs for applications, and discoveries in the applications areas. The investigators participate in USC's Minority Opportunities in Research (MORE) program.","title":"CRI: Reconfigurable computing infrastructure for high end and embedded computing applications","awardID":"0454407","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[269403,269404,"456874","563658","490011"],"PO":["550859"]},"109604":{"abstract":"This workshop will be the Second NSF Workshop on Future Spectrum Technology and Policy, and will be held on 25-27 May 2005 in Washington, DC. NSF sponsored a joint NSF-FCC workshop in May 2003 to foster more awareness within the research community of the research challenges related to spectrum policy. The focus of this latest meeting will be on reassessing the technical needs and research priorities for enabling new radio applications and capabilities as well as new U.S. policy concepts. This proposal includes funding to support participant travel to the workshop.","title":"NSF Future Spectrum Technology and Policy Workshop","awardID":"0535385","effectiveDate":"2005-07-01","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["529582"],"PO":["7594"]},"107679":{"abstract":"Improvements in computational capability have entailed making the components smaller and smaller, to the point where we are reaching the limits of the current top-down fabrication methodologies. Furthermore, fabrication methods currently in place are inherently limited to two dimensional arrangements of components. With the research to be pursued under this award, we plan to address both of those problems: We will develop methods to assemble computing components from the bottom-up, using chemical methods. In addition, we will extend our ability to assemble computational elements from two dimensions to three dimensions.<br\/><br\/>How do we plan to accomplish these ambitious goals? We will use the most effective and selective self-assembling system that is found in nature -- DNA. We are all aware that DNA serves as the genetic material of living organisms, from bacteria to humans. One of the key features of DNA that enables it to fulfill this role is complementarity between the two strands of the double helix: If one side of the helix contains a unit known as an A, the other side will always have another unit know as a T; likewise, two further complementary units known as G and C work in the same way. We now live in an era when it is very easy to synthesize DNA strands with particular sequences on them, so we can program a given strand on one side, and then make its complementary strand on the other side. Furthermore, DNA is a nanoscale object, with a width of about 2 nm, and a helical repeat of about 3.5 nm. In addition, DNA is the molecule whose intermolecular interactions are the most readily programmed, from the perspectives both of affinity (which molecules will bind to which others), and structure (what will they look like when they combine). This intermolecular programming is accomplished by using short single-stranded segments on the ends of each molecule, which are called 'sticky ends'.<br\/><br\/>But wait. Nature already makes linear DNA molecules in profusion. Synthetic molecules would just be relatively short 'lines' of matter, so what use would they be? The advantage of synthetic DNA molecules is that they can be programmed to associate into branched, rather than linear molecules. Thus, joining a bunch of linear DNA molecules together can lead to longer lines, but joining branched molecules together can lead to connected networks of DNA. In the past, we have built a variety of two dimensional crystalline arrays with specific patterns; to do this we have designed DNA strands to form two dimensional motifs that could they self-assembled using sticky ends. In the research to be pursued under this award, we will design and self-assemble motifs that will form motifs that self-assemble to produce three dimensional arrays, related to conventional crystals, such as sugar crystals, except that their repeating units will be much larger. We will characterize these molecules using the standard method for examining three-dimensional matter, x-ray crystallography.<br\/><br\/>How does the assembly of three dimensional DNA crystals relate to revolutionary computing? We have just discussed that DNA has outstanding architectural properties. However, it does not seem well-suited to serve as a computational component. By contrast, there are numerous newly discovered nanoscale-sized systems, such as carbon nanotubes or quantum dots that would seem to be ideal for computational purposes, if only we could organize them into circuitry for our purposes. Once we are able to assemble DNA into three-dimensional arrangements, we will undertake to use its architectural features to act as scaffolding in three dimensions for electronic components such as these. Thus, we will combine the outstanding architectural properties of DNA, which we know how to control, with demonstrated outstanding electronic components to form circuitry in three dimensions. This achievement ultimately will lead to extremely dense memory units and extremely rapid computation.","title":"NANO: EMT: Revolutionary 3D Nanoarchitectures to Organize the Assembly of Computing Elements","awardID":"0523290","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["554823"],"PO":["565223"]},"109637":{"abstract":"A workshop will be held to bring together noted scientists to provide recommendations on the synergy among scientific observing systems funded or proposed for funding by NSF, and the ability of that synergy, and those systems, to address grand environmental challenges identified by the National Research Council. The participants will explore the proposed and potential value of NSF nascent and current observing systems to address these challenges. Of critical importance will be the identification of gaps in knowledge that will remain or become more critical as observing systems are deployed, and how this may affect the role of non-observing system science. Of particular importance will be the identification of synergies among and within<br\/>observing systems and system requirements for inter-operability. The anticipated intellectual impact will be the identification of emergent frontiers, unrealized research opportunities, and unique infrastructure capacity that arise from the combined environmental research observing systems and networks. The workshop will have broad impacts by defining the potential of observing systems to generate solutions to environmental problems relevant to society. In addition, the resulting reports will contribute to the planning, funding, implementation, coordination, and use of the observing systems through formulating a set of recommendations for the AC-ERE, NSF, and the wider community.","title":"Coordination Meeting on Observing Systems for Environmental Solutions","awardID":"0535549","effectiveDate":"2005-07-15","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0604","name":"Division of OCEAN SCIENCES","abbr":"OCE"},"pgm":{"id":"1680","name":"OCEAN TECH & INTERDISC COORDIN"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"1440","name":"ENVIRONMENTAL ENGINEERING"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7304","name":"ERE General"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7350","name":"NAT ECOLOGICAL OBSERVATORY NET"}}],"PIcoPI":["560926","360958",290253,"528146"],"PO":["565067"]},"106007":{"abstract":"Sources of entropy that are not precisely reproducible nor uniformly distributed, such as biometrics, nontraditional passwords, or physical random functions, are increasingly suggested as tools in electronic and physical security. There are, however, many significant unresolved questions about exactly how such sources should be used and stored. This proposal focuses on investigating how to use them securely, reliably, privately and versatilely. The techniques studied will have applications well beyond biometric authentication, to settings where noisy data needs to be stored securely, compared privately, or used cryptographically.<br\/><br\/>A simple motivating scenario for our research is that of password-based authentication. In order to avoid security vulnerabilities inherent in storing passwords, systems often store their one-way hashes instead. When a user's password is entered for verification, it is first hashed and then compared to the stored hash value. The problem with passwords, of course, is that their entropy is low. The problem with using highentropy inputs, on the other hand, is that the readily available ones are hard to reproduce precisely: humans make typographical errors in long passphrases and forget some of the answers to multiple questions, while machines cannot precisely reproduce fingerprints and iris scans from one reading to the next. Therefore, the one-way hash function approach does not work, because even slight variations in the input will results in drastic changes of the hash value. Without additional techniques, one has no choice but to store the original<br\/>enrollment value and accept the inherent security vulnerabilities, or to exhaustively search all values close to the input value.<br\/><br\/>Intellectual Merits of the Proposed Project<br\/>The proposed research will allow verification of such noisy high-entropy inputs without requiring secret storage or performing brute-force search. What distinguishes our work from related prior work in the literature is that our approach is rigorous and versatile. The techniques we propose to study will allow the use of unreliable nonuniform inputs not only in the above password-authentication scenario, but also for keys is any cryptographic application. Moreover, the same techniques will have other applications, such as privacy-preserving data mining.<br\/><br\/>Our proposal builds on the recent work of the two PIs [42]. That work introduced new notions for using nonuniform and unreliable data cryptographically: secure sketches and fuzzy extractors. While the notions are already finding applications [40, 39], much work is needed to obtain and analyze practical constructions for a variety of input classes, to strengthen definitions, and to study specific new applications.<br\/><br\/>Broader Impacts of the Proposed Project<br\/>ON SECURE SYSTEMS. By removing the need for large-volume distributed secure storage, our work has the potential to significantly lower the costs and potential liabilities of systems that utilize biometric or other sensitive inputs for security (as detailed in the proposal description). Moreover, it may enable systems that have relied on low-entropy passwords to switch to more secure approaches, such as biometric-based key agreement.<br\/><br\/>ON PRIVACY. A significant drawback of many systems that require authentication is the loss of privacy that users experience (e.g., when having their social security numbers stored as passwords for their credit card accounts, or when having their fingerprints stored as passwords for secure doors). This work will remove the need to store private data in many applications. Moreover, as further detailed in the proposal description, the privacy protection will extend not only to the biometric (or similar) password, but also to the data protected by it, ensuring that no one without the right password will have access to the data.<br\/><br\/>ON EDUCATION. The two PIs regularly teach courses on cryptography and network security, and will be able incorporate the new results into the courses they teach. In addition, the proposal has a significant graduate student training component.","title":"Collaborative Research: Rigorous Cryptography from Biometrics and Other Noisy Data","awardID":"0515100","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["449084"],"PO":["499399"]},"104720":{"abstract":"Failures of computers or an unexpected order of messages leads to subtle bugs in distributed programs. This project is investigating algorithmic and implementation issues in monitoring and controlling multithreaded distributed computations. The techniques in this project are useful for testing distributed Java programs and for fault-tolerance during their execution. The project is investigating techniques in four areas: slicing, dependency tracking, global predicate detection and controlling a computation. Computation slicing is useful in reducing the size of the computation that needs to be analyzed. The project is developing online and distributed algorithms for slicing. Dependency tracking is required for online monitoring of global predicates and is currently done using vector clocks of dimension equal to the number of processes and threads in the system. The project is investigating a technique calledchain clocks that can track dependency in a scalable way even for a large-scale system. Global predicate detection is required to detect bugs during testing or runtime. The project is investigating detection of temporal logic predicates interpreted over the lattice of global states of a computation. Controlling a computation is useful during the testing phase to steer the computation toward software bugs and during the operation phase to steer it away from any existing software bugs.<br\/><br\/>The project is implementing a framework in Java and it will result in theoretical and practical advances in monitoring and testing of concurrent programs. The project is expected to significantly improve the quality and fault-tolerance of distributed software.","title":"CSR --- PDOS: Monitoring and Controlling Multithreaded Distributed Software","awardID":"0509024","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["561475"],"PO":["535244"]},"110055":{"abstract":"Lehigh University has been awarded Special Project funds to organize and hold a workshop on network security entitled, Automated Worm\/DDoS Eradication System Organized Information Exchange Workshop (AWESOME), in conjunction with a Cisco bootcamp on network security that Prof Chuah is hosting at Lehigh University from August 8-11, 2005. The objectives of the workshop are: <br\/><br\/>1. To attract more women and underrepresented minority students, especially those in the Tri-State Area, to network security research; <br\/>2. To minimize the learning curve of participants who are interested in network security research by providing them with tutorials on relevant topics and some hands-on experience with an NSF-funded research testbed; <br\/>3. To help participants expand their peer support network with a community of researchers who have similar objectives and experiences.","title":"Workshop on Automated Worm\/DDos Eradication System Organized Information Exchange Workshop (AWESOME), 8\/8-11\/05, Bethlehem, PA","awardID":"0538451","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["517746"],"PO":["564181"]},"105941":{"abstract":"ABSTRACT<br\/>0514772<br\/>Jorge Nocedal<br\/>Northwestern University<br\/><br\/>Active-Set and Interior Methods for Nonlinear Optimization<br\/><br\/>The goal of this research project is to advance the capabilities of algorithms for nonlinear optimization. First, it develops and analyzes a new active-set algorithm that overcomes some of the limitations of traditional sequential quadratic programming (SQP) methods. The new algorithm falls under the category of EQP methods, which decouple the active-set identification and step computation procedures. The algorithm <br\/>solves a linear program (LP) to provide a guess of the optimal active set, and then solves an equality constrained quadratic program (EQP) to attempt to achieve optimality. A key feature of the new algorithm is the use of two trust regions (one for the LP phase and one for the EQP phase that act quasi-independently.<br\/><br\/>The second project investigates nonlinear interior methods -- the other leading approach for large-scale optimization. Research focuses on one of the most difficult algorithmic questions: how to design a<br\/>procedure for controlling the barrier parameter that is effective in practice and is supported by global convergence guarantees. The proposed procedure selects the barrier parameter at every iteration<br\/>byminimizing certain \"quality functions\". New strategies for computing corrector steps allow for longer steps even when the initial point is poorly chosen. A globalization procedure that interferes with adaptive choices of the barrier parameter as little as possible will be developed.<br\/><br\/>The software developed as part of this project will be beneficial in the numerous areas of application of optimization algorithms. Large optimization problems arise in circuit simulation, computational chemistry, finance, PDE-based optimization, traffic equilibrium, and many other areas. The new algorithms developed<br\/>in this project will significantly expand the range and applicability of nonlinear optimization methods, and will stimulate future research in areas were large-scale optimization plays a crucial role.","title":"Active-Set and Interior Algorithms for Non-Linear Optimization","awardID":"0514772","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["517236"],"PO":["381214"]},"110088":{"abstract":"From mathematics word problems to great literature to science texts, being able to read and write is of paramount importance to academic success. In addition, with the Federal mandate of \"no child left behind\" an increasing national priority is learning objectives that prepare students to do well on national standardized tests, and many of these learning objectives rely on literacy skills. Yet literacy remains a critical unsolved issue in our educational system, with national test scores showing that non-white children (e.g., African Americans and Hispanics) still score well below their Caucasian counterparts. Increasingly, approaches to this problem derive from the recognition that primary school education is based on a set of mainstream oral practices and literacy-preparation skills. Because many children do not share the same cultural experiences typical of mainstream culture, educational practitioners have applied a cultural-historical approach to identify common characteristics of ethnic groups, and then designed cultural supports for literacy learning by children of diverse backgrounds. But while classroom practice has been influenced by this approach, and training programs for parents have been instituted along these lines, it has been rare to see technological learning environments that leverage diversity in any but the most superficial ways (for example identical content characters with different skin color, or content based on traditional associations such as the Anancy myths). The PI believes she can do much better. To this end, in the current project she will first conduct an in-depth investigation into African-American peer-oriented oral language and literacy practices, in order to understand how cultural practices support individual learning and development in this population. Using these findings, she will then design a virtual peer that engages in authentic cultural practices with African-American children as a bridge to school-based literacy. As the technology is developed, the PI will bring it into classrooms and community centers, formatively evaluating and iteratively redesigning as necessary until the virtual peer is capable of being like African-American children in important ways, can exploit that affinity to establish rapport, and can leverage this rapport and linguistic interaction to help the child acquire literacy skills. Project outcomes will include a rich comparative description of the language and nonverbal practices of African-American and Caucasian children in peer-emergent literacy interaction, a set of behaviors that allow a virtual human to establish rapport with users (including local-level moment-by-moment interactional behaviors and larger scale culturally specific practices), and a Flash virtual peer and set of virtual peer behaviors that have been shown to improve literacy in the children interacting with the virtual peer, both for Standard American English (SAE) speakers and African American Vernacular English (AAVE) speakers. A summative evaluation will address transfer from interaction with the virtual peer to other standard literacy contexts and tests. <br\/><br\/>Broader Impacts: This project addresses fundamental issues relating to acquisition of literacy skills by young children who are members of an underserved population, and will lead to new technology for improving their literacy readiness. The PI will pursue an innovative program of dissemination of results and research practices that involves undergraduates from HBCUs, local schools with high populations of African Americans, and local churches and community centers, in order to broaden in the short term participation by African-American undergraduates in the engineering research behind this project and in the longer term participation in STEM by young African-American school children.","title":"SGER: Multicultural Story Listening Systems","awardID":"0538610","effectiveDate":"2005-07-15","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":["496760"],"PO":["565227"]},"104764":{"abstract":"A five billion dollar industry has arisen over the last 25 years to develop and support tools and languages for synthesis, verification, and analysis of integrated circuits (ICs). Within the next five years, IC design will further change as it will be possible to provide enough transistors on a single chip for on the order of a hundred 16-bit processors. The systems will be designed to meet their performance goals through the exploitation of task level parallelism. These software-intensive systems will require modeling capabilities far beyond traditional computer-aided design tools. While software engineering techniques also contribute as design aids, they do not address the important aspect of performance; a key goal in the design of real, constrained systems. Central to the performance of these systems is the scheduling of the concurrent tasks onto concurrent processing elements. <br\/><br\/>Bottlenecks at one point in the system can cause system-wide performance degradations. This research will investigate defining a new level of modeling for the design of single-chip heterogeneous multiprocessing systems. The project will investigate the development of the underlying models that include processing resources, application tasks, and schedulers , and will develop a diagnostic aid based on the concurrent system model, and a simulator to be made available on a website. This research will define, demonstrate, and teach a new level of computer system modeling.","title":"CSR---SMA:Single-Chip Heterogeneous Multiprocessor Systems: Defining a New Level of Design","awardID":"0509193","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["507772"],"PO":["551712"]},"105996":{"abstract":"Multihop transmission is increasingly being incorporated into modern wireless communication networks. These networks are central to our nation's future communications and monitoring infrastructures. The<br\/>basic motivation for multihop is that transmissions occur over shorter distances -- and therefore with higher received signal strength -- via many intermediate nodes rather than over longer distances -- and<br\/>therefore with lower received signal strength -- between the source and destination of the information. However, multihop transmission involves complex interactions among channel coding at the physical<br\/>layer, distributed channel access at the link layer, and multihop routing at the network layer. These techniques have been studied largely in isolation by different communities, whereas this project<br\/>focuses on their interaction, especially in delay-constrained scenarios.<br\/><br\/>This research involves models for general wireless multihop networks, and develops tradeoffs for transmission along an individual routes of up to M + 1 nodes. Transmission between the end nodes can occur in a single hop, or up to M hops. Multihop transmission increases the received signal-to-noise ratio (SNR) at intermediate nodes; however, this observation does not take into account the important practical issues of power and bandwidth allocation, end-to-end delay, error propagation, or interference induced by other transmitters. Among other results, preliminary research indicates that the benefits of multihop are eroded by these issues, especially for high spectral efficiency, i.e., high data rates relative to the available bandwidth. The investigators take a comprehensive look at multihop transmission from the point of view of communication theory, mathematical networking, and networking practice, with the goal of offering solutions that will impact a major part of our society.","title":"Delay-Constrained Multihop Transmission in Wireless Networks: Interaction of Coding, Channel Access, and Routing","awardID":"0515012","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["508186","40607","540624","540625"],"PO":["432103"]},"103455":{"abstract":"SUMMARY<br\/>Intellectual Merit<br\/>Probabilistic considerations arise in the analysis of algorithms in at least two important ways. First of all, in a randomized algorithm the outcomes of random events are used to determine the progress of the algorithm. Randomization is now a standard tool of the computer scientist. A second area of consideration is when the problem instances come from some probability distribution and one wants to understand the average performance of a particular algorithm, which is often far better than its worst case. Both aspects are extremely important and this proposal describes a number of problems in these two areas.<br\/>Broader Impact<br\/>Results from this work will be disseminated at scientic workshops and at seminars at other institutions, both nationally and internationally. Results obtained will be published in journals and conference proceedings. The P.I. takes care to ensure that all of his recent papers are available on-line on his home-page. Work on Monte-Carlo Markov Chain analysis has an impact on Probability Theory and Sta- tistical Physics. In addition the proposal will have an impact on education, mainly at the graduate level. The PI tries to embed the knowledge gained from his research into courses and tries to involve his graduate students (currently four of them) as much as possible in any research that he does. Sometimes the work involved is of such a nature that it can lead to meaningful summer projects for bright undergraduates. In such cases the P.I. has sought and will seek additional support from the NSF and Carnegie Mellon University. In the recent past the P.I. has supervised such projects on Small-World networks, on Resource Discovery in Distributed Networks, on a<br\/>Directed Model of Web-Graphs and on a Graph Version of Nim. In addition, the P.I. is actively involved in the NSF funded Aladdin project in the Computer Science Department at CMU and this has a signicant out-reach component. Together with Danny Sleator, the P.I. runs a popular puzzle page:<br\/>http:\/\/www.cs.cmu.edu\/puzzle\/","title":"Probabilistic Considerations in the Analysis of Algorithms","awardID":"0502793","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["481434"],"PO":["499399"]},"105765":{"abstract":"Collaborative Research: Foundations of Solving Large Direct and Inverse Scattering Problems --- Algorithm Analysis and System Support<br\/><br\/><br\/><br\/>Summary<br\/><br\/>We propose to develop and implement new computational methods on large cluster-based high-end systems for solving the direct and inverse problems in electromagnetics motivated by industrial and military applications. We will address two sets of important and closely related technical issues for this high-end scientific computing project. First, the targeted problems for the proposed project are large-scale direct and inverse scattering problems that can be only solved in high-end systems. These problems involve<br\/>particularly electromagnetic wave propagation with high wave numbers. A major difficulty for solving the inverse problems by an optimization method is the ill-posedness and the presence of many local minima. We propose a novel approach for solving the inverse medium scattering problem of Maxwell's equations in three dimensions. Crucial to the approach will be the development of an efficient regularized iterative linearization algorithm (recursive linearization with respect to the wave number). A challenge in<br\/>developing our numerical methods is to deal with large and structured data sets. The second set of technical issues for solving the targeted problems is concerned with the lack of system support in high-end architectures to maintain high sustained performance of<br\/>computing due to increasingly high speed gap between the CPU and the memory and the I\/O storage. This challenge can also be found in many other large scientific computation problems on high-end systems. An equivalently important objective to the scientific computing in this proposal is to design and build effective system support by effectively allocating both CPU and memory resources, by establishing a global network RAM system in high-end architecture, and by providing exceptional system handlers to deal with dynamic and unexpectedlylarge memory demands from applications.<br\/><br\/>Intellectual merits of this proposal come from several aspects. (1) Our proposed numerical methods will address several scientific challenges in applied mathematics including electromagnetic wave propagation with high wave numbers, ill-posedness for inverse problems, and management of large data sets in multiple dimensions. (2) Processors and high-end systems have become increasingly complex, which makes the understanding of execution behavior more and more difficult. Our proposed system support based on both hardware counters and a system kernel instrumentation tool will address the system complexity issue, and provide insightful runtime system information for resource management systems with low overhead. (3) In order to effectively support high sustained performance and high productivity computing in clusters, our system support aims for several important resource management objectives, such as high memory utilization, low communication latency, and fast response time. (4) Although our system will be mainly tested by solving the large direct and inverse problems, it is also our aim to build it as a general purpose system so that it will become a fundamental software system infrastructure for many other large scientific applications in high-end systems.<br\/><br\/>Broader impact of this proposal will be: (1) Due to the fast development of high performance systems, computational electromagnetics has become a fundamental, vigorously growing technology in diverse science and engineering disciplines, such as<br\/>microwaves, millimeter waves, optics, and acoustics. Our computational models and cluster system support will provide an inexpensive and easily controllable ``virtual prototype\" of the structures\/media as opposed to costly, time-consuming physical<br\/>prototyping. (2) The proposed system resource management tools and system prototype will be disseminated in the high-end computing and systems community for a wide usage. (3) The research results will be timely introduced to both undergraduate and graduate curriculum development of scientific computing, parallel computing, and operating systems.","title":"Collaborative Research: Foundations of Solving Large Direct and Inverse Scattering Problems - Algorithm Analysis and System Support","awardID":"0514078","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["536664","512771"],"PO":["381214"]},"105787":{"abstract":"Our modern times are characterized by vast amounts of information needed to be stored, searched, encrypted, transmitted, compressed, and so on. The major increase in the amount of data we wish to handle has turned the feasible-but-costly chores of yesterday to infeasible ones, and the feasible chores of yesterday to costly<br\/>ones. Linear or even sub-linear algorithms are imperative. The stringent complexity<br\/>requirements imposed by the data explosion phenomena raises new challenges in a large variety of fields and applications. <br\/><br\/>The wide variety of these algorithmic tasks seems to require individual study of the separate challenges, carefully learning the specific properties of each distinct challenge, in a quest for the special angles by which an improved algorithmic respond may be given. A formidable and diverse challenge indeed.<br\/><br\/>Luckily, there are basic algorithmic building blocks that repeatedly emerge in many of the current algorithmic solutions. One of these recurring building blocks is the use of Fourier Transform, and in particular of the Fast Fourier Transform (FFT) algorithm. The FFT algorithm is famous for its efficiency, computing the Fourier Transform of a signal of length $n$ in time $\\Theta(n\\log n)$. Alas, in the settings of the Data Explosion Era, a super-linear time-complexity often does not suffice. However, examining usages of the FFT algorithm, reveals that in many cases, instead of computing all the entries in the Fourier Transform of a function, it would actually suffice to know only a few \"heavy\" entries. A task for which sub-linear algorithms have recently be devised.<br\/><br\/>We propose to <br\/>Explore new input-models for the recent sub-linear algorithms for finding the \"heavy\" entries of the Fourier transform of functions, which emerge in computer science applications.<br\/>Improve the time-complexity of the recent sub-linear algorithms for finding the \"heavy\" entries of the Fourier transform of functions, which emerge in computer science applications.<br\/>Devise faster algorithms for applications that can benefit from a sub-linear algorithm for finding the \"heavy\" entries in a Fourier transform. In particular, for application in which the current algorithm uses the FFT algorithm, while it would suffices to find only the \"heavy\" entries in the Fourier transform.<br\/>Explore applications of the improved algorithms to questions in complexity theory, cryptography, learning theory, and coding theory.<br\/><br\/>The intellectual merit of the proposed research is that if successful it will have significant impact on our understanding of basic issues in cryptography, coding theory, and complexity theory at large. The impact of speeding up algorithms for <br\/>data processing tasks would be tremendous.<br\/><br\/>The broader impact of the proposed research will be through the PI's disseminating the knowledge and understanding gained in courses taught at the graduate and undergraduate level on cryptography and algorithms at MIT, as well as in research seminars and conferences nationally and internationally. In addition, the PI and the graduate students working on this<br\/>research are both women.","title":"Learning Fourier Coefficients: Theory and Application","awardID":"0514167","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["562010"],"PO":["499399"]},"102157":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Collaborative: Next Generation CiteSeer <br\/><br\/>Lead Proposal: CNS 0454203<br\/>PI: Gregg Rothermel<br\/>Institution: University of Nebraska-Lincoln<br\/><br\/>Proposal CNS 0454348<br\/>PI: John M. Hatcliff<br\/>Institution: Kansas State University <br\/><br\/> <br\/>The investigators will create and disseminate a repository of software-related artifacts sufficient to support rigorous controlled experimentation with program analysis and software testing techniques for a broad community of researchers and educators. Software written in the C and Java programming languages along with supporting elements such as multiple versions, faults, test requirements, specifications, test cases, functional behavior specifications, and state-machine models of program behavior will be included. Methods to retrieve program information will be developed. This resource will be an enabling technology for controlled experimentation for program analysis, software testing, and education in software engineering and software testing. Broader impacts of this project include the impact of the resource on enabling a broad range of research and improving capacity for software education in a wide range of institutions.<br\/>universities.","title":"CRI: Collaborative Research : A Community Resource to Support Controlled Experimentation with Program Analysis and Software Testing Techniques","awardID":"0454203","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["561782","548132","518212"],"PO":["550859"]},"107734":{"abstract":"Akin to a complex engineered system, biological processes operate through many simultaneous interactions within complex networks. Traditionally, biologists have constructed \"models\" to capture this complexity and verify their intuitions as well as communicate how a particular biological system or subsystem actually works. To build these models, biologists rely on a very general and broad array of knowledge, and also augment it with depth and expertise obtained from small number of exemplar systems. With availability of large amount of high-throughput experimental data, biologists are also faced with the task of reconstructing models from data where relevant information may be deeply buried in layers of numerical information.<br\/>Biological models are often presented pictorially as graphs and flow charts with many components, each corresponding to a certain biochemical reaction. Such diagrams have also, but not always, been associated with mathematical models, mostly in the form of differential equations. The equations are used to perform simulations of the system, when they have a well-defined set of kinetic parameters. The model is refuted or validated depending on whether the simulated traces agree with biological data.<br\/><br\/>Often one is faced with situations, where there is no mathematical model, or the model is incomplete and they lack a complete set of parameters, and yet biologists do have detailed descriptive understanding of many of the components and their interactions. For instance, current microarray data analysis techniques draw the biologist's attention to targeted sets of genes but do not otherwise present global and dynamic perspectives (e.g., invariants) inferred collectively over a dataset. When ontologically invariants are inferred from experiments (using GOALIE redescription tool), such invariants can be compared with the known descriptive information to determine if we have complete and consistent theories about certain biological processes.<br\/>This project addresses these two scenarios by providing automated reasoning tools that bridge both computational and descriptive models in biology. The results from these tools and experimental analyses hint at the construction of efficiently testable predictions. The results of wet-lab experiments are then used to refine and amend the formal model. This feedback cycle between modeling and experimentation has proven important in obtaining a process-level understanding of the underlying cellular machinery.<br\/><br\/>The further characterization of specific parts of the mammalian cell cycle behavior (e.g. how a possibly unknown factor may allow the phosphorlyzation Cdk inhibitor p27 by Cdk2 at G1\/S.)<br\/>In the longer run, understanding the wider implications of the complex regulatory and metabolic architecture of the cell cycle will provide significant insights into new applications of biology and advanced computing. In addition, they will provide new perspectives on computing by exploiting biologically driven metaphors. More importantly, the approaches developed in the context of hybrid-system (HS) models and bio-ontology will find applications to swarm robotics, social-software, e-commerce, complex interactive engineered systems, computer-security, adaptive software, etc., although from our own historical perspective, we will remain engaged in proving the first successes of this approach in biomedical applications.","title":"BIC: EMT: Innovative Symbolic Hybrid Systems Models, Inspired by Biological Networks and Bio-Ontology","awardID":"0523851","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["417590",284568],"PO":["565223"]},"105798":{"abstract":"Project Abstract<br\/>0514213<br\/>G. W. Stewart and Dianne P. O'Leary<br\/>U of Maryland College Park<br\/><br\/><br\/>This work considers several computational problems associated with unitary and real orthogonal matrices.<br\/><br\/>The intellectual merit of the work lies in the mathematical and algorithmic ideas.<br\/><br\/>First, the investigators seek a unified view of the least squares family of solutions to data fitting problems. The purpose is to improve algorithms for solving such problems and to advance the understanding of<br\/>the relationship among the various data models. The investigators have derived a common framework for least squares, data least squares, total least squares, and regularized least squares, for problems with single or<br\/>multiple sets of observations. Using this, they will study the existence and uniqueness of solutions and the behavior of the solutions as parameters are varied. Efficient algorithms will be derived, both for small problems and for problems too big to be solved directly. In addition, problems that use norms other than least squares will be investigated.<br\/><br\/>Second, the investigators will study sparse QR factorizations of sparse matrices into the product of an orthogonal matrix and a right-triangular matrix. The usual QR factorization is quite storage intensive, since Q typically has many more nonzeros than the original matrix. Recently one of the investigators developed an algorithm that represents the QR factors in terms of the data in the original matrix, along with a small dense matrix, thus preserving sparsity. By terminating the algorithm early, an approximate factorization can be formed to any given tolerance. The investigators will produce quality software implementations of this<br\/>algorithm and test its performance on data from a variety of applications.<br\/><br\/>Third, the investigators will study several unitary matrix problems arising in quantum computing. The first set of problems comes from computing quantifiers for entanglement; in particular, the goal is to<br\/>compute the concurrence capacity for various operators of practical interest, including the Quantum Baker's map, by localizing the eigenvalues of certain unitary matrices or by solving an optimization problem. The<br\/>second set is motivated by adiabatic quantum computing and involves perturbation analysis on various homotopies between matrices in order to understand their behavior and prove a stability property for them.<br\/><br\/>Finally, the investigators will develop software for updating orthogonal matrix factorizations when rows or columns are added or deleted, either singly or in blocks (in collaboration with the LAPACK project).<br\/><br\/>The broader impact of the work arises from its applications in other problem areas and in its value to education. The mathematical and computational results obtained under the first topic will impact data fitting in science and engineering. The second topic has application to problems as diverse as astronomical image compression and medical information retrieval. The third might influence the design of practical quantum computers. The fourth has application, for example, to data assimilation in signal processing.<br\/><br\/>Outreach activities will include working with women and minority graduate students and developing projects for computational science education based on the research. Two students will be supported under this grant. The research will continue to be incorporated as appropriate into computational science courses and published as computational science educational projects. They will also influence a textbook on computational science to be begun next year.","title":"Computations with Unitary Matrices","awardID":"0514213","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["450658",279149],"PO":["381214"]},"108835":{"abstract":"This proposal requests funding to assist approximately 15 US-based graduate students to attend the 2nd Symposium on Networked Systems Design and Implementation (NSDI). Participation in NSDI and similar conferences is a valuable and important part of the graduate school experience. It provides students with the opportunity to interact with more senior researchers in the field, and exposes students to leading edge work in the field. The support requested in this proposal will enable the participation of students who would otherwise be unable to attend NSDI.","title":"Student Travel Support for the Second USENIX\/ACM Symposium on Networked Systems Design and Implementation","awardID":"0530882","effectiveDate":"2005-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["543394"],"PO":["402055"]},"107746":{"abstract":"This project involves connections between formal language theory, finite state machines and biomolecular protocols, and consists of three main tasks:<br\/><br\/>1.Developing an autonomous read-write transducer and automata with unbounded memory encoded on circular plasmids.<br\/><br\/>2.Scaling up the computational process allowing these finite state machines to perform parallel computation while immobilized on surfaces.<br\/><br\/>3.Taking the molecular computing concept one step further by showing that the output can be not just for a specific molecule, but also for a specific biological function, proving that biomolecular computing can be biologically relevant. The two main approaches include: (a) control over the open reading frame (ORF) of an enzyme-encoding gene, and (b) design of biologically meaningful output signals such as bacteria possessing either fluorescent proteins or antibiotic-resistance properties.<br\/><br\/>The project prototypes a molecular machine that is autonomous and produces an output that can potentially serve as an input for a new molecular automaton or as a template for biological organization and regulate gene expression. Methods include real time observation of the automaton dynamics and has a potential for an algorithmic control and modification of DNA strands and genes. Theoretically, the design of the automaton calls for investigations of new types of splicing systems. The process for scaling up the computations may lead to a new crypto system as well as more rapid tests for genetic disorders that are triggered by a combination of mutations. Very few individuals enter the field of DNA-based computation with proper preparation. We will attempt to provide some solutions to this problem during the course of the project. All graduates, undergraduates and high school students who participate in this work will be trained and prepared as unique interdisciplinary research scientists. We anticipate that three graduate students will receive graduate training through this award.","title":"BIC: Enzyme Driven Autonomous Biomolecular Computer","awardID":"0523928","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["506031",284602],"PO":["565223"]},"105205":{"abstract":"This project is a study of some problems in computational complexity theory, in particular<br\/>some problems in structural complexity theory, and a number of algorithmic problems related to graph isomorphism.<br\/><br\/>In graph isomorphism, the investigator proposes to study further notions of coloring schemes, and in particular how such coloring schemes relate to graph spectra. We also wish to study a number of structural complexity theoretic questions as related to graph isomorphism and graph automorphism. It is expected that techniques of group theory, linear algebra, and combinatorics will interact in this study. The PI in collaboration with Prof. Osamu Watanabe have proposed a notion of stringent relativization, which is a generalization of the Karp-Lipton notion of advice strings. This study is related to circuit complexity theory and techniques of derandomization and algebraic techniques. We would like to develop this notion further. The investigator also proposes to study further a notion of persistent NP-hardness. We envision this to be an intermediate level of complexity measure between worst-case hardness and average-case complexity in the framework of Levin and others.","title":"Some Problems in Complexity Theory","awardID":"0511679","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["517847"],"PO":["499399"]},"105689":{"abstract":"Scientists are faced with increasingly larger volumes of data to analyze. To analyze and validate various hypotheses, they need to create insightful visual representations of both observed data and simulated processes. Often, insight comes from comparing multiple visualizations. But data exploration through visualization requires scientists to assemble complex pipelines consisting of sequences of operations that transform the data into appropriate visual representations, and today, this process is far from interactive. It contains many error-prone and time-consuming tasks, greatly hindering the scientific discovery process.<br\/><br\/>The Visualization Management System (VMS) streamlines the creation, execution, and sharing of complex visualization pipelines. It extends traditional dataflow-based visualization systems in several significant directions: it enables interactive multiple-view visualizations by simplifying the creation and maintenance of visualization pipelines, and by optimizing their execution; it provides scalable mechanisms for generating and managing a large number of visualizations; it allows systematic maintenance of data product provenance (akin to a lab notebook) and enables the automatic re-execution of visualization pipelines. VMS gives scientists more control over the visualization process, simplifying many of their day-to-day activities such as allowing them: to resume their explorations where they left off; to apply identical operations to a new data set without having to redo a long sequence of operations; or to share their findings with colleagues.<br\/><br\/>VMS has the potential to substantially improve scientists' ability to carry out visualization and data exploration. By making VMS freely-available and by integrating it with the leading visualization systems, it can be widely used by the scientific community to support and facilitate scientific discoveries.","title":"Managing Complex Visualizations","awardID":"0513692","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}}],"PIcoPI":["521992","521991"],"PO":["565136"]},"116227":{"abstract":"In the past few decades, there have been tremendous advances in microprocessor technology and the use of digital controllers in the automation of physical plants. The increasing integration of such controllers results in highly complex systems involving both continuous and discrete event dynamics. In addition to discontinuities introduced by the computer, most physical processes exhibit discrete dynamics due to the action of elements ranging from valves, gears and switches in electromechanical systems to transcriptional regulators in genetic and metabolic networks. Such systems combining discrete and continuous features are called hybrid systems.<br\/><br\/>Formal verification is a very important issue during system design. Its goal is to prove that the system performs as expected. As the automated systems are growing in scale and complexity, the possibility of subtle errors is much greater. This project develops scalable, provably correct algorithms and software tools for safety verification and reachability analysis of hybrid systems. First, the project investigates the construction of discrete abstractions. While doing this, this work attempts to enlarge the class of known decidable hybrid systems. Second, enabled by recent advances in semialgebraic methods and convex optimization, the project investigates a new method for safety verification that does not require explicit calculation of trajectories or reachable sets, and provides a nested family of sufficient conditions for system safety that are polynomial-time checkable. Third, bringing ideas from motion planning based on randomized techniques, the project develops an algorithm for test case generation.<br\/><br\/>The algorithms developed in this project are implemented as a Reachability Analysis and VErification (Rave) toolkit and tested by considering two very difficult problems arising in the safety of multi-agent robotic systems. The results of this project also have an immediate impact in a wide range of areas where hybrid systems are used for modeling, such as automated highway systems, air-traffic management systems, genetic and metabolic networks, embedded automotive and avionic controllers, robotics, and real-time communication networks.","title":"Scalable algorithms for safety verification and reachability analysis of hybrid systems","awardID":"0611925","effectiveDate":"2005-07-01","expirationDate":"2008-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2801","name":"EMBEDDED & HYBRID SYSTEMS(EHS)"}}],"PIcoPI":["496541"],"PO":["561889"]},"109869":{"abstract":"This project will collect, organize and preserve historic materials, particularly film, that are part of the historical record of the field of Artificial Intelligence (AI). It will create an organized digital archive and use highlights selected from the archive to illustrate the intellectual history of AI in a coherent narrative. It is intended that these materials will be made publicly available, for instance through a web site or DVD. Sources for this project included notes, memos and technical reports from MIT and elsewhere, and in particular, a uncatalogued, unconserved and uncurated collection of films that recently came to light at MIT. <br\/><br\/>The project will proceed in two phases. In Phase 1, the project will gather films, inventory their condition and content, organize them into a historic time line, convert the collection into digital format, and create an on-line archive of the collection. In Phase 2, the project will create a connecting narrative at the level appropriate to a broad popular audience (e.g., at the level of viewership of PBS's NOVA series). This phase will also select a collection of anecdotal film clips from the larger collection to highlight some of the major moments in the history of AI. Lastly, the project will create a web site or DVD to showcase the selected clips, the connecting narrative, and other more technical materials.","title":"SGER: Rescuing the MIT Film History of AI","awardID":"0537285","effectiveDate":"2005-07-01","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":[290967,290968,"198445"],"PO":["387198"]},"107328":{"abstract":"This project from an HBCU, acquiring a parallel and configurable computing framework using conventional PCs equipped with a set of FPGAs, develops application implementations that execute much faster than the best single\/parallel PC implementation. Configurable computing systems allow the same hardware to be used for various applications for distinct parallel algorithms. Hence, rather than replacing expensive system hardware, new versions of a hardware architecture can be loaded into the same configurable integrated circuit reducing the system costs. A major goal in developing this system consists in automating the process of mapping algorithms onto a configurable computer in a transparent fashion. Projects using this implementation include:<br\/>-Parallel configurable digital signal processing,<br\/>-Parallel algorithm development for problems in applied mathematics and the computational sciences,<br\/>-Parallel implementation of 3D image reconstruction for PET scanner data,<br\/>-Modeling of a configurable digital signal processing subsystem for handheld electronic nose,<br\/>-Cubic spline interpolation, and<br\/>-Sensor-based supply chains.<br\/>Each of these applications will be mapped onto the parallel configurable computing system to perform comparisons with the best single\/parallel PC implementation and to make the appropriate optimizations to execute orders of magnitude faster on this parallel configurable computer. Moreover, the infrastructure supports the following research activities:<br\/>-Detection of cancerous tumors using 3-D PET image reconstruction,<br\/>-Onboard processing for control of spacecraft and other science data processing required by NASA scientists and engineers,<br\/>-High performance image processing,<br\/>-Odor remediation and assessment, and<br\/>-Sensor-based supply chains.<br\/><br\/>Broader Impact: The work exhibits potential to bring serious savings to product tracking through their life cycle. The Auto-ID Center has estimated potential global supply savings between $150 to $300 billion a year. Through RFID deployment, Wal-Mart alone could save $8.36 Billion annually, including $600 million through avoiding stock-outs and $575 million avoiding theft. Furthermore, enabling under-represented students and faculty, this research takes place at a major HBCU with collaboration with other universities and federal agencies.","title":"MRI: Acquisition of a Parallel Configurable Computer for Research in Engineering and the Computational Sciences","awardID":"0521365","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":["561800","344345"],"PO":["557609"]},"106008":{"abstract":"Analysis of Markov Chains and Algorithms for Ad-Hoc Networks<br\/>Dana Randall, Principal Investigator<br\/>Georgia Institute of Technology<br\/>Project Summary<br\/>The first research direction outlined in this proposal concerns central open questions in the design and analysis of Markov chains. The first is to improve the decomposition technique for analyzing convergence rates. Decomposition is a powerful tool for breaking a complicated chain into smaller pieces that are more amenable to analysis. Next, the proposal examines new sampling algorithms for contingency tables, or non-negative integral matrices that satisfy prescribed row and column sum constraints. We plan to explore the cell-bounded generalization where the entries in the matrices are further constrained to satisfy given upper bounds. These tables have applications in statistics, and the cell-bounded case includes well-studied computer science applications such as approximating the permanent and sampling bipartite graphs with given degree sequences. The second proposed direction is studying the behavior of protocols on ad hoc networks. Sampling graphs with known degree sequences is an important challenge in the context of the Internet and web graphs, where practitioners are trying to develop efficient protocols on graphs whose degrees satisfy conjectured power laws. An additional research focus will be to design power-efficient protocols for sensor networks that guarantee connectedness and efficient performance. The Adaptive Power Topology Control Algorithm is a local approach to building up networks in this context, but little is understood about the tradeoffs between the sparsity of the graphs and the optimization of power. Moreover, most of the analysis to date has been done only with the idealized assumption of the disk model for wireless<br\/>footprints. The proposed research will explore these tradeoffs and more realistic footprint assumptions. Intellectual merit: Markov chain Monte Carlo remains a popular method in many disciplines for studying large combinatorial systems. Recent developments for analyzing their convergence rates have established the first rigorous, polynomial time algorithms for many fundamental sampling problems. There remain many opportunities for furthering this research, both by developing new methods for analyzing these chains and designing new algorithms to address particular applications. While convergence rates are well understood from a mathematical perspective, new methods for systematically deriving polynomial bounds on their running times are still required. Remaining challenges include developing new sampling algorithms for various applications and designing new tools to aid their analysis. This remains one of the foremost areas where theoretical computer science can impact other scientific disciplines because of the large amounts of computational resources currently be expended on nonrigorous sampling heuristics. Ad hoc networks are gaining prominence in the field of algorithms as the Internet and wireless devices become central tools. There is great opportunity for developing rigorous protocols that are robust under a wide set of operational assumptions.<br\/>Broader impact: This research will be supplemented through an educational component. Starting in Fall 2005, the P.I. will be chairing the organizing committee of a DIMACS special focus on Discrete Random Systems, concentrating on this area of interdisciplinary research. In addition to the typical workshops bringing together leading researchers in the relevant areas, there will also be workshops promoting broader impact. One such workshop will provide an outreach to practitioners using clever heuristics in the hopes that collaborations will lead to the design of faster rigorous algorithms; another will focus on applications of Markov chains in other areas of computer science, including spectral methods used to study the Internet and web graphs.","title":"Analysis of Markov Chains and Algorithms for Ad-Hoc Networks","awardID":"0515105","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["518569"],"PO":["562944"]},"102543":{"abstract":"Proposed by the Council on Competitiveness is a $321,043 over 24 months project to investigate how partnerships between the NSF Cyber-infrastructure Centers (e.g. National Center for Supercomputing Applications (NCSA), San Diego Supercomputing Center (SDSC) and the Pittsburgh Supercomputing Center (PSC)) and U.S. industry have succeeded in advancing industrial productivity and innovation, and how these partnerships can be leveraged more effectively to support and stimulate U.S. competitiveness.","title":"SCI: High Performance Computing and Competitiveness project","awardID":"0456129","effectiveDate":"2005-07-15","expirationDate":"2008-12-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7368","name":"SCI TESTBEDS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"T273","name":"ENERGY-NATIONAL SECURITY"}}],"PIcoPI":[270395,270396],"PO":["438085"]},"105711":{"abstract":"An overwhelming and growing volume and diversity of biological data is available on the Web.<br\/>The objective of the proposed work is to help biology students and researchers make effective use<br\/>of this wealth of data by developing a proof-of-concept software assistant, BioLogica, capable of<br\/>inferring information from multiple and diverse data sources. A cross-disciplinary team has been<br\/>assembled to carry out this work. Automated logical deduction will be used to achieve semantic integration of multiple heterogeneous biological data sources and to answer queries that involve combination of data from diverse data sources. An axiomatic biological theory will be developed with the ability to 1) Express fundamental biological concepts and relationships<br\/>2) Represent metadata describing the capabilities of diverse data sources, 3) Understand the meaning of complex queries, 4)Decompose queries into simpler components to be answered by available sources and 5)Assemble answers by combining results from multiple component queries. While queries can be posed in English or conveyed via a graphical user interface, ultimately they are rephrased as conjectures, that is, theorems to be proven in the formal biological theory. The theorem is proven by an automatic theorem prover, and an answer to the query is extracted automatically from the proof. The project includes the development of a formal biological language and theory, the development of techniques to automate the formation of \"procedural attachments\" (software to access the data sources), and the discovery of domain-specific strategies to accelerate the theorem-proving process. BioLogica will demonstrate that the application of spatial and temporal inference and more general reasoning applied to a domain-specific formal theory provides an effective approach to the semantic integration of scientific data sources.<br\/><br\/>The formal knowledge base and prototype tool will be made available on the Web, and annual<br\/>workshops will be held to acquaint the broader community with use of the tool and to exchange<br\/>ideas about improving the underlying technology. A variety of planned student projects will<br\/>train students in techniques for integration of diverse biological data sources and in development<br\/>of bioinformatics tools. We will work with interested faculty to incorporate use of the tool into<br\/>courses. Among the social impacts are developing a software assistant that enables research biologists to deal with the bewildering multiplicity of available online data and making such data available to instructors and students. The natural-language component of the proposed prototype is capable of spoken, as well as typed, interaction. Thus BioLogica could be adapted for use by researchers and students with visual or other impairments. It is expected that the techniques introduced by BioLogica for the biological sciences will more generally be applicable to semantic integration of data sources in all the sciences.","title":"II(BIO): BioLogica--Deductive Integration of Heterogeneous Biological Data Sources","awardID":"0513857","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}}],"PIcoPI":[278923,"550115"],"PO":["565136"]},"105953":{"abstract":"ABSTRACT<br\/>0514819<br\/>PI: Xindong Wu; Co-PIs: Abdullah N. Arslan and Xingquan Zhu<br\/>U of Vermont & State Agricultural College<br\/><br\/>Pattern Matching with Wildcards and Length Constraints<br\/><br\/><br\/>This research defines a unique problem of pattern matching with wildcards and length constraints, and aims to design efficient algorithms for the problem. Given a pattern P and a text T, a substring S in T is a matching string of P if (1) the number of wildcards between each two consecutive pattern letters in S and (2)<br\/>the length of S are both bounded by the user's specifications. The project seeks to find the maximum number of ``distinct'' occurrences of P in T. This is a complex problem that integrates both local constraints<br\/>(in the form of gaps between consecutive pattern letters) and global length constraints in pattern matching.<br\/><br\/>The research team will start with an existing preliminary design and further investigate the pattern matching problem, by (1) exploring the time complexity of the problem, (2) designing new, efficient <br\/>algorithms to deal with some special cases, and (3) applying these efficient algorithms in practical problems in text indexing, gene sequence analysis, network security and stream data mining.","title":"Pattern Matching with Wildcards and Length Constraints","awardID":"0514819","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["526618",279531,279532],"PO":["381214"]},"104743":{"abstract":"Device drivers are operating system components that are typically developed independently by hardware vendors and third parties, often under the assumption that the driver is the only time-critical component in the operating system. Device drivers execute with full kernel privileges and can circumvent the system's scheduling mechanisms to meet their own requirements. This, in turn, interferes with the execution and timing of other applications and device drivers.<br\/><br\/>This project seeks to improve the state-of-the-art in device driver support for embedded real-time applications. The approach involves fitting the scheduling of device driver execution within the operating system via paradigms for which model-based schedulability testing can be performed. The research studies representative examples of existing device drivers to determine how they fit well-understood formal models for schedulability analysis. It explores how the drivers, the operating system's device driver scheduling mechanisms, and formal models can be improved to achieve more predictable performance for real-time applications. The research activities include formal analysis, prototype implementation of new architectural features, and performance testing.<br\/><br\/>This research strives to address problems that the device drivers in current operating systems pose for real-time analysis. Besides disseminating results through conventional meetings and publications, and training graduate and undergraduate students, this project seeks to advance real-time computing practice through the dissemination of real-time kernel code and benchmark programs.","title":"CSR--EHS: Next-Generation Real-Time Device Driver Architecture: Integrating Schedulability Theory with Driver Implementation Practices","awardID":"0509131","effectiveDate":"2005-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["564778","537229","550962"],"PO":["561889"]},"105975":{"abstract":"Algebraic and computational methods for error-correction<br\/><br\/>Madhu Sudan (MIT)<br\/><br\/>Errors are inescapable when storing information (such as on CDs or DVDs) or communicating information (through cellular phones or cable modems). Coping with errors, and devising methods to detect and automatically correct errors, is one of the persistent challenges to the theory of information. This project investigates a collection of fundamental problems in this theory. <br\/>The problems are unified by their goals as well as methods under consideration. The central goal is to improve the efficiency of communication and of the associated computational tasks for very general <br\/>error models. The methods to be investigated include algebraic techniques over finite fields, and techniques<br\/>from the theory of computer science.<br\/><br\/>Algebraic methods have long contributed to the foundations of error-correcting codes. The principal examples are the Reed-Solomon codes and their decoding algorithms which have paved the way for much of the reliability of digital storage media. All CDs and DVDs are encoded with Reed-Solomon codes, and CD- and DVD-players come equipped with error-correcting algorithms for these codes. Recent research, including some previous work of the PI, has shown that the algebraic methods can be pushed even further to correct more error, and deal with a further diversity of reliability information when dealing with erroneous channels. Yet some fundamental questions remain unanswered, even about Reed-Solomon codes. A simple question is: What is the fraction of random error that can be corrected in Reed-Solomon codes, with efficient algorithms? This, and other such fundamental questions about algebraic codes, are investigated in this project. The project also investigates the applicability of new techniques developed in theoretical computer science in the context of some classical challenges in coding theory.","title":"Algebraic and Computational Methods for Error-Correction","awardID":"0514915","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["425345"],"PO":["348215"]},"105986":{"abstract":"Abstract<br\/>--------<br\/><br\/>Many scientific and engineering endeavors call for estimating probabilities and distributions based on an observed data sample. When the sample size is large relative to the number of possible outcomes, for example when a biased coin is tossed many times, estimation is simple. However, in many applications the number <br\/>of possible outcomes is large compared to the sample size. For example, in language modeling - used in compression, speech recognition, and data mining - the number of words and contexts is large compared to the amount of text at hand. Estimation in this large-alphabet regime is much more complex, and has been studied for over two centuries. While some good estimators have been derived, for example those named after<br\/>Laplace, Krichevsky-Trofimov, and Good-Turing, very few optimality properties have been established for them, and each is known to perform poorly under some conditions. <br\/><br\/>Adopting an information-theoretic viewpoint, the investigators undertake a systematic study of these issues. They concentrate on two broad problems, concerning the estimation of: (1) the probability of each observed outcome and of the collection of outcomes not yet observed; (2) the underlying distribution, which does not associate probabilities with specific outcomes. For each problem they seek estimation algorithms that perform well in practice and have provable optimality properties such as small Kullback-Leibler divergence and other metrics between the underlying and estimated distributions. The problems they address are both theoretical, for example the data size required to estimate the underlying distribution to within a given confidence level, and computational, regarding the complexity and sequentiality of the derived algorithms.","title":"Predicting the Unlikely: Theory, Algorithms, and Applications","awardID":"0514973","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["486456"],"PO":["432103"]},"104787":{"abstract":"The need for minimizing energy consumption continues to be compelling in embedded, mobile, and server systems such as handheld devices, robots, spaceships, laptops, cluster servers, etc. This is due to the direct impact of constrained energy sources (e.g., battery size and weight), as well as cooling expenses in cluster-based systems to reduce heat dissipation. Energy management therefore plays a paramount role in not only hardware design but also in user-application, middleware and operating system design. This project focuses on techniques, algorithms and OS support to scale down energy needs whenever the system performance can be relaxed. Specifically, the project will study energy-aware resource reservations and slack-stealing algorithms to satisfy both performance needs and energy constraints.<br\/><br\/>Many energy management schemes also focus on a single resource that is dedicated to real-time or non-real-time processing. Unfortunately, in many practical systems such as Personal Digital Assistants (PDA), cellular phones, robots and personal computers, the combination of hard and soft real-time periodic tasks, aperiodic real-time tasks, interactive tasks and batch tasks must be supported. Each task may also require access to multiple resources. Therefore, this project will tackle the NP-hard problem of providing timely and simultaneous access to multiple resources by the use of practical abstractions and near-optimal heuristics aided by cooperative scheduling. Approaches where power management is carried out in different islands separately will also be compared against non-partitioning approaches. Finally, the project will address the need for explicit power management in system software for sensor networks.","title":"CSR---EHS: Power-Aware Real-Time Systems","awardID":"0509305","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["461012"],"PO":["561889"]},"102136":{"abstract":"Abstract<br\/>Proposal: CNS 0454056<br\/>PI: Carlo Tomasi<br\/>Institution: Duke University<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>CRI: A Core Experimental Facility for Computer Vision and Artificial Intelligence <br\/><br\/>Duke University will construct a facility for the measurement of three-dimensional shapes and the motion of human bodies, to be used for both training and performance evaluation of computer vision and artificial intelligence algorithms.<br\/>Shapes will be measured with a high-quality laser range finder. Structured light, stereoscopic and monocular images of the same scene or object also will be collected from sensor systems of varying quality, and whose positions and orientations relative to the laser range finder are precisely known through calibration. Similarly, the motions in space of one or a few people will be recorded with an accurate motion capture system, as well as with several sets of color and black-and-white cameras and structured-light systems recording at the same time. Research project on stereo vision, robot localization, visualization, gesture analysis and dermatology are planned.<br\/>The laser range finder and motion-capture system provide accurate training and ground-truth data, and the other sensors yield the type of input that a computer vision algorithm or an artificial intelligence inference system would use to determine or reason about a scene or an event. Because of the broad need for this data, the proposal also envisions the creation of a course on the principles underlying sensing and geometric measurements whose materials will also be made public, and of a web repository for the measurement data collected both at the facility and elsewhere.","title":"CRI: A Core Experimental Facility for Computer Vision and Artificial Intelligence","awardID":"0454056","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["513187","518533"],"PO":["539087"]},"102147":{"abstract":"The Standard ML programming language is important as a robust, high-level, general-purpose language whose design is based on firm and clear theoretical foundations, expressed in a concise, rigorous and complete formal definition. It has become a paradigmatic language for the programming language research community over the last twenty years. Standard ML of New Jersey is the most widely used and best supported implementation of Standard ML, and it has seen wide use in the programming language and formal methods research communities (among others) since the late 1980s. It is available as a free, open source software system with a liberal BSD-style license. To remain viable and competitive, a programming system needs to be continually maintained and improved, and the resources provided under this award will help to do this. In order to maintain and increase the value of SML\/NJ to its user community, this proposal will address the following needs: (1) to keep SML\/NJ in step with advancing technologies (e.g. support for 64-bit architectures, Unicode character and string types, improved interoperability with other languages, support for new operating system releases and new platforms), (2) to improve the system's performance, reliability, capabilities, and usability, both as a programming tool and as a research and education platform, and (3) to manage the open source development process (e.g. managing bug reports and fixes, integrating contributed code, testing, building releases, writing and revising documentation, etc.). In addition, the proposed equipment infrastructure, consisting of instances of supported architectures and operating systems, will help maintain the valuable diversity of platforms that SML\/NJ is known for.<br\/>This is awarded as a community resource. Broader impacts of this award include its wide use in research and education in the field of programming languages, and use in development of major systems.","title":"CRI: Standard ML Software Infrastructure","awardID":"0454136","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["523800",269299,"475199"],"PO":["564777"]},"102158":{"abstract":"Abstract<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: COLLABORATIVE RESEARCH: Automated facial expression measurement: toolbox and database <br\/><br\/>Lead Proposal: CNS 0454233<br\/>PI: Marian S. Bartlett<br\/>Institution: University of California-San Diego<br\/><br\/>Proposal CNS 0454183<br\/>PI: Mark Frank<br\/>Institution: Rutgers University New Brunswick<br\/><br\/><br\/>This community resource project will package and release to the CISE research community a collection of software tools for vision based perceptual primitives for human-computer interaction studies and a database of facial expressions that has been coded by facial expression experts. The tools enable recognition of basic emotions and facial actions from the Facial Action Coding Systems (FACS). The tools and database have been developed with experts from machine learning and facial expression fields. Broader impacts of this community resource include enabling research and education in this area for a broader community in the U.S., use of automated facial expression in education and machine tutoring applications, enabling advances in computer vision, enabling advances in behavioral science and medicine related to emotion, cognition, and human-machine communication, and applications in homeland security.","title":"CRI: COLLABORATIVE RESEARCH: Automated facial expression measurement: toolbox and database","awardID":"0454233","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"7704","name":"Science of Learning Activities"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["463668","523911"],"PO":["564316"]},"107735":{"abstract":"Intellectual Merit:<br\/><br\/>In a DNA memory, information is encoded into DNA sequences and retrieved through template-matching hybridization reactions among DNA oligonucleotides. The template matching, hybridization reaction between DNA oligonucleotides is important in biotechnology and medicine (DNA microarrays),and nanotechnology (DNA directed self-assembly of nanostructures).The project should lead to a better understanding of how information can be encoded into an ensemble of hybridizing DNA oligonucleotides, and to better models and<br\/>ways of characterizing large ensembles.<br\/><br\/>Simple protocols can be used to manipulate the contents of the memory to achieve information processing. In the statistical DNA memory, an ensemble of DNA molecules are trained with test tube protocols to reproduce particular input\/output mappings, similar to artificial neural networks. Rather than a one to one mapping, information is encoded probabilistically into the DNA sequences and their hybridization interactions. These two architectures will be experimentally tested and modeled computationally and physically, and finally simulated to better understand how information is stored and manipulated through DNA template-matching hybridization reactions.<br\/><br\/>Broader Impacts:<br\/><br\/>Graduate students will participate in the project, and thus, will be trained in the interdisciplinary knowledge and methods required of computational research using biological systems. They will be mentored to produce research publications, attend professional conferences and write graduate theses. <br\/><br\/>Curricular modules associated with the research will be developed for incorporation into both biological and computer curricula. These modules will provide instruction and demonstrations in the topic that show the interplay between the disciplines, as well as experimental, theoretical, and simulation methods. <br\/><br\/>The goal is to produce students that are better prepared for careers and challenges in the emerging synthesis of biology and computation. The project results will be disseminated through a dedicated web site, and through conference and journals in the relevant disciplines.","title":"BIC: Large-Scale DNA Associative Memories","awardID":"0523858","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[284570,"548478",284572,"524863"],"PO":["565223"]},"106537":{"abstract":"This award supports travel and expenses for selected participants in a Workshop on Privacy and Confidentiality, to be held July 9-15, 2005 at the University of Bologna Residential Center in Bertinoro, Italy. The workshop comes at a time when governments and organizations are struggling to expand access to statistical and other databases while simultaneously protecting medical and other administrative records, and combating breaches of cyberinfrastructure security, especially those involving unauthorized record linkage and individual identification and harm. There has been a long tradition of confidentiality associated with statistical databases, but the ever-expanding cyberinfrastructure raises new and far more challenging questions about the protection of privacy associated with electronic databases involving individuals, families and other groups, and organizations. The goal of this workshop is to bring together leading privacy researchers from the statistics and computer science communities to share expertise and map out feasible research goals. It will (1) help shape an interdisciplinary methodologically-oriented intellectual agenda for the area of privacy and confidentiality by establishing commonly understood terminology, goals, and methodological description, (2) stimulate new interdisciplinary collaborations on the topic, and (3) influence the directions that confidentiality research takes in the separate disciplines. Specific areas of investigation include finding mathematically\/statistically rigorous definitions of confidentiality, disclosure, and privacy that transcend specific problem and model details, finding minimal definitions of statistical utility, understanding the tension between privacy and utility, and understanding the role of auxiliary information (\"extra\" information known to the adversary) in defeating privacy objectives.","title":"Workshop on Privacy and Confidentiality; July 19-26, 2005,Italy.","awardID":"0517956","effectiveDate":"2005-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"1333","name":"METHOD, MEASURE & STATS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}}],"PIcoPI":["493252"],"PO":["553193"]},"116228":{"abstract":"Hierarchical Abstractions for Planning and Control of Robotic Swarms<br\/>Calin Belta<br\/><br\/>As a result of recent advances in computation, communication, sensor, and actuator technology, it is now possible to build teams of hundreds of small and inexpensive ground, air, and underwater robots. They are light, easy to transport and deploy, and can fit into small places. Such swarms of autonomous agents provide increased robustness to individual failures, the possibility to cover wide regions, and improve computational power through parallelism. However, planning and controlling such large teams of agents with limited communication and computation capabilities is a difficult problem that received a lot of attention in the past decade.<br\/>To accommodate large numbers of robots with nontrivial kinematics or dynamics moving in complicated environments, this project proposes hierarchical abstractions. At a lower level, continuous abstractions reduce the dimension of the problem by extracting a set of essential features of the swarm, while correctly capturing the robot constraints. At a higher level, discrete abstractions focus on the complexity of the environment and map the planning and control problem from the infinite dimensional world of continuous systems to the decidable world of finite state automata. The proposed algorithms lead to the development of HILLS, a High Level Specification Language for Swarms, and LARAD, a LAnguage for Robot Automated Deployment. In these frameworks, robotic motion plans are formulated in a high level language in terms of strings or temporal logic formulas in the language of a discrete system capturing the complexity of the environment. HILLS and LARAD are implemented as simulation packages and also used in experimental platforms for swarming robotics.<br\/>While aimed to providing a solution to the swarming problem, this project addresses fundamental issues in shape theory and the well-known \"n-body problem,\" which are traditionally studied in theoretical physics and find applications in areas such as atomic physics and celestial mechanics. On the other hand, using a unique approach to discrete abstractions, this work attempts to enlarge the class of known decidable continuous and hybrid systems. Finally, the hierarchical abstraction architectures of this project open a new direction in planning and control of mobile robots by creating a framework in which powerful discrete algorithms dealing with the complexity of the environment can be seamlessly combined with continuous control laws for nontrivial robot dynamics.<br\/>This research is highly interdisciplinary, covering topics ranging from traditionally \"continuous\" areas, such as geometric nonlinear control, to \"discrete\" areas such as formal analysis, as well as application areas at the boundary between robotics and biology. The educational plan is focused on building bridges among the above areas by introducing graduate and undergraduate courses on Hybrid Systems, Systems Biology, Geometric Planning and Control, and Bioinformatics. It also involves a rich spectrum of outreach activities, including mentoring of high school teachers, judging and advising high school students participating in robotics competitions, and mentoring under-represented minority undergraduate students.","title":"CAREER: Hierarchical Abstractions for Planning and Control of Robotic Swarms","awardID":"0611926","effectiveDate":"2005-07-01","expirationDate":"2012-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["496541"],"PO":["562760"]},"104810":{"abstract":"This project focuses on an important kind of networked embedded systems<br\/>called sensor networks. Such networks have become popular platforms<br\/>for continuous sensing and analysis of physical environments, leading to<br\/>a better understanding of natural phenomena, civil infrastructures,<br\/>animal habitats, and other important scientific and engineering issues.<br\/>The sensor data can also be used to improve environmental protection,<br\/>infrastructure safety and energy efficiency, to name a few<br\/>of the potential applications.<br\/><br\/>The ad-hoc and dynamic nature of networked embedded<br\/>systems make their communication protocols complex.<br\/>This research uses the methodology of <br\/>continuous error monitoring for continued improvement<br\/>of reliability after the deployment of sensor networks.<br\/>The project builds a framework consisting of compiler-based tools and software<br\/>techniques for the detection, diagnosis and correction of<br\/>programming errors on sensor networks.<br\/>The nature of the sensor-network applications requires the hardware components<br\/>and software protocols to be small and resource-constrained.<br\/>The project therefore makes resource efficiency<br\/>one of the critical design criteria.<br\/><br\/>The success of this project should contribute substantially to<br\/>the reliability of sensor networks whose potential applications are very broad.<br\/>The project also engages a broad community<br\/>of students in various disciplines, through Honor Seminars,<br\/>special mentoring programs and undergraduate research opportunities<br\/>such as Purdue's EPICS program (Engineering Projects in Community Services).","title":"CSR-EHS: Resource-Efficient Monitoring, Diagnosis, and Programming Support for Reliable Networked Embedded Systems","awardID":"0509394","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["408878","556632","540778"],"PO":["561889"]},"104843":{"abstract":"This project focuses on creating a programmable router processor designed to efficiently execute a variety of network algorithms with the goal to make it a feasible alternative to custom ASICs. To achieve the high throughput required by core routers, the research will focus on (1) fast data plane algorithms, (2) support for co-exploration of algorithms and architecture design, and (3) creating low power routers that do not sacrifice worst case throughput.<br\/><br\/>The project will spur the development of more revolutionary approaches to high speed network processors, allowing programmable processors to penetrate the core router market. This in turn can help reduce the cost of building and maintaining routers. The results will also help reduce the power costs to run these high speed network routers. In terms of academic impact, this research will bring collaboration between two hitherto separate academic communities (computer architecture and networking) to spark new ideas from this<br\/>interdisciplinary interaction.","title":"CSR-EHS - Building a High Throughput Programmable Network Processor Through Algorithm and Architecture Co-Exploration","awardID":"0509546","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["438592","438592","293079"],"PO":["493916"]},"112004":{"abstract":"What is novel and unique in this proposed research is the proposed design of knowledge-based prediction systems based on optimal vector space representations of proteins that have previously been represented by character strings. If an optimal representation of a character string can be found by a numerical sequence, then a great variety of methodologies from disciplines such as optimization, pattern discovery, and machine learning can be readily applied to new understanding of protein tertiary structure and function.<br\/>For this, kernel based nonlinear classifiers and nonlinear dimension reduction as well as visualization methods will be developed to provide scalable and elective prediction systems.The prediction systems will be specially tailored for several problems related to protein structure discovery such as protein secondary structure, relative solvent accessibility and disulfide connectivity, as well as prediction of protein-protein interaction. <br\/>In this proposal the P.I.s describe how they intend to accomplish this, so that their preliminary results can be extended to the more general structure and protein-protein interaction prediction problem. All results obtained will be made available to the research community in order to facilitate further research activity. Using existing web servers, the results will be made available to teaching faculty and graduate and undergraduate students in a suitable tutorial form. This will allow those interested to tailor the material for use in graduate and undergraduate research and class projects. The authors will incorporate the results into current and future course material as well. Special efforts will be made by the two women PIs to provide opportunities for women graduate students to participate in the proposed research and for development of the<br\/>related software, which has long range social impact beyond the scientific results.","title":"ALGORITHMS: Collaborative Research: Development of Vector Space based Methods for Protein Structure Prediction","awardID":"0549247","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["562362","296440"],"PO":["565272"]},"114446":{"abstract":"The Information Technology Research (ITR) program is a unique, interdisciplinary program at the National Science Foundation (NSF). It is designed to fund innovative information technology research and education. To assess and learn from the experiences of the ITR program, two related phases of work are proposed. In the first phase, an ITR Principal Investigator (PI) meeting will be held. This meeting will foster an exchange of information and collaborative experiences among ITR PIs and NSF Program Officers to learn the outcomes of the ITR program. Detailed field notes will be collected from the ITR PI meeting. They will be analyzed systematically, and then applied to the next phase. In the second phase, the factors that lead to greater success in ITR projects will be analyzed. A survey will be created and objective data will be collected to evaluate quantitatively how different attributes of projects, such as geographic dispersion, number of disciplines and coordination strategies, effect the way that projects are conducted and research outcomes achieved. Projects will be assessed in comparison with NSF's GPRA goals to improve research knowledge, research infrastructure, scientific and engineering training and personnel, and society's use and understanding of science. The intellectual merit lies in the use of diverse metrics and systematic user-centered research design to assess interdisciplinary research programs, such as ITR. The broader impacts include advancing understanding and policy for successful interdisciplinary, distributed scientific work.","title":"Collaborative Research: ITR PI Meeting and Research Assessment","awardID":"0603836","effectiveDate":"2005-07-01","expirationDate":"2008-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["499499"],"PO":["564456"]},"105866":{"abstract":"ABSTRACT<br\/>0514489<br\/>Daniel B. Szyld<br\/>Temple University<br\/><br\/>Asynchronous Parallel Methods with Overlap for Google Matrices, Dynamics of Biomolecules, and Other Markov Chains Problems<br\/><br\/><br\/>This project relates to computational problems with singular matrices, and in particular with those representing Markov chains. Two important applications drive this investigation. The first of these are the stochastic matrices representing the interconnections between web pages in the World Wide Web. This representation can be done in several ways, one of which is by the PageRank algorithm reportedly used by Google. A basis for the null space of this matrix, appropriately normalized, gives the ranking of the web pages used by the search engines.<br\/>It is proposed to solve such singular linear systems by parallel block iterative methods.<br\/>Schwarz methods have been shown to be very effective in the nonsingular case, thanks to the use of overlapping variables. These ideas will be applied to singular systems and Google-type matrices.<br\/>One of the computational tools to be developed to that end is a new combinatorial and algebraic algorithm to determine appropriate blocks of variables (with the inclusion of overlapping variables) so that the parallel iterative methods work well.<br\/>Asynchronous methods will also be studied in this context. The second important application comes from the dynamics of medium-sized biomolecules, including certain viruses, such as HIV. These molecules have different equilibrium states, and the probability models to go from one state to another give rise to singular matrices. Certain blocks of variables represent metastable configurations of these biomolecules, and<br\/>the proposed project involves new algorithms to find these metastable configurations.","title":"Asynchronous Parallel Methods with Overlapfor Google Matrices, Dynamics of Biomolecules,and Other Markov Chains Problems","awardID":"0514489","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["485287"],"PO":["381214"]},"104777":{"abstract":"The goal of this research project is to build an application<br\/>infrastructure and a suite of embedded system software that will<br\/>enable the rapid development of software control and monitoring<br\/>applications, such as heating, ventilation, and air conditioning<br\/>systems (HVAC), on low-power, lossy wireless sensor-actuator<br\/>networks. Such user-preference driven control applications require<br\/>the ability to adapt to noise, loss, and failures in a large-scale<br\/>distributed network, and demand a high degree of automation in<br\/>acquiring user preferences, making control decisions and<br\/>disseminating those decisions to the appropriate actuators.<br\/><br\/>The Sensor Control System (SCS) being developed provides a high-<br\/>level platform that allows users to express control application<br\/>requirements at the granularity of the entire network, enabling users<br\/>to focus on the requirements of their deployments by abstracting away<br\/>the low-level sensor network details. SCS combines techniques from<br\/>machine learning and statistics with high-level abstractions inspired<br\/>by work in software systems and databases, allowing for<br\/>mathematically-sound decision making despite loss and uncertainty<br\/>that is inherent in such systems. The project will demonstrate the approach on<br\/>a highly instrumented office environment. This research has the potential to<br\/>impact industry through the development of a new class of wireless<br\/>control systems, and academia through the infusion of cross-<br\/>disciplinary ideas into a variety of sub-fields of EECS. The<br\/>datasets, code and the sensor-actuator deployments developed during<br\/>the course of this research will be used in a range of educational<br\/>initiatives.","title":"CSR-EHS: Collaborative Research: A General, Efficient and Robust Platform for Enabling Control Applications in Sensor Networks","awardID":"0509261","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["525076"],"PO":["561889"]},"105767":{"abstract":"Intellectual Merit: The power consumption rate of computing devices has been increasing exponentially. This makes it increasingly difficult to supply energy to devices and to cool these devices. Poweraware computation is especially important in the domain of sensor networks which are composed of small battery-powered nodes. Power management in sensor networks is viewed as so critical that it must be dealt with at all layers of the protocol stack.<br\/><br\/>Many power management techniques have been proposed and implemented. Most of these techniques are similar in that they reduce or eliminate power to some or all components of the device. However, there is an inherent conflict between power management and performance; in general, the more power that is available, the better the performance that can be achieved. As a result, it is generally proposed that power reduction techniques be preferentially applied during times when performance is less<br\/>critical. However, this requires a policy to determine how essential performance is at any given time and how to apply a particular power reduction technique. For example, to use the frequency scaling technique, where the speed of the clock is changed dynamically, one needs a policy to set the speed at each point in time. There is a growing consensus that these policies must incorporate information provided by applications and high levels of the operating system, and that current tools and mechanisms for power management are inadequate and require more research. The authors propose to formalize power<br\/>management problems as optimization problems, and then develop algorithms that are optimal by these criteria. The goal of this research is to develop effective algorithms for specific problems within the domain of power management, as well as to build a toolkit of widely applicable algorithmic methods for problems that arise in energy-bounded and temperature-bounded computation. The authors propose to initially focus on problems that deal with speed scaling and power-down techniques, since these are currently the dominant techniques in practice.<br\/><br\/>Broader Impacts: The authors propose to both develop fundamental theoretical techniques, and to apply these techniques to attack timely and important applications in computer systems. Both PIs have an established track record of working closely with researchers in applied areas to ensure that the theoretical models developed match the associated real-world problems. This is essential for theoretical results to have an impact. This work will continue to foster this very productive cross-fertilization between<br\/>these experimental systems researchers and theoretical computer science. The students supported under this grant will be influenced by this philosophy of research. They will be trained to be proactive in working with researchers in applied domains to bring important and interesting problems into the theory community. They will also be encouraged to publish the resulting work in systems as well as theory conferences to ensure that new algorithmic discoveries have an impact. As part of this project, the authors also plan to continue outreach work to high schools students, encouraging underrepresented groups to choose careers in technology related fields. They have developed a talk outlining diverse opportunities within computer science and plan to involve graduate and undergraduate students in presenting this talk at local high schools. The work in this proposal is featured in the talk. In addition, they plan to involve students from underrepresented groups in research projects related to power management.","title":"Collaborative Research: Algorithmic Support for Power Aware Computing and Communication","awardID":"0514082","effectiveDate":"2005-07-15","expirationDate":"2009-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["409328"],"PO":["499399"]},"102137":{"abstract":"Abstract<br\/><br\/>Proposal: CNS 0454062<br\/>PI: David F. Kotz<br\/>Institution: Dartmouth College<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: A Community Resource for Archiving Wireless Data At Dartmouth: CRAWDAD<br\/> <br\/>Investigators at Dartmouth College will develop an archive of wireless network data and associated tools for collecting and processing the data, as a community resource for those involved in wireless network research and education. Most current research is based on analytical or simulation models; due to the complexity of radio propagation in the real world and a lack of understanding about the behavior of real wireless applications and users, these models are severely limited. On the other hand, the difficult logistical challenges involved in collecting detailed traces of wireless network activity preclude most people from working with experimental data. The investigators will build on experience collecting data from wireless networks that has bee released to the research community. <br\/>The project will construct a shared facility for storing large data sets collected from real wireless networks, develop common formats and tools for collecting, anonymizing, and analyzing this data, host visiting students who want to use the archive or collaborate on tool development, work with community leaders to encourage contribution to and use of the archive, coordinate with other community efforts to develop network trace formats and tools, and encourage educators to use the tools and data in course projects, and to share course modules.<br\/>This project will have broad academic and practical impact in the wireless network community through enabling research by users of the data resources, providing data and analysis software for educators, and involving women and minorities in the project.","title":"CRI: A Community Resource for Archiving Wireless Data At Dartmouth: CRAWDAD","awardID":"0454062","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["553543",269255],"PO":["402055"]},"105668":{"abstract":"ABSTRACT<br\/><br\/>The objective of the proposed research is to develop a general and robust machine learning system for integrated analysis of high-throughput biological data for the purpose of prediction of gene function and protein-protein interactions. Achieving this goal requires addressing multiple challenges that include data heterogeneity, variable data quality, high noise levels in data, and a paucity of training samples. These challenges have prevented the successful application of traditional machine learning methods to diverse biological data. The research team will leverage diverse bioinformatics, machine learning, and biology expertise of the co-PIs and collaborators to develop accurate and effective approaches optimized for integrated analysis of genomic data. For prediction of protein-protein interactions, this investigation will focus on Bayesian approaches based on successful preliminary research. For gene function prediction, the focus will be on developing novel machine learning methods. These learning methods will use heterogeneous biological data as well as protein-protein interactions predicted by the system. The proposed research will lead to development of a general bioinformatics system that will utilize diverse large-scale biological data, including gene expression microarrays, physical and genetic interactions datasets, sequence and literature data, to produce an accurate map of protein-protein interactions and predictions of function for each of the proteins. This system will address the critical need in genomics to extract accurate biological information from disparate high-throughput<br\/>data sources, enabling the first step in accurate and comprehensive study of cellular processes<br\/>on a whole-genome level. Additionally, the proposed analysis will provide genomics researchers with quantitative rankings of the relative reliability of high-throughput experimental technologies, thereby providing biologists with data on which high-throughput technologies are more accurate than others. A significant advantage of this plan is that the research team will work closely with biologists to evaluate the predictions and feed the information back into the investigation to further improve the system and the quality of the resulting predictions.<br\/><br\/>The proposed system will provide predictions that will drive biological experimentation, enabling genome-wide annotation of unknown genes. The system will be publicly available to genomics researchers through its integration with the Saccharomyces Genome Database, a model organism database for yeast, and also via distribution of this integrated framework to other model databases. The interdisciplinary approach of this proposal will further the impact of advanced<br\/>computer science on biology and will precipitate further interactions between the two fields, both through research and through interdisciplinary education.","title":"SEI (BIO): Integrated analysis of heterogeneous genomic data for accurate prediction of gene function and interactions between proteins","awardID":"0513552","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":["457635","450503"],"PO":["565136"]},"105679":{"abstract":"The interplay between information theory (IT) and computer science (CS)<br\/>dates back to the founding father of information theory, Claude E.<br\/>Shannon. Ever since Shannon's work on both information theory and<br\/>computer science, the research in the interplay between IT and CS has<br\/>continued and expanded in many exciting ways. In 2003 the first NSF<br\/>sponsored Workshop on the IT and CS Interface was held in Chicago,<br\/>while in 2004 a graduate course on analytic methods in information<br\/>theory and analysis of algorithms was organized at MSRI, Berkeley. We<br\/>build on this momentum and propose to work on problems of information<br\/>theory, combinatorics, and analysis of algorithms. Following Knuth's<br\/>and Hadamard's precept, we study such problems using techniques of<br\/>complex analysis. This program, which applies complex-analytic tools<br\/>to information theory, constitutes ``analytic information theory''.<br\/><br\/>This research is focused on some facets of source coding, such as the<br\/>redundancy rate problem, method of types, entropy evaluation, channel<br\/>capacity, and joint channel-source coding. The redundancy rate problem<br\/>for a class of sources is the determination of how far the actual code<br\/>length exceeds the optimal (ideal) code length, while the method of<br\/>types is a powerful technique in information theory, large deviations,<br\/>and analysis of algorithms. It is argued that counting types can be<br\/>accomplished efficiently by enumerating Eulerian paths (Markov types)<br\/>or binary trees with a given path length (universal types). Likewise,<br\/>analysis of the redundancy rate problem for memoryless and Markov<br\/>sources leads us to interesting generating functions such as tree<br\/>generating functions (e.g., arising in counting labeled rooted trees),<br\/>which are studied extensively in computer science.","title":"Crossroads of Information Theory and Computer Science: Analytic Algorithmics, Combinatorics, and Information Theory","awardID":"0513636","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["425343"],"PO":["432103"]},"116229":{"abstract":"The nascent field of synthetic biology is focused on creating small synthetic genetic networks<br\/>inserting them into living cells in order to \"program\" cellular behavior. Recent prototypes include<br\/>a toggle switch, an oscillator, logic gates, concentration band detectors, and even a pulse<br\/>generator. Synthetic gene networks are foreseen to have tremendous applications in<br\/>biotechnology, medicine, and defense related areas. Such engineered biological devices will<br\/>engage in simple computations and cell-cell communications to diagnose diseases, repair tissues,<br\/>detect and clean up environmental pollutants, and manufacture biomaterials.<br\/>The main challenge in synthetic biology is creating and tuning gene networks to desired<br\/>specifications. There is currently no formal mechanism to guarantee the behavior of these systems<br\/>and to tune their parameters a-priori to satisfy desired performances. The existing tools for formal<br\/>analysis cannot handle genetic networks successfully due to nonlinearities, uncertainties, scarce<br\/>knowledge of kinetic and regulatory parameters, and measurements corrupted by noise. However,<br\/>the adoption of synthetic gene networks in critical applications such as tissue engineering requires<br\/>that these systems meet strict safety guarantees.<br\/>In this project, we propose a hybrid systems approach to forward engineer and analyze synthetic<br\/>genetic networks. In this framework, interval-based specifications translate to reachability<br\/>analysis and safety verification, which are the central problems of formal analysis. By exploiting<br\/>the particular nonlinearities induced by chemical reactions and cooperative regulations, we first<br\/>reduce these infinite dimensional problems to finite searches on graphs by constructing discrete<br\/>abstractions, and then map them to parameter value intervals. This procedure will allow for<br\/>analysis under parameter uncertainty and will provide a provably correct methodology to tune the<br\/>parameters to achieve desired interval-based properties. We will validate our approach with two<br\/>experimental systems. We will improve the steady state digital response of a transcriptional<br\/>cascade and use the optimized cascades to build more robust toggle switches. We will also use<br\/>formal analysis to fine-tune the dynamic behavior of a pulse generator that incorporates cell-cell<br\/>communication and a feed-forward motif.","title":"BIC: Collaborative Research: Rational Design of Synthetic Gene Networks using Formal Analysis of Hybrid Systems","awardID":"0611927","effectiveDate":"2005-07-01","expirationDate":"2007-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["496541"],"PO":["521045"]},"109519":{"abstract":"Object recognition in Computer Vision, though being a main processing step in many tasks of robotics, surveillance, and other fields of automation, is still an unsolved problem. The recent results in human visual perception strongly suggest that contour extraction is a key step to object recognition. A development of a contour-based system for object recognition is proposed. The first step of the new approach concentrates on extraction of object contours from edge images that correspond to contours as perceived by humans. Since the extraction of complete contours may not be possible (e.g., due to occlusion), extraction is focused on meaningful parts of contours. The proposed approach uses a mixture of bottom up and top down processing for edge grouping. After each step of bottom-up processing in a pyramid architecture, top-down evaluation is applied to select the most promising grouping constellations. A promising grouping constellation is defined using cognitively motivated constraints. In accord with the cognitive simplicity principle known from Gestalt psychology, partial shape similarity will be used as a primary building block of such constraints. In accord with the newest results in human perception, grouping of edges to parts of object contours and recognition of the parts using shape similarity play a key role in object recognition. This means that object recognition is possible if only part of a contour is constructed, and the construction of the whole contour is not necessary for recognition. In particular, object recognition works in the presence of occlusion and segmentation errors. <br\/><br\/> The proposed solution to the object recognition problem can make a significant step to improve the application scope of vision systems. The results of this work will be applicable to vision systems, large image databases, and video analysis systems. The proposed research to find interdependence and structural information among visual parts may lead to further understanding of human visual perception and cognition. The proposed research will provide an excellent resource for interdisciplinary work for graduate and undergraduate students in computer science and psychology. The PIs will offer courses and seminars on proposed research topics that will bring the state-of-the-art knowledge and technology to the classrooms.","title":"Collaborative Research: From Edge Pixels to Recognition of Parts of Object Contours","awardID":"0534929","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["541851",289942],"PO":["564316"]},"110586":{"abstract":"Digital microfluidics systems (DMFS) are a new class of \"lab-on-a-chip\" systems that can manipulate discrete droplets. This exploratory SGER project will develop algorithms for the design, simulation, and optimization of DMFS. These systems have the potential to perform complex biochemical analyses in real time at dramatically reduced costs. Their low energy requirements, small reagent volumes, and small size make them attractive for use in a wide variety of applications including medical diagnostics, clinical monitoring, and environmental monitoring. For example, such systems can be used as pathogen monitoring devices that sense and identify pathogenic organisms in the environment.<br\/><br\/>The proposed project will establish the validity of the novel approach by developing specialized routing and scheduling algorithms for the coordination of droplets on a DMFS, by exploiting the structure of these problems. By developing models abstracted from the hardware and focusing on the algorithms, basic principles will be developed for designing array layouts and coordinating droplets that work across different hardware implementations. The developed droplet routing and scheduling algorithms can be directly used in software to control a DMFS. Further, they will ultimately lead to high-level CAD tools to design and simulate DMFS.<br\/><br\/>This research will extend robot coordination techniques to biotechnology applications. The proposed research will involve graduate students in research, and will be integrated into a Robot Motion Planning course taught by the PI. Outreach activities include plans to enhance summer robotics camps with The Children's Museum of Science and Technology in Troy.","title":"SGER: Towards Automated Droplet Coordination in Digital Microfluidic Systems","awardID":"0541224","effectiveDate":"2005-07-15","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["553786"],"PO":["335186"]},"110135":{"abstract":"One's doctoral advisor is one's academic parent. This relation defines an academic genealogy of researchers that describes the academic ancestors and descendents of a particular set of researchers. While many students have a single doctoral advisor, some have more than one, so the genealogy has a lattice structure that branches both forward and backward in time. This project will build an academic genealogy for the field of Artificial Intelligence (AI), a remarkably interdisciplinary field that draws on computer science, mathematics, electrical, control, and mechanical engineering, cognitive, perceptual, and developmental psychology, linguistics, and philosophy, among other fields. AI is made up of a highly diverse collection of intellectual threads, which can often be made clear through examination of intellectual heritage.<br\/>.<br\/>A landmark in the creation of the field of AI was a workshop held at Dartmouth College in the summer of 1956. A number of the attendees at that workshop became the founding researchers in AI and many of their students have gone on to become leaders in the field. Although some of the founders and early leaders of AI have died in recent years, we are fortunate that many are still alive and vigorously pursuing research. It is thus a propitious time to collect historical information from these AI pioneers.<br\/><br\/>Developing an academic genealogy of AI will be valuable and timely for the field. This project will provide a formal definition for the genealogy task, a representation for the data to be collected, criteria for starting points, and visualization methods. The resulting genealogical data will provide a useful resource for historians and social scientists studying the nature of science as well as the particulars of the field of AI. For instance, the student-advisor relation is an important special case of intellectual influence, and one that is approximated by the formal structure of the academic genealogy. This project will help reveal the human face of science to a wider audience and demonstrate the strength and limitations of the advisor-student relationship in the ongoing process of science. It will also help demonstrate how important new ideas enter a field \"from the side\", outside of the established links of the academic genealogy.","title":"Artificial Intelligence: An Academic Genealogy","awardID":"0538927","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["534124"],"PO":["387198"]},"104822":{"abstract":"Storage capacity and data volume have been doubling every 18 months during the past<br\/>two decades. A key challenging issue in building next-generation storage systems is to<br\/>manage massive amounts of feature-rich (non-text) data, which has dominated the<br\/>increasing volume of digital information. Comparing noisy, feature-rich data requires fast similarity match instead of exact match, and thus exploring such data requires similarity<br\/>search instead of exact search. Current file systems are designed for named text files;<br\/>they do not have mechanisms to manage feature-rich data. To date, there is no practical<br\/>storage system with the ability to do similarity search for noisy, high-dimensional data<br\/>and there is no index engine design for efficient similarity search. This research<br\/>addresses this problem by studying how to design and implement a content-addressable<br\/>and -searchable storage (CASS) system to manage and explore diverse feature-rich data.<br\/>The system includes a built-in similarity search engine for general-purpose, noisy, highdimensional<br\/>metadata using compact data structures and novel indexing methods. The<br\/>research will also develop segmentation methods and feature extraction methods for<br\/>audio, image and genomic data, and develop similarity search benchmarks and to<br\/>evaluate the CASS system.<br\/><br\/>This research will advance knowledge and understanding in the area of storage system<br\/>designs such as data structures, mechanisms, and APIs for managing, searching and<br\/>exploring noisy, high-dimensional feature-rich data. The research will accelerate the<br\/>development of next-generation storage systems which will revolutionize how to access,<br\/>search, explore and manage massive amounts of feature-rich data in many disciplines.","title":"CSR-PDOS-Content-Searchable Storage for Feature-Rich Data","awardID":"0509447","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["52508","497799","542007","457635"],"PO":["535244"]},"102523":{"abstract":"Intellectual Merit<br\/><br\/>The proposed research will develop sophisticated techniques for managing the lifetime of data in secure versioning systems, a practical application of the AON transform that leverages its semantics, rather than just its security properties. The problem of revocation will also be addressed in order to refine the security and granularity of key regression techniques.<br\/><br\/>Recent legislation has created new requirements for retaining and securing electronic information. More than 4,000 state, local, and federal acts govern archives. The specifics of each act vary by domain, but, when taken as a whole, they can be distilled down to a set of technical requirements. An archive must provide privacy, confidentiality, and non-repudiation for information. Archives must use strong encryption with authentication for data on disk, as well as a means for secure transmission. Legislation mandates an auditable trail of changes made to electronic records that can be accessed on-line, which requires versioning data over time and providing access to past versions. Governmental and corporate organizations must ensure that compliance does not degrade security, privacy, or the enforcement of retention policies. The combination of regulatory and organizational requirements bring up two technical problems for secure versioning systems. First, there is no efficient way to securely delete information, i.e. so that no computationally practical way to recover deleted data exists. Second, systems must provide an efficient means to change the accessibility of information throughout time. This includes downgrading information, e.g. declassifying information as part of the Freedom of Information Act, or, revoking privileges, e.g. disallowing future access to information after an employee leaves the company or transferring the rights to medical records from one health care provider to another. <br\/><br\/>Without technical solutions to these problems, organizations and individuals will be subject to information<br\/>leakage and will fail to comply with regulations. Data that are not securely deleted are recoverable and subject to subpoena or cryptographic attacks. After a legislated retention period, information often represents a legal and competitive liability. Also, some regulations require that personal medical and financial records be deleted based on time or circumstance. Again, this deletion must be permanent, and thus, secure. Existing solutions for secure deletion and for scoping access to information over the lifetime of data are inadequate. They either fail to meet requirements or they are intolerably inefficient. In this project, mew technologies will be created that efficiently implement secure deletion and revocation in versioning systems. A novel application of All-or-Nothing (AON) encryption will be applied that both provides authenticated, strong encryption of data on disk and pioneers efficient secure deletion. <br\/><br\/>Impact<br\/><br\/>The project will lead to curriculum development at Johns Hopkins, including a short course on the Policy and Technology in Data Storage. The PIs will offer three tutorials based on the short course; one geared toward governmental agencies, one toward health care providers, and one toward corporations. Tutorials will be organized and promoted through StorageNetworking.org, an initiative for the education of storage professionals and will be freely available to the community.<br\/><br\/> The proposed research will address problems introduced by recent legislation, which effect financial and medical organizations and all levels of government. Because the solutions are general to all storage systems that share content among versions, they apply to file systems, distributed archives, and databases. Open-source software produced by the project will permit anyone to construct a compliant<br\/>storage system, with security and deletion guarantees, at little expense.","title":"Securely Managing the Lifetime of Versions in Digital Archives","awardID":"0456027","effectiveDate":"2005-07-01","expirationDate":"2008-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["553563","339640","551737"],"PO":["371077"]},"105933":{"abstract":"Data mining (aka Knowledge Discovery in Databases, KDD) is a procedure to extract previously unknown and potentially useful information or pattern from huge data sets. KDD is usually a multiphase process involving numerous steps such as data preparation, data preprocessing, feature selection, rule induction, knowledge evaluation and deployment etc. Many novel data mining and learning algorithms have been developed, though vigorously, under rather add hoc and vague concepts. These algorithms, in most cases, are individual creations of different researchers, without much common methodological and fundamental framework. In other words, great majority of work in data mining is focused on algorithm development while neglecting the studies of fundamental theoretical issues concerning data, inter-data relationships, and quality of the implicit information hidden in the data or data redundancies. Thus, it is not easy to fully understand and evaluate how individual phase influences each other and the impact of each phase on the whole knowledge discovery process. For further development and breakthroughs in data mining and learning algorithms, a deep examination of its foundation is necessary. The central goal of the proposed research is to develop a unified rough set based data mining framework to explore various fundamental issues of data mining and learning algorithms. It aims to present the analytical capabilities of the methodology of rough sets in the context of data mining methodologies, techniques and applications. It will provide a unified framework to help better understand the whole KDD process.<br\/><br\/>Intellectual merit: Rough set theory is particularly suited to reasoning about imprecise or incomplete data and discovering relationships in the data. The simplicity and mathematical clarity of rough set theory makes it attractive for both theoreticians and application-oriented researchers. The main advantage of rough set theory is that it does not require any preliminary or additional information about the data, such as probability in statistics, basic probability assignment in Dempster-Shafer theory or the value of membership in fuzzy set theory. Rough set theory constitutes a sound basis for KDD and can be used in different phases of the KDD process. In particular, the formal techniques of rough set theory lead to many novel and promising breakthrough methods and algorithms for attribute functional, or<br\/>partial functional dependencies, their discovery, analysis, and characterization, feature election, feature extraction, data reduction, decision rule generation, and pattern extraction (templates, association rules) etc., which are the fundamental issues of the KDD process. Rough set theory represents a new innovative approach and can lead to the development of new learning algorithms to create novel uses and breakthroughs of data mining techniques.<br\/><br\/>Broader impacts: The proposed collaborative project is interdisciplinary in nature. It will synthesize often-disparate work in data mining, rough set theory and high performance computing. The PIs' strong multidisciplinary research collaboration experience will lead to widespread awareness and impact of the proposed research to rough set, data mining and high performance computing community. It will design and develop a wide-range of novel data mining algorithms and methods including data reduction, rule induction and classification ensemble in one unified framework to better understand the whole KDD<br\/>process. These algorithms and methods will significantly extend the application scope of data mining techniques and rough set theory and will result in the improved understanding of issues involved in designing efficient and innovative data mining and learning algorithms and methods. The proposed research will integrate tightly with teaching activities, the research results will be developed into undergraduate and graduate courses and research projects. Part of this approach includes the development of new cross-disciplinary courses that bring together computer science and mathematics for the understanding of principle and methods of theoretical foundations of data mining and rough set theory. The integration will help with training students in the issues involved in the rough set theory, design and implementation of novel data mining methods and algorithms, high performance computing. The active participation of students will allow for significant exposure to the latest research in data<br\/>mining.","title":"High Performance Rough Sets Data Analysis in Data Mining","awardID":"0514750","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["37090"],"PO":["565223"]},"112005":{"abstract":"ABSTRACT <br\/>0204109<br\/>Haesun Park<br\/>University of Minnesota- Twin Cities<br\/><br\/>Due to today's exponential growth of the internet and computing power, an information retrieval<br\/>system is expected to handle a tremendous amount of data, and users demand more efficient techniques<br\/>to obtain useful information from the flood of data. The goal of this proposed research is to find lower dimensional representations of text data in vector space based information retrieval. Dimension reduction is imperative for achieving high efficiency and effectiveness in manipulating the massive quantity of data in today's information retrieval system. A problem of fundamental importance here is to achieve better representation of the data with relatively severe dimension reduction, rather than simple dimension reduction through a lower rank approximation of a matrix. One difficulty is that it is not easy to measure by a theoretical formula how well a certain dimension reduction method provides a good representation of the original data, and it will be essential to conduct theoretical research in parallel with experimental study.","title":"Lower Dimensional Representation of Text Data for Efficient and Effective Information Retrieval","awardID":"0549253","effectiveDate":"2005-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["562362",296440],"PO":["321058"]},"105966":{"abstract":"Abstract<br\/>--------<br\/><br\/> Digital communication, embodied in such applications as cell phones and wireless Internet, by now pervades our daily lives, while digital storage devices, such as CDs, DVDs, and computer disk drives, have become the principal means of preserving our information. In the \"information age\" in which we all now live, the need for reliable transmission and storage of digital data is of paramount importance. What makes such reliable transmission and storage possibles are error-correcting codes, first conceived by Claude Shannon over 50 years ago. Indeed, as you are reading these lines, millions of error-correcting codes are decoded every minute, using efficient algorithms implemented in custom VLSI circuits. At least 75% of these circuits decode Reed-Solomon codes,invented by Irving Reed and Gustave Solomon in the 1960s. In the four decades since their invention, Reed-Solomon codes have been extensively studied and ingenious decoding algorithms for these codes have been developed. What has been realized only recently, however, is that Reed-Solomon codes can correct many more errors than previously thought possible! In a series of theoretical breakthroughs, Sudan, Guruswami-Sudan, and Koetter-Vardy have made state-of-the-art Reed-Solomon decoders out of date. At least in principle, we can now achieve much better performance with the same codes. The goal of this project is to follow-up on this exciting recent work and to follow this line of research through to its ultimate potential, in theory as well as in practice.<br\/><br\/> In order to attain this goal, we plan a broad line of attack. On one hand, the proposed investigation will address deep theoretical questions. Can one exceed the Guruswami-Sudan decoding radius? What is the optimal multiplicity assignment for algebraic soft-decision decoding? How can iterative decoding methods be applied to Reed-Solomon codes? On the other hand, we intend to go all the way to the first-ever VLSI implementation of a soft-decision Reed-Solomon decoder. The proposed VLSI architecture aims for high speed and low power dissipation. Thus complexity considerations, inherently motivated by the practice of VLSI design, will be paramount throughout our investigation. Specifically, The main topics to be investigated are: (1) Multivariate interpolation decoding beyond the Guruswami-Sudan radius; (2) Probabilistic model and multiplicity assignment schemes for algebraic soft-decision decoding; (3) Iterative methods for soft decision decoding of Reed-Solomon codes; (4) Analytic bounds on the performance of maximum-likelihood and suboptimal decoders; and (5) Complete VLSI implementation of a state-of-the-art soft-decision Reed-Solomon decoder in an FPGA and\/or ASIC.","title":"Collaborative Research: Next Generation Decoders for Reed-Solomon Codes","awardID":"0514881","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[279563],"PO":["432103"]},"102578":{"abstract":"0456285<br\/>Singer<br\/><br\/>This is a U.S.- China Conference consisting of two workshops on research and education partnership in symbolic computation proposed by Dr. Michael Singer, North Carolina State University and Professor Ziming Li, Chinese Academy of Sciences, China. One workshop will be at North Carolina State University in Raleigh, NC on October 16-22, 2005, and the other at the Mathematics Mechanization Research Center, Chinese Academy of Sciences, Beijing, China, from July 31-August 6, 2006. The workshops will focus on important issues related to symbolic computation and will include one junior scientist and 7 U.S. graduate students. A long-term research partnership is expected to develop between the U.S. and Chinese researchers and graduate students. This conference will be jointly supported by the NSF and the Chinese Academy of Sciences.","title":"Workshops for NCSU\/China Research and Educational Partnership In Symbolic Computation","awardID":"0456285","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"7299","name":"Catalyzing New Intl Collab"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["517853","520532","550491","485433"],"PO":["553073"]},"105977":{"abstract":"This project involves developing connections between Approximation Algorithms, Machine Learning and AI problems of planning under uncertainty. This work has two main related themes. The first is extending the technology of approximation algorithms to problems of greater interest to AI and Robotics. These include problems of path planning under uncertainty, clustering, and problems motivated by new challenges in machine learning. For instance, in the context of path planning, the classic Traveling Salesman Problem asks: what is the shortest route to visit all locations of interest in some deterministic environment and return back to the start? But what if we are developing a plan for a robot whose actions may be unreliable and to which unexpected events can occur? In that case, we would want to use a stochastic model of the environment such as in Markov<br\/>Decision Processes. Can one develop good approximations to the natural analog of the TSP in such settings? One of the goals of this work is to explore this and several related optimization problems. In a different context, in machine learning there are a number of problems that can be phrased as a form of graph partitioning, but where the graph is embedded in Rn and there is some geometric restriction on the form of cuts allowed.<br\/><br\/>The second theme of this work is in the other direction, applying techniques from computational learning theory, including online learning and sample-complexity analysis, to the design of algorithms for optimization problems with provable quality guarantees. These include problems in routing, in algorithmic mechanism design, and a number of problems in online optimization.<br\/><br\/>Other topics in this proposal include the exploring the relation between kernel methods in machine learning and dimensionality reduction, and investigating how online learning techniques can be used to converge to certain game-theoretic equilibria. The intellectual merit of the proposed work is that this research will advance our understanding of important problems in all three areas: approximation algorithms, machine learning, and planning under uncertainty. The broader impact is that by developing connections between these areas, it will bring them closer together, for instance by developing algorithms with good approximation guarantees for models of greater interest to robotics, or algorithms for clustering and graph partitioning problems of greater interest to machine<br\/>learning. This will in turn, we hope, allow future work by other researchers in each area to have a greater impact on each of the other areas.","title":"Machine Learning, Approximation Algorithms, and Planning under Uncertainty","awardID":"0514922","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["554195"],"PO":["499399"]},"105647":{"abstract":"This project will develop a novel system to investigate and analyze many important aspects of cumulus cloud dynamics, cloud evolution, and precipitation formation to an extent that has previously been impossible. Clouds and precipitation affect our daily lives, personal safety, commercial decisions, and our future sustainability on Earth. Clouds and precipitation are important at all regional scales: local, state, national, and global. For example, clouds influence the daily maximum and minimum temperatures over our homes and they modulate the global temperature by affecting the amount of incoming solar radiation and outgoing longwave radiation. As the inhabitants of earth become increasingly concerned about global warming and climate change on global and regional levels, it is necessary to understand the roles of clouds and precipitation in the Earth System in order to predict the future state of our planet.<br\/><br\/>However, understanding and predicting atmospheric phenomena are very difficult tasks which require the measurement and modeling of properties on a wide variety of scales (cloud scale, storm scale, mesoscale, globally), fusion of computational model data, measured data, and the simultaneous fusion of hundreds of scalar and vector fields that vary over time. Current tools for atmospheric visualization are not capable of integrating these various data sources, communicating the complex three-dimensional, time-varying information necessary to accurately understand and predict atmospheric events, or for the integration of visual representations into the scientific analysis and discovery process. <br\/><br\/>This project will provide a fundamental advance in visualization and interaction techniques to solve these multiscale, multifield, data fusion, analysis, time-critical decision making, and interaction problems. These new multiscale, multifield, atmospheric visualization tools will: incorporate novel, effective, photorealistic and illustrative multifield visualization techniques; fuse observational and model data; improve the understanding of cloud dynamics, cloud evolution and precipitation formation; create effective multiscale visual representations; be rapidly deployed for research, training, and education; and produce an environment for actionable, comprehensive and efficient visual analysis. <br\/><br\/>Both computer science and atmospheric science research challenges addressed in this project will benefit other fields by: <br\/><br\/>1. Improving understanding of cumulus entrainment and warm rain formation, leading to better parameterizations in weather forecasting models and possibly global climate models.<br\/>2. Improving training of students and atmospheric scientists to perform their science in three dimensional environments.<br\/>3. Unifying access to co-registered model and measured data across multiple scales, greatly improving the understanding of the atmosphere, and advancing atmospheric models and weather prediction.<br\/>4. Creating a fused, comparative visual analysis environment to reduce the ambiguity inherent in the use of a variety of data sources by calibrating multiple, measured and simulation data sources.<br\/>5. Creating a physically plausible, parameterized database of canonical cloud models for use in atmospheric science research, rendering research, illumination simulation and validation (e.g., headlamp visibility in various weather conditions) and in the visual effects industry.<br\/>6. Developing a new architecture and visualization tools for large scale, multiscale, multifield data integration, fusion, analysis, and experimentation for use by the larger atmospheric science community.<br\/>7. Developing modules for educating high school and undergraduate students about the principles of cloud and precipitation formation. <br\/><br\/>The techniques to be developed will significantly change the state-of-the-art of visualization and large-scale data analysis, and have a dramatic impact on many fields using multifield, multiscale data, including computational fluid dynamics, biology, medicine, astrophysics, and nanoscale-microscale integration. Advanced information communication through advanced visual analysis tools will increase the rate of scientific discovery by improving the effectiveness of scientists and forecasters.","title":"Collaborative Research: An Advanced Interactive Multifield, Multisource Atmospheric Visual Analysis Environment","awardID":"0513464","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}}],"PIcoPI":["499748","522848"],"PO":["565136"]},"104558":{"abstract":"Proposal No: 0508338<br\/>Title: Electronic Devices from Viral and Cytoskeletal Templates<br\/>Inst: University of Virginia<br\/>PI: Michael Reed<br\/><br\/><br\/>This research will employ well characterized, self-organizing biological systems as templates to fabricate active nanoscale electronic devices, including integrated electrical interconnects between the nanoscale devices and contact pads. The project applies biological processes and molecular genetic techniques to a practical engineering problem: the fabrication of a molecular-scale electronic device, with precise dimensions and predictable and reproducible performance, in a way that can be scaled to billions or trillions of integrated devices, at a low cost. The geometrically increasing number and performance of transistors on integrated circuits, driven by scaling of devices to smaller dimensions, can only be continued with new technologies that sidestep fundamental limits imposed by current manufacturing techniques relying on photographic pattern transfer. The proposed new technology harnesses self-assembly techniques, evolved by nature, to fabricate electronic switches. Also, methods will be developed to use cytoskeletal templates to fabricate conducting microtubules, useful for contacting this and other molecular electronic devices. This work addresses a vexing problem in interfacing the world of molecular devices, at the nanometer scale, to the micron and millimeter scales necessary in practical systems. Demonstration of the techniques developed in this research will significantly advance our ability to leverage biological systems in the design and development of more complex, self-organizing electronic systems.","title":"NER\/SNB: Electronic Devices from Viral and Cytoskeletal Templates","awardID":"0508338","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1676","name":"NANOSCALE:  EXPLORATORY RSRCH"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"5345","name":"BIOMEDICAL ENGINEERING"}}],"PIcoPI":["547383",276066,276067],"PO":["562984"]},"102138":{"abstract":"Abstract<br\/>Proposal: CNS 0454066<br\/>PI: Enrico Pontelli<br\/>Institution: New Mexico State University <br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: CRI: Computing Support for the Next Generation Application-driven Declarative Programming Systems <br\/><br\/><br\/>The PI's will acquire a Beowulf cluster to support research in Logic Programming, particular a new effort in Answer Set Programming (ASP). ASP addresses needs for constraint programming and non-monotonic reasoning that are not present in traditional logic programming. These needs arise in world situations in which knowledge of the world changes such that prior true statements become false - violating monotonic properties of traditional logic programming. The research to be performed will include research on scalable and transparent methods to exploit parallelism, research on reasoning in complex environments such as those expressing sensing and actuation or actions with time constraints, and development of a domain specific workbench to support the design data oriented processes in evolutionary biology, such as phylogenetic inference. The infrastructure will support research and education at New Mexico State University, a minority serving institution.","title":"CRI: Computing Support for the Next Generation Application-driven Declarative Programming Systems","awardID":"0454066","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["376366","482838","561141","435957","385617"],"PO":["539087"]},"105669":{"abstract":"The objective of this work is to extend significantly the capabilities of portals created by the geospatial community, by providing semantic integration over diverse data sets. For example, the Wisconsin Land Information System and the new Federal Geospatial One-Stop portals disseminate data and support procedures and simulations in emergency situations. However, geospatial data are complex and highly heterogeneous, having been developed independently by various levels of government and the private sector. This project includes metadata integration methods to enable more precise location of data sources over the web and to provide geospatial portals with query capabilities and semantic resolution for the types of information integration that could help in information discovery, problem-solving, or emergency response.<br\/><br\/>The research will develop methods and tools to support the integration of information in such a way that end users will have a homogeneous view over heterogeneous data sources. An ontology-based architecture will be developed with which each individual heterogeneous data source can be added to the network of information with relatively little effort. Ontology mappings will establish correspondences between terms in heterogeneous sources and those of standard models and ontologies. The approach also extends ontology mappings to incorporate semantics regarding spatial considerations, such as accuracy, for spatial integration.<br\/><br\/>Information integration is an area of research that stretches over databases, artificial intelligence, digital libraries, and the semantic web. This project will extend significantly the state of the art of information integration in general and of geospatial information integration in particular, by providing a robust and scalable framework that encompasses techniques and algorithms for integrating heterogeneous data sources using an ontology-based approach.<br\/><br\/>This project's goal of semantic integration for geospatial data fits into a broad vision for creating a cyberinfrastructure on the Web. In a geospatial cyberinfrastructure, data will be automatically located and semantically matched to other relevant data sources. Manual intervention will not be needed or will be minimal. With such a structure, emergency managers, government officials, and the general public will not be constrained to pre-formulated queries. Instead, ad hoc, exploratory queries and analyses will be possible. From an educational viewpoint, the project will significantly benefit the training of graduate and undergraduate students, in particular of women and minorities, and will include the design of new graduate and undergraduate courses.","title":"Collaborative Research: Information Integration for Locating and Querying Geospatial Data","awardID":"0513553","effectiveDate":"2005-07-15","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1629","name":"BE: NON-ANNOUNCEMENT RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}}],"PIcoPI":["554456"],"PO":["565136"]},"107737":{"abstract":"Systems biology is an emerging multidisciplinary field whose goal is to provide a systems-level understanding of biological systems by uncovering their structure, dynamics and control methods.<br\/>While many exciting and profound advances have been made in investigating robustness, network<br\/>structures and dynamics, and application to drug discovery, the field is still in its infancy.<br\/>An important open problem in systems biology is finding appropriate computational models that scale<br\/>well for both the simulation and formal analysis of biological processes. Currently, the majority of these<br\/>models are given in terms of large and complex sets of nonlinear differential equations, describing in painful<br\/>detail the underlying biological phenomena. Although an invaluable asset for integrating genomics and<br\/>proteomics data to reveal local interactions, such models are often not amenable to formal analysis and<br\/>render simulation at the organ or even the cell level impractical.<br\/>This proposal seeks to develop a hybrid-automata (HA) approach to modeling and analyzing complex biological systems. Excitable cell networks (heart cells in particular) will be used as an archetype of a complex biological system. Standard modeling methods capture the behavior of such cells using reaction-diffusion PDE systems, with the Hodgkin-Huxley (HH) formalism describing ion channel gating and currents. Initial results indicate that HA models, combining discrete and continuous processes, are able to successfully capture the morphology of the excitation event (action potential) of different cell types, including cardiac cells. They can also reproduce typical excitable cell characteristics, such as refractoriness (period of non-responsiveness to external stimulation) and restitution (adaptation to pacing rates). Multicellular ensembles of HA elements are used to simulate excitation wave propagation, including complex spiral waves underlying pathological conditions in the heart. The resulting simulation framework exhibits significantly improved computational efficiency, and opens the possibility to formal analysis based on HA theory.","title":"BioComp: Efficient Modeling and Analysis of Excitable Cell Networks using Hybrid Automata","awardID":"0523863","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["417606","451745",284580],"PO":["565223"]},"105912":{"abstract":"Theory and Application of Algebraic Feedback Shift Registers<br\/><br\/>Principal Investigator: Andrew Klapper, University of Kentucky<br\/><br\/>Pseudorandom sequences essential for digital communications and information technology. They are used in stream cipher cryptosystems; in spread spectrum systems in cellular telephones, GPS systems, and satellite<br\/>communications; and as codewords in error-correcting codes for digital communication. Pseudorandom sequences of large numbers are used in large simulations for such applications as weather prediction, reactor design, oil well exploration, radiation cancer therapy, traffic flow, and pricing of financial instruments. In each case sequences with particular properties are needed. Yet few general methods for efficient generation of pseudorandom sequences are known. This research involves the development and analysis of a large supply of pseudorandom sequences for a variety of applications in cryptography, coding theory, and simulations.<br\/><br\/>In 1994 A. Klapper and M. Goresky proposed \"feedback-with-carry shift registers\" (FCSRs), a class of pseudorandom generators which are easily implemented and which rapidly generate pseudorandom sequences with many desirable properties. These were later generalized to algebraic feedback shift registers (AFSRs). Many basic properties of FCSRs and AFSRs have been determined. This project addresses issues concerning FCSR and AFSR sequences including (1) design of \"combiners\", \"feedforward functions\", and clock-controlled circuits for the generation of cryptographically secure sequences, (2) design of fast and efficient FCSRs with good statistical properties for use in quasi-Monte Carlo (QMC), (3) analysis and design of several new AFSR generators with applications to stream ciphers and QMC, (4) solution of the register synthesis problem for AFSRs, and (5) development of new families of error correcting block and convolutional codes based on<br\/>AFSR sequences.","title":"Theory and Application of Algebraic Feedback Shift Registers","awardID":"0514660","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["408476"],"PO":["564898"]},"105703":{"abstract":"Generic tools and technologies for creating and maintaining data cooperatives- confederations whose purpose is distributed data sharing-will be developed to overcome the difficultiess encountered in the sharing of information in life sciences, specifically in bioinformatics.<br\/><br\/>The vision of large-scale data sharing has been a long-time goal of the bioinformatics field, much of it proceeding through data integration efforts. However, conventional approaches to data integration do not have the necessary flexibility and adaptability to make the existing and future plethora of data accessible and usable to typical biologists, while keeping it rapidly extensible to new concepts, domains, and types of queries, and thus fostering new research developments. The main reasons are that (1) different biologists work with different types of data and at differing levels of abstraction; (2) schemas in the bioinformatics world are typically large and complex; (3) queries and mappings may \"break\" without warning because of asynchronous updates; (4) it is logistically, economically and politically difficult to operate centralized data integration facilities. In response to these difficulties data cooperatives emphasize: decentralization for both scalability and flexibility, incremental development of resources such as schemas, mappings, and queries, rapid discovery mechanisms for finding the resources relevant to a topic, and tolerance for intermittent participation of members and for approximate consistency of mappings.<br\/>More specifically, the technical goals of the proposal include: (1)collaboratively developed yellow pages of biological topics; (2) schema templates, capturing the part of the structure of data pertaining to a specific interest and functioning also as visual templates from which a query form created; (3) incremental specification of mappings; (4) reasoning about uncertainty in mappings by measuring with statistical tools their degree of reliability and using it in query answering; (5) multi-path answering for queries with caching and replication in a large-scale data cooperative where the participation of individual members may not always be assured.<br\/><br\/>Data cooperatives will have broader impact through applications in a variety of scientific and industrial fields, but it is in the field of bioinformatics that they are likely to have an immediate and significant impact. Therefore, a specific data cooperative as a biological testbed for evaluating the proposed technologies. This testbed is based on a small set of databases which are already collaborating and exchanging data related to Plasmodium falciparum. Broader impact will be also be achieved through the proposed educational initiatives, specifically through a \"compu-tational orchestra\" bioinformatics course which will expose students to data integration issues through project work, and a workshop for the Greater Philadelphia Bioinformatics Alliance (GPBA). Minority involvement will also be encouraged through a GPBA internship program.","title":"II: Data Cooperatives: Rapid and Incremental Data Sharing with Applications to Bioinformatics","awardID":"0513778","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}}],"PIcoPI":["541878",278900,"541876","512143","517980"],"PO":["565136"]},"107914":{"abstract":"(21) The project is addressing the issues of teaching versus learning and lack of communication between mathematicians, teachers and mathematics education researchers by a systematic content analysis.The content analysis of problems in algebraic thinking drawn from current school curricula and sample student solutions is being conducted jointly by mathematicians, teachers, and researchers. The results of the analysis, an annotated commentary on the problems and the student work, are being disseminated in a form designed to catalyze other collaborations. Furthermore, the work is contributing to the development of instructional materials for professional development of teachers, for teacher education classes, and for university courses in TA training. The central products of the project are: (a) the development and dissemination of a methodology of collaboration between mathematicians, teachers, and mathematics education researchers and (b) concrete instructional materials that demonstrate the effectiveness of the methodology. The instructional materials are being designed to work with a broad range of students. An advisory board of international experts in both mathematics and mathematics education is guiding the project.","title":"Making Connections: Joint Analysis of School Mathematics Problems. A national model for collaboration between Mathematicians, Teachers, and Mathematics Education Researchers","awardID":"0525009","effectiveDate":"2005-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0507","name":"Division of SCIENCE EDUCATION RESOURCES IM","abbr":"SER"},"pgm":{"id":"1264","name":"ALGEBRA,NUMBER THEORY,AND COM"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1260","name":"INFRASTRUCTURE PROGRAM"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1746","name":"DISTINGUISHED TEACHING SCHOLAR"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1793","name":"MSP-OTHER AWARDS"}}],"PIcoPI":["443401"],"PO":["472260"]},"105615":{"abstract":"This project will develop novel data analysis algorithms that will enable scientists to discover new knowledge from large data sets of variable-size structured data. It will specifically focus on small molecules in organic chemistry that can be represented in various ways, such as the two-dimensional graph of covalent bonds. The project will develop new methods to explore chemical space and predict the physical, chemical, and biological properties of small organic molecules. <br\/><br\/>The research will directly address these problems by using annotated datasets of compounds in combination with machine learning approaches. Specifically, this project will develop new fingerprint representations of small molecules, for instance, by indexing the paths and trees contained in the molecular graphs, or by building histograms of three-dimensional distances between labeled pairs of atoms. Kernels methods - currently one of the leading methods in machine learning - will be used to measure similarity between fingerprints and to develop predictive algorithms for classification and regression tasks. The algorithms will be quantitatively evaluated in terms of their ability both to describe observed data and to predict new data. Selected applications to the prediction of critical temperatures, toxicity, mutagenicity, anti-cancer and other biological activity of small molecules will serve as testbeds for validating the techniques developed. Algorithms and data developed during the project will be made publicly available on the Web for research and scientific use. Educational activities included in this project will foster in computer science students an understanding of the increasingly important role of computer science and data mining in data-driven sciences such as chemistry. <br\/><br\/>New informatics methods for structured data will greatly benefit chemistry. The penetration of computational, artificial intelligence, and informatics methods in chemistry has been slower than in biology, because of the single-investigator nature of chemical research and the dominance of genome sequencing and other high-throughput projects in biology. Data on millions of compounds, however, are becoming readily available. By developing efficient fingerprints, kernels, and other machine learning methods for graphs and molecular structures, this project will address some of the most outstanding problems in the field and help accelerate the penetration of modern computational methods in chemistry. <br\/><br\/>The project has the potential for significant benefit to society. Small molecules have numerous applications in biology, pharmacology, and bioengineering. They can be used to probe and study biological pathways and systems and to develop new drugs. The algorithms developed by this project will provide basic building blocks and important steps towards understanding and predicting molecular properties from molecular structures. They will allow scientists to screen large data sets of compounds rapidly, while searching for compounds that satisfy particular structural or functional constraints. This will produce cost savings, accelerate the development of new drugs, and promote the understanding of chemical space.","title":"Mining Structured Data with Applications in Chemistry and Biology","awardID":"0513376","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["551111"],"PO":["565136"]},"104768":{"abstract":"The goal of this research project is to build an application infrastructure and a suite of embedded system software that will enable the rapid development of software control and monitoring applications, such as heating, ventilation, and air conditioning systems (HVAC), on low-power, lossy wireless sensor-actuator<br\/>networks. Such user-preference driven control applications require the ability to adapt to noise, loss, and failures in a large-scale distributed network, and demand a high degree of automation in<br\/>acquiring user preferences, making control decisions and disseminating those decisions to the appropriate actuators.<br\/><br\/>The Sensor Control System (SCS) being developed provides a high-level platform that allows users to express control application requirements at the granularity of the entire network, enabling users to focus on the requirements of their deployments by abstracting away the low-level sensor network details. SCS combines techniques from machine learning and statistics with high-level abstractions inspired by work in software systems and databases, allowing for mathematically-sound decision making despite loss and uncertainty<br\/>that is inherent in such systems. The project will demonstrate the approach on a highly instrumented office environment. This research has the potential to impact industry through the development of a new class of wireless control systems, and academia through the infusion of cross-disciplinary ideas into a variety of sub-fields of EECS. The datasets, code and the sensor-actuator deployments developed during<br\/>the course of this research will be used in a range of educational initiatives.","title":"CSR-EHS: Collaborative Research: A General, Efficient and Robust Platform for Enabling Control Applications in Sensor Networks","awardID":"0509220","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550401"],"PO":["561889"]},"101017":{"abstract":"Spectrum is among the world's most expensive natural resources and demand is skyrocketing due to the rapid proliferation of broadband wireless services. On the other hand, preliminary studies indicate the presence of a significant amount of white space, or unused space, in the radio spectrum. Thus, it is spectrum access, instead of true spectrum scarcity, that limits the potential growth of versatile wireless services. This project is motivated by this dilemma. Opportunistic utilization of the white space is studied, which has the great potential to mitigate the spectrum scarcity. The project focuses on modeling and protocol design. The expected results include new metrics to quantify the impact of exploiting white space and analytical models to capture the spatial and temporal characteristics of white space, and thus a better understanding on the potentials of opportunistic spectrum utilization. Furthermore, centralized and distributed algorithms will be developed that provide dynamic, efficient, and fair sharing of the opportunistically-available spectrum. Suitable solutions for different application scenarios will be identified. The results of the project will benefit the research community and provide theoretical data for policy-makers. The project will also enhance the education curriculum and foster the participation of under-represented groups in engineering.","title":"CAREER:Smart-Radio-Technology-Enabled Opportunistic Spectrum Utilization","awardID":"0448613","effectiveDate":"2005-07-01","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["436918"],"PO":["557315"]},"102139":{"abstract":"Abstract<br\/><br\/>Proposal: CNS 0454074<br\/>PI: Oliver Brock<br\/>Institution: University of Massachusetts - Amherst<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Experimental Platform for Robot Programming and Task Execution in Human Environments<br\/><br\/><br\/>The investigators will construct a robotic platform that combines mobility and dexterous manipulation capabilities with advanced sensing. This platform will serve as the experimental testbed for an integrated research effort into algorithms for programming mobile manipulators to perform complex tasks in human environment. The infrastructure will support research on five projects. (i) Probabilistic logic planning to develop plans using a representation of the world. This research will develop adaptive strategies to deal with uncertainty in the real world. (ii) Sensing for mobile manipulators deals with the challenge of coordinating the motions of the manipulator and the mobility operations that permit moving about the world. (iii) Sensing for mobile manipulators will explore how to move sensors such as cameras to get multiple views of a scene and improve understanding of the scene. (iv) Programming by demonstration will develop techniques to learn from human operators how to perform a complex task. (v) Programming by observation uses a hidden Markov model to deal with hierarchical task decomposition. Broader impacts of this project include the use of the facility in student thesis projects, REU projects that recruit from nearby women's colleges, and participation in the SPUR program that provides summer research opportunities to students from underrepresented groups.","title":"CRI: Experimental Platform for Robot Programming and Task Execution in Human Environments","awardID":"0454074","effectiveDate":"2005-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["376899","391847","460642","544475",269267],"PO":["557609"]},"120103":{"abstract":"NeTS-ProWiN: Collaborative Research - Dynamic Spectrum MAC with Multiparty Support in Adhoc Networks<br\/><br\/>Award 0435306<br\/><br\/>Saswati Sarkar, University of Pennsylvania<br\/><br\/>Abstract<br\/><br\/>Major advances in dynamic spectrum management and the inevitable deregulation of large portions of the radio spectrum will revolutionize future wireless networks, services, and applications. This will lead to an era of spectrum efficient cognitive radios that will enable the deployment of radically different radio architectures, algorithms, and protocols over the next decade. This project is studying the design of a new programmable media access control (MAC) layer for this new environment. The MAC must regulate how future programmable radio devices can efficiently interact with each other using spectrum-aware communication algorithms. In our study, we model the MAC design as decision problems using tools from decision sciences such as stochastic control, optimization, graph theory and estimation theory. We validate the design of the MAC through an experimental implementation consisting of programmable radios. The expected results from the research include the design of a programmable MAC system that can enable a class of new applications, including enhanced reliability communications and spectrum-efficient group communications. The resulting implementation of the proposed programmable MAC platform and its software will enable the development of new intelligent spectrum-aware algorithms and applications. The results of the research will provide a set of foundation algorithms that can be used by the community developing new spectrum-aware radio systems. The research facilitates several life-critical activities e.g., search and rescue missions and disaster relief operations. The research will also enrich the education curriculum of the participating institutions and foster the participation of under-represented groups in engineering.","title":"NeTS-ProWiN - Collaborative Research: Dynamic Spectrum MAC with Multiparty Support in Adhoc Networks","awardID":"0631289","effectiveDate":"2005-07-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"V818","name":"DEFENSE-NETS PROGRAM"}}],"PIcoPI":["529645"],"PO":["434241"]},"101722":{"abstract":"Abstract<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Enabling Globally-Distributed Electronic Collaboration (GloDEC) for Expertise Studies and Human and Social Dynamics Research Planning Grant <br\/>Proposal: CNS 0452180<br\/>PI: Becerra-Fernandez, Irma <br\/>Institution: Florida International University <br\/><br\/><br\/>Florida International University will use this planning award to evaluate feasibility and expansion of its knowledge management lab to support studies in collaborative expertise and human and social dynamics. Emphasis will be on expanding research programs in expertise in collaboration with other Florida universities, on understanding through computer models how components of expertise interact with social, communication and organizational structures, and on developing linkages between these two areas. Specific planning activities will explore inter-university PhD education and research, assess and recommend technology infrastructure for their needs, and plan an implementation for a student-centered collaborative, electronic learning community. Broader impacts include the potential for impact on education programs in management information systems. Florida International University has the largest contingent of Hispanic students among US doctoral granting institutions who can benefit from this innovative program.","title":"CRI: Enabling Globally-Distributed Electronic Collaboration (GloDEC) for Expertise Studies and Human and Social Dynamics Research Planning Grant","awardID":"0452180","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["397037","396906"],"PO":["539087"]},"104824":{"abstract":"The project will develop technologies for distributed computer and data intensive application environments such as those encountered in e-commerce, entertainment, financial, search engines, medical informatics, genomics, and web-based content delivery. Many of these emerging applications are quite data-intensive in nature. Data is being generated from multiple sources such as transactions, instruments, and through simulations. Not only datasets are increasing in size, they are becoming more dynamic and distributed. The designs used by current-day data-centers have several limitations to achieve good performance and scalability with low cost for handling the emerging trend of dynamic and geographically distributed data. The proposed research is built around computer architecture, networking, cluster computing, and data.<br\/><br\/>The project will carry out research along the following directions: designing advanced communication protocols and subsystems by taking advantage of novel mechanisms and features of modern networks; designing and developing novel data-center primitives such as soft shared state, distributed lock managers and global memory aggregators; designing dynamic content caching services; and designing constraint-based active resource adaptation services to provide Quality of Service (QoS) guarantees. Solutions developed along the proposed directions are planned to be integrated on an experimental data-center testbed and a simulated testbed and evaluated using a set of commercial workload.","title":"CSR: Designing Next Generation Data-Centers with Advanced Communication Protocols and Systems Services","awardID":"0509452","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["561947"],"PO":["551712"]},"104714":{"abstract":"The goal of this research is to remove the tight binding of user<br\/>state to specific computing hardware. This tight binding arose<br\/>at the birth of personal computing nearly three decades ago, and<br\/>has continued unchanged since that time. It has many negative<br\/>consequences: hardware upgrades are painful, disaster recovery is<br\/>complex, and total cost of ownership is high. It also forces<br\/>mobile computing into a paradigm of each user carrying personal<br\/>hardware, rather than traveling hands-free and taking advantage<br\/>of pervasive hardware.<br\/><br\/>The proposed research aims for a new vision of on-demand personal<br\/>computing called \"Internet Suspend\/Resume (ISR)\" that exploits<br\/>the ubiquitous presence of the Internet. This research explores<br\/>issues at the intersection of two well-established technologies:<br\/>virtual machine (VM) technology and distributed file system<br\/>technology. By layering a VM on a distributed file system that<br\/>performs aggressive client caching, a user's entire personal<br\/>computing environment (operating system, applications and user<br\/>files) can be accurately and safely delivered anywhere on the<br\/>Internet.<br\/><br\/>The proposed research spans operating systems, distributed<br\/>systems, mobile and pervasive computing, and security and<br\/>privacy. Its has three major thrusts: (a) Coping with Huge VM<br\/>State, (b) Preserving Privacy of VM State, and (c) Dynamically Varying<br\/>Client Thickness. The work in each thrust includes conceptual<br\/>development of novel techniques and algorithms, as well as their<br\/>implementation and experimental validation in the context of a<br\/>prototype. This prototype will be disseminated in open source<br\/>form, for use by other researchers and by industry.","title":"CSR---PDOS: Liberating Personal Computing from Hardware","awardID":"0509004","effectiveDate":"2005-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["496764",276437,"358529","475390"],"PO":["493916"]},"105946":{"abstract":"The research supported under this grant targets the thorough and effective understanding of message-passing algorithms which constitute a large and very potent class of estimation and detection techniques. Indeed, while message-passing (iterative) processing is very successful in practice, understanding its limitations and its sources of non-optimal behavior has been elusive. Despite the enormous impact that message-passing algorithms have, in particular in a communication scenario, practical systems currently rely almost exclusively on a simulation-based evaluation. In this situation, understanding the behavior and geometry of message-passing will not only reduce the necessity of simulations but provide powerful tools for system optimization.<br\/><br\/>This proposal draws on recent exciting developments that connect message-passing algorithms to the well established theory of convex optimization. As it turns out, message-passing algorithms are intimately related to a linear programming formulation of the inference task at hand. In fact, belief propagation algorithms may be interpreted as an efficient duality-based method to closely approximate the solution to a linear program. Once such connections are established the investigators will strive to understand message-passing algorithms from an entirely new and fruitful point of view. Also, the investigators have already shown that the connection to convex optimization is rooted in the basic property of message-passing algorithms, namely that they operate only locally in a given graphical model. Thus the findings resulting from the approach investigated here will apply to any reasonable locally-operating algorithm.","title":"Collaborative Research: Message-Passing Algorithms - from Practice to Theory and back to Practice","awardID":"0514801","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["333038"],"PO":["564898"]},"104736":{"abstract":"Model-based approaches to software development promise significant increase in productivity. However, current industrial efforts in Model Driven Architectures (MDA) generally lack rigorous foundations, and their full potential for embedded system development is yet to be demonstrated. The exploratory research in this project addresses these issues in the following research thrusts:<br\/><br\/>(1). The project develops formal foundations for a concrete MDA approach based on Domain-Specific Modeling Languages (DSMLs). A central component of the exploratory research investigates whether graph transformation systems can provide a suitable technology framework for practical model transformation and software generation.<br\/>(2). The project applies the results of research in the domain of distributed, real-time embedded systems by building specialized DSML-s for embedded applications, and demonstrating how software can be generated for different platforms from the same code base. <br\/><br\/>The project seeks to advance knowledge about constructing complex embedded software systems by exploring: domain-specific models for embedded applications, formal model transformation, and platform-dependent software synthesis from models. The broader, societal impact of this research lies in the importance of rigorous methods for building and maintaining complex real-time embedded system applications that are safe and reliable.","title":"CSR-EHS: Software Composition for Embedded Systems using Graph Transformations","awardID":"0509098","effectiveDate":"2005-07-15","expirationDate":"2008-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["553600",276491],"PO":["561889"]},"105715":{"abstract":"Determining the evolutionary relationships among a group of organisms is one of the major goals of integrative historical biology. In the century following Darwin , morphological structures were almost the sole source of data for a variety of analyses aimed at resolving this problem. Over the past few decades, molecular systematic techniques have taken center stage in the study of extant species, usually supporting but frequently overturning results based on morphology. In paleontology, however, morphological data (the shapes of fossils) remain effectively our only information<br\/>describing the evolutionary position of extinct organisms. The application of 3D techniques of data collection and analysis, as well as computer-based visualization methods, to data sets including both extant and extinct species will now permit improved levels of interpretation in paleontology. The team will develop new tools in each of these areas, using Old World monkeys as a \"test-bed\" or model organism, to approach significant problems in both computer science and the broad domain of comparative biology. New advances in the analysis and manipulation of surfaces and volumes in three-dimensions in computer graphics, visualization and computer vision can be applied to these problems. Simultaneously, collecting and interpreting morphological data for paleontology is a novel application for pushing the development of techniques in computer science.<br\/><br\/>analysis will be based on a large existing database of three-dimensional data (mostly skull surfaces) at the American Museum of<br\/>Natural History.<br\/><br\/>The interactive graphics, visualization and statistical analysis tools we propose are ever more widely needed as the amount of three-dimensional morphology data increases. The close interaction of geometric morphometrics and computer graphics will lead to new ideas about the representation of shape. In addition, the project develops new approaches to the problem of integrating morphology with molecular data in the study of evolution, applicable in many parts of the tree of life. In addition, with massive amounts of new data, new processing and analytic software, and new approaches to integrating morphology, answers to specific questions about the evolution of African monkeys might be obtained.<br\/><br\/>A large part of the project will be done at Lehman College of CUNY, a minority-serving institution in the Bronx, and minority undergraduates are already involved in the research. The software tools we will develop are sorely needed and will become part of the scientific infrastructure. The visualizations will form a basis for sharing research in evolution with the general public.","title":"SEI(SBE): Collaborative Research on Visualization of Evolutionary Transformation using 3D Morphometrics: African Monkeys as a Test Case","awardID":"0513894","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1629","name":"BE: NON-ANNOUNCEMENT RESEARCH"}}],"PIcoPI":["495182","538707","297417"],"PO":["565136"]},"105979":{"abstract":"Airborne networks can be used to transmit vehicle state information among aerospace vehicles, moving platforms, and ground stations for enhancing flight safety, efficiency, and security in a wide range of air transportation systems. Fundamental to the operation and design of airborne networks are issues on the effective dissemination of vehicle state information, efficient use of available radio spectrum for data transmission, and optimal decision-making across the networks. This research project seeks to establish theoretical foundations for some fundamental optimization problems in these issues.<br\/><br\/>For the effective information dissemination across airborne networks, the research will focus on the selection of aircraft as information centers that provide information for other aircraft. The proper selection of information centers requires that the service cost be minimized, or\/and the service time be bounded. For the efficient use of radio spectrum in airborne networks, the project explores the possibility of frequency reuse by properly assigning channels to active transmissions. The proposed research will successfully solve both the hidden terminal problem and the exposed terminal problem. Finally, for the distributed optimal decision-making across airborne networks, the project emphasizes proper modeling and the interplay of vehicle dynamics, communication link performance capabilities, as well as numerical schemes for distributed optimization. In addition, a series of educational activities are planned in order to bring the impacts of this project to the networking research community and classrooms.","title":"Collaborative Research: Fundamental Optimization Problems in Airborne Network Design and Application","awardID":"0514940","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["544621"],"PO":["562944"]},"107816":{"abstract":"Better understanding of the processes involved in the physiology of bacteria can potentially have tremendous impact on both therapeutic approaches to infectious diseases and metabolic engineering applications in biotechnology.<br\/><br\/>In this project, Drs. David L. Wild and Matthew J. Beal, of the Keck Graduate Institute in Claremont, California and the State University of New York in Buffalo, New York, respectively, are proposing to build statistical models of time series data, with a view to leveraging sophisticated Bayesian methods to \"reverse-engineer\" an organism's complex genetic regulatory networks from the raw measurements of gene expression and metabolite concentration.<br\/><br\/>Drs. Wild and Beal will apply their techniques to an ideal experimental system: the response of the bacterium E coli to acid stress, which enables pathogenic E. coli to survive passage through the acidic environment of the stomach and gastro-intestinal tract. They will collaborate with experimentalists Drs. Francesco Falciani and Mark Viant at the University of Birmingham, UK, who will provide data from both pathogenic and non-pathogenic strains of this bacterium. Predictions made by Wild and Beal's models can then be tested and explored back in the laboratory.<br\/><br\/>Recent advances in functional genomics technologies have given biologists unprecedented access to measurements of the inner workings of complex biological organisms. Using microarray expression profiling, it is now possible to measure the expression levels of tens of thousands of genes in just a single biological experiment, conducted over several days in the form of a time series. Contrast this to the situation only ten years ago when it was rather unusual for a biologist to measure the expression of more than just one or two carefully chosen genes. As well as high-throughput gene expression methods, the new technology of \"metabolomics\" has opened the door to measuring even more information in the form of the concentration of hundreds of metabolites that are also crucial players in the complex cellular processes under study.<br\/><br\/>This overwhelming amount of data challenges traditional methods of analysis, especially when one considers the element of time, because now one must consider how certain genes regulate the expression of other genes from one time point in the experiment to the next. <br\/><br\/>A key ingredient in Drs. Wild and Beal's models is the inclusion of \"hidden factors\" that help to explain the correlation structure of the observed measurements. These factors may correspond to unmeasured quantities that were not captured during the experiment and often reduce the number of direct gene-to-gene dependencies, leaving the resulting networks much more interpretable for the biologist. A natural question arises: how many hidden factors should be used to account for the dependencies in the observed data? This is answered by employing Bayesian model selection, a well-founded principle used in machine learning and statistics to choose between models of differing complexities. Their models also use a technique called Automatic Relevance Determination to further simplify the models so that only those genes and metabolites that are participating players in the process are retained in the final model.<br\/><br\/>Another advantage of the Bayesian framework is that existing information about known network connections and interactions, derived from the literature or commercial databases, can be included in the model. The output of the modeling procedure is a probabilistic reckoning of which genetic regulatory networks are plausible or not. These probabilities can be used to design future biological experiments targeted at specific genes, with a view to corroborating the model's in silico predictions or to simply probe a relatively uncharted network.","title":"BioCmp: Reconstructing Metabolic and Transcriptional Networks using Bayesian State Space Models","awardID":"0524331","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":[284791,"497659",284793],"PO":["565223"]},"105759":{"abstract":"Intellectual Merit: The power consumption rate of computing devices has been increasing exponentially. This makes it increasingly difficult to supply energy to devices and to cool these devices. Poweraware computation is especially important in the domain of sensor networks which are composed of small battery-powered nodes. Power management in sensor networks is viewed as so critical that it must be dealt with at all layers of the protocol stack.<br\/><br\/>Many power management techniques have been proposed and implemented. Most of these techniques are similar in that they reduce or eliminate power to some or all components of the device. However, there is an inherent conflict between power management and performance; in general, the more power that is available, the better the performance that can be achieved. As a result, it is generally proposed that power reduction techniques be preferentially applied during times when performance is less<br\/>critical. However, this requires a policy to determine how essential performance is at any given time and how to apply a particular power reduction technique. For example, to use the frequency scaling technique, where the speed of the clock is changed dynamically, one needs a policy to set the speed at each point in time. There is a growing consensus that these policies must incorporate information provided by applications and high levels of the operating system, and that current tools and mechanisms for power management are inadequate and require more research. The authors propose to formalize power<br\/>management problems as optimization problems, and then develop algorithms that are optimal by these criteria. The goal of this research is to develop effective algorithms for specific problems within the domain of power management, as well as to build a toolkit of widely applicable algorithmic methods for problems that arise in energy-bounded and temperature-bounded computation. The authors propose to initially focus on problems that deal with speed scaling and power-down techniques, since these are currently the dominant techniques in practice.<br\/><br\/>Broader Impacts: The authors propose to both develop fundamental theoretical techniques, and to apply these techniques to attack timely and important applications in computer systems. Both PIs have an established track record of working closely with researchers in applied areas to ensure that the theoretical models developed match the associated real-world problems. This is essential for theoretical results to have an impact. This work will continue to foster this very productive cross-fertilization between<br\/>these experimental systems researchers and theoretical computer science. The students supported under this grant will be influenced by this philosophy of research. They will be trained to be proactive in working with researchers in applied domains to bring important and interesting problems into the theory community. They will also be encouraged to publish the resulting work in systems as well as theory conferences to ensure that new algorithmic discoveries have an impact. As part of this project, the authors also plan to continue outreach work to high schools students, encouraging underrepresented groups to choose careers in technology related fields. They have developed a talk outlining diverse opportunities within computer science and plan to involve graduate and undergraduate students in presenting this talk at local high schools. The work in this proposal is featured in the talk. In addition, they plan to involve students from underrepresented groups in research projects related to power management.","title":"Collaborative Research: Algorithmic Support for Power Aware Computing and Communication","awardID":"0514058","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["534213"],"PO":["499399"]},"104219":{"abstract":"Automated analysis of public health data represents a critical need, but effective analysis must look beyond individual data points. Much of the data that is collected is structural, consisting not only of entities but also of relationships (e.g., spatial,temporal) between the entities. As a result, a need exists to develop methods for discovering knowledge and learning concepts specifically for this type of structural data. A graph-based data mining technique that can perform pattern discovery, concept learning, and hierarchical clustering on data represented as graphs. This approach, implemented in the Subdue system, has demonstrated success in a numbeof scientific and industrial databases. The proposed effort will investigate the viability ofgraph-based data mining approach as a foundation for representing and ministructural data found in public health databases and related applications.<br\/><br\/>The effort will contribute 1) an analysis of public health data that explores data<br\/>points, relationships between the data points, and integration of data from related<br\/>domains to strengthen the results, 2) design of a graph-based mining system that can handle streaming data in an online fashion, 3) development of a new approach to concept learning that processes training examples embedded n a single interconnected graph, and 4) construction of a toolset that can provide early detection and assessment of epidemics and other public health crises.<br\/>The project depends on a strong partnership between computer scientists and an expert in public health. A collaboration between the University of Texas at Arlington and the University of North Texas Health Science Center has already received initial support from the two schools The collaboration will be fostered through monthly seminars and research meetings. The results of this project will thus have an impact on the computerscience community and an equal, if not greater, impact on the domain community The code and data will be available for general dissemination over the Internet, and results will be integrated into the classroom and into a book on graph-based data mining.","title":"Collaborative Research: SEI: Graph-based Mining of Public Health Data","awardID":"0506635","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":[275035],"PO":["565136"]},"102515":{"abstract":"Thisi project will develop novel tools and technologies for automated ingestion and management of preservation processes, and demonstrate their use on scientific, historical, and educational collections covering widely different technical requirements. The technologies will be built on a novel robust, reliable, and secure layered architecture - ADAPT (Approach to Digital Archiving and Preservation Technology) - developed by the research group using open standards and web technologies. This architecture will evolve gracefully as the underlying technologies change and will interoperate with digital library and grid technologies. Specifically, The team will develop a distributed persistent archive that provides producers, site administrators, and preservation managers key functionalities for the long-term access and preservation of digital assets. A novel architecture for a deep archive, with provably high reliability and high resiliency against security attacks and system failures, will also be demonstrated. The tools and technologies will be tested and validated on four very distinct and rich collections: (i) an archive of videotaped oral histories provided by the Survivors of the Shoah Visual History Foundation; (ii) children's books in their original languages (current over 500 books in 30 languages) available through the University of Maryland International Children.s Digital Library (ICDL); (iii) a rich historical collection of photographs, drawings, maps, charts and textual documents available through the National Archives' Electronic Access Project (EAP); and (iv) a wide variety of unique earth science data available through the University of Maryland Global Land Cover Facility (GLCF).<br\/><br\/>Intellectual Merit. A large portion of the scientific, business, cultural, and government digital information being created today needs to be maintained and preserved for future use of periods ranging from a few years to decades and sometimes centuries. This project introduces a novel framework for planning, managing, and executing ingestion and preservation processes. Novel features include: (i) an automated distributed ingestion architecture that enables secure and verifiable ingestion of digital objects; (ii) a policy driven management of preservation processes with the ability to constantly audit secure replication, refreshing, and migration, and track media degradation, file corruption, and format obsolescence; (iii) a peer-to-peer architecture for a deep archive, which guarantees with high probability the integrity and survivability of every object in<br\/>the deep archive against failures and malicious corruption; and (iv) an evaluation strategy to assess various strategies developed under this project. <br\/><br\/>Broader Impact. Many communities are interested in long term preservation of their data, and are seeking technology approaches to deal with this challenging problem. This project will address issues of direct interest to all these communities. The research team will introduce technologies and tools to these communities through presentations at major related meetings. Moreover, this project involves collaborations with the National Archives and Records Administration (NARA), NASA at Goddard, and the Survivors of the Shoah Visual History Foundation. An impact on these three organizations will in fact have a much broader impact on many other communities. Strong letters of support from these three organizations are appended to this proposal. Various tools will be developed and released as well-documented open source software.","title":"Robust Technologies for Automated Ingestion and Long-Term Preservation of Digital Information","awardID":"0455995","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V753","name":"LIB CONGRESS-DIGITAL PRESERVAT"}}],"PIcoPI":["501013","543582","518152"],"PO":["371077"]},"105969":{"abstract":"Abstract<br\/>--------<br\/><br\/> Digital communication, embodied in such applications as cell phones and wireless Internet, by now pervades our daily lives, while digital storage devices, such as CDs, DVDs, and computer disk drives, have become the principal means of preserving our information. In the \"information age\" in which we all now live, the need for reliable transmission and storage of digital data is of paramount importance. What makes such reliable transmission and storage possibles are error-correcting codes, first conceived by Claude Shannon over 50 years ago. Indeed, as you are reading these lines, millions of error-correcting codes are decoded every minute, using efficient algorithms implemented in custom VLSI circuits. At least 75% of these circuits decode Reed-Solomon codes, invented by Irving Reed and Gustave Solomon in the 1960s. In the four decades since their invention, Reed-Solomon codes have been extensively studied and ingenious decoding algorithms for these codes have been developed. What has been realized only recently, however, is that Reed-Solomon codes can correct many more errors than previously thought possible! In a series of theoretical breakthroughs, Sudan, Guruswami-Sudan, and Koetter-Vardy have made state-of-the-art Reed-Solomon decoders out of date. At least in principle, we can now achieve much better performance with the same codes. The goal of this project is to follow-up on this exciting recent work and to follow this line of research through to its ultimate potential, in theory as well as in practice.<br\/><br\/> In order to attain this goal, we plan a broad line of attack. On one hand, the proposed investigation will address deep theoretical questions. Can one exceed the Guruswami-Sudan decoding radius? What is the optimal multiplicity assignment for algebraic soft-decision decoding? How can iterative decoding methods be applied to Reed-Solomon codes? On the other hand, we intend to go all the way to the first-ever VLSI implementation of a soft-decision Reed-Solomon decoder. The proposed VLSI architecture aims for high speed and low power dissipation. Thus complexity considerations, inherently motivated by the practice of VLSI design, will be paramount throughout our investigation. Specifically, The main topics to be investigated are: (1) Multivariate interpolation decoding beyond the Guruswami-Sudan radius; (2) Probabilistic model and multiplicity assignment schemes for algebraic soft-decision decoding; (3) Iterative methods for soft decision decoding of Reed-Solomon codes; (4) Analytic bounds on the performance of maximum-likelihood and suboptimal decoders; and (5) Complete VLSI implementation of a state-of-the-art soft-decision Reed-Solomon decoder in an FPGA and\/or ASIC.","title":"Collaborative Research: Next Generation Decoders for Reed-Solomon Codes","awardID":"0514890","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["485936"],"PO":["432103"]},"107707":{"abstract":"There has been considerable recent progress demonstrating a class of DNA molecular-effectors called \"DNA walkers\" that execute various forms of biped locomotion along self-assembled linear DNA nanostructures. In particular, the walker constructed by Reif's group at Duke is the ?rst DNA walker that executes autonomously, without external intervention. This project will extend the emerging nanotechnology of DNA walkers to provide a programmable network system for molecular transport and communication within self-assembled DNA lattices. Our system will perform a set of communication and transport operations. These include, among other things, the ability to (i) perform a class of ?nite state operations on the information provided by speci?ed sites of the 2D lattice, (ii) route ?nite state information and\/or nanoparticles to and from pairs of tiles of the DNA lattice or to other specified sites. Although the degree of programmability of the proposed constructs is restricted to ?nite state transitions, still the programmability far exceeds current experimental demonstrations of current molecular effectors and molecular-motor devices. We intend to make significant improvements to Duke's existing autonomous DNA walkers to allow them to have far more impact to nanoscience and nanotechnology, including extending the DNA walkers to be programmable in at least two senses: (a) They will be made to route to target sites following paths embedded in the 2D DNA lattices (in contrast, prior DNA walkers traversed only linear DNA nanostructures). (b) The DNA walkers will be able process information embedded along its path and perform finite state operations on the sequence of information it encounters. We will also self-assemble for the first time fully addressable two dimensional DNA lattices, on which the programmable paths of the walkers will be defined. We will also extend the DNA walkers to nano-transport devices: that is, (a) to transport a variety of types (e.g., metallic particles and proteins) of nano-particles, (b) to pick up and unload nano-particles at specific sites on the DNA lattice, and (c) to cooperatively carry a DNA lattice as a load. These much improved DNA walkers will be of two types: (a) One type is an extension of previous autonomous walkers developed at Duke and is based on protein enzymes; (b) the other type is made purely of nucleic acids and does not use protein enzymes. In addition to the design and computer simulation of these improved DNA walkers, the proposed work includes a series of experimental demonstrations of isolated capabilities of DNA walkers as listed above, as well as experimental demonstrations of general operations such as lattice-wide transporting\/processing of information and transporting of material. <br\/>The proposed work is inherently cross-disciplinary and will impact multiple ?elds including chemistry, biochemistry, physics, computer science, and robotics, with potential long-term applications in nano-engineering and possibly in areas of biomedical interests. It will provide exciting and challenging interdisciplinary training opportunities for Duke graduate and undergraduate students. This project will provide a transition from the existing simple nano-effectors to programmable nano-robotics network system, yielding capabilities that have broad impact to nano-engineering. There are numerous feasible practical applications of our proposed autonomous programmable molecular transport network. For example, such a programmable nano-particle transport system might be used in the future for constructing complex assemblies of various nano-electronic devices attached to specified sites on the DNA lattice and for detecting\/processing\/broadcasting molecular signals.","title":"NANO: EMT: A DNA-Based Autonomous Programmable Molecular Transport Network","awardID":"0523555","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["550822","531216"],"PO":["565223"]},"107729":{"abstract":"Self-assembly is the process by which small ``components'' spontaneously form intricate aggregate structures. DNA self-assembly is a key tool for nano-technology, nano-robotics, and molecular computation. More generally, biological organisms are self-organized chemical systems that carry out algorithms encoded in the genetic material, DNA. Biology thus provides clear proof that autonomous chemical systems can be programmed, and that they can function reliably on a grand scale -- biological organisms can be composed of as many as 1024 molecular components!<br\/><br\/>Recent experimental advances in the synthesis of artificial molecular systems have demonstrated that it is possible to program molecular self-assembly to carry out rudimentary logic. These experiments also suggested that the occurrence of errors is a major obstacle to scaling up DNA self-assembly and biologically inspired computation. This project will devise algorithmic tools and analysis techniques for error-correction and error-suppression in biologically inspired self-assembling and computational systems. It is our hope that our research will facilitate sophisticated tasks such as counting, growing molecular assemblies of pre-specified sizes (no larger, no smaller), and pattern recognition of complex chemical signals using inherently error-prone biomolecular operations at the nano-scale.<br\/><br\/>In order to design and analyze our error-correction mechanisms, we will use high level models which are both sufficiently realistic to be useful and sufficiently abstract to be amenable to analysis. The basic elements of our models will be DNA tiles, transcriptional circuits, and DNA hybridization catalysts. Thus, our research will target assembly of and computation with large molecules such as long chains of DNA rather than smaller molecules such as proteins and amino acids.<br\/><br\/>We will also develop course material on the basis of our research which will be taught at Caltech and at Stanford.","title":"NANO: Collaborative Research: EMT: Algorithmic error-correction in biologically inspired self-assembly and computation","awardID":"0523761","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["558957"],"PO":["521045"]},"100943":{"abstract":"This project explores data management methods for geosensor networks, i.e. large collections of very small, battery-driven sensor nodes deployed in the geographic environment that measure the temporal and spatial variations of physical quantities such as temperature or ozone levels. An important task of such geosensor networks is to collect, analyze and estimate information about continuous phenomena under observation such as a toxic cloud close to a chemical plant in real-time and in an energy-efficient way. The main thrust of this project is the integration of spatial data analysis techniques with in-network data query execution in sensor networks. The project investigates novel algorithms such as incremental, in-network kriging that redefines a traditional, highly computationally intensive spatial data estimation method for a distributed, collaborative and incremental processing between tiny, energy and bandwidth constrained sensor nodes. This work includes the modeling of location and sensing characteristics of sensor devices with regard to observed phenomena, the support of temporal-spatial estimation queries, and a focus on in-network data aggregation algorithms for complex spatial estimation queries. Combining high-level data query interfaces with advanced spatial analysis methods will allow domain scientists to use sensor networks effectively in environmental observation. The project has a broad impact on the community involving undergraduate and graduate students in spatial database research at the University of Maine as well as being a key component of a current IGERT program in the areas of sensor materials, sensor devices and sensor. More information about this project, publications, simulation software, and empirical studies are available on the project's web site (http:\/\/www.spatial.maine.edu\/~nittel\/career\/).","title":"CAREER: Data Management for Ad-Hoc Geosensor Networks","awardID":"0448183","effectiveDate":"2005-07-01","expirationDate":"2010-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["289731"],"PO":["469867"]},"103814":{"abstract":"PROPOSAL NUMBER.: 0504387<br\/>INSTITUTION: Rutgers University New Brunswick<br\/>NSF PROGRAM: STATISTICS<br\/>PRINCIPAL INVESTIGATOR and Co-PI: Shepp, Larry and Zhang, C Hui<br\/>PROPOSAL TITLE: Statistical Methods in Fast Functional MRI<br\/><br\/><br\/><br\/> Abstract<br\/><br\/>The proposed research will further advance and use statistical methods developed by the principle investigators and their collaborators to <br\/>sharply improve time-resolution for the functional magnetic resonance imaging. The objective remains to improve the time-resolution of <br\/>functional magnetic resonance imaging by sampling only a small fraction of the Fourier transform of the spin density, and using a prolate <br\/>wavelet filter to approximately obtain an integral representing the total activity of the difference in susceptibility between task and pre-task, over various regions of interest in the brain at successive time-points. The cost for this is a decrease in spatial resolution. A nearly optimal trajectory will be used for sampling a small cube in three-dimensional k-space about the origin. This sampling region is also nearly optimal. The use of a, again nearly optimal, prolate filter will provide a low spatial but high temporal resolution image of the deoxy-hemoglobin density. An aim of the project is to find one or more consistent locations in the brain where oxygen is consumed during <br\/>higher level processing by the brain of the image in the primary visual region. This region would then be scanned in a two-dimensional <br\/>experiment where a slice plane is chosen to go through the region which would then give convincing demonstration of feasibility of the <br\/>proposed methods.<br\/>The proposal focuses on developing statistical methods and related theory for fast functional magnetic resonance imaging, to sharply <br\/>improve the time-resolution of present techniques. Fast fMRI is expected to have profound and far-reaching consequences in the <br\/>understanding of brain function, a problem of central scientific interest at the present time.","title":"Statistical Methods in Fast Functional MRI","awardID":"0504387","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1269","name":"STATISTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"T331","name":"NSA-DYNAMIC LINK ANALYSIS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"T306","name":"NSA-DYNAMIC LINK ANALYSIS FROM"}}],"PIcoPI":["513597","480973"],"PO":["565309"]},"104507":{"abstract":"ABSTRACT<br\/>TITLE: NER: EXPERIMENTAL REALIZATION OF PROTECTED QUBITS<br\/>PROJECT ID: 508129<br\/>PI NAME: M. GERSHENSON,<br\/>INSTUTUTION: RUTGERS UNIVERSITY. NJ<br\/><br\/>The recent discovery of several powerful quantum algorithms indicates that a \"quantum-mechanical\" computer might have an enormous advantage over its conventional classical counterpart in solving many difficult problems. However, for successful implementation of quantum computing, the quantum computer elements (a.k.a. qubits) should be sufficiently decoupled (\"protected\") from the environment. The ultimate goal of this proposal is to develop the first scalable element for quantum computation that would be sufficiently protection from environmental noise. The proposed approach to the solid-state realization of a \"die-hard\" Schrodinger cat is based on nanoscale Josephson junctions protected by nontrivial symmetries. Development of a fabrication-friendly design of protected superconducting qubits and fabrication of this novel nanodevice will be crucial for successful realization of quantum computation. The proposed experiments will provide a testing ground for the physical realization of ideas of symmetry-based protection developed in string theory, with the aim of applying them towards computer science. Implementation of the proposed research will contribute to better understanding of the decoherence processes in numerous quantum nanodevices operating at low temperatures. The multi-component Educational and Outreach Program, an essential part of the project, is designed to nurture an appreciation for nanoscience, to develop innovative curricula and training modules in nanoscience and nano-engineering, and to disseminate the curricula with a view of creating a more scientifically literate general public.","title":"NER: Experimental Realization of Protected Qubits","awardID":"0508129","effectiveDate":"2005-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1676","name":"NANOSCALE:  EXPLORATORY RSRCH"}}],"PIcoPI":[275928,"445659"],"PO":["562984"]},"105959":{"abstract":"Title: Collaborative Research: Multiuser Wireless Security<br\/>Abstract:<br\/><br\/>Wireless communications has had a profound effect on various aspects of our society and our way of life; from how we do business, to how we educate; from how we handle emergency situations, to how we care for patients and elderly. The promise of being able to exchange any amount of information, anytime, anywhere is prompting more and more subscribers to rely solely on their wireless devices for communicating sensitive nformation. As a result, offering a wide range of wireless services to more subscribers while preserving the security of information is becoming essential. Wireless system design efforts so far has aimed solely on providing high capacity. This research nvestigates the fundamental design principles of high capacity ireless systems that ensure secure information delivery.<br\/><br\/>This research includes the development of a comprehensive framework for the design of the multiuser physical layer that aims at achieving high capacity and secure transmissions for all users. The research addresses the existence of intruders that aim to disrupt communication by creating intentional interference, i.e., jammers; intruders that intercept the communication and attempt to decode the information of the users, i.e., eavesdroppers; and intruders that listen in on the communication and then create intentional interference accordingly, i.e., correlated jammers. Assuming a variety of levels at which the intruders are capable to harm the multiuser system, and using these scenarios as additional design constraints in the physical layer design, this research seeks to establish the performance limits of a variety of multiuser systems in nonfading and fading channels in the presence of security threats; the ways in which the system entities, i.e., transmitters and receivers, can collectively cope with security threats at the physical layer; and the jointly optimum transmit strategies for the users that render the intruders ineffective. The investigators study a variety of multiuser channels including scalar, waveform and vector multiple access and broadcast scenarios, as well as relay and cooperative communication scenarios and identify the fundamental design trade-offs for capacity versus security for a variety of wireless networks.","title":"Collaborative Research: Multiuser Wireless Security","awardID":"0514846","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["548269"],"PO":["564898"]},"100944":{"abstract":"The next wave of information systems to be deployed over Wide-Area Networks will feature data sources residing on Internet-enabled handheld devices, mobile laptops, and other small devices. These hosts will manage important data sets (e.g., sensor readings, medical information, financial data), and change network location as their owners move with them across geographical regions. The goal of this project is the design, implementation, and evaluation of NetTraveler, a database middleware infrastructure that locates, dynamically federates and integrates data sources residing on mobile devices and enterprise servers. The core innovations in NetTraveler are its software abstractions, protocols, and algorithms that enable NetTraveler to efficiently run queries over sites that might be mobile hosts. NetTraveler provides a context-aware query processing framework to schedule query operators based on the nature of hosts, as specified in a dynamic profile submitted by each host. NetTraveler prevents loss of query processing effort due to intermittent connectivity by means of algorithms that either cache query results, or save minimal state information necessary to resume query execution once a previously disconnected source reconnects again. Regarding broader impacts, this project combines research and educational activities to provide Hispanic graduate and undergraduate students at UPRM with the opportunity to become immerse in database research. New courses will be developed in middleware systems, and application development for mobile hosts. Expected outcomes of these activities include the source code for NetTraveler, course materials, tutorials, seminars, and research papers. All materials for these activities are freely available from the project Web site (http:\/\/www.ece.uprm.edu\/~manuel\/nettraveler\/).","title":"CAREER: NetTraveler: A Database Middleware System for Ubiquitous Data Access on Wide-Area Networks","awardID":"0448184","effectiveDate":"2005-07-01","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[266269],"PO":["469867"]},"100966":{"abstract":"From the very early years of computer science it has been clear that<br\/>computational problems vary greatly in difficulty, ranging from easy to<br\/>moderately hard to intractably hard to outright unsolvable. <br\/>Computational complexity theory is the branch of computer science that<br\/>seeks to rigorously and mathematically study the inherent difficulty of<br\/>computational problems, measured as the amount of a certain resource (or<br\/>resources) required for their solution, and organize problems into<br\/>classes based on their inherent difficulty. Examples of relevant<br\/>resources include processing time, storage space and inter-computer<br\/>communication. Communication complexity involves the latter<br\/>resource and asks how many bits must be communicated between different<br\/>computers to solve a problem whose input is split between these<br\/>computers. <br\/><br\/>There is plenty of intrinsic interest in communication complexity,<br\/>because of the large variety of Internet-driven applications where it is<br\/>relevant. In addition, over the last decade and a half, communication<br\/>complexity has been emerging as an area that unites many seemingly<br\/>disparate areas of theoretical computer science some of which do not<br\/>directly involve communication; examples are circuit complexity, query<br\/>complexity, quantum computing, algorithmic game theory, optimization and<br\/>distributed computing. <br\/><br\/>In recent years, information theory has proved itself to be a powerful<br\/>tool in the study of communication complexity, and therefore in the<br\/>various other areas of complexity theory that communication impacts. I<br\/>have already explored this theme in my recent work and was an author of<br\/>a recent paper that formally introduced the concept of information<br\/>complexity and described its relation to communication complexity.<br\/>Other subsequent work of mine has applied information theoretic<br\/>techniques to prove theorems about communication complexity and applied<br\/>these theorems in turn to explore the inherent difficulty of problems<br\/>not directly involving communication.<br\/><br\/>The main goal of this project is to continue this line of work in two<br\/>ways. First, I shall continue to develop and extend what I call the<br\/>information complexity framework and attempt to apply it to models of<br\/>computation where it hasn't yet been applied successfully. A noteworthy<br\/>subgoal of the first goal will be to use this framework to understand<br\/>the limitations of quantum computation. Second, I shall seek to study<br\/>certain concrete problems in communication complexity that have known<br\/>connections to other areas of complexity theory. Two specific areas I<br\/>shall focus on are (1) the complexity of data stream computations and<br\/>(2) circuit complexity, with an emphasis on the power of shallow<br\/>circuits.","title":"CAREER: Information Theoretic Methods in Communication and Computational Complexity","awardID":"0448277","effectiveDate":"2005-07-01","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["538173"],"PO":["565157"]},"110525":{"abstract":"This is a one year interagency funded SGER award.","title":"SGER: Limitations of Anonymity and Knowledge Discovery","awardID":"0540989","effectiveDate":"2005-07-15","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"T028","name":"CIA-KDD WORKING GROUP"}}],"PIcoPI":["541892"],"PO":["565136"]},"100867":{"abstract":"Sensor networks are ideal candidates for a wide range of applications such as critical infrastructure protection. It is necessary to guarantee the trustworthiness and resilience of sensor networks as well as the sensing applications, especially when the failure of these applications may result in catastrophic events with impacts affecting safety, security, the economy and society at large. The objective of this project is to develop practical techniques for building trustworthy and resilient sensor networks as well as instructional materials that facilitate the education of these techniques. The research activities are focused on practical broadcast authentication, trustworthy and resilient clock synchronization, and light-weight and collaborative intrusion detection in sensor networks. To handle the unique challenges (e.g., resource constraints, threat of node compromises) in sensor network security, this project seeks effective integration of cryptographic","title":"CAREER: Towards Trustworthy and Resilient Sensor Networks","awardID":"0447761","effectiveDate":"2005-07-01","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}}],"PIcoPI":["553866"],"PO":["557315"]},"100999":{"abstract":"Increasingly important social processes are migrating to an increasingly decentralized, inter-organizational information infrastructure. This work investigates the resulting trust issues. Are there grounds for one entity, Alice, to trust a remote entity Bob, with a particular task? When such grounds exist, how can this information be securely transmitted across the boundaries that separate Bob from Alice? Do Alice's applications and tools enable her to make the appropriate trust judgment?<br\/><br\/>This research begins with the technology of public key infrastructure (PKI), because it can communicate assertions without sharing secrets, and trusted computing platforms, because of they can potentially create islands of trust within a distributed infrastructure. The research will catalog archetypical examples of mismatches between current technology and human trust requirements, designe a series of experimental systems that seek to eliminate these mismatches, build the strongest design candidates, validate these designs in real pilot systems, and formalize the resulting design criteria. By creating a framework that ties PKI and trusted computing to the core problem of human trust, this project will advance understanding of the technology necessary to support trust and hasten the realization of that trustworthiness in our society's information infrastructure.","title":"CAREER: Effective Trust Judgments Across Boundaries","awardID":"0448499","effectiveDate":"2005-07-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}}],"PIcoPI":["406591"],"PO":["562302"]},"105927":{"abstract":"Computational geometry algorithms traditionally have been designed for centralized data settings, where the data are available to algorithms locally and in a persistent form. Yet, a growing number of emerging applications no longer fit such a conventional data model. For instance, in sensor networks and location-aware mobile computing, data is geographically distributed, and in high-volume data monitoring, such as analysis of Internet traffic or web clicks, data must be processed as a stream, without being stored. Motivated by these technological trends, the investigator develops distributed algorithms for geometric computing.<br\/><br\/>Designing mathematically grounded geometric algorithms for distributed or streaming data is challenging because the algorithms must operate with limited computational resources. In particular, nodes in a sensor network have very limited battery power, memory, and bandwidth, and so collecting data from the network requires nodes to construct approximations of their spatial measurements. Similarly, data stream algorithms must process the data in a single pass and compute synopsis data structures that summarize important features of the data. This research develops novel geometric algorithms and data structures that deal with insufficient resources (bandwidth, memory, power) in a graceful manner, so that the solution quality adapts to the resources available --- the better the resources, higher the solution quality. In particular, the research proposes such resource-adaptive methods to discover epsilon-cuts in sensor networks, compute bounded-memory approximations of sensor observations, discover hierarchical heavy hitters in multi-dimensional data streams, and shape-preserving clustering methods for geometric streams. Many challenges of national importance concern the protection of our physical as well as cyber infrastructures. Being able to monitor these systems remotely and analyze their data with flexible, resource-adaptive, and programmable software tools is critically important. Because many of these systems deal with distributed or streaming data, this research has direct relevance to those applications.","title":"Geometric Computing over Distributed and Streaming Data","awardID":"0514738","effectiveDate":"2005-07-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["536698"],"PO":["565157"]},"105949":{"abstract":"Title: Collaborative Research: Multiuser Wireless Security<br\/>Abstract:<br\/><br\/>Wireless communications has had a profound effect on various aspects of our society and our way of life; from how we do business, to how we educate; from how we handle emergency situations, to how we care for patients and elderly. The promise of being able to exchange any amount of information, anytime, anywhere is prompting more and more subscribers to rely solely on their wireless devices for communicating sensitive nformation. As a result, offering a wide range of wireless services to more subscribers while preserving the security of information is becoming essential. Wireless system design efforts so far has aimed solely on providing high capacity. This research nvestigates the fundamental design principles of high capacity ireless systems that ensure secure information delivery.<br\/><br\/>This research includes the development of a comprehensive framework for the design of the multiuser physical layer that aims at achieving high capacity and secure transmissions for all users. The research addresses the existence of intruders that aim to disrupt communication by creating intentional interference, i.e., jammers; intruders that intercept the communication and attempt to decode the information of the users, i.e., eavesdroppers; and intruders that listen in on the communication and then create intentional interference accordingly, i.e., correlated jammers. Assuming a variety of levels at which the intruders are capable to harm the multiuser system, and using these scenarios as additional design constraints in the physical layer design, this research seeks to establish the performance limits of a variety of multiuser systems in nonfading and fading channels in the presence of security threats; the ways in which the system entities, i.e., transmitters and receivers, can collectively cope with security threats at the physical layer; and the jointly optimum transmit strategies for the users that render the intruders ineffective. The investigators study a variety of multiuser channels including scalar, waveform and vector multiple access and broadcast scenarios, as well as relay and cooperative communication scenarios and identify the fundamental design trade-offs for capacity versus security for a variety of wireless networks.","title":"Collaborative Research: Multiuser Wireless Security","awardID":"0514813","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["550356"],"PO":["564898"]},"111989":{"abstract":"The goal of this research is to develop fundamental theory and algorithms for efficient and reliable data broadcast scheduling. Broadcast is an effective and efficient means for information dissemination over many networks. It is becoming more and more important with the increasing deployment of wireless local and wide area networks. Scheduling of data items is crucial in maximizing the efficiency of broadcast. This research will address a few fundamental and important issues that have not been well studied, i.e. broadcast schedules that can combat channel errors effectively, theory and schedule design for scheduling over multiple channels, and, scheduling for streaming data. This project presents an integration of theory and techniques of error correcting codes to the broadcast scheduling problem, which no previous works on this topic have applied. This research will exploit, and contribute towards, theoretical studies in information and coding theory, probability analysis and combinatorics.","title":"NR: Collaborative Research: Scheduling for Efficient and Reliable Data Broadcast","awardID":"0549202","effectiveDate":"2005-07-29","expirationDate":"2007-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4097","name":"NETWORKING RESEARCH"}}],"PIcoPI":[296397],"PO":["565090"]},"102947":{"abstract":"New techniques for the control of multi-agent systems in <br\/>uncertain environments\"<br\/><br\/>ABSTRACT<br\/><br\/>The research develops new techniques for the control of multi-agent systems in uncertain environments. The problem area is of critical national and scientific importance, since modern engineered systems deployed for many critical applications are comprised of multiple agents who need to coordinate their actions to achieve a control objective in an uncertain environment. Such systems are used for many applications, including surveillance, environmental monitoring, inventory control, and control in hazardous environments, among many others, in fields such as transportation, medical diagnostics, civil engineering, communications networks, and defense systems.<br\/><br\/>The main scientific ideas being developed are (1) a new technique to tame the complexity of controlling multi-agent systems by a focus on aggregate behavior, i.e. not so much on which agent carries out a given action as on how many agents carry out a given action, and (2) a new technique to based on the use of common randomness between the agents to increase their ability to hedge against uncertainty in the environment.<br\/>The aggregation techniques to tame complexity that are being developed rely on mean-field methods, which have proven well-suited to exposing phase transitions in physical systems, but have so far not been systematically used for control. The phenomenon of symmetry breaking, which occurs when different portions of a homogeneous system must use different control rules to obtain optimal system wide performance, is of particular interest. The techniques to hedge against uncertainty that are being developed rely on game-theoretic models, as well as methods to extract common randomness from ambient noise and the randomized choices of other agents.","title":"New Techniques for the Control of Multi-Agent Systems in Uncertain Environments","awardID":"0500234","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1518","name":"CONTROL, NETWORKS, & COMP INTE"}}],"PIcoPI":["560233"],"PO":["432103"]},"104806":{"abstract":"The quest for high performance drives parallel scientific computing software design. Well over 60% of the high-performance computing (HPC) community writes programs using the MPI library; to gain performance, they are known to perform many manual optimizations. Even groups that generate MPI from high level descriptions ultimately seem to generate MPI code, due to its eminent portability. However, since performance does not port, manual tweaks are inevitable. This, together with the vastness and evolving nature of the MPI standard, and the innate complexity of concurrent programming introduces costly bugs.<br\/><br\/>Formal methods are enjoying an explosive growth precisely to help eliminate these kinds of bugs. Already they find applications in diverse areas ranging from formally verifying optimizing compiler transformations, formally debugging device driver codes, and solidifying industrial standards. The project will investigate and employ a number of complementary formal approaches to HPC software design: erect formal standards for MPI so that designers are properly educated, take advantage of the standards and write comprehensive MPI platform tests, extract finite-state models from MPI programs and analyze them automatically for deadlocks and race conditions, and will instrument the MPI-based program with correctness assertions that can be checked at run-time.<br\/><br\/>The project will advance the state of the art in formal methods by developing algorithms that take advantage of semantic properties of communication libraries. It will help advance the state of the art in parallel scientific programming by encouraging the use of formal assertions, and encouraging the use of formal analysis in lieu of brute-force execution.","title":"CSR-SMA: Toward Reliable and Efficient Message Passing Software Through Formal Analysis","awardID":"0509379","effectiveDate":"2005-07-01","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["561775","552996"],"PO":["535244"]},"105906":{"abstract":"Many problems in combinatorial optimization are NP-hard, and unlikely to have efficient algorithms to find the optimal solution. These include multitudes of problems that arise in practice in industry, including locating facilities to service customer demands, scheduling service personnel to make onsite repairs, dispatching vehicles, constructing low-cost networks with specific connectivity properties, routing wires on chips, and many, many others. The typical approaches for solving these problems are to design some heuristic that in practice provides solutions that are good enough, or, if enough time and resources are available and an optimal solution is sufficiently valuable, to design<br\/>an exhaustive search algorithm to find the optimal solution.<br\/><br\/>In order to mathematically ground the study of heuristics, computer scientists have considered approximation algorithms. These are efficient, polynomial-time algorithms that produce solutions that are provably close in value to the optimal solution. In particular, an _-approximation algorithm produces a solution whose value is within a factor of _ of the value of an optimal solution. The parameter _ is sometimes called the performance guarantee of the algorithm. <br\/><br\/>To prove that the algorithm produces a near-optimal solution, a bound on the optimal value must be used. This bound is often a quantity that can be computed efficiently on its own, and in many interesting cases is based on a linear programming relaxation of the problem. When a polynomial-time computable bound, such as a linear programming bound, is used to prove a strong performance guarantee, this also has implications for practitioners who solve problems to optimality using exhaustive search, since a strong bound is useful in quickly pruning the search space.<br\/><br\/>The goal of the proposal is to study some anomalies in the current state of knowledge of approximation algorithms, since as in other areas of science, consideration of anomalies leads most quickly to new advances. Several known approximation algorithms use as their bound quantities that are not polynomial-time computable; or, even if polynomial-time computable, are difficult to show apriori are bounds on the problem being considered. Included in the study are problems of locating facilities that can serve a bounded amount of customer demand (the capacitated facility location problem), designing low-cost networks (the Steiner tree problem), and the routing of vehicles to minimize the average time at which a vehicle arrives at a customer (the minimum<br\/>latency problem). The goal of the research is to show that these algorithms can be restated in terms of polynomial-time computable bounds, and, as often as possible, bounds involving linear programming relaxations.<br\/><br\/>Intellectual merit: Significant methodological innovations will likely be needed to resolve<br\/>these issues. Additionally, the PI expects to find simpler, better, more general, and more practical approximation algorithms as a result of this research. Furthermore, if polynomial-time computable bounds can be found for some of these problems, it will lead to better exhaustive search algorithms for finding the optimal solution for these problems. The PI is one of the leaders in the field of approximation algorithms and has an extensive track record of finding new methods and improved algorithms.<br\/><br\/>Broader impact: The PI will train of graduate researchers in the broader area of combinatorial optimization and in conducting original research in the area of algorithms. Since one of the goals of the proposal is the simplification of current results, the broad dissemination of the results of the research through conference and journal publication should increase understanding in the area. Potentially the results of the research will be part of a graduate class that the PI has taught on several occasions on the subject of approximation algorithms; notes from previous versions of the class are publically available and have been widely used. Finally, since many of the problems to be considered have practical importance, the improved bounds developed will affect the heuristics and exact optimization techniques used in practice.","title":"Resolving Anomalies in Approximation Algorithms","awardID":"0514628","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["485172"],"PO":["562944"]},"102518":{"abstract":"For many industries, engineering design and manufacturing data needs to be preserved over 50-to-75 year lifespans. Traditional digital data management techniques are usually dependent on the proprietary formats of commercial software systems and cannot guarantee the readability and utility of data over long periods. Hence, while 3D CAD modeling has become indispensable, the engineering part print (i.e., the 2D drawing) still remains a principal method of design knowledge archival. The rich knowledge in 3D CAD about features, manufacturing processes and artifact behavior are simply lost in translation. The research team proposes to develop a representation and algorithmic techniques to archive 3D CAD objects. The approach is to augment shape-based representations with formal models of engineering semantics and domain-driven segmentation algorithms. The rationale is that low-level shape information is repres ntationally straightforward, and easily preserved; whereas native CAD\/CAM formats are proprietary and notoriously hard to preserve, even across incremental product versions. Engineering semantics will be based on emerging W3C, Semantic Web, ISO and NIST standards formalisms that will need to be extended to handle new datatypes specific to complex engineering artifacts. These will be mathematical logics described in the non-proprietary syntax of international standards, and thus also easier to preserve than proprietary representations. Lastly, techniques from databases will be extended to extract and populate these representations from existing design tools and engineering documentation.<br\/><br\/>This approach form the basis of a Digital Archive Toolkit for Engineering (DATE), a set of tools to be developed and tested in collaboration with the government\/industrial partner Honeywell FM&T. Working with data from the United States Department of Energy, the PIs will work with the agency-wide Advanced Design and Production Technologies Initiative (ADAPT) team to apply techniques developed in this research to indexing and storage of engineering data used at the Kansas City Plant (KCP). The objective is to extract data to create Digital Engineering Archives, enable answers to meaningful engineering queries on archives of 3D engineering knowledge, and support long-term engineering knowledge preservation.<br\/><br\/>Intellectual Merits<br\/><br\/>The relationships among shape and form, structure and function, and behavior and semantics are among the most fundamental questions studied by science and engineering. It is precisely these relationships that must be captured in Digital Engineering Archives. While these problems are large in scope, by focusing on the vital domain of discrete part manufacturing., the proposed project is poised to produce theoretical results, novel techniques, in addition to prototype systems. The aggregate output of the proposed project includes advancements in representation and retrieval algorithms, as well as contributions in basic computer science and engineering design and manufacturing. The scope of the research spans pattern recognition, knowledge representation, database semantics, as well as CAD\/CAM\/CAPP\/PDM\/PLM. <br\/><br\/>Broader Impacts<br\/><br\/>This research bridges the print information gap by developing a dynamic, self-describing and information-rich alternative to the part print to create Digital Engineering Archives. The availability of such a format will enable the creation of systems for archival, retrieval and reuse of engineering knowledge and activities for design teams operating throughout the product life cycle, which may be 10, 25, or 50 or more years. The proposed DATE system will have an immediate impact on the ability of industry to create data archives. Lastly, the team's collaboration with industrial and government partners position this project to have a significant positive effect on emerging NIST, W3C and ISO standards, and on commercial CAD\/CAM\/PDM\/PLM software systems.<br\/><br\/>Educational Impact<br\/><br\/> The project aims to create a novel experience for its graduate and undergraduate students, as well as for the investigators from industry, government and academia. Leveraging Drexel's cooperative education programs (both graduate and undergraduate), the investigators plan extensive personnel exchanges with DOE, NIST and other agencies. Lastly, the team will develop educational and tutorial materials to demonstrate to the engineering community how to apply computer and information science to create and maintain Digital Engineering Archives.","title":"Digital Engineering Archives","awardID":"0456001","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"T451","name":"LIB OF CONGRESS-DIGITAL ARCHIN"}}],"PIcoPI":["560676","366025"],"PO":["371077"]},"105917":{"abstract":"Data mining (aka Knowledge Discovery in Databases, KDD) is a procedure to extract previously unknown and potentially useful information or pattern from huge data sets. KDD is usually a multiphase process involving numerous steps such as data preparation, data preprocessing, feature selection, rule induction, knowledge evaluation and deployment etc. Many novel data mining and learning algorithms have been developed, though vigorously, under rather add hoc and vague concepts. These algorithms, in most cases, are individual creations of different researchers, without much common methodological and fundamental framework. In other words, great majority of work in data mining is focused on algorithm development while neglecting the studies of fundamental theoretical issues concerning data, inter-data relationships, and quality of the implicit information hidden in the data or data redundancies. Thus, it is not easy to fully understand and evaluate how individual phase influences each other and the impact of each phase on the whole knowledge discovery process. For further development and breakthroughs in data mining and learning algorithms, a deep examination of its foundation is necessary. The central goal of the proposed research is to develop a unified rough set based data mining framework to explore various fundamental issues of data mining and learning algorithms. It aims to present the analytical capabilities of the methodology of rough sets in the context of data mining methodologies, techniques and applications. It will provide a unified framework to help better understand the whole KDD process.<br\/><br\/>Intellectual merit: Rough set theory is particularly suited to reasoning about imprecise or incomplete data and discovering relationships in the data. The simplicity and mathematical clarity of rough set theory makes it attractive for both theoreticians and application-oriented researchers. The main advantage of rough set theory is that it does not require any preliminary or additional information about the data, such as probability in statistics, basic probability assignment in Dempster-Shafer theory or the value of membership in fuzzy set theory. Rough set theory constitutes a sound basis for KDD and can be used in different phases of the KDD process. In particular, the formal techniques of rough set theory lead to many novel and promising breakthrough methods and algorithms for attribute functional, or<br\/>partial functional dependencies, their discovery, analysis, and characterization, feature election, feature extraction, data reduction, decision rule generation, and pattern extraction (templates, association rules) etc., which are the fundamental issues of the KDD process. Rough set theory represents a new innovative approach and can lead to the development of new learning algorithms to create novel uses and breakthroughs of data mining techniques.<br\/><br\/>Broader impacts: The proposed collaborative project is interdisciplinary in nature. It will synthesize often-disparate work in data mining, rough set theory and high performance computing. The PIs' strong multidisciplinary research collaboration experience will lead to widespread awareness and impact of the proposed research to rough set, data mining and high performance computing community. It will design and develop a wide-range of novel data mining algorithms and methods including data reduction, rule induction and classification ensemble in one unified framework to better understand the whole KDD<br\/>process. These algorithms and methods will significantly extend the application scope of data mining techniques and rough set theory and will result in the improved understanding of issues involved in designing efficient and innovative data mining and learning algorithms and methods. The proposed research will integrate tightly with teaching activities, the research results will be developed into undergraduate and graduate courses and research projects. Part of this approach includes the development of new cross-disciplinary courses that bring together computer science and mathematics for the understanding of principle and methods of theoretical foundations of data mining and rough set theory. The integration will help with training students in the issues involved in the rough set theory, design and implementation of novel data mining methods and algorithms, high performance computing. The active participation of students will allow for significant exposure to the latest research in data<br\/>mining.","title":"High Performance Rough Sets Data Analysis in Data Mining","awardID":"0514679","effectiveDate":"2005-07-15","expirationDate":"2008-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["554592"],"PO":["409909"]},"105928":{"abstract":"ALGORITHMIC RIEMANNIAN GEOMETRY FOR A STATISTICAL<br\/>ANALYSIS OF IMAGES<br\/>Abstract<br\/><br\/>This project is concerned with the investigation of novel algorithmic representations of images and geometrical signal processing techniques for the automated analysis of image content. The investigators develop a new framework for an appearance-based analysis of imaged objects in terms of their shapes and textures using methods and tools derived from differential geometry and statistics. A statistical formulation is of the essence due to the large variability of shapes and textures frequently encountered in imagery of interest. The use of differential geometric methods in image processing is still incipient, but very promising, as solid evidence exists that such methodology is particularly well suited for the study of multidimensional, nonlinear features such as shapes and textures.<br\/><br\/>In recent years, the investigators have developed a statistical shape analysis program; shapes are viewed as elements of a shape space whose geometry is exploited for shape analysis. The investigators treat textures in a similar manner by creating a Riemannian manifold of textures and integrate both representations into a single shape-texture model for the algorithmic analysis of image content. Images are decomposed into their spectral components and local spectral histograms are treated as elements of an infinite-dimensional statistical manifold equipped with a geometric structure induced by non-parametric Fisher information. Differential geometric constructs are utilized to develop algorithms for: (i) statistical inferences and learning of shape-texture features; (ii) Bayesian detection and recognition of objects using shape-texture priors; (iii) dimensionality reduction techniques for efficient processing.","title":"Algorithmic Riemannian Geometry for a Statistical Analysis of Images","awardID":"0514743","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["538549","550508","561924"],"PO":["565157"]},"105719":{"abstract":"Mobile wireless communication is becoming the enabler of Internet access. Unfortunately, mobile networks are not equipped to provide the high-speed access that Internet users expect due to fundamental problems associated with wireless communication: scarce bandwidth and poor quality. To overcome these problems, this research develops a family of algorithms that allow the transmitter to respond to changes in the propagation channel. The major innovation in this work is the development of new methods for compressing information about the propagation channel. This information is sent from the receiver back to the transmitter to help the transmitter adjust the transmitted signal. <br\/><br\/>The approach to this research is to develop a framework for source coding with a new source: the wireless channel. This differs from traditional source coding in that the objective is to improve communication theoretic system performance as opposed to improving the fidelity of the reconstruction. The objectives of the research are to determine what channel state information should be quantized; develop algorithms for quantizing the essential parameters of the channel; derive suitable communication theoretic notions of fidelity of the quantization such as mutual information and bit error rate; characterize the tradeoff between feedback rate and network performance; and confront practical issues introduced by estimation error, errors and delay in the feedback channel, and implementation constraints.<br\/><br\/>The broader impacts of the work are expected in diverse areas including research in the form of new algorithms, theoretical results, and insights; industry through mobile network applications developed by industry partners; and education through better trained engineers, research experiences for minority students, mentoring, and enhanced learning worldwide thanks to publicly available courseware.","title":"Collaborative Research: Quantizing Wireless Channels","awardID":"0513916","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[278947],"PO":["564898"]},"110725":{"abstract":"Existing wireless networks provide dynamically varying resources with limited quality of service support for multimedia applications, which often results in unsatisfactory user experience. This research addresses these challenges by introducing a new communication paradigm that allows competing wireless stations to dynamically exchange communication and computation resources. This paradigm was inspired by a successful economics concept known as \"coopetition\", which suggests that a judicious mixture of competition and cooperation is advantageous in competitive environments. When applied to wireless multimedia systems, this research fundamentally changes the passive way stations currently adapt their transmission strategies to match available wireless and power resources, by enabling them to proactively influence the wireless systems dynamics through resource and information exchange. <br\/>To achieve this, the investigator models wireless stations as rational players competing for available wireless resources in a dynamic repeated game. Optimal resource exchange policies are determined for different application requirements, wireless conditions, multimedia content, and different levels of collaboration and advantages of forming coalitions among applications\/stations are explored. In the new proactive paradigm, the resource exchanges are made possible by adapting the transmission strategies of the participating stations. Currently, these strategies are optimized at the lower OSI layers without considering the multimedia traffic characteristics. This research develops a new cross-layer optimization framework that adapts to varying channel conditions and available resources, while explicitly considering the specific characteristics and requirements of wireless multimedia applications. Moreover, to enable different resource exchanges, user-centric scalable multimedia coding and streaming algorithms are investigated that consider the user experience and stations resources. This research considers both existing wireless networks and next-generation Spectrum Agile Radio networks in which stations can utilize multiple channels, thereby dynamically gathering additional resources to satisfy multimedia requirements.","title":"CAREER: New Paradigm for Wireless Multimedia Communication Systems with Resource and Information Exchanges","awardID":"0541867","effectiveDate":"2005-07-01","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["558132"],"PO":["564898"]},"101947":{"abstract":"This award supports purchase of two items of equipment: a 3D printer, and a laser cutter. A 3D printer is a device that operates much like a conventional printer, but the file sent to the printer describes a 3D object, and the printer actually produces as output a physical model of the 3D object. A laser cutter is a something like a high-tech band saw. Both will be used in four ways: (1) To support the investigators research in computational geometry, by making physical models of complex polyhedra, protein models, and origami-like folded objects; (2) To support the research of other faculty in the Sciences; (2) To support faculty research and student projects in the Smith College Engineering program; (3) To facilitate faculty and student projects in Art, Architecture, and Arts & Technology. The equipment will enable new research by faculty across the College, expose many undergraduate students to this technology via senior projects, and affect K-12 students through the PI's work on using folding and unfolding to teach a variety of topics at the 6th- through 12th-grade levels.","title":"CRI: 3D Printer and Laser Cutter","awardID":"0453208","effectiveDate":"2005-07-15","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["271511"],"PO":["564181"]},"110439":{"abstract":"The rapid and enormous improvements in sensing, control, high-performance computing, and communications have led to an explosion of potential applications of robotics, the study of machines with sensing, intelligence and mobility. These new applications present an exciting array of fundamental challenges that will shape both the research agenda and the technology to be developed in robotics. This project seeks funding to host a demonstration of significant accomplishments, which will be an opportunity to articulate the achievements of the community to scientific peers and the news media. It is timed to coincide with the presentation of a worldwide study partially sponsored by NSF, assessing the state-of-the-art in robotics science, technology and practice, internationally. The workshop will identify the significant accomplishments in robotics research and the potential for growth in this emerging industry.<br\/><br\/>The goal of this 2005 PI Demo Workshop is to bring together several PIs currently funded by the National Science Foundation In the area of robotics to:<br\/>1. Present the research issues that are fundamental for further progress; specify areas where major breakthroughs appear possible; and identify research initiatives and facilities needed.<br\/>2. Demonstrate current research results and significant accomplishments.<br\/>3. Provide an opportunity to NSF program officers, other foundations and funding agencies, and industry representatives to learn more about the current research efforts and successes of projects funded by NSF.<br\/><br\/>The workshop will create demonstrations which will archived in on-line as well as other tape media format. It will also articulate the challenges and achievements of the robotics field to the broader community, which spans high school students to the general public. The PI Demo Workshop will promote the benefits of robotics research in other sciences and the society in general.","title":"2005 PI Demo Workshop; September 16, 2005; Arlington, VA","awardID":"0540550","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["553286"],"PO":["335186"]},"104807":{"abstract":"The goal of this research project is to build an application<br\/>infrastructure and a suite of embedded system software that will<br\/>enable the rapid development of software control and monitoring<br\/>applications, such as heating, ventilation, and air conditioning<br\/>systems (HVAC), on low-power, lossy wireless sensor-actuator<br\/>networks. Such user-preference driven control applications require<br\/>the ability to adapt to noise, loss, and failures in a large-scale<br\/>distributed network, and demand a high degree of automation in<br\/>acquiring user preferences, making control decisions and<br\/>disseminating those decisions to the appropriate actuators.<br\/><br\/>The Sensor Control System (SCS) being developed provides a high-<br\/>level platform that allows users to express control application<br\/>requirements at the granularity of the entire network, enabling users<br\/>to focus on the requirements of their deployments by abstracting away<br\/>the low-level sensor network details. SCS combines techniques from<br\/>machine learning and statistics with high-level abstractions inspired<br\/>by work in software systems and databases, allowing for<br\/>mathematically-sound decision making despite loss and uncertainty<br\/>that is inherent in such systems. The project will demonstrate the approach on<br\/>a highly instrumented office environment. This research has the potential to<br\/>impact industry through the development of a new class of wireless<br\/>control systems, and academia through the infusion of cross-<br\/>disciplinary ideas into a variety of sub-fields of EECS. The<br\/>datasets, code and the sensor-actuator deployments developed during<br\/>the course of this research will be used in a range of educational<br\/>initiatives.","title":"CSR-EHS: Collaborative Research: A General, Efficient and Robust Platform for Enabling Control Applications in Sensor Networks","awardID":"0509383","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["549916"],"PO":["561889"]},"102508":{"abstract":"Intellectual Merit<br\/><br\/>Digital preservation stands as one of the grand challenges of the early 21st Century. National libraries and archives from around the world are grappling with the preservation of digital assets. Funding agencies worldwide recognize the preservation imperative and are funding a wide array of research and development projects. As digital libraries and digitization initiatives have grown over the past decade, an increasing number of researchers, information professionals, and policy makers have addressed the issues around creating, maintaining, and, most recently, preserving digital objects. Although most of the preservation research to date has centered on text and still images, video is becoming a more common medium of expression for science and education and offers unique preservation challenges. This project builds on earlier work with digital video files and their surrogates (http:\/\/www.open-video.org), seeking ways in which to preserve a video work's context and highlighting its essence, thus making it more understandable and accessible to future generations. Long-term provision of contextualized access that makes digital objects understandable over time is essential to longterm preservation. This project will focus on developing a preservation framework for digital video context that can be used by archivists to make preservation decisions and guide the development of finding aids.The work will include a demonstration of this framework by applying it to two important digital video collections: the complete series of NASA broadcast educational videos and the complete set of juried ACM SIGCHI videos presented at annual conferences from 1983 to the present. The framework and demonstrable examples using the test collections will be made available to inform video archival decisions in the immediate years ahead. <br\/><br\/>Broader Impacts<br\/><br\/>This work will have several important outcomes for a variety of stakeholders. First, it will address the important context aspect of digital preservation on both theoretical and practical fronts. This should improve archival decision making and finding aid creation and suggest ways to leverage technology further to make them more efficient and effective. Second, this work will define preservation parameters for digital video, an increasingly important medium that has to date received little attention. Third, this work will have high payoff by focusing attention on two important and substantial collections of video content. These collections are already used in analog or broadcast forms by millions of students and teachers (NASA) and in analog or digital local forms by thousands of human-computer interaction students and faculty. The NASA videos in particular were created to encourage women and minority participation in science and broader and more effective availability will enhance this mission.. Fourth, this project will bring together diverse communities of practice ranging from academics in multiple disciplines to publishers to educational technologists. The synergies such crossdisciplinary collaborations offer will not only result in a more robust framework for preservation but will also produce better understanding of digital preservation issues across these different communities. Finally, this project will have substantial impact because it is tuned to the requirements of a new and innovative funding model.","title":"Preserving Video Objects and Context: A Demonstration Project","awardID":"0455970","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}}],"PIcoPI":["531548","279365",270293],"PO":["371077"]},"104818":{"abstract":"It is well-known that most applications do not exploit the full performance potential of computer hardware. A promising new approach for addressing some of these problems is provided by library generators. Although all library generators have much in common, there is currently little code reuse in their implementation. This project will develop an infrastructure, called Proteus to generate highly optimized libraries for four important problem domains: dense linear algebra, sorting, data mining, and network packet processing. The platforms of interst for these environments include sequential processors with deep memory hierarchies, multi-core systems that provide small-scale shared-memory parallelism, and custom network processors.<br\/><br\/>Research advances on several fronts are required to make Proteus a reality: a simple domain language for linear algebra in the proposal and appropriate DSL's for data mining and network packet processing. Once domain-specific optimizations are done, the programs will be translated into a lower-level language called the X language, to be developed under the project, that will represent in a compact way the search space as well as the strategy for adapting the code to the hardware. Proteus will contain tools for gathering information about the target machine, for guiding the selection process either statically or dynamically, for implementing domain-independent optimizations of the X code, and for generating C code as the final output.<br\/><br\/>The Proteus infrastructure will enable rapid prototyping of library generators for different domains. The libraries that will beproduced in the four different domains will be useful and interesting in their own right. In addition, tools will be developed for gathering information about the target machines and the computing environment, can be used by themselves in other projects.","title":"CSR-AES: Collaborative Research: Library Generators for Advanced Execution Systems","awardID":"0509432","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[276726,"563535","533380","297854","550505"],"PO":["493916"]},"100947":{"abstract":"ABSTRACT:<br\/>Project ID: CCF-0448189 <br\/>PI: Igor Markov<br\/>Title: CAREER: Spatial Optimization of Computing Systems <br\/>Instritution: University of Michigan Ann Arbor <br\/><br\/><br\/>Consumer electronics and high-end computing applications of the future demand improved performance, better energy-efficiency and lower cost. A number of design styles have emerged to meet related tradeoffs, including such recent advances as structured ASICs. However, a fundamental change is upon us with the impending slow-down of Moore's law in 10-15 years-the size and performance of individual transistors are not going to improve as fast as today, and competitive semiconductor products will have to use spatial resources on a chip more efficiently. Such improvements will require synergies between computer and electrical engineering, as well as large-scale mathematical and combinatorial optimization. The challenges to interconnect scaling that arise in semiconductor technology nodes below 180nm give us a glimpse of problems to come. First, interconnect delays do not improve as fast as gate delays. Therefore, in chips that started shipping two years ago, critical interconnect delay exceeds gate delay. Moreover, today interconnects occupy a much larger volume on a chip than gates, and therefore gates must be spaced further apart to facilitate routing. By mitigating capacitance between adjacent wires, \"unused space\" improves power dissipation and signal delay, while decreasing signal noise. These global effects have not been expected in the late 1990s and devalue, to a large extent, the ongoing miniaturization of CMOS transistors. In technology nodes below 65nm, wires become so slow relative to gates that they must be heavily buffered during layout, making buffers most heavily used gates. In other words, most of the gate-area is used for communication rather than for computation. Since buffer locations and densities are not known in advance, additional unused space must be left throughout the chip, and interconnect becomes even longer.<br\/><br\/>The scaling effects outlined above already require a rethinking of methodologies for designing circuits and systems. Logic circuits must be synthesized so as to minimize communication and interconnect rather than gate area. At the system level, global communication and global wires must be minimized. Latency can be hidden by deep pipelining, non-traditional signaling strategies and such fundamental new concepts as networks on a chip. An additional thrust of this proposal is to carry over spatial planning to much earlier stages of chip and system design than is common today. The project will develop new software tools for optimizing shapes and relative locations of large modules, and inter-module communication at the system level. Spatial planning additionally requires attention to large fixed-shape modules, e.g., embedded memories and design Intellectual Property, that are difficult to pack and may require alignment. The more futuristic part of the research will consider spatial planning and physical optimization in the context of carbon-based nano-devices, quantum dots and quantum circuits. These technologies employ radically different types of interconnect and imply different design constraints and optimization.","title":"CAREER: Spatial Optimization of Computing Systems","awardID":"0448189","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["508348"],"PO":["562984"]},"109390":{"abstract":"Facilitated and encouraged by new understanding of brain function, by the advent of powerful low-cost computers, and by appreciation of the needs and potentials of people with disabilities, the pace and volume of brain-computer interface (BCI) research have grown rapidly in recent years. But effective BCI research requires highly interdisciplinary interactions involving neuroscientists, clinical neurologists, psychologists, systems and rehabilitation engineers, applied mathematicians, computer scientists, and clinical rehabilitation specialists. Because no standard venue existed to bring these groups together to share results, to exchange ideas, and to develop the requisite close and productive collaborations the NIH sponsored, and the Wadsworth Center in Albany NY organized, the first two International BCI Meetings in 1999 and 2002. The impact of these meetings, which brought together researchers from all over the world and included all relevant disciplines, is indicated by the striking fact that more than half of all the BCI research articles ever published have appeared in the last two years and that three quarters of these articles include among their authors at least one person who participated in one or both of these two meetings.<br\/><br\/>The Third International BCI Meeting, to be held June 14-19, 2005, in Rensselaerville NY with core funding provided by NIH, will continue and augment the impact of the first two BCI Meetings by providing an intensive four and a half day program that once again fosters interdisciplinary interactions among researchers in this field while focusing on issues critical to the current state of BCI research. Specific topics of interest this year include: advantages and disadvantages for BCI use of different brain signals and signal acquisition methods; selection of appropriate signal processing methods for extracting features from the brain signals, and translating these features into device commands; problems that must be solved for clinical use of BCIs and for the realization of applications of most value to users; and the continued development of powerful, flexible, and convenient software\/hardware systems to support both laboratory research and clinical applications. As in the first two BCI Meetings, a central objective of this year's event is to promote the education and development of young researchers through participation of numerous graduate students and postdoctoral fellows. The organizers expect about 150 participants, of whom approximately 75 will be students. The PI is especially eager to encourage participation by women and minority researchers, as well as researchers with disabilities. NSF support will be used to fund participation by about 30 of these students, and will play a key role in nurturing these young researchers at a critical stage in their professional development.<br\/><br\/>Broader Impacts: BCIs translate signals recorded from the brain into useful outputs such as control of the movement of a computer cursor, selection of letters or icons on a computer screen, or even operation of a neuroprosthesis. BCI technology thus is a potentially powerful new communication and control option for people with severe motor disabilities or disorders such as amyotrophic lateral sclerosis (ALS), brainstem stroke, cerebral palsy, and spinal cord injury, who may have little or even no muscle control and therefore no means of communication with the external world. BCIs can provide communication and control technology that does not depend on neuromuscular output, and would therefore be of tremendous practical value to these people.","title":"Third International Brain-Computer Interface Meeting","awardID":"0534128","effectiveDate":"2005-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":["353970"],"PO":["565227"]},"95394":{"abstract":"The goal of this collaborative research project (0414981, W. Meng, SUNY Binghamton and 0414939, C. Yu, UIC) is to develop technologies for providing integrated access to Web databases. The approach consists of developing highly automated solutions to the following tasks: discovering Web databases from the Web, clustering them according to their application domains, integrating the search interfaces of the Web databases in the same domain into an integrated search interface, mapping each query submitted to an integrated interface to its underlying Web databases, extracting and annotating the search result records from the result pages returned from the local Web databases, and merging the results to form an integrated response for presentation to the user. Web services based interfaces of Web databases, if available, will be utilized. The evaluation of the developed algorithms is based on real Web databases from different domains. This research is expected to produce new algorithms, useful datasets, a software toolkit, and several operational Web database metasearch engines (including some for the Air Force Research Lab at Rome). The developed technology can be used in many applications including comparison shopping and collecting data from the deep Web. Research results will be incorporated into several courses the PIs teach and students will be recruited to participate in the research activities. Research results will be disseminated through published papers as well as a textbook on Web-based search technology. The project Web site (http:\/\/www.cs.binghamton.edu\/~meng\/DMSE.html) provides access to research results as well as datasets and demo systems.","title":"Collaborative Research: Achieving Information Integration of Web Databases Through the Construction of Metasearch Engines","awardID":"0414939","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}}],"PIcoPI":["389044"],"PO":["563751"]},"106060":{"abstract":"ABSTRACT<br\/>PROPOSAL NO: 0515392 <br\/>PI: Gary Bernstein <br\/>INST: University of Notre Dame <br\/>TITLE: Quilt Packaging: A New Paradigm for the Integration of Heterogeneous Communications Systems-in-Package <br\/><br\/>Chip-to-chip communications is a bottleneck for integrated systems throughput. Several companies have developed their own technology for increasing the signal bandwidth between ICs, including IBM, Sun Microsystems, and SiliconPipe. The PIs paln to developed a unique method of interconnecting ICs that will result in several important benefits compared with those under development by industry. <br\/><br\/>The approach is a micromachining technique resulting in hundreds or thousands of small metal nodules that protrude out from the sides of IC die, allowing ICs to be connected (soldered or welded) together to form a \"quilt\" of die, such that signals traverse between the ICs at very high speeds and with negligible power dissipation. The technique has been called Quilt Packaging (QP) in the proposal.<br\/><br\/>Besides the increase in speed and performance (predicted to be at least 150 GHz), power dissipation of systems can be reduced by eliminating most of the high-power pad drivers currently in use in ICs. This same reduction in circuitry will decrease the chip real-estate, resulting in lower-cost ICs and higher yields. Additionally, since several QP-connected ICs will reside in one package, the total number of IC packages will be reduced resulting in lower system cost, smaller size, and lower weight. These attributes may have an important impact on military, space, and commercial hand-held systems. One more important feature of Quilt Packaging is the use of heterogeneous materials as panels in the quilt. This may lead to entirely new systems-in-package for optical, RF, or digital communications applications. Additionally, new<br\/>architectures not possible prior to the advent of Quilt Packaging will be made possible. <br\/><br\/>The PIs have already demonstrated a simple QP process, and have made die with protruding nodules. They are currently in the process of designing microwave test beds to demonstrate the predicted high-frequency characteristics. This proposal requests funds to make further advances in the process, perform advanced high-frequency measurements, and demonstrate real silicon ICs with quarter-micron ring oscillators that incorporate QP nodules such that the signals jump between two connected ICs between each stage.","title":"Quilt Packaging: A New Paradigm for the Integration of Heterogeneous Communications Systems-in-Package [UND_FY05_009]","awardID":"0515392","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["557409","489796","512755"],"PO":["562984"]},"105281":{"abstract":"Severe motor disabilities significantly impact the quality of life for millions of people worldwide. The most profound motor disability, so-called locked-in syndrome, describes people who are completely paralyzed and unable to speak - they are intelligent and alert, but unable to communicate even their most basic needs. Several technologies, based on detecting minute electrical changes in brain signals, have been assessed for whether they can provide a channel of communication for people with locked-in syndrome; although the results from these studies are encouraging, the interfaces are slow, highly error prone, and often require weeks or months of training before control is achieved. Functional Near-Infrared (fNIR) imaging is a new and promising brain-imaging technology that measures small changes in blood volume and oxygenation in the brain. The technology has been explored for augmented cognition and for diagnosis, but has not been investigated for its control potential. Initial studies in able-bodied and locked-in subjects suggest that fNIR control is more accurate, easier to activate, and does not require any training. The overall goal of this research is to fully characterize and test fNIR imaging for control application. To this end, the PIs will first conduct a comprehensive study to determine optimal fNIR activation methods (e.g., determination of mental tasks that can be easily performed and that result in detectable brain activations). The product of this study will be a screening protocol that can be used to determine the most controllable brain area and device configuration for an individual user. The PIs will also work to improve fNIR imaging methods, by conducting offline and online studies to determine optimal sensitivity, cortical depth, filters, and signal processing algorithms for fNIR control. The product of this study will be an fNIR imaging device that is as accurate, sensitive, and robust as possible, as well as a set of heuristics for optimally configuring the device for a particular user. Finally, the PIs will demonstrate fNIR control in real-world applications by determining optimal mappings of fNIR signals to traditional assistive technology control interfaces such as scanning or logical interfaces, cursor movement, and direct selection. The results from all of these studies will be validated by combining and incorporating the findings into a comprehensive test of the fNIR system, by implementing a system for in-home use to control devices such as light switches, a television, and an MP3 player, which will be tested with five locked-in subjects.<br\/><br\/>Broader Impacts: For people with severe motor disabilities, the implications of researching and improving access to assistive technologies are profound. This research will make a significant contribution to the area of brain-computer interfaces by introducing a new, unexplored brain imaging method for control. It will also add to the body of knowledge for assistive technology and human-computer interfaces, by establishing protocols and mappings between an fNIR device and control interfaces. There are many people with less profound motor impairments (e.g., palsy), who might also be helped by an fNIR input device. Further research in fNIR control could lead to control of prosthetics that could restore movement in paralyzed limbs. Additionally, interface strategies for low-bandwidth, high error rate contexts may have significant application in other domains such as mobile and wearable computing systems and hands-free device operation.","title":"Collaborative Proposal: Functional Near Infrared Imaging for Communication and Control","awardID":"0512003","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":["334091"],"PO":["565227"]},"109560":{"abstract":"The proposed research builds on results obtained from NSF grant IIS-0308067. The goal of the research is to gain deeper understanding of how dynamic agility can be achieved in mobile machines interacting with people and operating in normal home and workplace environments. The work utilizes and further develops a novel dynamically stable rolling machine research platform referred to as a \"ballbot.\" In the spirit of minimalism, the machine balances and locomotes using only a single spherical wheel. This enables the machine to be tall and narrow, have a high center of mass, respond compliantly to nudges and shoves, and rapidly move in any direction. The purpose is to illuminate many of the issues surrounding the operation of agile machines in human environments; in particular the ability to traverse narrow, cluttered rooms and to function robustly in the presence of people. A key aspect of the project is to investigate the addition of a pair of dynamically significant two-degree-of-freedom arms. This will allow the development of control strategies for movement of the ballbot body by dynamic swinging of the arms. Automatic recovery from unplanned collisions with a spectrum of obstacle types as well as purposeful manipulation of the human environment, e.g., carrying unknown loads while balancing, and opening\/closing drawers and doors will be investigated using only intrinsic sensing abilities. Insights will be gained toward the development of agile motive platforms that can be combined with the research community's ongoing work in perception, navigation, and cognition, to yield truly capable intelligent mobile robots for use in physical contact with people. Such robots, if realizable and economically viable, would function as aids to elderly or disabled persons; provide guidance and assistance in public spaces; help with education and entertainment; perform domestic cleaning and housekeeping; or fetch and carry everyday objects","title":"Dynamically Stable Single Wheeled Robots in Human Environments","awardID":"0535183","effectiveDate":"2005-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["524687"],"PO":["429182"]},"106051":{"abstract":"Intellectual Merit: Algorithm design investigates the most efficient ways to solve specific computational problems, whereas complexity theory investigates relationships between general classes of computational problems. This proposal will investigate situations in which answering questions in complexity require us to understand specific algorithmic problems, and designing efficient algorithms require us to answer questions in complexity. Recently, there have been several results that expose the connection between complexity and algorithms. Examples include the connections between algebraic circuit lower bounds and polynomial-identity testing, better exponential algorithms for _ -SAT and circuit lower bounds, limitations of widely used backtracking algorithms and proof complexity, and constructions of error-correcting codes and constructions of pseudorandom generators. The proposed work will elaborate on these connections between combinatorial constructions, efficient algorithms, and complexity. It will use these connections to further our understanding of both algorithms and complexity. It will also seek new connections in the study of randomness in computing, proof complexity, the exact complexity of N P-complete problems, and formal models of algorithm paradigms.<br\/><br\/>This proposal will investigate issues where algorithm design is key to new results in complexity. Such issues include:<br\/>_<br\/>Which instances of optimization problems are the most intractable ones? Exactly how difficult are these problems?<br\/><br\/>What are good heuristic methods for solving optimization problems? When and how well do they work?<br\/><br\/>Can we distinguish between the powers of various general algorithmic methods (e.g., dynamic programming, greedy algorithms, back-tracking, local search, linear-programming relaxation) for solving these problems?<br\/><br\/>How much does randomness help in solving problems?<br\/><br\/><br\/>What is the relationship between the theory of sub exponential time algorithms and fixed parameter tractability? What other consequences would the existence of sub exponential algorithms have for complexity and cryptography?<br\/><br\/>While complete answers to most of these questions will probably not be possible in the foreseeable future, researchers in complexity, including the PIs, have made substantial progress on all of them. In particular, it is becoming apparent that these questions are so interrelated that it is impossible to address any one issue in isolation. Instead, success will require a multi-pronged effort that reveals the interconnections, and uses progress in one direction to obtain similar progress on others.<br\/><br\/>Broader Impact: Search and optimization are central to any computational issue in science and engineering. For example, finding the most probable folding of a protein, finding the smallest area of a VLSI chip, and finding the optimal way to classify data are all combinatorial optimization problems. The same algorithmic techniques are used to solve such problems in a wide variety of application domains. However, many of these techniques are heuristic in that factors that determine the performance are not well understood. This is more than academic issue since the lack of understanding prevents users from matching application areas to the most suitable algorithmic techniques. The work in this proposal is intended to further this understanding and hence may indirectly lead to improvements in many diverse application domains.<br\/>This proposal will also train graduate students to be top researchers and educators like many of our alumni.<br\/>1","title":"Duality between Complexity and Algorithms","awardID":"0515332","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["515841","515842"],"PO":["499399"]},"109472":{"abstract":"This award funds the US participation in a US\/EU Workshop on Optical Networking. (The European Union (EU) funds European participants). The Workshop participants will include experts from the optical networking research community in the US and the EU. The proposed Workshop, tentatively scheduled for June 27-28, 2005, in Brussels, will serve the purpose of: (a) determining the future research needs and opportunities in optical networking, and (b) exploring methods which can facilitate stronger research collaboration between US and EU researchers in optical networking.<br\/><br\/>The Workshop Co-Chair from the US is Professor Biswanath Mukherjee. Attendance at the Workshop will be by invitation only and is limited to 30 participants--15 from the US and another 15 from EU. A Technical Program Committee (TPC) will determine the Workshop's Technical Program<br\/><br\/>Technical topics to be covered at the Workshop will include (1) optical network architectures; (2) experimental optical systems research (hardware systems); and (3) optical network control and management (software systems). Special attention will be paid to collaborations in these areas. The Workshop will address forward-looking and high-impact research. The Workshop will also explore how NSF and EU agencies might facilitate strong collaborative research between US and EU researchers in optical networking. The Final Workshop Report will detail important research challenges, both fundamental and technological, that are likely to be at the forefront of this field for many years to come.","title":"US\/EU Workshop on Optical Networking, June 27-28, 2005, Brussels","awardID":"0534685","effectiveDate":"2005-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["518063"],"PO":["565090"]},"99401":{"abstract":"Increasingly complex scientific and engineering problems demand cyberinfrastructure (CI) that fully integrates the tools, services, systems and support to reach the next level of scientific discoveries in areas ranging from computing a biologically faithful and multiscale virtual lung to managing the full range of environmental, social and economic issues from environmental hazards, such as severe storms and earthquakes. NCSA's unique expertise with science applications and high end data and computational resources forms the foundation for responding to the new imperative for CI: the secure, integrated provision of high-end resources coupled by high-end services for large-scale computational and data oriented research that can extend to other national facilities such as SDSC, and ETF. This integrated environment will enable a broad range of new scientific and engineering discoveries by transforming the processes and tools researchers and communities use to achieve meaningful scientific insight. <br\/><br\/>To meet this goal, integration within this powerful new environment must be driven by scientific and engineering requirements. NCSA will accomplish this with a structured community engagement process and community partnerships that will lead to a stable and robust CI that includes prototyping, deployment, support, and documentation. This approach will provide significant economies of scale in building a national CI because it will enable clear definition and effective sharing of common services allowing extension of these capabilities to new communities. <br\/><br\/>The high-end computing and data resources at NCSA, tightly coupled through an integrated CI, will address resource demands that range from hundreds to thousands of simulations of moderate size to single hero simulations requiring weeks of dedicated computational resources. It will seamlessly provide the extensive data assimilation, storage and management, analysis, mining, and visualization services required to discover new scientific results. <br\/><br\/>The NCSA environment and support for applications will be augmented with the systems and expertise that will be available through the NCSA\/SDSC CyberInfrastructure Partnership. Expertise at each center will be directed at providing needed middleware, data and visualization services to the national community in a secure fashion. This will include traditional support for porting and optimization of codes. To provide additional strategic input into the partnership, a joint international Cyberinfrastucture Technology Watch Group and a Joint User Advisory Committee will be created. Extensive collaborations with ETF will further strengthen the national cyberinfrastructure. <br\/><br\/>NCSA is providing a deep and broad CI for the national community that will enable new discoveries and insights across a wide spectrum of science and engineering disciplines. The community engagement process will ensure that this balanced, services-based high end environment will meet individual user and community needs. Collaboration with SDSC, the ETF, and other partners will augment and enhance the cyberinfrastructure. Scientists and engineers will have extensive training and educational opportunities to learn about the latest CI developments through advanced e-learning technologies. <br\/><br\/>NCSA will extend its engagement from existing to new communities that will drive the development of the CI in support of scientific discovery. Relevant CI services will then be made available to other communities, and in the process, an expanded national CI base formed. Pro-active efforts to engage women, minorities and people with disabilities will be accelerated through partnerships with numerous professional organizations including the Minority Serving Institutions Consortium. NCSA staff and partners will actively disseminate information through the web, national conferences and workshops, museum and planetarium exhibits, and PBS shows highlighting the direct benefits of scientific discovery to the health, safety, and economic competitiveness of society.","title":"SCI: Cyberinfrastructure in Support of Research: A New Imperative","awardID":"0438712","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7368","name":"SCI TESTBEDS"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7475","name":"HEC CORE FACILITIES"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T482","name":"ARCHIVES-NARA'S ELEC RECORDS"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T929","name":"NARA-ERA"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"T275","name":"ARCHIVES-ELECTRONIC RECORDS"}}],"PIcoPI":["436368","426398","528809",262170,"467389","469704"],"PO":["525727"]},"106041":{"abstract":"The objectives of this project are (1) to understand the power and limits of quantum algorithms, especially for group theoretic problems, and (2) to find new ways of implementing powerful quantum computing primitives. The project is in theoretical computer science, combining computational complexity theory, algorithms, and quantum informatics. It aims to find new quantum algorithms for important problems, and study the possibilities of ultrafast quantum computation via small-depth quantum circuits. A number of group theoretic algorithms, such as Hidden Subgroup, Hidden Translation, Group Intersection, have quantum algorithms for only a small class of groups. Recent techniques may widen the class of problems amenable to quantum solution. Our goal is to apply these techniques and develop new techniques for exanding the class of groups with fast quantum algorithms. Long quantum computations, corresponding to deep quantum circuits, may be difficult to maintain due to decoherence, The project therefore also focuses on computations that can<br\/>be performed by very shallow circuits that are (of necessity) enhanced with highly nonlocal interactions (multibit gates) as primitives. One such multibit gate that has been shown to be quite powerful for shallow quantum circuits is the fanout gate, which copies a bit from one register to several other registers in one step. Recent work by the PI has shown that the fanout gate arises naturally from applying certain potentially feasible variants of well-understood spin-spin interactions between particles in a group. The project will study new ways in which fanout and other, similarly powerful, multibit gates may be ultimately implemented. The project will also continue work by the PI and others, studying the intrinsic strengths and weaknesses of shallow quantum circuits in general.<br\/>Intellectual merit of the proposed activity<br\/>Results of the project are rigorously mathematical in nature. They will be based both on the PI's expertise in computational complexity theory and on his contact with other computer scientists. The project will contribute to a basic understanding of feasible quantum computation.<br\/>Broader impacts resulting from the proposed activity<br\/>The project will promote collaborations between the University of South Carolina, an EPSCoR institution, and other institutions top-tier institutions, as well as collaboration among different departments at USC (Mathematics, Physics, and Computer Science). Graduate students will be intimately involved with the research, contributing to their Masters and Doctoral degrees. Results will be submitted to journals and conferences, and will be quickly disseminated on line in well-known archives such as<br\/>http:\/\/arxiv.org\/quant-ph and the Electronic Colloquium for Computational Complexity.","title":"Complexity and Quantum Algorithms","awardID":"0515269","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["409161"],"PO":["499399"]},"109374":{"abstract":"Object recognition in Computer Vision, though being a main processing step in many tasks of robotics, surveillance, and other fields of automation, is still an unsolved problem. The recent results in human visual perception strongly suggest that contour extraction is a key step to object recognition. A development of a contour-based system for object recognition is proposed. The first step of the new approach concentrates on extraction of object contours from edge images that correspond to contours as perceived by humans. Since the extraction of complete contours may not be possible (e.g., due to occlusion), extraction is focused on meaningful parts of contours. The proposed approach uses a mixture of bottom up and top down processing for edge grouping. After each step of bottom-up processing in a pyramid architecture, top-down evaluation is applied to select the most promising grouping constellations. A promising grouping constellation is defined using cognitively motivated constraints. In accord with the cognitive simplicity principle known from Gestalt psychology, partial shape similarity will be used as a primary building block of such constraints. In accord with the newest results in human perception, grouping of edges to parts of object contours and recognition of the parts using shape similarity play a key role in object recognition. This means that object recognition is possible if only part of a contour is constructed, and the construction of the whole contour is not necessary for recognition. In particular, object recognition works in the presence of occlusion and segmentation errors. <br\/><br\/> The proposed solution to the object recognition problem can make a significant step to improve the application scope of vision systems. The results of this work will be applicable to vision systems, large image databases, and video analysis systems. The proposed research to find interdependence and structural information among visual parts may lead to further understanding of human visual perception and cognition. The proposed research will provide an excellent resource for interdisciplinary work for graduate and undergraduate students in computer science and psychology. The PIs will offer courses and seminars on proposed research topics that will bring the state-of-the-art knowledge and technology to the classrooms.","title":"Collaborative Research: From Edge Pixels to Recognition of Parts of Object Contours","awardID":"0533968","effectiveDate":"2005-07-15","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["416761"],"PO":["564316"]},"102181":{"abstract":"Abstract<br\/>Proposal: CNS 0454416<br\/>PI: Sirisha Medidi<br\/>Institution: Washington State University <br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Secure Wireless Ad-hoc Networking and Sensor Networking Systems <br\/><br\/>Investigators at Washington State University will acquire laboratory equipment for sensor networking research. They will establish a laboratory with an-hoc network capabilities consisting of several wireless enabled computers and personal digital assistants (PDAs), and a sensor network of 50 motes. The PI's will conduct experimental research on topics such as: (i) detecting misbehavior in ad-hoc networks, (ii) identifying and isolating malicious nodes in ad-hoc networks, (iii) distinguishing congestion from misbehavior in ad-hoc networks, (iv) quality of service routing in ad-hoc networks, (v) secure routing in ad-hoc networks, (vi) energy-efficient information dissemination in sensor networks, and (vii) receiver initiated MAC for sensor networks. The infrastructure will allow them to do performance testing and validation of their research in a realistic testbed. Broader impacts of the research include addressing the growing applications of sensor networks in research and societal applications. The facilities will be available for education and course work, including courses that Washington State University offers jointly with the nearby University of Idaho in Moscow, ID.","title":"CRI: Secure Wireless Ad-hoc Networking and Sensor Networking","awardID":"0454416","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[269411,"381076"],"PO":["543507"]},"107692":{"abstract":"Building a quantum computer is widely considered one of the great experimental and intellectual<br\/>challenges for physics and computer science over the next few decades. This proposal<br\/>aims to jump-start this process by designing quantum devices whose inherit physics enforces self-correction of the quantum errors arising due to decoherence, imprecise device fabrication, and<br\/>faulty quantum control.<br\/><br\/>Intellectual merit: Understanding how robust computation can emerge from simple physical<br\/>systems is one of the great challenges addressed by our proposal for self-correcting quantum<br\/>computers, but this problem is ubiquitous across many fields, including both nanotechnology and<br\/>the biological sciences, where higher level function emerges from noisy constituent components.<br\/>Further, the substance of a self-correcting quantum computer will represent a new phase of matter<br\/>for quantum many-body systems, and insights gained in this proposal are likely to be essential to<br\/>understand the properties of many other quantum many-body systems.<br\/><br\/>Broader Impact: Since the ideas of self-correcting quantum computing have the potential to<br\/>revolutionize how we conceive of building a quantum computer, the broad impact of this proposal<br\/>will be disseminated to educators and journalists in workshops and public lectures. In addition development<br\/>of a seminar and courseware bridge the gap between theoretical fault-tolerance and the<br\/>fault-tolerance of physical devices will be pursued. Finally, this proposal would further enhance the<br\/>education and interdisciplinary research environment at the University of Washington by bringing<br\/>one of the PIs (Bacon) to the campus, who naturally rests between the Computer Science, Physics<br\/>and Chemistry departments.","title":"QnTM: EMT: Self-Correcting Fault-Tolerant Quantum Computers","awardID":"0523359","effectiveDate":"2005-07-15","expirationDate":"2008-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["555935","483594","555935","483594"],"PO":["565157"]},"106020":{"abstract":"Project Abstract<br\/>Exploiting Multiple Antennas in Multiuser Wireless Networks Huaiyu Dai and Brian L. Hughes, NC State University Multiple-input multiple-output (MIMO) techniques exploit the presence of multiple antennas at the transmitter and receiver to improve performance in wireless communication systems. Most work on MIMO has thus far focused on point-to-point communications and physical-layer performance metrics. However, many important wireless applications involve substantial contention among multiple users in complex time-varying environments, in which network-level metrics such as throughput, latency and heterogeneous user quality-of-service may be more relevant. While much is known about point-to-point MIMO communication, the design of efficient multiuser MIMO networks is far less well understood and entails greater challenges. <br\/>Intellectual Merit: This project deals with joint analysis and design of physical (PHY) and medium-access-control (MAC) layer protocols for multiuser MIMO networks. The aim is to understand how PHY and MAC-layer protocols combine to determine overall performance, and how these protocols can be jointly designed to optimize performance. Three main issues are addressed: (a) fundamental performance tradeoffs between PHY-layer techniques and MAC-layer techniques under diverse channel conditions, (b) joint design of PHY and MAC-layer protocols to promote collaboration between layers rather than competition, and (c) the impact and optimal exploitation of imperfect channel feedback. <br\/>Broader Impacts: This project investigates how best to deploy multiple-antenna techniques in wireless networks, which may have a significant impact on emerging wireless standards for several applications, including cellular telephony, wireless LANs and sensor networks. Material from this project will be incorporated into wireless courses at NCSU, and several university programs will be exploited to encourage participation by underrepresented groups. The PIs will also participate in outreach programs to disseminate research findings to industry and the broader public.","title":"Exploiting Multiple Antennas in Multiuser Wireless Networks","awardID":"0515164","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["560179","544836"],"PO":["432103"]},"95333":{"abstract":"This collaborative research project conducted jointly by the Michigan State University and the University of Michigan at Dearborn investigates the issues and techniques for supporting efficient similarity searches in multidimensional Non-ordered Discrete Data Spaces (NDDS). Similarity searches in NDDSs are becoming increasingly important for applications based on multidimensional discrete vectors, such as genome sequence databases, biometrics and E-commerce. Efficient similarity searches require robust indexing techniques in order to provide fast access to data. The currently existing indexing methods are either not suitable for an NDDS (e.g., the R*-tree) or too generic to provide good performance for an NDDS (e.g., metric trees). The main goal of this project is to study the fundamental properties of NDDSs and develop indexing methods exploiting these properties to support efficient similarity searches in NDDSs. A set of essential geometric concepts for an NDDS is introduced based on extended methods for traditional (ordered) continuous data spaces. A number of promising data-partitioning-based and space-partitioning-based indexing techniques (including index tree structures, building strategies, search algorithms and performance models) using these concepts for NDDSs are explored and compared. Other related issues including supporting various types of queries, adopting different distance measures, indexing hybrid data spaces with mixed ordered and non-ordered dimensions, developing efficient bulk loading techniques and utilizing effective compression schemes are studied. This research will provide new database indexing techniques to solve relevant issues in scientific, medical and commerce fields that require fast access to large volumes of NDDS data. Research results, including software tools or programs and experimental data will be disseminated via the projects' Web sites (http:\/\/www.cse.msu\/~pramanik\/nsf05\/nsf05msu.html and <br\/>http:\/\/www.engin.umd.umich.edu\/~qzhu\/nsf05\/nsf05umd.html).","title":"Collaborative Research: Supporting Efficient Similarity Searches for Multidimensional Non-ordered Discrete Data Spaces","awardID":"0414576","effectiveDate":"2005-07-15","expirationDate":"2009-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7496","name":"COLLABORATIVE SYSTEMS"}}],"PIcoPI":["550626"],"PO":["563727"]},"102170":{"abstract":"Abstract<br\/>Proposal: CNS 0454333<br\/>PI: Lydia Kavraki<br\/>Institution: William Marsh Rice University <br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Acquisition of Rice Physical and Biological Computing (PBC) Cluster <br\/><br\/>Rice University investigators will acquire a 32 node computer cluster to advance research on representations, algorithms, system architectures, and new enabling technologies for solving complex geometric problems, particularly those arising from physical and biological sciences. The main collaborative projects of the Rice group are: (a) the modeling and manipulation of deformable objects that range from elastic objects to bio-molecules, (b) the development of robotics-inspired methods for the study of molecular motion and function, and (c) the reduced-dimension modeling of bio-molecular systems. The infrastructure will enable them to develop and use computational tools to represent, simulate, and interact with the physical world at all scales. Broader impacts of this project include its use in graduate training, supporting collaborations with Rice and the Texas Medical Center, and use in parallel and distributed computing topics courses. The facility will also support students at Rice participating in the CRA Distributed Mentoring project.","title":"CRI: Acquisition of Rice Physical and Biological Computing (PBC) Cluster","awardID":"0454333","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["551029","551026","557516","540484","429913"],"PO":["563727"]},"95422":{"abstract":"Real data often show a more complex structure than is assumed in much of statistics, machine learning, or data mining. Objects may be characterized by diverse types of information such as numerical quantities, text, and properties of a network neighborhood. The goal of the project is to develop techniques to integrate information components that differ both quantitatively and qualitatively. Classification algorithms that are based on homogeneous attributes can be evaluated exclusively by their overall classification quality. In the presence of qualitatively and quantitatively diverse information, the search space of all possible combinations of techniques and parameters is too large to be evaluated by any reasonable amount of test data. Three goals are pursued: (1) defining intermediate, homogeneous attributes that allow effective use of uniform classification and clustering techniques; (2) developing robust criteria that allow identification of suitable intermediate attributes and do not exclusively rely on overall classification accuracy; and (3) developing efficient and effective approaches to generate intermediate attributes from data with network connectivity, time-dependent data, text and other types of data. Starting with a specific classification problem in bioinformatics, the project attempts to find solutions that are applicable to a wide range of data mining problems. The work is ideally suited to teach students a broad range of research activities from fundamental concepts to applications, both in thesis and course work. Results will be of relevance to a large number of practical applications in bioinformatics and other sciences. The project Web site (http:\/\/www.cs.ndsu.nodak.edu\/~adenton\/IDM\/) is used for dissemination of up-to-date results, including software, demonstration of newly developed techniques, comprehensive examples based on biological data to benefit researchers from biological sciences, and generally understandable examples (such as for a movie database) will enhance outreach, and demonstrate generality of results.","title":"Data Mining in the Presence of Quantitatively and Qualitatively Diverse Information","awardID":"0415190","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["543472"],"PO":["563751"]},"95334":{"abstract":"This collaborative research project conducted jointly by the Michigan State University and the University of Michigan at Dearborn investigates the issues and techniques for supporting efficient similarity searches in multidimensional Non-ordered Discrete Data Spaces (NDDS). Similarity searches in NDDSs are becoming increasingly important for applications based on multidimensional discrete vectors, such as genome sequence databases, biometrics and E-commerce. Efficient similarity searches require robust indexing techniques in order to provide fast access to data. The currently existing indexing methods are either not suitable for an NDDS (e.g., the R*-tree) or too generic to provide good performance for an NDDS (e.g., metric trees). The main goal of this project is to study the fundamental properties of NDDSs and develop indexing methods exploiting these properties to support efficient similarity searches in NDDSs. A set of essential geometric concepts for an NDDS is introduced based on extended methods for traditional (ordered) continuous data spaces. A number of promising data-partitioning-based and space-partitioning-based indexing techniques (including index tree structures, building strategies, search algorithms and performance models) using these concepts for NDDSs are explored and compared. Other related issues including supporting various types of queries, adopting different distance measures, indexing hybrid data spaces with mixed ordered and non-ordered dimensions, developing efficient bulk loading techniques and utilizing effective compression schemes are studied. This research will provide new database indexing techniques to solve relevant issues in scientific, medical and commerce fields that require fast access to large volumes of NDDS data. Research results, including software tools or programs and experimental data will be disseminated via the projects' Web sites (http:\/\/www.cse.msu\/~pramanik\/nsf05\/nsf05msu.html and <br\/>http:\/\/www.engin.umd.umich.edu\/~qzhu\/nsf05\/nsf05umd.html).","title":"Collaborative Research: Supporting Efficient Similarity Searches for Multidimensional Non-ordered Discrete Data Spaces","awardID":"0414594","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7496","name":"COLLABORATIVE SYSTEMS"}}],"PIcoPI":["550712"],"PO":["563727"]},"109211":{"abstract":"This award is for a High Confidence Medical Device Software and Systems (HCMDSS) workshop, Philadelphia, June 2-3, that provides an open, working forum for leaders and visionaries concerned with information technology in medical devices and systems. Participants are drawn from industry, research laboratories, academia, hospitals, and government. The goal of the workshop is to develop a roadmap for information technology research to overcome crucial issues and challenges in the design, manufacture, certification, and use of future medical device software and systems. The workshop is sponsored in cooperation with the High Confidence Software and Systems (HCSS) Coordinating Group (CG) under the NSTC Networking and Information Technology Research and Development (NITRD) Subcommittee.","title":"High Confidence Medical Device Software and Systems Workshop (HCMDSS) 2005; Philadelphia, PA","awardID":"0532968","effectiveDate":"2005-07-15","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"V931","name":"NSA-AREA OF PROG VERIFICATION"}}],"PIcoPI":["553656","526896"],"PO":["561889"]},"109376":{"abstract":"The December 26, 2004 Sumatran-Andaman earthquake and subsequent tsunami led to the discovery of two critical scenarios. There is a deficit of information management systems within the government sector countries on the coast of the Indian Ocean. Globally, there is no disaster management software that can handle a variety of disaster information management needs. This project explores intellectual challenges of data management, in the context of Sahana, an open source disaster information management system that was deployed in Sri Lanka within days of the disaster. <br\/><br\/>The first challenge is the design of the data management component for disaster information management. While database design is not a challenge, understanding the scope of information that should be accessed during and after a disaster is a challenge. The second challenge is a plan for data acquisition, data cleaning, <br\/>integration and quality, in the context of a National Data Center (NDC) for Sri Lanka. While solutions to similar challenges have been reported in the literature, the lack of computerized information management systems in Sri Lanka makes the task a challenge. This US research team has expertise in wide area database <br\/>management (Raschid) and open source development and Web services (Weerawarana). Karunaratne from the University of Colombo School of Computing (UCSC) and Madurapperuma from the the University of <br\/>Moratuwa in Sri Lanka provide domain expertise. The need for broader impact is met via the development of open source global disaster management systems and the design of a national information infrastructure for Sri Lanka. The project Web site http:\/\/www.umiacs.umd.edu\/labs\/CLIP\/Handle\/SGERtsunami.html is to be used for communication about the exploratory project and for disseminating results.","title":"SGER: Addressing the Data Management Challenges of Disaster Information Management Within the Context of a Pilot National Data Center (NDC) for Sri Lanka","awardID":"0533986","effectiveDate":"2005-07-01","expirationDate":"2006-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}}],"PIcoPI":["543580"],"PO":["371077"]},"102171":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Collaborative: Next Generation CiteSeer <br\/><br\/>Lead Proposal: CNS 0454203<br\/>PI: Gregg Rothermel<br\/>Institution: University of Nebraska-Lincoln<br\/><br\/>Proposal CNS 0454348<br\/>PI: John M. Hatcliff<br\/>Institution: Kansas State University <br\/><br\/><br\/>The investigators will create and disseminate a repository of software-related artifacts sufficient to support rigorous controlled experimentation with program analysis and software testing techniques for a broad community of researchers and educators. Software written in the C and Java programming languages along with supporting elements such as multiple versions, faults, test requirements, specifications, test cases, functional behavior specifications, and state-machine models of program behavior will be included. Methods to retrieve program information will be developed. This resource will be an enabling technology for controlled experimentation for program analysis, software testing, and education in software engineering and software testing. Broader impacts of this project include the impact of the resource on enabling a broad range of research and improving capacity for software education in a wide range of institutions.<br\/>universities.","title":"CRI: Collaborative Research: A Community Resource to Support Controlled Experimentation with Program Analysis and Software Testing Techniques","awardID":"0454348","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["563763"],"PO":["550859"]},"102183":{"abstract":"Abstract<br\/>Proposal: CNS 0454425<br\/>PI: Dan Suciu<br\/>Institution: University of Washington<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Global-scale Data Sharing using Statistics and Probabilities <br\/><br\/>This project will address the problem of semantic heterogeneity that occurs in large-scale data integration by exploring the scalability of novel techniques to very large amounts of data. Two such techniques will be considered. One is corpus-based schema matching, where a large collection (corpus) of schemas is stored, analyzed, and preprocessed in order to enhance automatic schema matching. The second technique consists of probabilistic-based query answering, which efficiently computes complex SQL queries on probabilistic databases. To study the scalability of these techniques to large-scale data integration tasks, a significant fragment of the Web will be downloaded, and stored locally, on a cluster of servers. Data instances and their schemas will be extracted automatically from these Web pages. The resulting corpus of schemas will be matched using a variety of techniques, and the matches interpreted probabilistically.<br\/>The resulting data organization is called the semantic cache. Users will be able to formulate rich queries over the semantic cache, for example in a language like SQL. Each query will be evaluated on the global data, and given a probabilistic interpretation. The answers will be returned to a user ranked according to their probabilities. This project has potential, if successful, to impact a variety of applications where large scale data integration is currently impossible to achieve, such as from scientific data sharing, electronic commerce, and emergency management systems.","title":"CRI: Global-scale Data Sharing using Statistics and Probabilities","awardID":"0454425","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["531543","278933"],"PO":["565136"]},"107683":{"abstract":"Artificial molecular-scale robots capable of cooperative and adaptive behavior currently reside firmly in the realm of science fiction. However, we will demonstrate that molecules behaving similarly to macroscopic robots can indeed display simple forms of cooperation and adaptation, when challenged with certain tasks. These \"molecular robots\" which we call \"snakes\" consists of chains of nucleic acids incorporating multiple catalytic units. They move (\"walk\") over landscapes covered with substrates, leaving a trail of cleaved substrates behind them. We will study the ability of these molecular robots to integrate additional catalytic segments (limbs) from the solution, or to form dimers, when we force them to perform longer walks. Those snakes that acquire additional limbs, or form cooperating dimers, while surviving harsh conditions, will be allowed to amplify. Such experiments will help create in the future molecular robots that display swarm intelligence and that are capable of distributive computing.","title":"BIC: EMT: Cooperative and Adaptive Behaviors By Molecular Robots","awardID":"0523317","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["558957","459219"],"PO":["521045"]},"109531":{"abstract":"This project explores a novel high mobility locomotion system concept for unstructured environments. The Intelligent Mobility Platform with Active Spoke System (IMPASS) utilizes rimless wheels with individually actuated spokes to provide the ability to step over large obstacles like legs, adapt to uneven surfaces like tracks, yet retaining the speed and simplicity of wheels. Since this system lacks the complexity of legs and has a large effective (wheel) diameter, this highly adaptive system can move over extreme terrain with ease while maintaining respectable travel speeds, and thus has great potential for search-and-rescue missions, scientific exploration, and anti-terror response applications. This research project will research, demonstrate and evaluate this novel concept with a focus on studying the issues of mobility, intelligent motion planning, and design. The intellectual merits of the proposed activities lie in the development of an original concept of a new type of high mobility locomotion system and an improved understanding of mobility and intelligent motion planning issues. The broader impacts of this research include the many important application areas of the high mobility system enabled by extending human reach capabilities into unknown and hazardous environments. This effort will integrate research with education and promote teaching and learning by integrating the prototype design into the two-semester undergraduate senior design project. The robot prototypes developed will be demonstrated to the public at the FIRST robotic competitions and at local high schools to promote interest in science and engineering.","title":"Intelligent Mobility Platform with Active Spoke System","awardID":"0535012","effectiveDate":"2005-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["435880"],"PO":["564316"]},"109564":{"abstract":"This project is a collaborative effort by computer scientists and engineers from Texas A&M and UC Berkeley consulting with natural scientists and documentary filmmakers. The goal is to advance the fundamental understanding of automated and collaborative systems that combine sensors, actuators, and human input to observe and record detailed natural behavior in remote settings. Currently, scientific study of animals in situ requires vigilant observation of detailed animal behavior over weeks or months. When animals live in remote and\/or inhospitable locations, observation can be an arduous, expensive, dangerous, and lonely experience for scientists. The project proposes a new class of hybrid teleoperated\/autonomous robotic \"observatories\" that allow groups of scientists, via the internet, to remotely observe, record, and index detailed animal activity. Such observatories are made possible by emerging advances in robotic cameras, long-range wireless networking, and distributed sensors. The project will investigate the algorithmic foundations for such observatories: new metrics, models, data structures, and algorithms, that will comprise a robust, mathematical framework for collaborative observation. The project will build on past work to extend and formally characterize hybrid models of collaborative and automated observation that draw on computational geometry, stochastic modeling and optimization. The project will advance fundamental understanding of networked robotics and develop efficient algorithms for collaborative observation that combines human and sensor input. This effort is intended to benefit biological scientists and facilitate collaboration among researchers. It will produce working prototypes that will be accessible via the internet to scientists, students, and the public worldwide.","title":"CONE: Collaborative Observatory for Natural Environments","awardID":"0535218","effectiveDate":"2005-07-01","expirationDate":"2009-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["520937"],"PO":["387198"]},"102172":{"abstract":"Abstract<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Proposal: CNS 0454355<br\/>PI: Sumeet Dua<br\/>Institution: Louisiana Tech University <br\/>Title: CRI: Planning for Heterogeneous Data Repository for Computing Research <br\/><br\/> <br\/>Investigators at Louisiana Tech University will plan an infrastructure project for an infrastructure proposal to build a heterogeneous repository of biomedical related data. The anticipated project will incorporate data such as free-text notes, instrument readings, and confocal images. The data will support a computing science research program addressing problems in data integration, data mining, database modeling, query translation, query optimization, rule-based databases, object recognition, information relevance feedback, extraction and indexing, and content-based queries. Partners in the planning process in addition to Louisiana Tech University at Ruston, LA will include the University of Louisiana at Lafayette, LA, Grambling State University at Grambling, LA, the University of New Orleans at New Orleans, LA, Louisiana State University at Baton Rouge, LA, Louisiana State University Health Sciences Center<br\/>at New Orleans, and Children's Hospital at New Orleans, LA. Broader impacts of this project include participation of minority institutions and potential partnerships with additional higher education and industry groups.","title":"CRI: Planning for Heterogeneous Data Repository for Computing Research","awardID":"0454355","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[269376,"554655",269378,269379],"PO":["563751"]},"105451":{"abstract":"0512974<br\/>Jaron<br\/><br\/>This award provides continued funding to the American Institute for Medical and Biological Engineering (AIMBE) to help promote scientific collaboration and improve communication among Biomedical Engineering researchers and their professional societies from 48 countries worldwide. AIMBE is the U.S. member organization to the International Federation of Medical and Biological Engineering and this award provides funds to support travel of AIMBE'S appointed representative to participate in meetings of IFMBE's Administrative Council. Representation on the Administrative Council provides the United States with the opportunity to develop new lines of communications and collaborative projects among biomedical engineering researchers globally, thereby extending the U.S. leadership role in medical science and technology research and development. This is particularly relevant to investigations concerning emerging medical technologies.<br\/><br\/>Proceeds from an award are to support the cost of attending Council meetings and regional conferences in Asia-Pacific, Latin and South America, and Europe. The Council meetings entail strategic planning of the organization, planning of scientific meetings, publications, dissemination of scientific information and initiation of advanced projects in critical and emerging areas. The regional conferences are designed to promote collaboration and networking among Bioengineering researchers and educators from the United States and developing countries and as a mechanism to implement the critical projects.<br\/><br\/>The funding level is modest compared to the anticipated value for the U.S. community of biomedical engineering researchers. Also, the project is expected to promote the role of the National Science Foundation in advancing research and education in this field globally while assisting NSF in achieving its strategic objectives.","title":"United States Representation to IFMBE and IUPESM","awardID":"0512974","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"5345","name":"BIOMEDICAL ENGINEERING"}}],"PIcoPI":["363926"],"PO":["319455"]},"95402":{"abstract":"The goal of this collaborative research project (0414981, W. Meng, SUNY Binghamton and 0414939, C. Yu, UIC) is to develop technologies for providing integrated access to Web databases. The approach consists of developing highly automated solutions to the following tasks: discovering Web databases from the Web, clustering them according to their application domains, integrating the search interfaces of the Web databases in the same domain into an integrated search interface, mapping each query submitted to an integrated interface to its underlying Web databases, extracting and annotating the search result records from the result pages returned from the local Web databases, and merging the results to form an integrated response for presentation to the user. Web services based interfaces of Web databases, if available, will be utilized. The evaluation of the developed algorithms is based on real Web databases from different domains. This research is expected to produce new algorithms, useful datasets, a software toolkit, and several operational Web database metasearch engines (including some for the Air Force Research Lab at Rome). The developed technology can be used in many applications including comparison shopping and collecting data from the deep Web. Research results will be incorporated into several courses the PIs teach and students will be recruited to participate in the research activities. Research results will be disseminated through published papers as well as a textbook on Web-based search technology. The project Web site (http:\/\/www.cs.binghamton.edu\/~meng\/DMSE.html) provides access to research results as well as datasets and demo systems.","title":"Collaborative Research: Achieving Information Integration of Web Databases Through the Construction of Metasearch Engines","awardID":"0414981","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}}],"PIcoPI":["434747","434750"],"PO":["563751"]},"106001":{"abstract":"This research project investigates the design, analysis, and implementation of resource efficient integrative transceivers and retransmission diversity in broadband multi-input-multi-output (MIMO) wireless communications. The development and exploitation of MIMO technologies and Automatic Repeat reQuest (ARQ) protocols have been popular subjects of wireless research. Hybrid ARQ is effective as protection against packet error in wireless communications, while MIMO transceivers have demonstrated significant performance gains at the wireless physical layer. However, traditional approaches treat MIMO schemes and ARQ as independent mechanisms in wireless networks. The design integration of hybrid ARQ protocols with MIMO transceivers has received scant coverage and has not been well utilized. As more and more mainstream products begin to adopt MIMO technologies in wireless LAN and other wireless systems, there is an urgent need to exploit and achieve the full potential benefit offered by integrating wireless ARQ and MIMO designs. This research project investigates the efficient integration of ARQ with broadband MIMO physical layer.<br\/><br\/>Simultaneously considering both designs of ARQ protocols and MIMO transceivers, the investigations of this project are systematic, integrative, and broadly applicable. Unlike in traditional wireless designs where ARQ and MIMO technology are treated separately, this integrative design approach opens a new door to developing future advanced wireless systems that are power and bandwidth efficient. While the advantages of cross-layer optimization are well known, the integrative design of ARQ protocols and their corresponding MIMO transceivers provides a concrete and tangible approach to cross-layer network design. Focusing on bandwidth and power efficiency, the project goal is to optimize and adapt ARQ (Automatic Repeat reQuest) protocols for MIMO wireless systems to improve performance and scalability. The research presents new capacity analysis and transceiver optimization of progressive MIMO precoding to better exploit the ARQ retransmission diversity. The investigators develop design of bandwidth and power efficient transceiver technologies in full integration with new scalable ARQ designs to significantly improve the performance of end user applications. The results of this project provide new formulation of wireless MIMO channel estimation algorithms particularly tailored for bandwidth efficient and scalable hybrid ARQ protocols in future broadband wireless communication systems.","title":"Integrative Design of Broadband MIMO Wireless Transceivers and Spectrally Efficient Retransmission Diversities","awardID":"0515058","effectiveDate":"2005-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["551148"],"PO":["564898"]},"106012":{"abstract":"Sources of entropy that are not precisely reproducible nor uniformly distributed, such as biometrics, nontraditional passwords, or physical random functions, are increasingly suggested as tools in electronic and physical security. There are, however, many significant unresolved questions about exactly how such sources should be used and stored. This proposal focuses on investigating how to use them securely, reliably, privately and versatilely. The techniques studied will have applications well beyond biometric authentication, to settings where noisy data needs to be stored securely, compared privately, or used cryptographically.<br\/><br\/>A simple motivating scenario for our research is that of password-based authentication. In order to avoid security vulnerabilities inherent in storing passwords, systems often store their one-way hashes instead. When a user's password is entered for verification, it is first hashed and then compared to the stored hash value. The problem with passwords, of course, is that their entropy is low. The problem with using highentropy inputs, on the other hand, is that the readily available ones are hard to reproduce precisely: humans make typographical errors in long passphrases and forget some of the answers to multiple questions, while machines cannot precisely reproduce fingerprints and iris scans from one reading to the next. Therefore, the one-way hash function approach does not work, because even slight variations in the input will results in drastic changes of the hash value. Without additional techniques, one has no choice but to store the original<br\/>enrollment value and accept the inherent security vulnerabilities, or to exhaustively search all values close to the input value.<br\/><br\/>Intellectual Merits of the Proposed Project<br\/>The proposed research will allow verification of such noisy high-entropy inputs without requiring secret storage or performing brute-force search. What distinguishes our work from related prior work in the literature is that our approach is rigorous and versatile. The techniques we propose to study will allow the use of unreliable nonuniform inputs not only in the above password-authentication scenario, but also for keys is any cryptographic application. Moreover, the same techniques will have other applications, such as privacy-preserving data mining.<br\/><br\/>Our proposal builds on the recent work of the two PIs [42]. That work introduced new notions for using nonuniform and unreliable data cryptographically: secure sketches and fuzzy extractors. While the notions are already finding applications [40, 39], much work is needed to obtain and analyze practical constructions for a variety of input classes, to strengthen definitions, and to study specific new applications.<br\/><br\/>Broader Impacts of the Proposed Project<br\/>ON SECURE SYSTEMS. By removing the need for large-volume distributed secure storage, our work has the potential to significantly lower the costs and potential liabilities of systems that utilize biometric or other sensitive inputs for security (as detailed in the proposal description). Moreover, it may enable systems that have relied on low-entropy passwords to switch to more secure approaches, such as biometric-based key agreement.<br\/><br\/>ON PRIVACY. A significant drawback of many systems that require authentication is the loss of privacy that users experience (e.g., when having their social security numbers stored as passwords for their credit card accounts, or when having their fingerprints stored as passwords for secure doors). This work will remove the need to store private data in many applications. Moreover, as further detailed in the proposal description, the privacy protection will extend not only to the biometric (or similar) password, but also to the data protected by it, ensuring that no one without the right password will have access to the data.<br\/><br\/>ON EDUCATION. The two PIs regularly teach courses on cryptography and network security, and will be able incorporate the new results into the courses they teach. In addition, the proposal has a significant graduate student training component.","title":"Collaborative Research: Rigorous Cryptography from Biometrics and Other Noisy Data","awardID":"0515121","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["550227"],"PO":["499399"]},"106023":{"abstract":"The area of approximation algorithms developed at a spectacular rate over the last fifteen years. Besides the obvious ties of approximation algorithms to real applications, another opportunity is using methodology developed in this area to make conceptual breakthroughs in other areas. A prominent example of this is the natural linkup with the relatively new area of algorithmic game theory, which attempts to answer issues raised by the new generation of distributed systems which include the Internet, P2P systems, the web, and wireless systems.<br\/>Recent exciting work on network coding also establishes strong links with approximation algorithms. Additionally, this area still retains its excitement - very basic insights are still being obtained, though at a slower rate than in the 1990's, and several fundamental problems still remain unsolved. The PI's research involves both aspects: work on the interface of areas as well as work on some fundamental problems remaining open in approximation algorithms.","title":"Approximation Algorithms and Algorithmic Game Theory","awardID":"0515186","effectiveDate":"2005-07-15","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["516928"],"PO":["348215"]},"109169":{"abstract":"The 9th International Workshop on Software and Compilers for Embedded Systems (SCOPES) will take place September 29th through October 1st, in Dallas. The scope of the workshop is software for embedded systems, with emphasis on code generation (compilers) for embedded processors. This award supports the first meeting of this international embedded systems workshop series to be held in the United States, and it supports participation by graduate and undergraduate students.","title":"Proposed Conference Support for SCOPES 05","awardID":"0532686","effectiveDate":"2005-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["13545","397863"],"PO":["561889"]},"104780":{"abstract":"Studies of application behavior reveal the nested repetition of large and small program phases, with significant variation among phases in such characteristics as memory reference patterns, memory and energy usage, I\/O activity, and occupancy of micro-architectural resources. Reliably predicting and exploiting phased behavior can allow an advanced execution system to allocate resources in a way that better matches program needs, or to transform programs so that their needs better match the available resources. This project will develop the technology required to accurately detect and exploit phased behavior in a wide range of applications. Specifically, the project will combine hardware and run-time monitoring mechanisms, off-line trace analysis, and profile driven adaptation. <br\/><br\/>The proposed research will evolve a more general-purpose suite of detection mechanisms, comparing alternative approaches and reconciling conflicting indications of phase boundaries. Once program phases have been detected and marked, the proposed research will investigate three principal techniques to optimize program performance. Data reorganization will improve memory hierarchy performance by reducing latency and increasing effective memory bandwidth. In particular, phase-based array regrouping and structure splitting will allow applications to better utilize the cache and minimize direct access to memory. The system will include a compiler for program transformation and a run-time monitoring system based on advanced hardware support.<br\/><br\/>Scheduling and task assignment will serve to minimize load imbalance and communication bandwidth requirements. Relative loads among processes in data-parallel applications commonly vary from one phase to the next. Characterization of these loads will enable both static and dynamic load balancing. Identification of memory reference patterns will also allow locality-sensitive assignment of tasks, thereby increasing CPU utilization while decreasing bandwidth requirements. I\/O prefetching will exploit phase-specific patterns of access to disk data, thereby improving disk utilization and minimizing time lost waiting for I\/O. Profile-driven algorithms will build on the PIs' prior work in on-line content delivery. The proposed techniques will be incorporated into popular run-time systems.","title":"CSR---AES: Program Phase Detection and Exploitation","awardID":"0509270","effectiveDate":"2005-07-01","expirationDate":"2010-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["481348","556692","550486","550397","548271"],"PO":["535244"]},"105770":{"abstract":"ABSTRACT<br\/>0514092<br\/>Si-Qing Zheng<br\/>University of Texas @ Dallas<br\/><br\/>Optimization of Hypergraphs and Combinatorial Designs with Applications<br\/><br\/>Hypergraph theory started to take shape around 1960. It has been seen that the theory of hypergraphs is a powerful tool for the solution of integers optimization problems, such as scheduling problems, location <br\/>problems, etc. when the incidence matrices of the corresponding graphs have some special properties. Combinatorial block designs, whose root can be traced much earlier than hypergraphs, are considered as special hypergraphs. They already found applications such as experiment designs, coding theory, etc. In this project, the investigator intends to extend the hypergraph and combinatorial design theories, motivated <br\/>by a new class of applications, namely powerful interconnection structures in high-performance computing and communications based on multiconnect components. The new class of interconnection structures <br\/>considered are called hypernetworks. A typical multiconnect component is an optical device connecting a set of nodes using TDM, WDM, CDM and SDM techniques. Additional examples of multiconnect component include a <br\/>wireless LAN, a \"cluster\" of communicating mobile stations and a high-performance subnetwork in grid computing. A hypernetwork is represented by a hypergraph. In a point-to-point network, each connection, which connects a pair of nodes, has a fixed capacity dedicated to it. In a hypernetwork, the nodes connected by a <br\/>multiconnect component or multipoint subnet are considered fully connected. Due to shared resources (channels, transceivers. etc.), hypernetworks have more balanced link loading, higher throughput and <br\/>reduced time delay. Furthermore, hypernetworks support multipoint communications more efficiently. A hypernetwork, when abstracted as a hypergraph, is characterized by several parameters, such as vertex <br\/>degree, hyperedge rank, diameter, bisection connectivity. regularity, uniformity, symmetry, etc. Since combinatorial block designs are special hypergraphs with many properties that are desirable in <br\/>hypernetworks, it is quite natural to consider hypernetworks whose underlying theoretic models are block designs. The investigator propose to use hypergraph and combinatorial block design theories to guide the <br\/>design of hypernetworks. Each design aspect has a spectrum of design choices. The design of a particular type of hypernetworks depend in application demands, technology feasibility, and cost-effectiveness. <br\/>Since desirable hypernetwork features are interrelated and many of them have contradicting requirements, trade-offs must be considered. Thus, hypernetwork design problems are constrained optimization problems. The goal of this project is to investigate the potential and limit of hypernetworks by applying new combinatorial optimization techniques.<br\/>The proposed research have implications in the construction of new computing and communication systems using emerging technologies. The proposed work is theoretical in nature, and it addresses fundamental issues in information processing and communication.","title":"Optimization of Hypergraphs and Combinatorial Designs with Applications","awardID":"0514092","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["338379"],"PO":["381214"]},"102140":{"abstract":"Abstract<br\/><br\/>Proposal: CNS 0454081<br\/>PI: Constantine Manikopoulos<br\/>Institution: New Jersey Institute of Technology<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>CRI: 'SmartCampus' A Wireless Mobile Community System with People-To-People-To- Places Services<br\/><br\/>This project will create a mobile, wireless NJIT campus community system that will serve as a dispersed laboratory for the study of location-based, online communities with People-To-People-To-Places (P3) Services, in terms of: 1) community building; 2) co-ordination of mobile teams; 3) user privacy [personal location data]; and 4) security. This campus-wide facility will allow the integration of activities across many laboratories and enable anywhere, anytime participation by both students and faculty. SmartCampus will aid the analysis and understanding of the underlying technical and social issues and their interactions, taking into account rising privacy concerns. The latter will be addressed from two perspectives: 1) improving system security and trust with the development of ConexGuard, a novel SmartCampus security-on-demand framework for its heterogeneous environment, and 2) creating privacy-sensitive applications that exploit relevant contextual factors - properties of people and places, and relationships between them - thus addressing issues distinct to P3-Systems. The team includes social scientists, computer scientists and electrical engineers in an interdisciplinary effort addressing issues of emerging interest - particularly privacy in community systems. Broader impacts include education and field testing in Newark New jersey.","title":"CRI: 'SmartCampus' - A Wireless Mobile Community System with People-To-People-To-Places Services","awardID":"0454081","effectiveDate":"2005-07-15","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["352199","291803","348215",269272,"538322","538322","383971"],"PO":["564778"]},"102151":{"abstract":"Abstract<br\/>Proposal: CNS 0454170<br\/>PI: Bin Wang<br\/>Institution: Wright State University <br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Instrumentation of a Hierarchical Wireless Sensor Network Test-bed for Research and Education <br\/><br\/><br\/>Project: <br\/>The PI's will acquire a two tier, hierarchical sensor network based on motes. The testbed will have a core network, core access network, and sensor field. The core network is the wireless LAN already in existence; the core access network will consist of several network service access points emulated by laptop computers, and the sensor nodes will be constructed from motes, sensor boards and network interface boards. Research supported will include development, analysis and deployment of sensor network protocols and applications. The PI's will study whether a proposed hierarchical architecture improves connectivity, energy conservation, and reliability; they will evaluate security protocols; they will design new security mechanisms and resilience schemes in defense of faults and attacks; and develop region-based event monitoring applications. The infrastructure will support education at Wright State University by use in undergraduate capstone courses and graduate level independent research.","title":"CRI: Instrumentation of a Hierarchical Wireless Sensor Network Test-bed for Research and Education","awardID":"0454170","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["465463","559204","352533"],"PO":["539087"]},"105671":{"abstract":"Determining the evolutionary relationships among a group of organisms is one of the major goals of integrative historical biology. In the century following Darwin , morphological structures were almost the sole source of data for a variety of analyses aimed at resolving this problem. Over the past few decades, molecular systematic techniques have taken center stage in the study of extant species, usually supporting but frequently overturning results based on morphology. In paleontology, however, morphological data (the shapes of fossils) remain effectively our only information<br\/>describing the evolutionary position of extinct organisms. The application of 3D techniques of data collection and analysis, as well as computer-based visualization methods, to data sets including both extant and extinct species will now permit improved levels of interpretation in paleontology. The team will develop new tools in each of these areas, using Old World monkeys as a \"test-bed\" or model organism, to approach significant problems in both computer science and the broad domain of comparative biology. New advances in the analysis and manipulation of surfaces and volumes in three-dimensions in computer graphics, visualization and computer vision can be applied to these problems. Simultaneously, collecting and interpreting morphological data for paleontology is a novel application for pushing the development of techniques in computer science.<br\/><br\/>analysis will be based on a large existing database of three-dimensional data (mostly skull surfaces) at the American Museum of<br\/>Natural History.<br\/><br\/>The interactive graphics, visualization and statistical analysis tools we propose are ever more widely needed as the amount of three-dimensional morphology data increases. The close interaction of geometric morphometrics and computer graphics will lead to new ideas about the representation of shape. In addition, the project develops new approaches to the problem of integrating morphology with molecular data in the study of evolution, applicable in many parts of the tree of life. In addition, with massive amounts of new data, new processing and analytic software, and new approaches to integrating morphology, answers to specific questions about the evolution of African monkeys might be obtained.<br\/><br\/>A large part of the project will be done at Lehman College of CUNY, a minority-serving institution in the Bronx, and minority undergraduates are already involved in the research. The software tools we will develop are sorely needed and will become part of the scientific infrastructure. The visualizations will form a basis for sharing research in evolution with the general public.","title":"SEI (SBE): Collaborative Research on Visualization of Evolutionary Transformation using 3D Morphometrics: African Monkeys as a Test Case","awardID":"0513565","effectiveDate":"2005-07-01","expirationDate":"2010-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1629","name":"BE: NON-ANNOUNCEMENT RESEARCH"}}],"PIcoPI":["340163"],"PO":["565136"]},"106046":{"abstract":"When an analog source such as an audio or video signal is transmitted over a noisy channel, typically the source is first quantized and compressed (source coded) and then encoded to make it robust to the noisy channel (channel coded). For a class of channels called ergodic channels, it is optimal to separately perform source and channel coding. This principle has served as an engineering guideline to design several modern communication systems. However non-ergodic channels are encountered in several practical communication systems, for example in broadcasting a source to many users. For this class of channels, the separation based approach is not necessarily optimal. The focus of this work in understanding and providing guidelines for performing source and channel coding for such channels.<br\/><br\/>Three separate but related problems in coding for non-ergodic channels are considered. The first is that of minimizing a measure of end-to-end distortion such as expected mean squared error in communicating a source over a non-ergodic noisy channel. The focus is on the design of analog and hybrid analog to digital joint source-channel coding techniques which can outperform conventional separation based digital approaches. Then the design of practical universal channel codes for non-ergodic channels with memory is considered and it is shown that coded decision feedback signal processing is a promising technique to solve these problems. Finally, the design of universal codes for distributed compression where the underlying correlation model is not perfectly known is considered.<br\/><br\/>Apart from standard educational activities, a key aspect of the broader impact is in creating avenues for our students to interact with researchers from other universities.","title":"Universal Source and Channel Codes for Non-Ergodic Channels","awardID":"0515296","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["551055"],"PO":["432103"]},"106057":{"abstract":"This proposal concerns algorithmic and structural problems at the intersection of graph theory and string algorithms. The problems center on the derivation of a set of strings under certain operations, of which recombination is the most important, and graphs are used both to display the solution, and to obtain the solution. It is the non-trivial role of graphs in obtaining the solution that forms the basis for this proposal. We have recently exploited structural aspects of certain graphs, and exploited graph algorithms to solve a number of string problems. The proposed project will more deeply study the relevant graph structure and graph algorithms and connect it to solve a specific class of algorithmic string problems. The specific string problems of concern originate in population genetics at one biological scale, and in phylogenetics at a different biological scale.<br\/><br\/>Broader Impact<br\/>The algorithms and software that we will develop may allow biologists to better understand the history and role of recombination, gene-conversion and lateral gene transfer. As just two examples, the tools will facilitate the tasks of gene finding by association mapping, and in understanding how lateral gene transfer helps bacteria to rapidly develop antibiotic resistance.","title":"Graph Structure and String Algorithms for String and Graph Reconstruction Problems","awardID":"0515378","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["518691"],"PO":["562944"]},"99407":{"abstract":"Cyberinfrastructure refers to the ability to access and integrate today's hardware, software, and human <br\/>information technology resources in order to facilitate science and engineering goals. This proposal <br\/>describes a plan of action for building and delivering cyberinfrastructure. The efforts of the San Diego Supercomputer Center (SDSC) will focus on the provisioning of a deeply integrated environment, <br\/>which coordinates hardware, software, and human resources to engage and support users, and the <br\/>development of critical infrastructure to drive research and education. <br\/><br\/>SDSC activities will have particular focus on building the data cyberinfrastructure so critical to <br\/>managing, understanding, and gaining knowledge from today's deluge of data. The Center will also focus on building the community for cyberinfrastructure: extending the reach of researchers and educators through deep collaborations, professional-level services and software, large-scale data management and <br\/>computational facilities, and the engagement and support of new communities of social scientists and <br\/>computer scientists who can play an important role in cyberinfrastructure as process-builders. <br\/><br\/>Cyberinfrastructure extends the reach of researchers and educators past the capabilities, challenges, and <br\/>limitations of their home environment.. Cyberinfrastructure provides a vehicle for accessing additional <br\/>infrastructure that benefits users significantly in their research and education efforts. SDSC will be a core <br\/>resource for cyberinfrastructure through highly valued, applied, and multi-disciplinary expertise and <br\/>community leadership; large-scale data and computational resources; and robust, professional, user-<br\/>focused software. SDSC will provide: <br\/><br\/>o A broad spectrum of cyberinfrastructure software and services that enable users to coordinate <br\/>available technological tools in their home environment with the larger landscape of cyberinfrastructure resources and impact their science efforts. <br\/><br\/>o A Synthesis Center in partnership with the California Institute for Telecommunications and <br\/>Information Technology (Cal-IT2) [1] for leveraging infrastructure across projects and disciplines. <br\/>o Intensive strategic collaboration projects among domain scientists and computer scientists that <br\/>bridge science and technology goals. <br\/><br\/>o An Advanced Cyberinfrastructure Laboratory that will be a flexible environment to support research, development, experimentation, and evaluation with cyberinfrastructure technology and tools. <br\/><br\/>o A comprehensive Integrated Data Cyberinfrastructure to serve a broad class of existing and emerging data-oriented users. <br\/><br\/>The customers and beneficiaries of cyberinfrastructure are its users, and the ultimate metric of success <br\/>for cyberinfrastructure is the ability to focus on the challenges of the science rather than the challenges of <br\/>the tools in driving new discovery. SDSC will build and deliver key services, infrastructure, and <br\/>innovations that will enhance the ultimate success of cyberinfrastructure. Together with a community of <br\/>collaborators including the National Center for Supercomputing Applications (NCSA), TeraGrid\/ETF, <br\/>project-focused community efforts, and a broad group of domain scientists, engineers and computer <br\/>scientists, SDSC will provide a cohesive combination of expertise, hardware, and software to facilitate the <br\/>transformation of the cyberinfrastructure vision into reality.","title":"SCI: Delivering Cyberinfrastructure: From Vision to Reality","awardID":"0438741","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7475","name":"HEC CORE FACILITIES"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T465","name":"AIR FORCE-HPC MODERNIZATION PR"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T482","name":"ARCHIVES-NARA'S ELEC RECORDS"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T576","name":"ENERGY-ASC ALLIANCE CTR PROG"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T657","name":"ENERGY-SDSC"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T928","name":"ENERGY-SDSC"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T929","name":"NARA-ERA"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"T934","name":"ENERGY-ASCA"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"T021","name":"ENERGY-USE OF DATA STAR & PLAN"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"T155","name":"AIR FORCE-HPC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"T156","name":"AIR FORCE-HPC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"T187","name":"ENERGY-ASCI"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"T275","name":"ARCHIVES-ELECTRONIC RECORDS"}}],"PIcoPI":["562621",262196,"559497",262198],"PO":["525727"]},"104770":{"abstract":"System software, such as operating system (OS) kernels and middleware for distributed systems, is quite difficult to develop and maintain because it is: (1) asynchronous and event-driven in nature; (2) often written in the type-unsafe C programming language; and (3) complicated by numerous data structures, caches, locks, and reference counts, all intended to improve performance. System software also provides the critical infrastructure on which all other applications must run, and should therefore elicit a sense of high confidence from its developers and users. Static analysis techniques have matured to the point where they can catch a number of misuse errors (e.g., incorrect usage of an API), and model checking has proven effective in catching more insidious data-dependent bugs.<br\/><br\/>The goal of this project is to develop advanced model-checking techniques, coupled with OS code instrumentation, to achieve always-on monitoring of system software. The proposed effort will result in the development of compiler and model-checking tools that allow formal modeling and verification to be applied to complex software packages. Embedded in the tools will be heuristics to quantify confidence levels for software components, and methods to balance confidence levels with the intensity of runtime checking. Techniques will be developed to explore models and produce actual test cases from CCMs, possibly feeding results back into model updates, until an equilibrium is reached. Finally, the formal verification of an embedded Linux kernel, which is used in numerous critical systems around the world, will be conducted.","title":"CSR---AES: Runtime Monitoring and Model Checking for High-Confidence System Software","awardID":"0509230","effectiveDate":"2005-07-01","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["417606","543574","451745","552012","531979"],"PO":["493916"]},"105991":{"abstract":"Sensor network is a new computing paradigm that has many potential applications. Although much work has been done in this area, a great deal of effort has been put in simulation and experimental study. Some researchers proposed algorithms for sensor networks, but unfortunately many of them are based on the same assumptions as in traditional distributed computer systems including mobile ad-hoc networks. Sensor networks, however, are very different from the computing systems we have built before in terms of energy concerns, large scale, unreliable communication, etc. Thus, most of the algorithm design foundations are not suitable for sensor networks.<br\/>We have two observations for a sensor network. First, a sensor network is composed of fragile nodes with limited computation capability, operated on limited battery power, and connected via lossy wireless networks. Therefore, due to the energy consumption of transmitting information to a single point, the imprecise knowledge of the network, and the dynamic status of the network, it is prohibitive to run centralized algorithms on a sensor network; instead, a localized algorithm is preferable. Second, since a sensor network consists of a large number of nodes, algorithm analysis is also different from that of traditional computer systems. Communication complexity is important because communication captures the energy consumption. In additon, analysis should not rely on the specific location of each node, but be based on the random distribution of the sensors. The random analysis, instead of the fixed graph structure, should be used for performance evaluation.<br\/>Intellectual merit: We propose to address those theoretical challenges by examining the computational<br\/>limitations and capabilities, algorithm design, and performance analysis for sensor networks based on localized diffusion-like operation and random analysis. Specifically, we look into three sensor network problems: clock synchronization, robot navigation and task assignment, and information diffusion in a mobile sensor network. The first problem is a classic topic for distributed systems, which has attracted much attention for the past several decades. By solving it, it can help us to understand the fundamental limitations and capabilities of a sensor network. The second application addresses one of the most important aspects of a sensor network: data dissemination. We design localized, fault-tolerant, and very simple algorithms for the above two problems. The third one estimates the speed for information diffusion in a random network, which is very important in analyzing the performance for a localized algorithm. The analysis techniques will benefit the algorithm design and analysis for other problems in sensor network applications and infrastructure design.<br\/>We believe our effort is a first step toward understanding sensor networks, and exploring how to design and analyze algorithms for sensor networks. Much theoretical work has done on general networks, but most relies on the assumptions that are not appropriate for sensor networks. Localized and fault-tolerant algorithm design and analysis are very important and promising for the future prevalence of sensor network deployment. This research will help to solve and answer the fundamental problems and limits in sensor networks, for example, clock synchronization, data dissemination, information diffusion, and so on.<br\/>Broader impact: The project will integrate research and education by introducing sensor network and more advanced algorithm design techniques to the students. It will help to supplement one undergraduate network course and design two graduate level courses. Results from the project will be disseminated via conferences, journals, and the Internet. Furthermore, this project will stimulate the collaboration with people from various disciplines, e.g., networking, computational geometry, online algorithm, matrix analysis, robotics, and so forth.","title":"Localized Algorithm Design and Analysis","awardID":"0514985","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["551983"],"PO":["562944"]},"102130":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Developing the Lemur Toolkit into a Community Resource <br\/>Proposal: CNS 0454018<br\/>PI: Jamie Callan<br\/>Institution: Carnegie-Mellon University <br\/><br\/>The PI will build on the Lemur Information Retrieval Toolkit, a researcher oriented toolkit developed at the University of Massachusetts and Carnegie Mellon (See http:\/\/www.lemurproject.org\/) to create a new community resource that a broad range of computer scientists can use. Uses of the resource include information retrieval research and education activities such as search engines, text retrieval, indexing, and user interfaces. The project will add capabilities need by human language technology researchers. It will address usability, resource needs, and documentation. Compute science education will be fostered by creating tools that can be used in classes. A large-scale server with Internet access via client-server programs will support a wide range of institutions. This will be an open-source project enabling a wide community to engage in improvements and use.","title":"CRI: Developing the Lemur Toolkit into a Community Resource","awardID":"0454018","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["507668","541869"],"PO":["563751"]},"104792":{"abstract":"This project addresses the problem checking if a system's behavior meets its specification. The technical focus areas of the project are to integrate computing and networking to support reliable, complex distributed systems and to enable data driven applications for predicting system behavior. Specifically, the techniques will enable an assessment of the probability of systems failure due to software errors. Large-scale distributed systems and sensor networks have a crucial role in modern society. However, such systems are highly prone to failure because of software errors. This project will provide powerful ways of debugging and monitoring such systems to reduce errors. Specifically, it will provide ways of rigorously relating the tests to specifications and extending their coverage. This will enable software developers to catch errors in coding due to interactions between components in distributed systems. Moreover, the project will focus on efficient ways of distributed monitoring of systems so as to provide for timely checking while conserving communication bandwidth. A goal is to get the techniques to be sufficiently efficient that they can be applied to networks of embedded systems. The project will build new tools and experiment with applications in order to test the ideas.","title":"CSR---SMA: Dynamic Analysis and Control for Robust Scalable Open Distributed Systems","awardID":"0509321","effectiveDate":"2005-07-01","expirationDate":"2009-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["460466","549774","553664"],"PO":["493916"]},"107740":{"abstract":"A central tenet of molecular biology is that a protein's function is determined by its structure. The Protein Structure Initiative (http:\/\/www.nigms.nih.gov\/psi\/) and other recent efforts have targeted the accurate determination of protein structures for all genes encoded in genomes. The result has been a rapid increase in the number of proteins for which the 3D structure is known, which has enabled a new computational approach to the study of protein structure and function based upon recurring amino-acid packing patterns or spatial motifs in a collection of known protein structures. These spatial motifs may correlate with experimental measurements of protein function or with specific protein families. Our preliminary work supports the premise that such spatial motifs may be a more suitable starting point for protein function research than sequence level motifs. In this project, we propose to undertake a comprehensive analysis of protein structures. We will mine the protein structures available in the PDB (Protein Data Bank) for spatial motifs, and construct each protein's signature as a combination of such motifs. Similarity measures between the signatures can serve as the basis of various predictions of protein structural and functional classifications. We will look for family specific motifs (measured by enrichment significance) and significant associations between occurrences of spatial motifs.<br\/>This project will integrate novel techniques to link recurring structural patterns in protein families with protein function. The proposed studies will have significant impact on modern structural biology. Recent studies, including those conducted by our group, indicate that protein spatial motifs can be used for effective protein annotation, comparison, and classification. It will advance the frontiers of biology research by providing a novel, high-throughput mechanism for discovering, evaluating and annotating functionally significant spatial motifs derived from protein classifications - making a fundamental impact on the way biologists propose and test hypotheses that relate protein structure and function. Broader Impacts of this research include interdisciplinary collaboration and training, a multitude of educational impacts, and outreach to underrepresented minorities in the sciences.","title":"BioComp: Identifying Spatial Motifs for Classification of Protein Structure and Function","awardID":"0523875","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["426282","501061","234536"],"PO":["565223"]},"104594":{"abstract":"ABSTRACT<br\/>Proposal ID: 0508506<br\/>Title: NER: Exploratory Research in Molecular Communication between Nano-machines<br\/>Institution: University of California at Irvine<br\/>PI name: Tutsuya Suda<br\/><br\/>This project explores the possibility of molecular communication as a solution for communication between nanomachines. Nanomachines are artificial or biological nano-scale devices that perform simple computation, sensing, or actuation. Molecular communication provides a mechanism for nanomachines to communicate over a short distance (adjacent nanomachines to tens of micrometers) using molecules as a communication carrier.<br\/><br\/>The class of molecular communication systems that the PI considers in the project consists of sender nanomachines, receiver nanomachines, carrier molecules, and the environment that these operate in. Senders and receivers include biological (such as cells or bacteria) and biologically derived (such as molecular motors or sensors taken from biological systems) nanomachines that are capable of emitting and capturing molecules. Carrier molecules are proteins, ions, or DNA. The environment is the aqueous solution that is typically found within and between cells.<br\/><br\/>In the project, the P.I. develops research ideas for molecular communication, designs a few possible molecular communication systems, evaluates the feasibility through simulations, empirical studies, collaborates with researchers in the field to identify promising options, and develops a full proposal to submit to NSF for the next year's competition.<br\/><br\/>Intellectual merits and broader impacts of the proposed projects are the following:<br\/>Creation of new research area: Researchers have so far focused on understanding biological nanomachines and artificially creating counterparts of biological nanomachines. In this project, the P.I. investigates molecular communication as a solution for nano-scale communication between nanomachines. Using molecules as communication carriers is new, and creating such paradigm shifting research is intellectually challenging and rewarding.<br\/>Interdisciplinary research: The project is interdisciplinary and involves biotechnology, nano technology and information technology. This is intellectually challenging and rewarding.<br\/>Creation of new applications for bio and nano technologies: Communicating nanomachines enables a new set of applications. If multiple nanomachines communicate, they may cooperate and perform complex tasks such as nano-scale sensing, molecular computing, or nanomedicine. The proposed research enables creation of such important applications, and its impact is broad.<br\/>Creation of new courses at the graduate program level: The project produces a broad impact on education by identifying a new area of science and technology. The proposed research leads to the creation of new courses of the multidisciplinary nature at a graduate program level that provides a broad scope of nano scale science.","title":"NER: Exploratory Research in Molecular Communication between Nanomachines","awardID":"0508506","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1676","name":"NANOSCALE:  EXPLORATORY RSRCH"}}],"PIcoPI":["409909"],"PO":["562984"]},"102174":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: DEter Cyber Community of Researchers <br\/>Proposal: CNS 0454381<br\/>PI: B. Clifford Neuman<br\/>Institution: University of Southern California <br\/> <br\/>This project will expand the DEter testbed to enable improving its usefulness for science research, conducting larger experiments, and expanding the research and education community that can access it. DEter is a shared laboratory for researchers from academia, government and industry to experiment with cyber security technologies under realistic conditions. The testbed provides tools and resources that enable repeatable experiments and comparisons of approaches. DEter has strong security, contained experiments and usage policies for its use in research on computer network attack and defense mechanisms. Broader impacts of the project include improvements in cybersecurity for computing and critical infrastructure. Educational impacts will also follow from the use of DEter in hands-on projects at colleges and universities.","title":"CRI: DEter Cyber Community of Researchers","awardID":"0454381","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[269383,"561510"],"PO":["539087"]},"108962":{"abstract":"This SGER proposal is aimed at developing a wireless sensor network, consisting of both acoustic and seismic sensors, for monitoring eruptions of active and hazardous volcanoes. We intend to develop and deploy a wireless sensor array at Volc'an Tungurahua, an active volcano in central Ecuador, in July-August 2005. Wireless sensor networks have the potential to greatly enhance our understanding of volcanic activity.<br\/><br\/>This project will develop new wireless technologies for monitoring volcanic activity, which will lead to a richer scientific understanding of volcanic processes. The proposed large-aperture seismoacoustic sensor array will be deployed on the upper flanks of Tungurahua volcano, in an area that would otherwise not be readily accessible for multi-station data acquisition. The improved station coverage is in a critical (near-field) zone of the volcano and will enable much improved determination of source locations and mechanisms for explosions and other earthquakes in the shallow subsurface and volcanic conduit.","title":"SGER: A Wireless Sensor Network for Monitoring Volcanic Eruptions","awardID":"0531631","effectiveDate":"2005-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["297596"],"PO":["292741"]},"107873":{"abstract":"Self-assembly is the process by which small ``components'' spontaneously form intricate aggregate structures. DNA self-assembly is a key tool for nano-technology, nano-robotics, and molecular computation. More generally, biological organisms are self-organized chemical systems that carry out algorithms encoded in the genetic material, DNA. Biology thus provides clear proof that autonomous chemical systems can be programmed, and that they can function reliably on a grand scale -- biological organisms can be composed of as many as 1024 molecular components!<br\/><br\/>Recent experimental advances in the synthesis of artificial molecular systems have demonstrated that it is possible to program molecular self-assembly to carry out rudimentary logic. These experiments also suggested that the occurrence of errors is a major obstacle to scaling up DNA self-assembly and biologically inspired computation. This project will devise algorithmic tools and analysis techniques for error-correction and error-suppression in biologically inspired self-assembling and computational systems. It is our hope that our research will facilitate sophisticated tasks such as counting, growing molecular assemblies of pre-specified sizes (no larger, no smaller), and pattern recognition of complex chemical signals using inherently error-prone biomolecular operations at the nano-scale.<br\/><br\/>In order to design and analyze our error-correction mechanisms, we will use high level models which are both sufficiently realistic to be useful and sufficiently abstract to be amenable to analysis. The basic elements of our models will be DNA tiles, transcriptional circuits, and DNA hybridization catalysts. Thus, our research will target assembly of and computation with large molecules such as long chains of DNA rather than smaller molecules such as proteins and amino acids.<br\/><br\/>We will also develop course material on the basis of our research which will be taught at Caltech and at Stanford.","title":"NANO: Collaborative Research: Algorithmic error-correction in biologically inspired self-assembly and computation","awardID":"0524783","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["429621"],"PO":["565223"]},"109512":{"abstract":"Recent successes in computer vision, machine learning, and combinatorial optimization are leveraged to tackle once more the image interpretation problem as defined in the early days of computer vision. Interpretation is cast as a problem of simultaneous image segmentation and region classification: Given an image and a list of class labels, the goal is to compute the most probable image segmentation and labeling, one label per segment. This is learning in context in that learning techniques are used to recognize several objects in the context of complex, cluttered images<br\/><br\/>A manual image labeling method is proposed that enlists the help of both web surfers and the students in a junior-high school and a high school to tackle this labor intensive task, while at the same time exposing young pupils to computer vision research.<br\/><br\/>The proposed work has intellectual merit of relevance to the fields of computer vision, artificial intelligence, and cognition in general. In particular, the notion of defining ``words for pictures'' that this proposal offers may establish a new, fruitful bridge to text retrieval research and widen the discourse computer vision has been entertaining with other areas of science.<br\/><br\/>The understanding of visual perception in its more semantic sense of ``image interpretation'' will undeniably have a broader impact on society. From a practical point of view, image understanding systems are useful for information retrieval, surveillance, medical imaging, and in many other endeavors. In addition, the proposed activities include collaboration with industry and government agencies and involve postdocs, graduate and undergraduate students. These activities also explicitly involve younger pupils in grades 6 through 12, and will hopefully help attract them to computer vision.<br\/><br\/>0535152\/0535166<br\/><br\/>This project addresses the problem of category-level object recognition in images: Its aim is to develop effective methodologies for representing object classes; learning the corresponding object models from cluttered sample images in a semi-supervised manner; and efficiently and robustly recognizing instances of these models in novel images despite clutter, occlusion, viewpoint and illumination changes, and individual variations within each class. <br\/><br\/>Intellectual Merit. The scientific objective of this project is to develop a representation of the salient parts of an object and their relationships that can effectively be learned from<br\/>heavily cluttered data in a weakly supervised way, correctly captures within-class variability and appearance changes due to variations in viewpoint and illumination, and effectively supports inference over object models and the automated construction of efficient classification machines.<br\/><br\/>Broader Impacts. This project will investigate applications of category-level object recognition to image retrieval, video annotation, human-computer interaction; surveillance and security; and robotics via international academic and industrial collaborations. Contributions to education and outreach will include training PhD students and post-doctoral researchers, and involving underrepresented groups in graduate research and undergraduate data collection and empirical evaluation projects.","title":"Visual Learning in Context","awardID":"0534897","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["513187"],"PO":["500200"]},"106047":{"abstract":"This research investigates the role of feedback in communication, the intrinsic loss of information in the joints of a communication network, and the use of quantum entanglement as a communication resource for the quantum multiple access channel.<br\/><br\/>Approaches to the unsolved problem of Gaussian feedback capacity (using convex duality, optimal filtering, and Schalkwijk-Kailath style feedback correction to obtain capacity) and to the Gaussian relay channel (using binning in an optimal way to make optimal use of the downlink capacity) are being pursued. Applications include sensor networks.","title":"Backward and Forward Information Flow in Networks","awardID":"0515303","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["321034"],"PO":["432103"]},"104771":{"abstract":"The main objective and intellectual merit of this project lies in developing <br\/>a new paradigm and run-time system for distributed computing in sensor networks, <br\/>called environmentally immersive programming. Environmentally immersive <br\/>programming is an object-based paradigm that allows integration of objects <br\/>that live in physical time and space as components in the computational <br\/>environment of an application. A new middleware, called EnviroSuite, is <br\/>built to demonstrate the first working environmentally immersive programming <br\/>system. Geared for environmental monitoring applications, this programming <br\/>system allows virtual objects to be dynamically instantiated and logically <br\/>attached to selected external entities or conditions in the physical environment<br\/>for monitoring purposes. Each such object encapsulates the monitored entity's <br\/>aggregate state, performs entity-specific computation, and serves as the <br\/>virtual communication port of the particular physical entity in the application <br\/>programmer's world. An important part of the proposed work is to address the <br\/>problem of unique representation of physical entities that are external to <br\/>the computing system, but whose presence is inferred via sensory input. The <br\/>complexity of this problem arises from the need to maintain consistency and <br\/>persistent state in the face of external object movement, message loss and <br\/>sensor false alarms. The utility of the middleware developed in this project <br\/>is demonstrated on realistic sensor network testbed using current sensor node<br\/>hardware.","title":"CSR--EHS--Environmentally Immersive Programming: A New Programming Paradigm for Deeply Embedded Systems","awardID":"0509233","effectiveDate":"2005-07-01","expirationDate":"2006-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550342","553633"],"PO":["561889"]},"105992":{"abstract":"Many optimization problems are NP-hard, and computing approximate solutions is an attractive way <br\/>to cope with NP-hardness. The effort to understand the approximation properties of NP problems has <br\/>occupied the center stage of theoretical computer science in the past decade. Despite many successes in this field, the status of some of the basic problems ---- metric tsp, vertex cover, graph coloring, sparsest cut etc.---is still open. The project consists of designing new approaches for computing approximate solutions to these problems. Any results for these central problems should generalize to many other problems.<br\/>The tools used involve sophisticated geometric arguments, and \"lift and project\" technique from polyhedral combinatorics. Another goal is to develop a comprehensive framework for designing approximation algorithms without relying on semidefinite programming (SDP). Many recent approximation algorithms<br\/>use SDP, which is not particularly efficient in practice. The goal in this project is to replace SDP with simpler algorithms based upon eigenvalue computations. Another aspect of the project is to prove lowerbounds to complement any new algorithms, or to rule out the existence of some of the above algorithms. The lowerbounds attempted would be both for all polynomial-time algorithms ---this would <br\/>use PCPs---and for specific algorithms arising from lift and project methods. (The latter consists of <br\/>viewing lift and project methods as a weak computational model.) <br\/><br\/>Broader impact of this project include dissemination efforts such as new innovative courses in graduate and <br\/>undergraduate education, new text book, and survey articles on current research.","title":"New directions in Approximation Algorithms for NP-hard problems","awardID":"0514993","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["549429"],"PO":["348215"]},"105541":{"abstract":"This project will develop a novel system to investigate and analyze many important aspects of cumulus cloud dynamics, cloud evolution, and precipitation formation to an extent that has previously been impossible. Clouds and precipitation affect our daily lives, personal safety, commercial decisions, and our future sustainability on Earth. Clouds and precipitation are important at all regional scales: local, state, national, and global. For example, clouds influence the daily maximum and minimum temperatures over our homes and they modulate the global temperature by affecting the amount of incoming solar radiation and outgoing longwave radiation. As the inhabitants of earth become increasingly concerned about global warming and climate change on global and regional levels, it is necessary to understand the roles of clouds and precipitation in the Earth System in order to predict the future state of our planet.<br\/><br\/>However, understanding and predicting atmospheric phenomena are very difficult tasks which require the measurement and modeling of properties on a wide variety of scales (cloud scale, storm scale, mesoscale, globally), fusion of computational model data, measured data, and the simultaneous fusion of hundreds of scalar and vector fields that vary over time. Current tools for atmospheric visualization are not capable of integrating these various data sources, communicating the complex three-dimensional, time-varying information necessary to accurately understand and predict atmospheric events, or for the integration of visual representations into the scientific analysis and discovery process. <br\/><br\/>This project will provide a fundamental advance in visualization and interaction techniques to solve these multiscale, multifield, data fusion, analysis, time-critical decision making, and interaction problems. These new multiscale, multifield, atmospheric visualization tools will: incorporate novel, effective, photorealistic and illustrative multifield visualization techniques; fuse observational and model data; improve the understanding of cloud dynamics, cloud evolution and precipitation formation; create effective multiscale visual representations; be rapidly deployed for research, training, and education; and produce an environment for actionable, comprehensive and efficient visual analysis. <br\/><br\/>Both computer science and atmospheric science research challenges addressed in this project will benefit other fields by: <br\/><br\/>1. Improving understanding of cumulus entrainment and warm rain formation, leading to better parameterizations in weather forecasting models and possibly global climate models.<br\/>2. Improving training of students and atmospheric scientists to perform their science in three dimensional environments.<br\/>3. Unifying access to co-registered model and measured data across multiple scales, greatly improving the understanding of the atmosphere, and advancing atmospheric models and weather prediction.<br\/>4. Creating a fused, comparative visual analysis environment to reduce the ambiguity inherent in the use of a variety of data sources by calibrating multiple, measured and simulation data sources.<br\/>5. Creating a physically plausible, parameterized database of canonical cloud models for use in atmospheric science research, rendering research, illumination simulation and validation (e.g., headlamp visibility in various weather conditions) and in the visual effects industry.<br\/>6. Developing a new architecture and visualization tools for large scale, multiscale, multifield data integration, fusion, analysis, and experimentation for use by the larger atmospheric science community.<br\/>7. Developing modules for educating high school and undergraduate students about the principles of cloud and precipitation formation. <br\/><br\/>The techniques to be developed will significantly change the state-of-the-art of visualization and large-scale data analysis, and have a dramatic impact on many fields using multifield, multiscale data, including computational fluid dynamics, biology, medicine, astrophysics, and nanoscale-microscale integration. Advanced information communication through advanced visual analysis tools will increase the rate of scientific discovery by improving the effectiveness of scientists and forecasters.","title":"Collaborative Research: An Advanced Interactive Multifield, Multisource Atmospheric Visual Analysis Environment","awardID":"0513212","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}}],"PIcoPI":["558600"],"PO":["565136"]},"105794":{"abstract":"Mobile wireless communication is becoming the enabler of Internet access. Unfortunately, mobile networks are not equipped to provide the high-speed access that Internet users expect due to fundamental problems associated with wireless communication: scarce bandwidth and poor quality. To overcome these problems, this research develops a family of algorithms that allow the transmitter to respond to changes in the propagation channel. The major innovation in this work is the development of new methods for compressing information about the propagation channel. This information is sent from the receiver back to the transmitter to help the transmitter adjust the transmitted signal. <br\/><br\/>The approach to this research is to develop a framework for source coding with a new source: the wireless channel. This differs from traditional source coding in that the objective is to improve communication theoretic system performance as opposed to improving the fidelity of the reconstruction. The objectives of the research are to determine what channel state information should be quantized; develop algorithms for quantizing the essential parameters of the channel; derive suitable communication theoretic notions of fidelity of the quantization such as mutual information and bit error rate; characterize the tradeoff between feedback rate and network performance; and confront practical issues introduced by estimation error, errors and delay in the feedback channel, and implementation constraints.<br\/><br\/>The broader impacts of the work are expected in diverse areas including research in the form of new algorithms, theoretical results, and insights; industry through mobile network applications developed by industry partners; and education through better trained engineers, research experiences for minority students, mentoring, and enhanced learning worldwide thanks to publicly available courseware.","title":"Collaborative: Quantizing Wireless Channels","awardID":"0514194","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["550451"],"PO":["432103"]},"105684":{"abstract":"Determining the evolutionary relationships among a group of organisms is one of the major goals of integrative historical biology. In the century following Darwin , morphological structures were almost the sole source of data for a variety of analyses aimed at resolving this problem. Over the past few decades, molecular systematic techniques have taken center stage in the study of extant species, usually supporting but frequently overturning results based on morphology. In paleontology, however, morphological data (the shapes of fossils) remain effectively our only information<br\/>describing the evolutionary position of extinct organisms. The application of 3D techniques of data collection and analysis, as well as computer-based visualization methods, to data sets including both extant and extinct species will now permit improved levels of interpretation in paleontology. The team will develop new tools in each of these areas, using Old World monkeys as a \"test-bed\" or model organism, to approach significant problems in both computer science and the broad domain of comparative biology. New advances in the analysis and manipulation of surfaces and volumes in three-dimensions in computer graphics, visualization and computer vision can be applied to these problems. Simultaneously, collecting and interpreting morphological data for paleontology is a novel application for pushing the development of techniques in computer science.<br\/><br\/>analysis will be based on a large existing database of three-dimensional data (mostly skull surfaces) at the American Museum of<br\/>Natural History.<br\/><br\/>The interactive graphics, visualization and statistical analysis tools we propose are ever more widely needed as the amount of three-dimensional morphology data increases. The close interaction of geometric morphometrics and computer graphics will lead to new ideas about the representation of shape. In addition, the project develops new approaches to the problem of integrating morphology with molecular data in the study of evolution, applicable in many parts of the tree of life. In addition, with massive amounts of new data, new processing and analytic software, and new approaches to integrating morphology, answers to specific questions about the evolution of African monkeys might be obtained.<br\/><br\/>A large part of the project will be done at Lehman College of CUNY, a minority-serving institution in the Bronx, and minority undergraduates are already involved in the research. The software tools we will develop are sorely needed and will become part of the scientific infrastructure. The visualizations will form a basis for sharing research in evolution with the general public.","title":"SEI(SBE): Collaborative Research on Visualization of Evolutionary Transformation using 3D Morphometrics: African Monkeys as a Test Case","awardID":"0513660","effectiveDate":"2005-07-01","expirationDate":"2010-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1629","name":"BE: NON-ANNOUNCEMENT RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7602","name":"INFORMATION INTEGRATION"}}],"PIcoPI":["417227","485996","485997","437043","485998"],"PO":["565136"]},"105696":{"abstract":"The design and development of powerful mechanisms for managing and mining large datasets of moving objects is an emerging direction in science and engineering informatics. The subject becomes increasingly important as the world, as well as national security threats, become more mobile. This project is investigating the issues related to the design and development of innovative methods for querying, analyzing, and mining of spatiotemporal information to find typical characteristics of the moving objects' trajectories, and uncover suspicious motion in large datasets of moving objects. The moving objects datasets are in the form of either stored data or transient data streams. The project is designing and implementing the MotionEye system prototype which consists of four subsystems: MotionQuest(DB) and MotionQuest(Stream) for querying and hypothesis validation in moving objects databases and data streams respectively, and MotionMine(DB) and MotionMine(Stream) for data mining in moving objects databases and data streams respectively. The project is investigating efficient and effective approaches to the implementation of these subsystems. The project is also striving to ensure that the developed technology will not sacrifice individual privacy. The project will enable the development of more advanced information systems in homeland security, law enforcement, traffic control, and other domains that deal with moving objects.","title":"SEI(IIS): MotionEye: Querying and Mining Large Datasets of Moving Objects","awardID":"0513736","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["557651"],"PO":["563727"]},"102187":{"abstract":"Abstract<br\/>Proposal: CNS 0454437<br\/>PI: Kara L. Nance<br\/>Institution: University of Alaska Fairbanks<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: ASSERT: Advanced System Security Education Research and Training Laboratory<br\/><br\/>This planning award will enable the PI's team to plan for a regular proposal to support research in information assurance. The planning funds will enable the PI's to visit other lower 48 labs such as Purdue, West Point, and University of Southern California to enable them to plan a facility that complements existing capabilities in information assurance research. The PI's will work with other disciplines at the university, state government, the private sector and other stakeholders to plan a research activity that serves the unique needs of Alaska. The project builds on a prior NSF Collaborative Capacity Building Grant that enabled the institution to obtain certification as an NSA Center of Academic Excellence in Information Assurance Education. The planned full proposal will build on the education activities and resources developed under that award.","title":"CRI: ASSERT: Advanced System Security Education Research and Training Laboratory","awardID":"0454437","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["528503","528504"],"PO":["539087"]},"109722":{"abstract":"This SGER (Small Grants for Exploratory Research), modeling auditory processing in children with learning disabilities and its corresponding treatments, builds, validates, and experiments with computational models the processing of audio signals in the auditory nervous system of the human brain. With a long term goal for capturing into a computational model the knowledge related to the acoustic processing streams acquired by a speech and hearing scientist through experiments with the human auditory system, the short-term goals include:<br\/>Understanding what portions of the auditory processing system ought to be included in the model, building a neural network model in consultation with researchers in speech and hearing sciences, and Validating the model by running computer simulations against experimentally observed data. Such a model is expected to predict brainstem responses to audio signals that come in via the human ear and partition into its finer spectral components in the cochlea from where the impulses travel via the auditory nerve and several specialized nuclei in the brainstem to the auditory cortex. The fact that the audio signals split into two distinct \"what\" and \"where\" streams in the auditory cortex or en-route to it motivates the research. Although many computational models of the auditory system exists, the novelty of the proposed research lies in<br\/>Including the ventral and dorsal streams in the computational model and Validating the model against auditory brainstem responses observed in children with learning disorders.<br\/><br\/>Broader Impact: A validated model \"opens the door\" to study the impact of proposed treatments for children with learning disabilities based on auditory deficiencies. Using this approach, computer approximations could serve as a fast and relative inexpensive precursor to possible treatments. Students will also gain expertise in the area.<br\/><br\/>Intellectual Merit: This research will expand the discipline of neuro-computational modeling and should contribute in understanding the causes of learning disabilities in children, thus enabling development of the corresponding treatments.","title":"SGER: Computational Models to study auditory processing and learning disorders in children","awardID":"0536258","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"5761","name":"INDUSTRY\/UNIV COOP RES CENTERS"}}],"PIcoPI":["364273"],"PO":["557609"]},"109854":{"abstract":"ABSTRACT<br\/>0537160<br\/>Sandeep K. Shukla<br\/>Virginia Polytechnic Institute @ State University<br\/><br\/>Today's computing is no longer limited to servers, work stations and desktop computing, but they are embedded everywhere, most importantly in the medical space, as well as in the safety critical systems such as avionics control, automotive control etc. However, historically hardware and software have been built separately, and both being extremely complex, the methodologies that ensure correct construction of hardware or software are themselves research topics. Yet, we are faced with the need for formal and mathematical techniques to model and analyze systems which integrate both hardware and software,<br\/>because of the increasing reliance of human society on ubiquitous and pervasive computing.<br\/>In 2003 the First International ACM\/IEEE Formal Methods and Models for Co-Design (MEMOCODE) conference, for the first time, brought together researchers who apply formal methods in software engineering fields, and those who apply formal methods to design and validate hardware systems. It was felt by attendees that formal method practitioners from both sides have a lot to share in terms of knowledge, experience and techniques. In 2004 the Second International ACM\/MEMOCODE was held in San<br\/>Diego, California. It was emphasized that the hardware-software co-design problem imposes an ever than before need for this symbiosis, which was the original aims of creating this series of conferences. More importantly, the US researchers present at the conference last two years felt that there is a lack of research activity in this important area of formal methods in the US, compared to the initiatives and research activities in Europe.","title":"CPA: Conference Support: 3rd ACM\/IEEE International Conference on Formal Methods and Models for Codesig; Universite of Verona, Italy","awardID":"0537160","effectiveDate":"2005-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["384407"],"PO":["564388"]},"109755":{"abstract":"This project aims to involve the wireless and mobile network research communities in a discussion of new architectures and disruptive technologies for the future Internet, leading to a white paper with future R&D recommendations. The objective is to solicit service requirements and innovative network architecture concepts from wireless, mobile and sensor net researchers at universities and industrial\/government research labs, and to consolidate this input into a coherent vision\/agenda for future research and experimental infrastructure needs.","title":"Planning Grant: New Architectures and Disruptive Technologies for the Future Internet - A Wireless & Mobile Network Community Perspective","awardID":"0536545","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["558959","564746"],"PO":["565090"]},"107698":{"abstract":"Evolution has a remarkable ability to design organisms possessing novel features that enable them to survive, and even thrive, in challenging environments. To a human observer, the biochemical and physiological complexity of living organisms is amazing, leading some to disbelieve that such biological designs could have been accomplished by evolution alone. Considering the levels of biocomplexity found throughout nature - the development of an embryo from a single-cell; the delicate balance of an ecosystem in a rainforest; all the way to the reasoning power of the human brain, complex and elegant adaptive designs abound. As we slowly understand more details about how evolution produces such complex traits, it becomes clear that this is a powerful constructive force that we need to fully understand in order to harness its potential in solving difficult computational design problems.<br\/>The twin goals of this project are to learn more about the mechanisms by which evolution produces innovative complex features, and then to apply this newfound understanding to develop a new generation of evolution-based algorithms for solving computational problems. In the process, we will use a mathematical framework for the study of complexity in biological systems to better understand the natural design process. We will start this work by answering fundamental questions in evolutionary biology and ecology including: Why do complex designs arise faster in multi-species ecosystems? If organisms have the capacity to modify their environment and communicate with one another, will this spur open-ended complexity growth? and How can we harness these forces to solve design problems?<br\/>The evolving system that we use in this study is an artificial one, based on self-replicating computer programs, that nonetheless exhibit the key characteristics of natural evolving systems - most notably the ability to produce novel complex traits and innovative solutions to problems. These \"digital organisms\" exist in a user-defined computational environment, and their genomes are composed can theoretically perform any computable mathematical function. Indeed, we have witnessed a wide variety of unexpected and innovative adaptations arise through evolution in Avida. This system allows us to explore fundamental questions in evolutionary biology with speed, detail and precision that would not be possible in any natural system. Ideally it is a tool that will allow us to fully understand the origins of biocomplexity, and constructively make use of these same forces that were able to produce human intelligence in the natural world.","title":"BIC: EMT: Reimagining Evolutionary Computation","awardID":"0523449","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["425363","548851"],"PO":["565223"]},"106015":{"abstract":"Broader impact We seek a better understanding of the information transfer underlying price determination<br\/>in economic activity. Economic activity can be seen as comprising complex chains of exchanges, or markets, for which money and prices are the essential lubricant. Our working assumption and motivation is that a collection of more or less simple procedures underlie the actions of participants in the actual economy. Accordingly, the goal of this work is to better understand price adjustment mechanisms that operate<br\/>independently on each price yet achieve a coordinated outcome. The proposed research has two main parts.<br\/>I. To explain why price changes in market economies might work effectively by showing simple distributed<br\/>algorithms that cause prices to converge quickly toward equilibrium values.<br\/>II. To consider price (toll) setting in networks, motivated in part by routing and road networks.<br\/>Intelectual merit In both areas, the focus will be on tatonnement-style algorithms. These are methods in<br\/>which each price is adjusted independently. Local algorithms are of particular interest; these are algorithms<br\/>in which the price adjustment is based only on information available to the corresponding price setter. A<br\/>prime obstacle to such methods is the 1978 lower bound of Saari and Simon which seemingly shows that such<br\/>algorithms have unduly high information requirements. The approach used in this work sidesteps that lower<br\/>bound, by using variable rate updates. The following is a summary of the issues that will be investigated.<br\/>I. MARKET PRICES<br\/>Continuous Markets: Develop fast, convergent, local algorithms, using updates based on excess demand.<br\/>Discrete Markets: What is the effect of indivisibility of goods and discreteness of prices on such<br\/>algorithms?<br\/>Markets over Time: How can time be incorporated into the model, and how does this affect the properties<br\/>of the tatonnement algorithms?<br\/>Production: How does incorporating production into the model affects the properties of tatonnement<br\/>algorithms?<br\/>Markets as Networks: Is it useful to view trading relationships as a graph?<br\/>This approach based on local, variable updates shows potential with regard both to enabling asynchronous<br\/>price updates, and yielding a dynamic algorithm (that is one that can respond effectively to external changes,<br\/>such as in utilities or resources).<br\/>II. NETWORK PRICES. Here, the situation is fairly well understood when there is full knowledge of user<br\/>utilities. In this work, the focus will be on the situation when these are unknown or changing. This work<br\/>asks:<br\/>When can tolls be set effectively based on user behavior as manifested in congestion?<br\/>Local tatonnement algorithms are again a main interest, although more global algorithms, corresponding to<br\/>a single network owner, will also be considered. This work will examine a variety of other issues including:<br\/>Types of user traffic: e.g. elastic versus inelastic traffic, and splittable versus unsplittable demands (a<br\/>demand is a single user's traffic).<br\/>Measures of traffic quality: rates of flow versus transit delays.<br\/>Network mechanisms: In particular, the use of buffering.","title":"Information, Prices, Markets, and Local Algorithms","awardID":"0515127","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["518072","450928"],"PO":["499399"]},"101010":{"abstract":"Distributed real-time embedded (DRE) systems play a crucial role in many mission-critical applications such as disaster recovery command and control, power grid management, and agile manufacturing. These applications have stringent requirements for end-to-end timeliness and availability whose assurance is essential to their proper operation. In recent years, many DRE systems become open to the Internet and volatile physical environments where system workloads may vary significantly at run-time. Such systems require a paradigm shift from classical real-time computing to adaptive solutions that handle workload variations dynamically. This CAREER research develops adaptive Quality of Service control, a control-theoretic framework for adaptive DRE systems. This framework includes a suite of adaptive strategies and algorithms, formal models and analysis techniques, and a middleware architecture that integrates multiple adaptation strategies via distributed software feedback control loops. This research will significantly impact the field of DRE systems as it will enable DRE systems to achieve and maintain critical performance assurances despite dramatic changes in operational conditions and system failures. As a result, mission-critical DRE systems will be able to provide significantly more robust and reliable services in a broad range of highly unpredictable environments.","title":"CAREER: Adaptive Quality of Service Control in Distributed Real-Time Embedded Systems","awardID":"0448554","effectiveDate":"2005-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}}],"PIcoPI":["560533"],"PO":["535244"]},"102154":{"abstract":"The computer science department at the University of Texas at El Paso will acquire a department infrastructure of computers and network equipment that will support the group's principal research areas in high-assurance systems and high-performance computing as well as a broad range of educational programs. Research in high-assurance systems includes computer security topics such as network protocols, encryption, anonymizing data sets, electronic voting and electronic money; software engineering for large-scale systems, software tools for Web Services; and usablility and documentation studies. Research in high-performance computing includes performance analysis, self-organizing systems, and computational chemistry applications. Broader impacts of this project include supporting the department as it begins a PhD program, reaching a significant group of minority computer science students, positioning the department to prepare for NSA designation as a Center of Excellence in Information Assurance, and increasing the capability of the department and its students for experimental research.","title":"CRI: A Reconfigurable Computer Network to Support Research","awardID":"0454189","effectiveDate":"2005-07-15","expirationDate":"2009-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["408502",269319],"PO":["550859"]},"102165":{"abstract":"This project will acquire two high-performance clusters with a high-performance storage subsystem, including servers, workstations, laptops, and PCs for new labs. One cluster can be dedicated to research that build on a stable system platform, while the second can be used for disruptive research and experiments. Each cluster will have 16 nodes, several RAID servers, etc. The proposed infrastructure will support the PIs in their work in several research areas, including Grid computing, sensor networks, mobile computing, and visualization. They also have a number of collaborative projects with units outside the computer science department. The group will join Planetlab to leverage their resources in distributed computing research. Addressing broader impacts, the resources will be available to other researchers at SUNY at Binghamton and will be used in developing new undergraduate and graduate courses.","title":"CRI: Heterogeneous High Performance Infrastructure for Computer Systems Research","awardID":"0454298","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["494776","434747","451774","553586","434750"],"PO":["550859"]},"108502":{"abstract":"This Special Project award gives the Association for Computing Machinery travel funds for undergraduate, graduate student, and MSI faculty to attend the 2005 Richard Tapia Celebration of Diversity in Computing Conference, October 19-20, 2005 in Albuquerque, New Mexico. It gives students an opportunity to interact with the many researchers giving technical presentations at the conference. In addition, the students themselves can present their work in poster sessions and a PhD Consortium. The Tapia Conference thus provides a unique, supportive environment for students from the underrepresented minority groups in Computing Native Americans, Hispanics, and African Americans to engage in intellectual discourse with a range of researchers. It also gives MSI faculty a chance to make technical contacts, and network.","title":"The Richard Tapia Celebration of Diversity in Computing Conference 2005; Albuquerque, NM; October 19-20, 2005","awardID":"0528045","effectiveDate":"2005-07-15","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["292630",286853],"PO":["561855"]},"102089":{"abstract":"Wireless embedded systems are invigorating CISE research areas from operating systems, distributed embedded computing, architecture, and networking to signal processing, algorithms, and data management, and opening up new broad-impact applications ranging from wide area environmental management to biomedical monitoring. These systems are increasingly focused on a particularly powerful and exciting class of deployment: the heterogeneous or tiered sensor network. A heterogeneous sensor network contains nodes with different capabilities, such as tiny, low-power \"motes\" and higher-powered, \"microservers\". Motes are inexpensive and require no infrastructure for long-term deployments, but also extremely constrained in memory, CPU power, and communication. Microservers, in contrast, are more efficient than motes at many computation- and memory-intensive tasks, and more readily interfaced to high-bandwidth peripherals, such as high-rate ADCs and network interfaces; but their higher energy consumption requires power infrastructure, such as solar panels, in long-term deployments. A heterogeneous system containing both motes and microservers can combine the advantages of both devices, using motes to achieve the desired spatial sensing density and microservers to achieve the desired processing power. <br\/><br\/>Wireless embedded sensor systems present the CISE community with an array of intertwined research challenges: real time sensing of complex and diverse phenomena, embedded computing constrained in bandwidth, energy, memory, and storage, controlled mobility, and the autonomous coordination of vast numbers of network nodes. But implementing and testing sensor network applications is daunting even aside from these challenges: many of the constraints that yield low-power, long-lifetime systems also undermine traditional methods for instrumenting and understanding program behavior. Coordinated community infrastructure can thus act as a tremendous research accelerator, and without shared infrastructure, research in the field will be significantly hampered. This project addresses a pressing need: wireless sensor network research demands a concentrated effort to develop a community resource for heterogeneous sensor systems.<br\/><br\/>The investigators will develop a community resource based on Emstar, a highly resilient application methodology for microservers and general heterogeneous deployments. Emstar smoothly combines simulation, emulation, and deployment, leading to qualitatively easier debugging and application analysis. An EmTOS component seamlessly integrates motes and microservers; the EmView visualizer provides unprecedented visibility into wireless sensor network communication patterns.<br\/><br\/>This will build on the investigators Emstar prototype which has proven its value in deployments of heterogeneous networks.<br\/>With expanded functionality, integration, hardening, enhanced usability, and longer-term support, its broad array of tools will become accessible to the computer and information science and engineering community. The project will expand Emstar's flexibility, completeness, robustness, documentation, and programmability; extend its functionality by learning from targeted deployments such as seismic arrays, mobile environmental sensing, and medical informatics; and develop a robust, active Emstar community through workshops, tutorials, and mailing lists. The proposed community resource will act as a tremendous accelerator for research into heterogeneous wireless sensor networks, enabling quick and thorough exploration of socially important applications including environmental monitoring, medical and public health systems, and industrial and civic infrastructure development and management. The project will also support and develop undergraduate and graduate level project courses using Emstar, and explicitly involve an undergraduate research program targeting underrepresented minorities.","title":"CRI: Emstar: A Community Resource for Heterogeneous Embedded Sensor Network Development","awardID":"0453809","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["560926","539241","515835","541961",269121],"PO":["402055"]},"109613":{"abstract":"The objective of this proposal is to facilitate outreach between the software<br\/>engineering and embedded systems research communities by providing travel support for<br\/>outstanding students of one community to participate at the flagship conference of the<br\/>other. The proposed travel stipend intends to improve the mutual awareness of the ACM<br\/>software engineering (SIGSOFT) and embedded systems (SIGBED) research<br\/>communities to the opportunities and challenges emerging in their research areas.<br\/><br\/>Supporting the crossover of ideas between these communities has been an important<br\/>goal. The outstanding scientist and recognized National leader in computer science<br\/>who has been working for this goal until his untimely death in 2004 has been Dr Frank<br\/>Anger, Deputy Director of the Division of Computing and Communication Foundations<br\/>in the Directorate of Computer and Information Science and Engineering (CISE) at the<br\/>National Science Foundation (NSF). The basic idea for this travel support has been<br\/>conceived in the spirit of his work and his legacy.","title":"Embedded Systems and Software Engineering Outreach","awardID":"0535418","effectiveDate":"2005-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[290193,"564429"],"PO":["561889"]},"106027":{"abstract":"This project covers ongoing and new work in three diverse areas in the theory of computation: (1) Boolean decision trees and related models (2) Testing whether two polynomials are algebraically equivalent, and (3) Computational aspects of economic mechanisms.<br\/><br\/>The decision tree model of computation is perhaps the simplest model of computation: the cost of a computation is measured entirely in terms of access to the input. The model encapsulates a common situation in which a computation is being performed and the time needed for the computation depends primarily on the number of calls to a single expensive subroutine. The focus is on computation of boolean functions, whose variables are 0-1 valued. The power of the decision tree model can be enhanced by adding random sampling, and also by using quantum superposition. The aim is to get a deeper and more precise understanding of the advantage that these enhancements provide. In addition, related models where computations must be robust in<br\/>the presence of noise will be studied.<br\/><br\/>A fundamental algorithmic problem in algebraic computation is to determine whether two<br\/>algebraic circuits, consisting of addition, subtraction and multiplication gates, compute the same multivariate polynomial. It is not known whether there is an efficient deterministic algorithm for this problem. The following algorithmic problem, which turns out to be equivalent to the above problem, will be investigated: given k _ k matrices A1; : : : ;An with integer entries, is there a nonsingular matrix in their linear span?<br\/><br\/>Economic mechanism design is an area that involves designing systems for implementing economic transactions among many self-interested agents, so as to achieve certain economic or social ends. With the rise of the internet, algorithmic issues have become increasingly prominent. The investigations will be aimed at further understanding the kinds of transactions that can be implemented in principle (ignoring the computational and communication resources needed), and also developing further methods for finding computationally efficient mechanisms that achieve the desired economic goals as closely as possible.<br\/><br\/>Intellectual Merit of Proposed Research: The first two areas of study include longstanding open problems in theory of computing. Study of decision trees is aimed at uncovering intrinsic limits in the improved computational efficiency that can be realized by using random sampling and quantum superposition; these bear on foundational issues of algorithmic design. Progress on the singular subspaces problem will be of substantial interest both in computer science and mathematics. The work on economic mechanism design will unify and extend existing techniques in the field, and provide new approaches and analytical methods.<br\/><br\/>Broader impact of the activities The proposed work will extend the mathematical foun-<br\/>dations of computer science, and has the potential to impact algorithm design methodology. The work on mechanism design is highly interdisciplinary, lying at the juncture of economics, mathematics and computer science, and the work on quantum computation has some connections to physics. The activities will integrate research and education by means of substantial involvement of graduate students in the research activities, and the development of research-related curriculum.","title":"Investigations in Concrete Complexity and Truthful Mechanism Design","awardID":"0515201","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["518427"],"PO":["499399"]},"106049":{"abstract":"Fractal-geometric aspects of complexity classes and central questions in computational complexity will be studied using resource-bounded dimension, refining the resource-bounded measure approach. Specific topics include:<br\/><br\/>Polynomial-time dimension will be used to understand the fractal geometry of NP and other complexity classes, with an emphasis on the self-similarity between a complexity class and its complete sets.<br\/> <br\/>Hypotheses such as \"NP has positive p-dimension\" that imply only average-case hardness will be compared to resource-bounded measure hypotheses and investigated for their consequences in computational complexity.<br\/><br\/>Zero-one laws for the resource-bounded dimensions of complexity classes will be investigated unconditionally and under derandomization assumptions.<br\/><br\/>The contrasting behavior of polynomial-time reductions in different orders of scaled dimension will be further studied to yield new insight into completeness phenomena and the failure of the random oracle hypothesis.<br\/><br\/>Moving beyond limitations of the martingale approach, compressibility will be employed to develop resource-bounded measure and dimension in small complexity classes.<br\/><br\/>The proposed research also includes some related investigations that go beyond computational complexity:<br\/><br\/>The equivalence of dimension and log-loss prediction will be used to apply .nite-state dimension to universal prediction. Relationships between VC-dimension and fractal dimension will be developed and applied in computational learning theory.<br\/><br\/>Correspondence principles for constructive dimension and Hausdor. dimension will be developed to provide a new Kolmogorov-complexity method to simplify proofs and establish new results in classical fractal geometry.<br\/><br\/>Intellectual merit: As evidenced by its online bibliography, resource-bounded measure has been a central approach to computational complexity with over 100 papers by 60 authors during the past 15 years. The proposed research is a challenging program that refines this approach using resource-bounded dimension. This will yield further quantitative insight into the structure of complexity classes, new understanding of the limitations of resource-bounded measure, and better assessment of the reasonableness of resource-bounded measure hypotheses.<br\/><br\/>Broader impacts: This research will also establish new links between computational complexity, information theory, learning theory, and fractal geometry, with the expected long-term effect of increasing interdisciplinary interaction among those fields. Concepts and ideas from previously distinct areas will be applied through the unifying concept of resource-bounded dimension to yield new ways of thinking. The research results will be made widely available as technical reports and conference publications. Online bibliographies of all relevant papers will be maintained to help researchers with better access to the literature. Graduate students from underrepresented groups will play a significant role in the project.","title":"Fractal Geometry in Complexity Classes","awardID":"0515313","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["410155"],"PO":["499399"]},"109338":{"abstract":"In 2004, an alliance between faculty from the UCLA Graduate School of Education and Information Studies (GSEIS), the UCLA Henry Samueli School of Engineering and Applied Science (HSEAS), and officials from the Los Angeles Unified School District (LAUSD)-the Computer Science Equity Collaborative (CSEC)--was formed. The purpose of the alliance is to address the under-representation of female, African-American, and Latino\/a students in computer science (CS) at the high school level.<br\/><br\/>The first endeavor of the CSEC was a summer institute for LAUSD Advanced Placement (AP) CS teachers that resulted in 11 brand-new AP CS courses in LAUSD high schools with high minority student populations. Teachers were given instruction in Java as well as being engaged in discussions on recruiting and retaining underrepresented students. As a result of the summer institute, the number of AP CS courses in the LAUSD doubled, the number of LAUSD female students enrolled in AP CS tripled, the number of LAUSD Latino\/a students tripled, and the number of LAUSD African-American students enrolled in AP CS doubled. Principals of the schools committed to providing support for these courses.<br\/><br\/>UCLA has been awarded Special Projects funds to support its participation in a second summer institute, to provide a UCLA-based summer conference meeting of principals and other critical stakeholders who are interested in expanding access to AP CS for under-represented students, to conduct follow-through research in year-round schools that continue to operate through the summer, and to further develop new collaborative initiatives.","title":"The Impact of Access: Expanding Opportunities for African-American, Latino\/a, and Female Students to Learn Higher Level Computer Science in High Schools","awardID":"0533626","effectiveDate":"2005-07-15","expirationDate":"2005-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["528136"],"PO":[289456]},"105994":{"abstract":"Airborne networks can be used to transmit vehicle state information among aerospace vehicles, moving platforms, and ground stations for enhancing flight safety, efficiency, and security in a wide range of air transportation systems. Fundamental to the operation and design of airborne networks are issues on the effective dissemination of vehicle state information, efficient use of available radio spectrum for data transmission, and optimal decision-making across the networks. This research project seeks to establish theoretical foundations for some fundamental optimization problems in these issues.<br\/><br\/>For the effective information dissemination across airborne networks, the research will focus on the selection of aircraft as information centers that provide information for other aircraft. The proper selection of information centers requires that the service cost be minimized, or\/and the service time be bounded. For the efficient use of radio spectrum in airborne networks, the project explores the possibility of frequency reuse by properly assigning channels to active transmissions. The proposed research will successfully solve both the hidden terminal problem and the exposed terminal problem. Finally, for the distributed optimal decision-making across airborne networks, the project emphasizes proper modeling and the interplay of vehicle dynamics, communication link performance capabilities, as well as numerical schemes for distributed optimization. In addition, a series of educational activities are planned in order to bring the impacts of this project to the networking research community and classrooms.","title":"Collaborative Research: Fundamental Optimization Problems in Airborne Network Design and Applications","awardID":"0515009","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[279627],"PO":["499399"]},"104795":{"abstract":"The project concentrates on the development of reliable physical<br\/>computing systems (PCS), a class of safety-critical embedded systems<br\/>that interact with the physical environment in significant ways via<br\/>sensors and actuators. The target application domain is medical<br\/>devices. The existing technology for PCS design does not effectively<br\/>support the development of reliable and robust medical device systems<br\/>as they become more complex. The new techniques will also have<br\/>applicability to other domains such as avionic systems, manufacturing,<br\/>and automotive controllers.<br\/><br\/>The project advances formal modeling and model-driven validation<br\/>techniques to make it possible to develop highly dependable PCS. The<br\/>approach is to extend hybrid systems modeling to PCS by explicitly<br\/>modeling failures. These models are then used for test generation and<br\/>run-time monitoring and checking. The project will also lay<br\/>foundations for a rigorous certification process of such systems by<br\/>developing static and dynamic dependability metrics. Finally, an open<br\/>platform is being developed for creation and sharing of the artifacts<br\/>of requirements, designs, models, test suites for generic infusion<br\/>pumps (GIP). The platform will foster a community interested in highly<br\/>dependable medical devices and ultimately enable transition of the<br\/>technology into practice. A significant part of the project is the<br\/>collaboration with the developers of Plug-and-Play (PnP) for operating<br\/>room of the future.<br\/><br\/>In the short term, the project will improve the quality of infusion<br\/>pumps and other computer-controlled medical devices, while long term<br\/>impact will be a wider use of formal approaches in the<br\/>engineering of embedded systems.","title":"CSR-EHS: Techniques for Assuring the Safety and Reliability of Physical Computing Systems and Applications to Medical Devices","awardID":"0509327","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[276658,"526896","553656","553657","382346"],"PO":["561889"]},"107721":{"abstract":"The goal of this proposal is to develop new computational paradigms for self-assembly, inspired by how living cells self-assemble highly patterned but adaptive three-dimensional (3D) structures during tissue morphogenesis. We will apply these paradigms to engineer man-made systems that require decentralized coordination and structure formation - for example, programming swarms of robots that assemble temporary scaffolds, space enclosures, and physical distribution networks that may be used for various construction and industrial applications in the future. We will also apply these models to directly study tissue morphogenesis - for example, to investigate how local mechanical and chemical cues can cause global adaptation of tissue morphology and even switching of morphology in endothelial cells. Through this effort, we aim to better understand how genetic information and environmental cues combine at the agent (\"cell\") level, to generate structures that exhibit optimized form and function at the whole-system level.","title":"BIC: Programmable Myriads: self-assembling cellular robots, inspired by tissue morphogenesis","awardID":"0523676","effectiveDate":"2005-07-15","expirationDate":"2008-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["563386"],"PO":["565223"]},"102155":{"abstract":"Abstract<br\/>Proposal: CNS 0454195<br\/>PI: Thomas L. Martin<br\/>Institution: Virginia Polytechnic Institute and State University (Virginia Tech)<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Electronic Textiles for Wearable and Pervasive Computing <br\/><br\/>The PI at Virginia Polytechnic Institute and State University is a leader in emerging research in electronic textiles. At present, the PI's team is handweaving textiles with embedded wiring, sensors, actuators, and processing elements. The project will acquire a computer controlled, 60-inch industrial loom, a 15-needle embroidery machine, and a software controlled sewing machine that can handle novel fibers. These acquisitions will enable faster and more reliable production of experimental textiles; the equipment is similar to industrial equipment so the researchers will both gain experience with manufacturing needs and position their research for future impacts on practice. The research supported includes textiles for health monitoring, monitoring physical therapy, monitoring gait, and sound detection and localization.","title":"CRI: Electronic Textiles for Wearable and Pervasive Computing","awardID":"0454195","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["485861","553340"],"PO":["564456"]},"105675":{"abstract":"The objective of this work is to extend significantly the capabilities of portals created by the geospatial community, by providing semantic integration over diverse data sets. For example, the Wisconsin Land Information System and the new Federal Geospatial One-Stop portals disseminate data and support procedures and simulations in emergency situations. However, geospatial data are complex and highly heterogeneous, having been developed independently by various levels of government and the private sector. This project includes metadata integration methods to enable more precise location of data sources over the web and to provide geospatial portals with query capabilities and semantic resolution for the types of information integration that could help in information discovery, problem-solving, or emergency response.<br\/><br\/>The research will develop methods and tools to support the integration of information in such a way that end users will have a homogeneous view over heterogeneous data sources. An ontology-based architecture will be developed with which each individual heterogeneous data source can be added to the network of information with relatively little effort. Ontology mappings will establish correspondences between terms in heterogeneous sources and those of standard models and ontologies. The approach also extends ontology mappings to incorporate semantics regarding spatial considerations, such as accuracy, for spatial integration.<br\/><br\/>Information integration is an area of research that stretches over databases, artificial intelligence, digital libraries, and the semantic web. This project will extend significantly the state of the art of information integration in general and of geospatial information integration in particular, by providing a robust and scalable framework that encompasses techniques and algorithms for integrating heterogeneous data sources using an ontology-based approach.<br\/><br\/>This project's goal of semantic integration for geospatial data fits into a broad vision for creating a cyberinfrastructure on the Web. In a geospatial cyberinfrastructure, data will be automatically located and semantically matched to other relevant data sources. Manual intervention will not be needed or will be minimal. With such a structure, emergency managers, government officials, and the general public will not be constrained to pre-formulated queries. Instead, ad hoc, exploratory queries and analyses will be possible. From an educational viewpoint, the project will significantly benefit the training of graduate and undergraduate students, in particular of women and minorities, and will include the design of new graduate and undergraduate courses.","title":"Collaborative Research: Information Integration for Locating and Querying Geospatial Data","awardID":"0513605","effectiveDate":"2005-07-15","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1629","name":"BE: NON-ANNOUNCEMENT RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}}],"PIcoPI":["433344"],"PO":["565136"]},"107743":{"abstract":"Protein modeling is becoming increasingly important for the characterization of biological systems at the molecular level. It is now widely accepted that the key to understanding biological function, and subsequently disease, is to understand the structure, flexibility, kinetics, and dynamics of the body's worker molecules, proteins. Recent successes in understanding the structure and dynamics of proteins using computational analysis underscore the importance of developing computational methodologies to explore the fundamental principles of protein flexibility. Representing a protein by a single configuration is less and less attractive, as such a representation does not represent the dynamical interplay of different conformations that can regulate protein function and does not fully characterize the protein's interaction with neighboring molecules.<br\/><br\/>This proposal will develop a computational framework for the characterization of protein flexibility at equilibrium conditions through a combination of geometric\/robotic and biophysics methods. The proposed work lies at the intersection of computer science and modern biophysics, and will benefit both communities. On the computational side, it will lead to new methodologies and paradigms to model physical systems with high flexibility, complex geometry, multiple constraints, and continuous motion. Successful robotics and computational geometry methods will be adapted to support the large-scale analysis required for biological applications. On the biophysical side, novel theories and methodologies will be developed and tested for modeling proteins at different resolutions. Quantitative connections between theory and experiment will be pursued. The proposed work has the potential to dramatically affect major unresolved problems on the inner workings of protein systems.","title":"BioComp: A Computational Framework for the Characterization of Biological Systems at the Molecular Level","awardID":"0523908","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["557516","540484"],"PO":["565223"]},"105686":{"abstract":"The design and development of powerful mechanisms for managing and mining large datasets of moving objects is an emerging direction in science and engineering informatics. The subject becomes increasingly important as the world, as well as national security threats, become more mobile. This project is investigating the issues related to the design and development of innovative methods for querying, analyzing, and mining of spatiotemporal information to find typical characteristics of the moving objects' trajectories, and uncover suspicious motion in large datasets of moving objects. The moving objects datasets are in the form of either stored data or transient data streams. The project is designing and implementing the MotionEye system prototype which consists of four subsystems: MotionQuest(DB) and MotionQuest(Stream) for querying and hypothesis validation in moving objects databases and data streams respectively, and MotionMine(DB) and MotionMine(Stream) for data mining in moving objects databases and data streams respectively. The project is investigating efficient and effective approaches to the implementation of these subsystems. The project is also striving to ensure that the developed technology will not sacrifice individual privacy. The project will enable the development of more advanced information systems in homeland security, law enforcement, traffic control, and other domains that deal with moving objects.","title":"SEI(IIS): MotionEye: Querying and Mining Large Datasets of Moving Objects","awardID":"0513678","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":["563535"],"PO":["563727"]},"114398":{"abstract":"Teams are the building blocks of innovation in modern organizations, yet we know little about how the knowledge that flows through team members and their social networks results in the successful implementation of creative ideas, tasks, or procedures. One explanation for this gap in research about the antecedents of innovation is that scholars have often focused on either the team level of analysis or the organizational level of analysis, not on the intersection of teams and their managers, colleagues, and other contacts who make up the organization. Recent developments in the field of social networks offer both theory and methods for building a more comprehensive model of the innovation process in organizations, including conditions under which information and communication technologies should be used for sharing task-relevant knowledge within and outside of the team. One feature of organizations, in particular, that has complicated our understanding of the innovation process is a rise in the number of spatially and temporally separated employees. Geographic dispersion increases coordination costs by making the exchange of knowledge through distant tie s more difficult, however it also has the potential to expose individuals to unique sources of knowledge through local ties, thus creating opportunities for innovation.<br\/><br\/>In this project, the PI will explicitly address the tradeoffs of geographic dispersion in organizational teams (i.e., coordination and innovation). His premise is that integrating social network theory with existing research on teams and organizations provides a more complete picture of how knowledge sharing through geographically dispersed teams and networks can foster innovation. In order to test the proposed model of dispersed innovation in organizations, multi-year field studies will be conducted within firms that: (a) have a clearly defined sample of collocated and dispersed teams, (b) maintain a corporate database with background information on team members, (c) administer surveys to assess knowledge sharing within and outside of teams, and (d) employ a standardized measure of innovation to permit evaluation of performance quality across teams. In the final year, the PI will compare and contrast results from these field studies with findings from another longitudinal project that explores predictors of success for scientific collaborations dispersed across multiple institutions. Building on the his prior work, which showed that knowledge sharing outside of teams was more strongly associated with innovation when teams were more geographically dispersed, this project will extend current thinking in four important ways by: (1) examining the direction of knowledge sharing instead of assuming knowledge flows equally in and out of teams, (2) identifying the sources of knowledge to learn specifically where contacts are located, (3) probing the awareness of knowledge to see the extent to which team members know what others know, and (4) considering the dynamics of knowledge sharing to learn when it is critical for teams to share knowledge externally.<br\/><br\/>Broader Impacts: As demand mounts for knowledge-intensive work in the US and other countries, innovation in organizations will continue to be a key component of global economic growth. Understanding how organizations can build capabilities for a dispersed innovation process through teams and networks will benefit industry, students, and researchers alike. To ensure access to the substance of this research, online tools developed to collect and analyze the team and network data will be used in the classroom and made publicly available along with measures of geographic dispersion, knowledge sharing, and innovation.","title":"Career: Fostering Innovation in Organizations Through Geographically Dispersed Teams and Networks","awardID":"0603667","effectiveDate":"2005-07-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7496","name":"COLLABORATIVE SYSTEMS"}}],"PIcoPI":["499499"],"PO":["564456"]},"109504":{"abstract":"This project is a collaborative effort by computer scientists and engineers from Texas A&M and UC Berkeley consulting with natural scientists and documentary filmmakers. The goal is to advance the fundamental understanding of automated and collaborative systems that combine sensors, actuators, and human input to observe and record detailed natural behavior in remote settings. Currently, scientific study of animals in situ requires vigilant observation of detailed animal behavior over weeks or months. When animals live in remote and\/or inhospitable locations, observation can be an arduous, expensive, dangerous, and lonely experience for scientists. The project proposes a new class of hybrid teleoperated\/autonomous robotic \"observatories\" that allow groups of scientists, via the internet, to remotely observe, record, and index detailed animal activity. Such observatories are made possible by emerging advances in robotic cameras, long-range wireless networking, and distributed sensors. The project will investigate the algorithmic foundations for such observatories: new metrics, models, data structures, and algorithms, that will comprise a robust, mathematical framework for collaborative observation. The project will build on past work to extend and formally characterize hybrid models of collaborative and automated observation that draw on computational geometry, stochastic modeling and optimization. The project will advance fundamental understanding of networked robotics and develop efficient algorithms for collaborative observation that combines human and sensor input. This effort is intended to benefit biological scientists and facilitate collaboration among researchers. It will produce working prototypes that will be accessible via the internet to scientists, students, and the public worldwide.","title":"CONE: Collaborative Observatory for Natural Environments","awardID":"0534848","effectiveDate":"2005-07-01","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550010"],"PO":["403839"]},"106006":{"abstract":"This project focuses on NP-hard optimization problems arising from the rapid development of Computer<br\/>Networks. This research proposes to investigate the development of efficient combinatorial algorithms<br\/>for wired and wireless networks. The project also involves improving the existing algorithms for<br\/>fundamental graph optimization problems that have direct applications in networking. The PI proposes<br\/>to concentrate on fast generation of approximate solutions. In addition, the algorithms for wireless<br\/>networks must use only local information.<br\/>Specifically, the PI intends to accomplish the following research items in this project:<br\/>.. devise efficient approximation algorithms to increase bandwidth utilization in quality-of-service<br\/>multicasting.<br\/>.. devise efficient distributed algorithms for maintaining connectivity in ad hoc wireless networks<br\/>and sensor networks, while minimizing the energy consumption and extending the network's<br\/>lifetime. The PI will analyze the communication complexity of the algorithms and their behavior<br\/>with imprecise input data and node failure. The research will start by concentrating on static<br\/>networks, while always considering extensions to mobile networks. Once the problems arising<br\/>from static ad-hoc wireless networks are better understood, the PI will concentrate on mobility.<br\/>.. devise efficient approximation algorithms for several graph connectivity problems.<br\/>.. identify and solve new optimization problems arising in the next generation of computer networks.<br\/>The new methods will be grounded on provably good algorithms with guaranteed worst-case performance,<br\/>and extensive empirical studies that thoroughly test the algorithms' efficiency in simulations<br\/>on available industrial data. The developed algorithms and their implementations will be widely distributed<br\/>to practitioners in order to enable a more complete understanding of the optimization problems<br\/>in these application areas. The intelectual merit of this proposal consists of advances in the techniques<br\/>of design and analysis of algorithms and the understanding of the underlying combinatorial structure<br\/>of several graph optimization problems.<br\/>The main educational goal of this project is to improve the preparation of graduate and undergraduate<br\/>CS majors at Illinois Institute of Technology (IIT) in theoretical computer science. The grant would<br\/>be used to support a research group of graduate and undergraduate students.<br\/>The project's impact will be increased by supporting seminars and workshops which benefit a diversity<br\/>of students and researchers. The PI's service agenda is centered on recruitment of talented students<br\/>for theoretical research, as part of the broader impacts of the proposed research.","title":"Approximation Algorithms for Networks","awardID":"0515088","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["409681"],"PO":["562944"]},"104840":{"abstract":"Computer systems are becoming pervasive as well as growing rapidly in scale, complexity, distribution, and heterogeneity. For these reasons, it is becoming increasingly important for engineers to reason quantitatively about the various systems' properties of interest and curb their design, development, and deployment costs. In an ideal situation important system characteristics, such as performance and reliability, would be assessed at system design time, before significant time and cost have been devoted to a project. However, making useful (quantitative) predictions in early design stages is difficult at best, due ti the interplay between many relevant factors, such as complex properties of software components, the potential effects on software of the firmware (hardware, OS, device drivers), as well as the potentially conflicting desired system attributes.<br\/><br\/>Attacking even a small subset of these problems is challenging enough and would result in significant advances to the state of the art in complex systems engineering. Hence, this project proposes to focus efforts on design-time evaluation of architectures with respect to one key attribute - reliability. Here, reliability is defined as the probability that the system will perform its intended functionality under specified design limits. The proposed approach will enable an engineer to build a multi-faceted, hierarchical model of a system and assess its reliability in an incremental, scalable fashion. Although several software reliability techniques exist, they are insufficient. To address these deficiencies, the project will develop a technique that will couple software architectural models (well understood by system designers) with augmented Hidden Markov Models (which allow us to reason about numerous uncertainties existing in early design phases), and will augment this methodology with the relevant attributes of the firmware in support of more complete and meaningful reliability models.<br\/><br\/>The project will evaluate the results the methods developed along two measures of interest: tractability (intended to address scalability issues existing in real, complex systems) and sensitivity (intended to address issues of confidence in the researchers predictions under numerous uncertainties existing at design time) and will apply the results to real problems from the domain of mobile robotics, a problem domain that is representative of many complex, distributed, and embedded systems.","title":"CSR-SMA: Engineering Reliability Into Hybrid Systems via Rich Design Models","awardID":"0509539","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["410098","560335","551145"],"PO":["551712"]},"105940":{"abstract":"In a wide variety of computational settings, where the data is most naturally viewed as coming from a distribution, it is often crucial to determine whether the underlying distribution satisfies various properties. Examples of such properties include whether two distributions are close or far in statistical distance, whether a joint distribution is independent, and whether a distribution has high entropy. For most such properties, standard statistical techniques which approximate the distribution lead to algorithms which use a number of samples that is nearly linear in the domain size. Until very recently, distributions over large domains, for which linear sample complexity can be daunting, have received surprisingly little attention. However, new interest in these questions comes from many directions, including data mining, Physics and machine learning applications in Neurobiology. Recent results have shown that one can achieve results which are significantly more efficient than the standard techniques for the case of large domains.<br\/>The intellectual merit of this research will lead to an understanding of the sample, time and space complexity required to identify various natural properties of a probability distribution. The proposed research will focus on determining which properties can be understood with a number of samples that is sublinear in the domain size, and will lead to an understanding of the aspects of algorithm design that are specific to these constraints. The questions that will be considered range from considering the complexity of testing previously unstudied properties, finding general techniques which apply to classes of distribution testing problems, investigating new models of distribution testing, understanding structural aspects of the testing problems that can be solved in sublinear time, and further understanding the relationship between the computational complexity and sample complexity.<br\/>The broader impact of this proposal includes educational and workforce development components. The educational component of this proposal involves designing course material on algorithms for testing distributions that would be appropriate for students at all levels. Enough material has been collected to develop a graduate course that highlights the body of techniques that have emerged in this field. Some of the recent advances are a perfect fit for conveying fundamental ideas behind randomized algorithms to undergraduates. The PI has been awarded two teaching awards for her efforts at undergraduate education. The PI will continue her efforts as an advisor and mentor to undergraduates, graduate students and postdoctoral researchers, which in the past have included several women who have gone on to successful research careers. The PI is currently co-organizing a Dagstuhl workshop on Sublinear Algorithms. A priority has been placed on inviting and supporting graduate students interested in working in the area. Finally, the ideas will also be disseminated through the use of the web, seminar, workshop and conference presentations, journal articles and survey articles aimed at a wider audience.","title":"The Complexity of Testing Distributions","awardID":"0514771","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["517766"],"PO":["562944"]},"105951":{"abstract":"Abstract<br\/>--------<br\/><br\/> Digital communication, embodied in such applications as cell phones and wireless Internet, by now pervades our daily lives, while digital storage devices, such as CDs, DVDs, and computer disk drives, have become the principal means of preserving our information. In the \"information age\" in which we all now live, the need for reliable transmission and storage of digital data is of paramount importance. What makes such reliable transmission and storage possibles are error-correcting codes, first conceived by Claude Shannon over 50 years ago. Indeed, as you are reading these lines, millions of error-correcting codes are decoded every minute, using efficient algorithms implemented in custom VLSI circuits. At least 75% of these circuits decode Reed-Solomon codes, invented by Irving Reed and Gustave Solomon in the 1960s. In the four decades since their invention, Reed-Solomon codes have been extensively studied and ingenious decoding algorithms for these codes have been developed. What has been realized only recently, however, is that Reed-Solomon codes can correct many more errors than previously thought possible! In a series of theoretical breakthroughs, Sudan, Guruswami-Sudan, and Koetter-Vardy have made state-of-the-art Reed-Solomon decoders out of date. At least in principle, we can now achieve much better performance with the same codes. The goal of this project is to follow-up on this exciting recent work and to follow this line of research through to its ultimate potential, in theory as well as in practice.<br\/><br\/> In order to attain this goal, we plan a broad line of attack. On one hand, the proposed investigation will address deep theoretical questions. Can one exceed the Guruswami-Sudan decoding radius? What is the optimal multiplicity assignment for algebraic soft-decision decoding? How can iterative decoding methods be applied to Reed-Solomon codes? On the other hand, we intend to go all the way to the first-ever VLSI implementation of a soft-decision Reed-Solomon decoder. The proposed VLSI architecture aims for high speed and low power dissipation. Thus complexity considerations, inherently motivated by the practice of VLSI design, will be paramount throughout our investigation. Specifically, The main topics to be investigated are: (1) Multivariate interpolation decoding beyond the Guruswami-Sudan radius; (2) Probabilistic model and multiplicity assignment schemes for algebraic soft-decision decoding; (3) Iterative methods for soft decision decoding of Reed-Solomon codes; (4) Analytic bounds on the performance of maximum-likelihood and suboptimal decoders; and (5) Complete VLSI implementation of a state-of-the-art soft-decision Reed-Solomon decoder in an FPGA and\/or ASIC.","title":"Next Generation Decoders for Reed-Solomon Codes -- Collaborative Research","awardID":"0514816","effectiveDate":"2005-07-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["385127"],"PO":["432103"]},"105962":{"abstract":"The research supported under this grant targets the thorough and effective understanding of message-passing algorithms which constitute a large and very potent class of estimation and detection techniques. Indeed, while message-passing (iterative) processing is very successful in practice, understanding its limitations and its sources of non-optimal behavior has been elusive. Despite the enormous impact that message-passing algorithms have, in particular in a communication scenario, practical systems currently rely almost exclusively on a simulation-based evaluation. In this situation, understanding the behavior and geometry of message-passing will not only reduce the necessity of simulations but provide powerful tools for system optimization.<br\/><br\/>This proposal draws on recent exciting developments that connect message-passing algorithms to the well established theory of convex optimization. As it turns out, message-passing algorithms are intimately related to a linear programming formulation of the inference task at hand. In fact, belief propagation algorithms may be interpreted as an efficient duality-based method to closely approximate the solution to a linear program. Once such connections are established the investigators will strive to understand message-passing algorithms from an entirely new and fruitful point of view. Also, the investigators have already shown that the connection to convex optimization is rooted in the basic property of message-passing algorithms, namely that they operate only locally in a given graphical model. Thus the findings resulting from the approach investigated here will apply to any reasonable locally-operating algorithm.","title":"Collaborative Effort: Message-Passing Algorithms: From Practice to Theory and Back to Practice","awardID":"0514869","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["320975","475205"],"PO":["564898"]},"102112":{"abstract":"The PI will conduct research on Field Programmable Gate Arrays (FPGAs) including partitioning algorithms among several FPGAs, computers and memories; developing hardware efficient algorithms; and exploring interactive visualization methods. The research will be applied to bioinformatics. The PI has experience with using FPGAs for global sequence comparison. They will consider other bioinformatics algorithms such as simulated annealing for task partitioning, BLAST and FASTA for sequences for applications of FPGAs. The project is working with the North Carolina bioinformatics program. New university initiatives in bioinformatics including a new $35 million center at UNC Charlotte will provide a setting to maximize impact.<br\/>The research will be integrated into education programs and courses at the university; the PI will use team-based projects and problem based learning to enhance education. The PI plans a WWW site to share results and process data from other universities through a JAVA interface.","title":"CRI: Field Programmable Gate Array (FPGA) based Hypercomputing Acceleration Platform for Bioinformatics","awardID":"0453916","effectiveDate":"2005-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["530662","434653","434652"],"PO":["539087"]},"101023":{"abstract":"Quantum information processing (QIP) holds the promise of distinctly new and more powerful information systems, including quantum computation, quantum communications, and quantum information theory. <br\/>However, this promise can only be achieved if the theoretical advances up to the present can be transformed into actual technology. This requires steady experimental advances over the next few years, which in turn requires a far better practical understanding of realistic quantum systems for QIP. The aim of this proposal is to provide such an understanding.<br\/><br\/>Intellectual Merit.<br\/>QIP is still a young field, but one that has grown explosively over the last ten years. From the discovery by Peter Shor that quantum computers could factor large numbers, a problem widely believed to be intractable on ordinary computers, and on which the well-known public-key encryption algorithm RSA is based, there has been rapid theoretical development, demonstrating numerous applications of quantum systems to information processing tasks.<br\/>Experimental development, while also rapid, still lags far behind theory in almost all areas. This is largely due to the inherent difficulty of controlling large quantum systems while isolating them from quantum noise (or decoherence). In addition to improved experiments, careful theoretical treatment is needed to understand the sources of noise in realistic quantum systems, and their effects on QIP. The PI will draw on extensive experience in decoherence to improve both the theoretical and numerical tools for such treatments, and apply them to a wide variety of related problems relating to near-term QIP experiments.<br\/><br\/>Broader Impact <br\/>As a new field, QIP has so far penetrated only shallowly into the academic community and public consciousness. It is important to train the next generation of quantum engineers. To these ends, the PI will:<br\/>-create two new courses: one, an introductory graduate course in quantum information and computation, the other an advanced topics course. <br\/>-The extensive lecture notes will be made publicly available<br\/>-PI will also host the SQuInT summer retreat for graduate students in QIP at USC, andHe will also reach out to the larger academic community at USC<br\/>-PI will serve as associate editor of IEEE Transactions on Computers, editing a special issue on QIP. <br\/>-The software library will also be made available for the use of others in the field, and provided with a suitable easy-to-use interface.","title":"CAREER: Realistic Models and Simulations of Systems for Quantum Information Processing","awardID":"0448658","effectiveDate":"2005-07-01","expirationDate":"2011-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["381587"],"PO":["565157"]},"104796":{"abstract":"A substantial advance in software and tools is essential for effective application of future High-End Computing (HEC) systems exhibiting tens of thousands (or even millions) of execution sites\/threads in an integrated structure. Conventional practices, largely restricted to SPMD-based messagepassing programming models in the context of distributed memory, provide limited degree of system-wide application control but often prove inefficient and are difficult to program, debug, and tune performance for large complex problems. These challenges will only increase as clock speeds, memory densities, system scale, memory hierarchy, and problem complexity increase significantly throughout this decade to peta-flops scale before 2010.<br\/><br\/>This research project addresses the underlying sources of performance degradation (e.g. latency, overhead, and starvation) and the difficulties of programmer productivity (e.g. explicit locality management and scheduling, performance tuning, fragmented memory, and synchronous global barriers) to dramatically enhance the broad effectiveness of parallel processing for high end computing. The project will develop a hierarchical threaded virtual machine (HTVM) that includes a dynamic, multithreaded execution model and programming model, providing an architecture abstraction for HEC system software and tools development. The project will conduct research and development of a continuous compilation and runtime software technology that are critical to enable the dynamic adaptivity of the HEC system. The research project addresses the underlying sources of performance degradation (e.g. latency, overhead, and starvation) and the difficulties of programmer productivity (e.g. explicit locality management and scheduling, performance scalability, load balancing efficient synchronization, etc.), and and will implement these methods in a novel framework of system software and tools to dramatically enhance the broad effectiveness of parallel processing for high end computing.","title":"CSR---AES: Dynamic Adaptive Multithreading: Continuous Compilation and Runtime Scheduling for High End Computing","awardID":"0509332","effectiveDate":"2005-07-15","expirationDate":"2010-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["380937","563330","475422"],"PO":["493916"]},"106864":{"abstract":"The PI is working to attract more talented middle and high school students and undergraduates to careers in math and science by involving them in activities that use the integration of cutting edge research as a pedagogical tool. His activities have their foundation in the deeper results obtained by him and his research group and amplify these efforts in K-12 outreach and innovative undergraduate education. He is directing annual Summer Institutes for approximately fifteen high school students and undergraduate students. These students are actively engaged in an integrated structured research environment that he has successfully employed in the past. The PI also is directing a nationwide collaborative initiative between the American Mathematical Society and Science Service consisting of outreach visits to communities across America. On these visits, he lectures along side some of the nation's most distinguished scientists, such as winners of the Nobel Prize and the National Medal of Science, to middle school and high school students in innovative hands-on classroom activities. Each visit also features a game show, loosely based on the famous \"Who Wants to Be a Millionaire\" TV program (aptly named \"Who Wants to Be a Mathematician?\"). Students compete for prizes and scholarships (provided by the American Mathematical Society), with local winners competing in a year end \"Tournament of Champions\" in Washington D.C.","title":"Educational Outreach Activities in Mathematics","awardID":"0519428","effectiveDate":"2005-07-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0507","name":"Division of SCIENCE EDUCATION RESOURCES IM","abbr":"SER"},"pgm":{"id":"1264","name":"ALGEBRA,NUMBER THEORY,AND COM"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1260","name":"INFRASTRUCTURE PROGRAM"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1746","name":"DISTINGUISHED TEACHING SCHOLAR"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7271","name":"TEACHER PROFESSIONAL CONTINUUM"}}],"PIcoPI":["533105"],"PO":["472260"]},"105896":{"abstract":"Whether it is the geometry of a human face, the boundary of a brain tumor or the decision interface of multivariate financial data, isosurfaces are important objects for understanding and analyzing all types of data. The marching cubes algorithm is a very effective and widely used technique for extracting isosurfaces from scientific data sets. The purpose of this research project is to exploit some special properties of the isosurfaces produced by the marching cubes algorithm in order to develop particularly efficient geometry processing tools for the analysis, manipulation and processing of these surfaces. The intellectual merit of the research project consists of the development of new methods for extracting meaning and understanding from data by new and efficient digital geometry processing tools for the special case of isosurfaces produced by the marching cubes algorithm. Since isosurfaces are widely used in many areas of science, the results of this research will have broad impact on data analysis in general.<br\/><br\/>It has recently been discovered that the isosurfaces produced by the marching cubes (MC) algorithm have some special properties that allow for the development of particularly efficient geometry processing tools. These properties include (1) some special aspects of a network of orthogonal polygons which allow curve techniques to be lifted to surfaces and (2) a local function property provided by a modification of the original MC method which allows conventional approximation techniques to be applied to these special isosurfaces. This research will concentrate on three aspects of geometry processing for surfaces, namely (1) parameterizations which are necessary for texture mapping, remeshing, compression and many other useful surface operations; (2) methods of estimating curvature and other related surface properties which will serve as the basic tools for many useful higher level surface processing techniques and (3) algorithms for smoothing and fairing surfaces.","title":"Geometry Processing for IsoSurfaces","awardID":"0514606","effectiveDate":"2005-07-15","expirationDate":"2009-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["353596",279394],"PO":["565157"]},"102145":{"abstract":"Abstract<br\/><br\/>Collaborative Proposal<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: Collaborative Proposal: CRI: Planning Proposal: Community Resources to Support Research in Automated Authorship Attribution <br\/><br\/>Proposal: CNS 0454197<br\/>PI: Shlomo Argamon<br\/>Institution: Illinois Institute of Technology<br\/><br\/>Proposal CNS 1454126<br\/>PI: David Madigan<br\/>Institution: Rutgers University New Brunswick<br\/><br\/><br\/>Abstract:<br\/>The investigators will conduct a planning activity for a corpus supporting research on authorship attribution. Planning activities include surveying existing resources and determining what practitioners need; assessing the costs of obtaining, cleaning, annotating, distributing and maintaining a test corpus, conducting a workshop to assess the plans. The workshop builds on the activities of a Working Group that is already discussing research in this area. Applications of reliable and valid authorship attribution include literary scholarship, intelligence, criminal law, civil law, and computer security.","title":"Collaborative Research: CRI: Planning Proposal: Community Resources to Support Research in Automated Authorship Attribution","awardID":"0454126","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["533344"],"PO":["539087"]},"102156":{"abstract":"Abstract<br\/><br\/>Collaborative Proposal<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: Collaborative Proposal: CRI: Planning Proposal: Community Resources to Support Research in Automated Authorship Attribution <br\/><br\/>Proposal: CNS 0454197<br\/>PI: Shlomo Argamon<br\/>Institution: Illinois Institute of Technology<br\/><br\/>Proposal CNS 1454126<br\/>PI: David Madigan<br\/>Institution: Rutgers University New Brunswick<br\/><br\/><br\/>Abstract:<br\/>The investigators will conduct a planning activity for a corpus supporting research on authorship attribution. Planning activities include surveying existing resources and determining what practitioners need; assessing the costs of obtaining, cleaning, annotating, distributing and maintaining a test corpus, conducting a workshop to assess the plans. The workshop builds on the activities of a Working Group that is already discussing research in this area. Applications of reliable and valid authorship attribution include literary scholarship, intelligence, criminal law, civil law, and computer security.","title":"Collaborative Proposal: CRI: Planning Proposal: Community Resources to Support Research in Automated Authorship Attribution","awardID":"0454197","effectiveDate":"2005-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["359770"],"PO":["539087"]},"107854":{"abstract":"Intellectual Merit<br\/><br\/>The much appreciated and extensively followed roadmap for quantum computing [http:\/\/qist.lanl.gov ]and quantum information science brings out the importance of cavity quantum electrodynamics with<br\/>neutral atoms and quantum transport in nano-systems.Both the systems are extremely important as the qubits are very well defined and well characterized entanglement can be created.Besides the technological advances in these areas are making the experimental work catch fast with theory.<br\/><br\/>The proposed work on cavity QED is facilitated by the very recent success on trapping atoms<br\/>in a high quality cavity to hold these atoms for much longer times an important milestone<br\/>for the purposes of quantum information science.Further the quality of the cavity has been<br\/>improved which is crucial for better control of decoherence.<br\/><br\/>Broader Impact<br\/><br\/>Beyond the important research goals of this proposal,the broader educational goals are<br\/>to provide opportunities for postdoc,graduate and undergraduate students.In addition,<br\/>lectures on quantum information will be given by the principal investigators to local high<br\/>school students.","title":"QnTM: Manipulation of Quantum Entanglement & Decoherence in Neutral Atoms and Quantum Dots","awardID":"0524673","effectiveDate":"2005-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["380854","380855"],"PO":["521045"]}}