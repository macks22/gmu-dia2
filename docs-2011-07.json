{"185141":{"abstract":"This grant will support travel and subsistence for graduate students to attend a doctoral symposium held in conjunction with the ESEC\/FSE conference taking place this year in Hungary. FSE is the Foundations of Software Engineering symposium, which is a key conference for the core software science and engineering areas of the NSF\/CISE\/CCF Software and Hardware Foundations (SHF) program. Periodically, FSE is held together with ESEC, the European Conference on Software Engineering, which is the case this year. <br\/>The funding will increase the number of US citizens and permanent residents who are able to attend the doctoral symposium, where a distinguished international panel of faculty members will provide feedback on dissertation research. The grant will also help the students travel to the main conference, which is important to their careers. The international experience for the students is also important to the students careers, and contributes to the goal of developing globally aware workforce.","title":"Travel Grant to ESEC\/FSE Doctoral Symposia","awardID":"1138306","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[496766],"PO":["564388"]},"175373":{"abstract":"The inability to change core functionality beyond a simple packet forwarding service has hindered innovation in the Internet. Although the architectural simplicity of the Internet has likely led to its massive success, it has also had unintended consequences. For example, there is little intrinsic support to measure and monitor the Internet, which makes effective management challenging and has confounded efforts to gain a deep understanding of the Internet's structure and behavior. In addition, real-time applications often rely on overlay networks and application-level optimizations to deliver acceptable performance to users. Moreover, various network security devices are deployed as extrinsic, isolated systems to detect and respond to attacks; methods to detect and react to security threats could be much more effective if there were coordinated detection and response mechanisms integral to the Internet.<br\/><br\/>Intellectual Merit: This project develops a new approach for enhancing the functionality of Internet routers, toward the goal of enabling the development of future applications and services. The aim of the research is to provide programmable mechanisms on routers and similar Internet devices to enable service providers, application developers, and researchers to harness in-network capabilities. A new framework for programming network routers will be developed based on primitive functions that enable and expose new and useful capabilities in the Internet. These primitives will be designed to provide in-network support for measurement and monitoring of the Internet, real-time applications, and network security.<br\/><br\/>Broader Impact: A reference design and implementation of the programming framework will be made openly available to researchers and deployed and evaluated in the Global Environment for Network Innovations (GENI) experimental network. In this setting, researchers will be able to use it to test research ideas that require or could benefit from additional in-network capabilities. In addition, tutorials will be held to introduce researchers to the system. Undergraduate students will participate in the research activities of this project. Moreover, new course materials will be developed and tailored to undergraduates at smaller institutions. Two sets of coursework will be developed: one for use with a popular reconfigurable hardware system (the NetFPGA) and another for use with the router programming framework to be developed. The educational materials will be made openly available to other undergraduate institutions and workshops will be held to disseminate the materials to the educational community.","title":"CAREER: Expanding the Functionality of Internet Routers","awardID":"1054985","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[470237],"PO":["564993"]},"185185":{"abstract":"The project studies the use of advanced Human-Machine Interface kits such as helmet displays and Virtual Reality goggles with low-cost autonomous Umanned Aerial Vehicles (UAV) and assesses their impact on the ground operator's cognitive load, and explores the addition of the capability to enable the UAV with a certain amount of cognitive ability so that it can suggest places in the scene to make observations.<br\/><br\/>This project impacts search and rescue, disaster mitigation, mapping and assessment, and broadens the perspectives of faculty and students through an international collaborative effort.","title":"RAPID: Low Cost Personal Remote Sensing for Cognitive Disaster Assessment with Enhanced Human-Machine Interface","awardID":"1138632","effectiveDate":"2011-07-15","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":[496882],"PO":["543539"]},"178783":{"abstract":"PostDoctoral Research Fellowship","title":"PostDoctoral Research Fellowship","awardID":"1103803","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7335","name":"WORKFORCE IN THE MATHEMAT SCI"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[479299],"PO":["565231"]},"177342":{"abstract":"How can intelligent control be created for autonomous robots that will allow them to respond flexibly and adaptively to changing environments? In the animal world, relatively simple animals such as soft-bodied invertebrates, are capable of coordinating their many possible movements, flexibly shifting and sequencing multiple behaviors as conditions change, and learning to alter their behavior based on experience. For robots, however, this remains a challenge, which is addressed in this project using a novel control architecture that can produce sensory driven or cyclic movements.<br\/><br\/>Traditional control architectures for robotics have three layers: high level deliberative planning, low-level reactive control, and an intermediate level for sequencing and simple decision-making. Creating intermediate level controllers for intelligent behavior is particularly challenging, and a major obstacle to progress in autonomous robotics. This problem will be addressed using a novel neural-inspired control architecture, stable heteroclinic channels (SHCs) that can flexibly and robustly orchestrate multiple degrees of freedom for multifunctionality, and can readily handle behavioral hierarchies, temporal decision-making, and learning. Their properties also allow them to incorporate some of the best aspects of the two traditional approaches to robotic control: finite state machines and central pattern generator (limit cycle) controllers. SHC-based dynamical architectures underlying multifunctionality will be analyzed in a soft-bodied animal that is tractable to experimentation, and principles from these neurobiological architectures will be used to implement multifunctional behavior in a novel hyper-redundant, soft-bodied robot platform.<br\/><br\/>There are many possible applications for adaptive, flexibly-controlled soft-bodied, or hyper-redundant, robots that are able to coordinate their many degrees of freedom in varying ways to accomplish multiple functions. A multifunctional worm-like robot could crawl through pipes of varying diameter and at any angle with respect to gravity and make sharp turns at intersections. A hollow hyper-redundant robot could inspect water mains from the inside, without interrupting water flow. Such a robot could be used for oil and gas pipeline inspections to avoid costly and environmentally disastrous leaks. Smaller versions could be developed for endoscopic diagnosis of the gastrointestinal tract. The proposed work will lead to a single controller framework that can robustly coordinate multiple coupled actuated mechanisms within a robot, and describe the sequencing of distinct behaviors in both animals and robots.","title":"RI: Medium: Dynamical Coordination and Sequencing of Multifunctionality in Animals and Robots","awardID":"1065489","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[475447,475448],"PO":["564318"]},"177243":{"abstract":"Over the last decade, scripting languages have assumed a huge role. Initially web developers used Perl and Python to enrich the content of web servers; later Ruby on Rails took the scene by storm. Over the same period, JavaScript has become the dominant language on the client side of the web. Additionally, the popularity of scripting language has inspired developers to use them for the construction of many other kinds of systems, including mission-critical real-time systems. <br\/><br\/>While scripting languages are productive tools for the exploration of design ideas, their use also introduces several new kinds of problems into the software cycle. Most basically, scripting languages tend to lack a type system, which tends to raise the debugging and maintenance costs for systems. Worse, even though scripting languages tend to be safe, their flexible primitive operations induce difficult-to-predict behavior in programs and thus creates novel kinds of security holes. At the same time, scripting languages do not come with a well-defined semantics, making it nearly impossible to validate the soundness of a program analysis for safety or security properties.<br\/><br\/>In response to these observations, this proposal promises to re-engineer the semantics of scripting languages. Specifically, the PIs propose to investigate the construction of executable semantics for three scripting languages: JavaScript, Python, and Racket. They will use the language-level test suites to check that the semantics model the implementations adequately. In addition, the PIs will use the semantics to design and validate type systems and program analyses for these scripting languages. <br\/><br\/>Over the long run, the proposal should impact the world at large in three ways. First, the type systems and analyses for scripting languages should help software developers improve the safety of their software and reduce their maintenance cost. Second, the semantics for the scripting languages will help researchers validate their ideas concerning program analyses. Finally, the PIs will develop a process for semantic re-engineering that should be useful to many additional scripting language communities.","title":"SHF: Medium: Collaborative Research: Semantics Engineering for Scripting Languages","awardID":"1064922","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[475166],"PO":["564588"]},"187176":{"abstract":"Towards enhancing trustworthiness of wireless integrated circuits, this project investigates the problem of hardware Trojans in the analog\/RF domain. Hardware Trojans are maliciously-intended modifications to fabricated integrated circuits, making them capable of additional functionality which is unknown to the designer and user, but which can be exploited by the perpetrator after chip deployment to sabotage or incapacitate it, or to steal sensitive information. The motivation for this research is two-fold: First, partly because of design outsourcing and migration of fabrication to low-cost areas across the globe, and partly because of increased reliance on external intellectual property and design automation software, the integrated circuit supply chain is now considered far more vulnerable to such malicious modifications than ever before. Second, wireless integrated circuits constitute an indispensable part of modern electronic systems and their ability to communicate data (possibly encrypted) over public channels makes them a prime attack candidate. To address this problem, this project focuses on (i) delineating the threat and potential impact of hardware Trojans in wireless cryptographic ICs, (ii) elucidating the shortcomings of existing test methods in exposing them, (iii) developing preventive countermeasures for obfuscating the chip design and complicating the development of hardware Trojans, and (iv) devising efficient hardware Trojan detection methods based on statistical analysis and machine learning. The anticipated impact of this research lies in the attainment of a better understanding of the hardware Trojan threat and in the development of appropriate remedies, thus enabling secure deployment of wireless integrated circuits and fostering technology trustworthiness.","title":"TC: Small: THWART: Trojan Hardware in Wireless ICs - Analysis and Remedies for Trust","awardID":"1149465","effectiveDate":"2011-07-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["550120"],"PO":["565264"]},"176176":{"abstract":"The Manually Annotated Sub-Corpus (MASC) is a shared corpus that supports research across<br\/>several disciplines: linguistics, computational linguistics, psycholinguistics, sociolinguistics and<br\/>machine learning. It includes a wide variety of present-day American English texts annotated for<br\/>several linguistic phenomena. Because MASC provides a unique resource, considerable<br\/>community momentum has grown up around it. This project builds upon this momentum to<br\/>enable the corpus to grow on its own, and to address the need for additional annotations. The<br\/>major activities are to : (1) provide web-based mechanisms to facilitate community contribution<br\/>and use of MASC annotations; (2) develop means to more fully automate the annotation<br\/>validation process; (3) extend the WordNet annotations to cover adjectives, to support research<br\/>on evaluation of ?subjective? annotations and harmonization of WordNet with other resources;<br\/>(3) promote use of MASC and new annotations by diverse groups, by sponsoring shared tasks<br\/>that exploit the corpus? unique characteristics and supporting beta-testers of software, data, and<br\/>annotations; and (4) aggressively develop an ?Open Language Data? community around MASC<br\/>through workshops, tutorials, and active participation in relevant community activities.<br\/><br\/>MASC provides an unparalleled resource for training and testing of tools for natural language<br\/>processing, which can enable a major leap in the productivity of NLP research and ultimately<br\/>impact the way people use and interact with computers. It is the first fully open, communitydriven<br\/>resource in the field. All data and annotations are freely distributed in a manner that<br\/>permits immediate and easy accessibility for users around the globe.","title":"RUI: CRI: CI-ADDO-EN: Collaborative Research: MASC: A Community Resource For and By the People","awardID":"1059312","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["501442"],"PO":["565215"]},"176198":{"abstract":"Network telescopes, which observe unsolicited Internet traffic sent to<br\/>unassigned address space, are one of the few types of instrumentation<br\/>that allow global visibility into a wide range of security-related events.<br\/>CAIDA researchers are expanding their telescope instrumentation to enable<br\/>researchers to exploit this unique global data source and improve our<br\/>understanding of security-related events such as large-scale attacks and<br\/>malware spread. This innovative shift in network monitoring addresses<br\/>sevearal pervasive challenges in network traffic research: collection<br\/>and storage, efficient curation, and privacy-protected sharing of large<br\/>volumes of data.<br\/><br\/>CAIDA researchers are developing a taxonomy of observed traffic, including<br\/>classes of DoS attacks, vulnerability scans, and malware spread. Such<br\/>a taxonomy facilitates real-time detection of events that merit more<br\/>comprehensive measurement and analysis and notification of interested<br\/>researchers. This project will also enable infrastructure for vetted<br\/>researchers to run analysis programs approximately one hour after data<br\/>collection. CAIDA has developed a privacy-sensitive data sharing framework<br\/>that integrates privacy-enhancing technology with proven and standard<br\/>privacy principles and obligations of data seekers and data providers.<br\/><br\/>The results of this project will contribute to developing early detection,<br\/>reaction and mitigation strategies supporting the global fight against<br\/>pervasive malware. Creation of educational data kits out of telescope<br\/>data samples will link this research to educational outcomes. Most<br\/>importantly, this project provides convenient remote access to a wealth<br\/>of data, computing resources and Internet expertise aimed at supporting<br\/>network security research.","title":"II-EN: A Real-Time Lens into Dark Space of the Internet","awardID":"1059439","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7683","name":"SOFTWARE DEVELOPEMENT FOR CI"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["521741"],"PO":["564778"]},"178288":{"abstract":"Inspired by a novel technique introduced by statistical physicists, in recent work with M. Bayati (Stanford University) and D. Gamarnik (MIT), the PI introduced a combinatorial interpolation scheme between random hypergraph ensembles along with a simplified analysis. The scheme has been effective in solving hard open problems concerning the existence of appropriately rescaled optima for the random instances of various combinatorial optimization problems, such as the Independence number, MAX-CUT, and Graph Coloring. The proposed research addresses questions pressing for a qualitative, deeper understanding of the applicability of these recent techniques. A second direction of research concerns exploiting submodularity in information theoretic as well as algorithmic contexts. On the one hand, in ongoing collaboration with M. Madiman (Yale University), the PI proposes tackling sumset and sum-product inequalities in additive combinatorics from an information theoretic point of view. On the other, new submodular linear ordering problems have been introduced in joint work with S. Iwata (Kyoto University), offering a new perspective on classic, well-studied, hard-to-approximate linear ordering problems in combinatorics and the theory of computing.<br\/><br\/><br\/>The full breadth of the proposed interdisciplinary research spans various topics in combinatorics, theory of computing, information theory, probability and statistical physics. Much of the research will include collaboration with researchers from other universities as well as students from Georgia Tech. In addition to mentoring postdoctoral researchers and advising Ph.D. students, the PI has been regularly engaging undergraduate students in various research and educational projects; concrete approaches to challenging conjectures in combinatorics concerning Young tableaux and Latin partitions, as well as graph homomorphisms, form a part of the PI's research with students. The PI will continue his commitment to the dissemination of knowledge, by way of hosting and delivering research seminars, colloquia, expository lecture series as well as focused workshops. Upcoming examples include co-organizing a conference on Mathematical Challenges in Graphical Models and Message Passing Algorithms at IPAM (UCLA), and hosting a workshop on Modern Aspects of Submodularity at Georgia Tech.","title":"Random graph interpolation, Sumset inequalities and Submodular problems","awardID":"1101447","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1263","name":"PROBABILITY"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7970","name":"Combinatorics"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["565106"],"PO":["565280"]},"185230":{"abstract":"The purpose of this project is to investigate the use of social media during a crisis and during post-catastrophe relief and recovery efforts, in circumstances in which much traditional communication infrastructure has been disrupted. Specifically, the goals of the project are (a) to better understand how internet-based interactive communication tools can increase preparedness for disaster and (b) to understand how public perceptions of social media as a focal point for volunteerism may be changed by their use in a crisis context. Ikegami will visit Japan to study the effects of the disaster on-site, collaborate with Japanese researchers, meet with key persons in government agencies and NGOs, and analyze reports in the Japanese language. She will study how agencies as well as NGOs used rapidly shifting social media to handle this unprecedented disaster. <br\/><br\/>Intellectual Merit: The project will integrate theoretical research on social capital with research on virtual networks created by new media. The findings will inform the areas of crisis management, volunteerism, social networks, and other areas of human-centered computing. <br\/><br\/>Broader Impacts. The project will lead to new knowledge about the role of social media in crisis response as well as inform policy makers and nonprofit organizations about how to best use social media to engage civic involvement in disaster relief efforts.","title":"Social Networking Services and Virtual Organizations in the Crisis and Immediate Post-Catastrophe Response Processes of the 3\/11 Japan Disaster","awardID":"1138825","effectiveDate":"2011-07-15","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":[497001],"PO":["564456"]},"175297":{"abstract":"Modern distributed systems are extremely complex, due in large part to<br\/>individual node complexity, node unreliability and asynchrony, and<br\/>unpredictable network message delays and orderings. Further complicating<br\/>development of these systems is both the presence of multiple potentially<br\/>incompatible versions of the systems, and the need to build correct systems<br\/>also exhibiting high performance. Prior testing and simulation frameworks are<br\/>characterized either by extensive manual effort, or automated search for<br\/>violations of a binary decision problem---the presence or absence of a bug. <br\/><br\/>We are developing automated and interactive techniques for helping developers<br\/>understand the behavior of distributed systems implementations. By leveraging<br\/>the evolved frameworks, and instrumenting implementations in structured,<br\/>straightforward ways, we are building development tools focused on<br\/>understanding system behavior rather than merely identifying correctness<br\/>errors. This change in focus will enable more general tools that improve<br\/>development productivity in addition to testing productivity.<br\/><br\/>This research is proceeding on three fronts: 1) developing automated tools <br\/>using data mining with repeated executions to extract execution behaviors and<br\/>performance, 2) developing flexible execution descriptions suitable for both<br\/>use-case descriptions and automated processing, allowing more intuitive<br\/>interaction between users and their tools, and 3) incorporating testing tools<br\/>with revision control systems, enabling multi-version analysis and long-term<br\/>progress tracking. When completed, this research will reduce developer effort<br\/>necessary to design, update, and debug distributed systems, and may inspire<br\/>creation of a new class of systems debuggers analysing not just correctness,<br\/>but also performance and complexity.","title":"CAREER: Analyzing Distributed Systems Behavior Using Repeated Execution","awardID":"1054567","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[470080,"519700"],"PO":["565255"]},"177255":{"abstract":"American Sign Language (ASL) animations have the potential to make information accessible to many deaf adults in the United States who possess only limited English literacy. In this research, which involves collaboration across three institutions, the PIs' goal is to gain a better understanding of ASL linguistics through computational techniques while advancing the state of the art in the generation of ASL animations for accessibility applications for people who are deaf. To these ends, the PIs will develop linguistically based models of two aspects of ASL production: movements required for head gestures and facial expressions that carry essential grammatical information and frequently extend over domains larger than a single sign, and the timing and coordination of manual and non-manual elements of ASL signing. Preliminary work has shown that these issues significantly affect how well signers understand ASL animations, and that these aspects of current ASL animation technologies require improvement. How should the face of a human or animated character be articulated to perform, with accuracy, the linguistically meaningful facial expressions that are part of ASL grammar? How should the onsets, offsets, and transitions of these movements be produced? How should the facial expressions and hand movements be temporally coordinated so that the ASL production is as grammatically correct and understandable as possible? To answer open questions such as these, the PIs' novel approach will apply techniques from computer vision to linguistically annotated video data collected from human signers, in order to produce models for use in animation-production. The PIs will expand their existing annotated video ASL corpora through new data collection and annotation, and will analyze these data to study the use, timing, and synchronization of manual and non-manual components of ASL production. The annotated videos will be used to train high quality computer vision models for recognition of linguistically significant facial expressions and timing subtleties. Parameters of these computer vision models will be used to hypothesize computational models of ASL timing and facial movements, to be incorporated into ASL-animation generation software and evaluated by native signers. The models will be iteratively refined in cycles of user-based studies and incorporated into ASL animation technologies to more accurately mimic human signing. Project outcomes will include high quality models of the movement of virtual human characters for animations of ASL performance. The analysis of video corpora of ASL will produce new linguistic insights into the micro-facial expressions and the temporal coordination of the face and hands in ASL production, while advances in the analysis of ASL prosody will contribute to an understanding of the fundamental commonalities and modality-specific differences between signed and spoken languages that is essential to a full understanding of the human language faculty. The creation of new modeling approaches and recognition techniques will advance the field of computer vision, by benefiting the identification and tracking of the human face and body in video during the rapid and complex movements of ASL (and other forms of human movement).<br\/><br\/>Broader Impacts: This research will lead to significant improvements to technology for generating linguistically accurate ASL animations, which will make information, applications, websites, and services more accessible to the large number of deaf individuals with relatively low English literacy. Advances in computer vision techniques for recognizing ASL in videos of humans will have general applicability in human-computer interaction, recognition and animation of facial expressions, and computer vision. The corpora created in this project will enable students and researchers in both linguistics and computer science (including those without access to the requisite technological and human resources to carry out their own data collection from native signers and time-intensive linguistic annotations) to engage in research on ASL. The techniques to be developed will also enable partial automation of the time-consuming creation of annotated ASL video corpora. As in the PIs' earlier work, the proposed research will create opportunities for people who are deaf and members of other underrepresented groups to participate in scientific research.","title":"HCC: Medium: Collaborative Research: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing","awardID":"1065009","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":[475201],"PO":["565227"]},"178355":{"abstract":"Viewers who browse web pages on the Internet are shown display ads such as images and video. Traditionally, web page publishers and advertisers negotiate a priori through sales teams to determine which ads are shown. An emerging way to buy and sell display ads is via automated ad exchanges, which are marketplaces where publishers and advertisers trade ad impressions via real time auctions. This project explores computational, informational and economic aspects of such ad exchange markets. It will abstract suitable models for such markets and study the fundamental challenges, including problems in auction design, online optimization, risk-bounded pricing, real time bidding strategies, and even cryptography. Specific examples include the design of an optimal auction in the presence of a hierarchy of intermediaries who are also auctioneers, determining at each level of this hierarchy which intermediary to call for bids, and proving the integrity of auctions at each level of the hierarchy. Solving these problems requires new methods, concepts and tools from Economics, Finance and Optimization, and Computer Science. <br\/><br\/>Ad exchanges will impact nearly every user on the Internet. Progress on research challenges described here has the potential to directly impact such systems and the experience of nearly every user on the Internet. Further, facing the technical challenges will bring together Computer Scientists and Economists, and also push these disciplines to address very high performance challenges. For example, auction and optimization solutions have to work in tens of milliseconds, the time it takes for users to experience a web page access. This calls for new algorithmic techniques beyond the current start of the art. Finally, a detailed analysis of the role of information in ad exchanges-how much or little information is relevant for the working of the marketplace-is of great interest to Internet users and ultimately the society.","title":"ICES: Small: Auctions and Optimizations in Ad Exchanges","awardID":"1101677","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":["507886"],"PO":["565251"]},"177288":{"abstract":"The last few years have witnessed an explosive growth in the popularity and capabilities of mobile handheld devices such as smart-phones, tablets, and laptops. The rapidly expanding computational power of these portable devices has enabled their users to access, process, and share content anytime, anywhere. However, these devices have already put significant stress on many of today's 3G\/4G cellular networks, which, if left unaddressed, will result in increasingly poor quality-of-service and user experience in the future. <br\/><br\/>To address this problem, the project investigates how to exploit the key attributes of mobile content sharing environments to dramatically increase network performance. Preliminary studies show that jointly utilizing the wireless channel and mobility can yield greater gains than even the sum of exploiting mobility and the channel independently. In this case, the whole is greater than the sum of its parts! Thus, the main focus of this research is to (i) develop the analytical foundation for double opportunism - i.e., joint exploitation of the wireless channel and mobility-considering the practical realities of the wireless environment for content distribution networks; (ii) develop low-complexity algorithms, heuristics, and distributed protocols that are provably efficient; (iii) investigate the impact of imperfect knowledge on performance; (iv) validate the results via testbed deployment.<br\/><br\/>The project is expected to lead to fundamental breakthroughs in the design and performance of wireless networks for content sharing. The algorithms and protocols developed by exploiting double opportunism, and the theories underlying them, will have a significant impact on the wireless industry.","title":"NeTS: Medium: Collaborative Research: Mobile Content Sharing Networks:Theory to Implementation","awardID":"1065136","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["548182","523431"],"PO":["557315"]},"181171":{"abstract":"Resource Description Framework (RDF) has, in recent years, become an increasingly important data and knowledge representation formalism for a broad range of applications, including the World Wide Web. With rapidly growth in the size of RDF datasets, there is growing need for scalable and efficient technologies for storing, indexing, and querying RDF datasets that are trillions of triples in size. This project, led by Dr. Praveen Rao of University of Missouri-Kansas City, aims to address this need by developing: (1) A novel approach to storing, indexing, and querying of RDF data that treats graphs as first-class citizens to reduce the cost and number of joins required for graph pattern matching using RDF signatures, RDF signature indexes and line graphs; (2) A new approach for parallel SPARQL query processing on cloud platforms using data distribution schemes based on RDF signatures, location index for quickly finding RDF graphs of interest across computing nodes, and a gossip-driven query execution model and (3) A new approach for selectivity estimation of RDF graph patterns for query optimization based on new gossip algorithms for cardinality estimation of RDF graph patterns and a divide-andconquer method for effective load balancing and improved accuracy. <br\/><br\/><br\/>The broader impacts of this project include new courses covering topics in RDF data management and cloud computing, a scalable RDF reasoning tool over cancer data for oncologists, new cloud services for very large RDF data stores, increased opportunities for research-based advanced training of undergraduate and graduate students, including women. The results of this research, including publications, software, and data sets will be freely shared with the broader community. Additional information about the can be accessed through the project website at http:\/\/vortex.sce.umkc.edu\/ric.html.","title":"III: Small: Scalable RDF Query Processing Using a Cloud Infrastructure","awardID":"1115871","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[485479],"PO":["565136"]},"181061":{"abstract":"The goal of this project is to create multicore computer processors<br\/>with near-mainframe reliability at commodity costs, where the primary<br\/>costs are additional hardware and additional power consumption. Should<br\/>this project achieve these goals, it would transform computing.<br\/>Reliable processors would no longer be a niche product; instead, they<br\/>would be the commodity products used in desktops, laptops,<br\/>smartphones, etc.<br\/><br\/>The research thrusts in this project include error detection, error<br\/>recovery, diagnosis of permanent faults, and self-repair for<br\/>tolerating permanent faults. The end result of this project will be a<br\/>hardware prototype of a low-cost, reliable multicore processor.<br\/>Building an artifact is a vital part of this project for two<br\/>reasons. First, a primary project goal is to keep power and area costs<br\/>low, and these costs cannot be accurately determined using high-level<br\/>simulation. Second, to transfer this technology to industry and<br\/>transform the industry, the chip makers must be convinced that the<br\/>solutions are viable. Chip makers have historically been<br\/>hesitant to accept ideas that have not been built, not least because<br\/>important issues can be hidden in a high-level simulator.","title":"SHF: Small: Commodity Processors with Mainframe Reliability","awardID":"1115367","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["536996"],"PO":["366560"]},"184263":{"abstract":"This award provides funding for a new Research Experiences for Teachers (RET) Site focused on robotics at the University of California-Davis. Forty five in-service and pre-service computer science and STEM teachers will be paired with faculty mentors to conduct cutting-edge research related to computing and robotics during a six week summer program. The RET teachers will also develop curricular materials for the integration of computing concepts into STEM disciplines and implement related research-based activities in their classrooms during the academic year. The program includes two sequences of workshops and a summer symposium to explore the impact on the teachers' knowledge, experiences, and instructional practice and to reach out to other teachers in the local community. The RET Fellows will become master teachers and assist in providing other secondary school teachers with professional development in computing, thereby broadening the impact and sustainability of the project.<br\/><br\/>Intellectual merit: The intellectual merit of this project lies with the strong research mentor team and excellent infrastructure at the university for the project. The project builds on prior research and pilot studies. The projects are compelling and of current interest and are exciting for teachers and their students. The new research-based curriculum implemented In schools during the academic year should provide important research-based methods for developing computational thinking competencies at the K-12 level.<br\/><br\/>Broader impacts: The program plans to target in-service teachers from the Yolo county school district as well as pre-service education students at the university. The target schools have a large population of underrepresented and economically disadvantaged students who will be impacted. National and international dissemination of the results and the curricular materials will open the pathways for replication and broader impact that is pervasive and fundamental to moving the underlying concepts into the K-12 arena.","title":"RET Site: Computing Research Experiences for STEM Teachers (CREST)","awardID":"1132709","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1359","name":"RES EXP FOR TEACHERS(RET)-SITE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":[494127,"535738"],"PO":["564181"]},"175320":{"abstract":"There has been an explosive growth of cross-layer designs proposed for wireless networks. These designs break the layered structure to actively exploit the dependence between protocol layers in wireless networks. However, the large number of cross-layer designs creates serious coexistence issues. The violation of layered structure may not comply with restrictions that constrain the coexistence among many cross-layer designs and other network systems, causing significant issues, such as degraded performance, inconsistent distributed decision making, network partition, and instability. The objective of this project is to systematically and rigorously categorize and analyze coexistence restrictions of cross-layer designs in wireless networks. In this project, coexistence restrictions of various cross-layer designs are theoretically modeled and analyzed. Different kinds of coexistence restrictions are defined, the conditions for their occurrences and their impact on network operations are revealed, and methods to check coexistence issues are developed. The project also seeks restriction-compliant protocol design techniques. This project serves as a major effort in the understandings of cross-layer designs in wireless networks and is the pioneer in providing systematic analysis of coexistence restrictions of cross-layer designs. The result of this project can be used to evaluate cross-layer designs? limitations and potential problems. This will promote the acceptance of good cross-layer designs in real systems and prevent architecture failures in design integration. In addition, this project provides practical techniques for designing more compatible cross-layer systems. Ultimately, this will greatly enhance the flexibility and robustness of current and future wireless network systems.","title":"CAREER: Study of Coexistence Restrictions of Cross-layer Designs in Wireless Networks","awardID":"1054697","effectiveDate":"2011-07-15","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["521677"],"PO":["557315"]},"177333":{"abstract":"The last few years have witnessed an explosive growth in the popularity and capabilities of mobile handheld devices such as smart-phones, tablets, and laptops. The rapidly expanding computational power of these portable devices has enabled their users to access, process, and share content anytime, anywhere. However, these devices have already put significant stress on many of today's 3G\/4G cellular networks, which, if left unaddressed, will result in increasingly poor quality-of-service and user experience in the future. <br\/><br\/>To address this problem, the project investigates how to exploit the key attributes of mobile content sharing environments to dramatically increase network performance. Preliminary studies show that jointly utilizing the wireless channel and mobility can yield greater gains than even the sum of exploiting mobility and the channel independently. In this case, the whole is greater than the sum of its parts! Thus, the main focus of this research is to (i) develop the analytical foundation for double opportunism - i.e., joint exploitation of the wireless channel and mobility-considering the practical realities of the wireless environment for content distribution networks; (ii) develop low-complexity algorithms, heuristics, and distributed protocols that are provably efficient; (iii) investigate the impact of imperfect knowledge on performance; (iv) validate the results via testbed deployment.<br\/><br\/>The project is expected to lead to fundamental breakthroughs in the design and performance of wireless networks for content sharing. The algorithms and protocols developed by exploiting double opportunism, and the theories underlying them, will have a significant impact on the wireless industry.","title":"NeTS: Medium: Collaborative Research: Mobile Content Sharing Networks: Theory to Implementation","awardID":"1065444","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["7594","563625"],"PO":["565303"]},"185198":{"abstract":"Proposal #: CNS 11-38674<br\/>PI(s): Andrews, Anneliese<br\/> Mahoor, Mohammad<br\/>Institution: University of Denver<br\/>Title: RAPID: CRAWLER Robot with Dual-Use Limbed Locomotion and Manipulation<br\/>for Void Inspection<br\/>Project Proposed:<br\/>This RAPID project, developing and fabricating a custom robotic tool based on the ongoing work in the CRAWLER robot with reconfigurable attachments, aims to deploy the tool in the areas affected by the 2011 tsunami and nuclear disaster in Fukushima, Japan. The robot, to be donated to International Rescue System Institute at Tohoku University in Sendai, will be based on the recent improvements that would make CRAWLER more resistant to water and more cleanable and maintainable. The system expands on the present robotic systems developed by the team to enable the use of multi-camera, orthogonal vision system for emergency responders that would be attached to the robot for enhanced situational awareness. This new vision subsystem constitutes part of its novelty. The Japanese-USA academic researcher team will be engaged in some of the following activities:<br\/>- Develop methods to analyze the efficacy of the improved robot?s vision system.<br\/>- Engineer and deploy the robot to collect data about the degree of contamination. <br\/>- Deploy the robots in Japan through Japanese colleagues at the International Rescue System Institute at Tohoku University in Sendai.<br\/>The investigators collaborate with Dr. Satoshi Tadokoro (Tohoku University), a search and rescue researcher. A support letter has been submitted by Dr. Tadokoro for the proposed joint research. The project is expected to fabricate an improved version of the robots (developed under a separate NSF grant) in extreme environment tests, and donate this robot to the Japanese lab for joint experimentation and in-situ testing. Additional funding is also requested to travel to Japan for collaborative research and experimentation. Proposed are also interactions and coordinating efforts with Robin Murphy (TAMU) who is organizing a workshop following up on the disaster in Japan (the related travel funds are not part of this proposal).<br\/>Broader Impacts: <br\/>This proposal promises an immediate benefit to society by supporting economic recovery efforts in Japan through a participatory research paradigm. Moreover, long term benefits for future disasters are in evidence since emergency response and unmanned systems are both formative domains and the data collected will advance the discovery and understanding of intelligent, human-centered systems in unpredictable situations. In addition, the use of the proposed tool should establish an important milestone in robotics allowing the nuclear power industry to be better positioned to rapidly respond to disasters in the future. Finally, the project aims to train undergraduate and graduate students and expose them to high-impact application areas. The PIs have strong track record in advising underrepresented students.","title":"RAPID: CRAWLER Robot with Dual-Use Limbed Locomotion and Manipulation for Void Inspection","awardID":"1138674","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["554621","522995"],"PO":["565303"]},"177256":{"abstract":"American Sign Language (ASL) animations have the potential to make information accessible to many deaf adults in the United States who possess only limited English literacy. In this research, which involves collaboration across three institutions, the PIs' goal is to gain a better understanding of ASL linguistics through computational techniques while advancing the state of the art in the generation of ASL animations for accessibility applications for people who are deaf. To these ends, the PIs will develop linguistically based models of two aspects of ASL production: movements required for head gestures and facial expressions that carry essential grammatical information and frequently extend over domains larger than a single sign, and the timing and coordination of manual and non-manual elements of ASL signing. Preliminary work has shown that these issues significantly affect how well signers understand ASL animations, and that these aspects of current ASL animation technologies require improvement. How should the face of a human or animated character be articulated to perform, with accuracy, the linguistically meaningful facial expressions that are part of ASL grammar? How should the onsets, offsets, and transitions of these movements be produced? How should the facial expressions and hand movements be temporally coordinated so that the ASL production is as grammatically correct and understandable as possible? To answer open questions such as these, the PIs' novel approach will apply techniques from computer vision to linguistically annotated video data collected from human signers, in order to produce models for use in animation-production. The PIs will expand their existing annotated video ASL corpora through new data collection and annotation, and will analyze these data to study the use, timing, and synchronization of manual and non-manual components of ASL production. The annotated videos will be used to train high quality computer vision models for recognition of linguistically significant facial expressions and timing subtleties. Parameters of these computer vision models will be used to hypothesize computational models of ASL timing and facial movements, to be incorporated into ASL-animation generation software and evaluated by native signers. The models will be iteratively refined in cycles of user-based studies and incorporated into ASL animation technologies to more accurately mimic human signing. Project outcomes will include high quality models of the movement of virtual human characters for animations of ASL performance. The analysis of video corpora of ASL will produce new linguistic insights into the micro-facial expressions and the temporal coordination of the face and hands in ASL production, while advances in the analysis of ASL prosody will contribute to an understanding of the fundamental commonalities and modality-specific differences between signed and spoken languages that is essential to a full understanding of the human language faculty. The creation of new modeling approaches and recognition techniques will advance the field of computer vision, by benefiting the identification and tracking of the human face and body in video during the rapid and complex movements of ASL (and other forms of human movement).<br\/><br\/>Broader Impacts: This research will lead to significant improvements to technology for generating linguistically accurate ASL animations, which will make information, applications, websites, and services more accessible to the large number of deaf individuals with relatively low English literacy. Advances in computer vision techniques for recognizing ASL in videos of humans will have general applicability in human-computer interaction, recognition and animation of facial expressions, and computer vision. The corpora created in this project will enable students and researchers in both linguistics and computer science (including those without access to the requisite technological and human resources to carry out their own data collection from native signers and time-intensive linguistic annotations) to engage in research on ASL. The techniques to be developed will also enable partial automation of the time-consuming creation of annotated ASL video corpora. As in the PIs' earlier work, the proposed research will create opportunities for people who are deaf and members of other underrepresented groups to participate in scientific research.","title":"HCC: Medium: Collaborative Research: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing","awardID":"1065013","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[475203],"PO":["565227"]},"177289":{"abstract":"The advent of new microprocessor chip architectures (e.g., GPUs, multi-core\/many-core chips architectures), next generation large scale integrated problems such as datacenter applications points to the need for a comprehensive rethinking of the approach to architecture design, programming, and software design. The research proposed in this document will be concerned with the characteristic behavior of future applications, highlighted by the performance, scalability, and how computer systems (execution and architecture models, and compilers\/runtime software technologies) can be appropriately targeted. This work will result in the characterization of the features and requirements of such applications, in the development of suitable execution and architecture models that match their needs, and in the development of a compilation technology and its associated tools that will work in coordination with the proposed architecture models for the targeted application domains. The outcome of the proposed research will be an in-depth understanding of the needs of future applications, the demonstration of new architecture ideas, as well as the design of compilation methods and tools.","title":"SHF: MEDIUM: Collaborative Research: Architecture, Programmability and Performance of Large Scale Parallel Systems","awardID":"1065147","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[475291],"PO":["366560"]},"185880":{"abstract":"This project covers travel and lodging support for graduate students to attend the workshop on Future Internet Architectures. The Future Internet Workshop (FIW) is focused on presentations from students on their new work on questions of Future Internet Architecture, providing an opportunity for students to interact with both other students and with senior people from the telecommunications industry (e.g., Cisco, Deutsche Telekom, Comcast, etc.). The workshop to be held on June 9th and 10th at the University of Pennsylvania in Philadelphia, PA hosts 22 students.<br\/><br\/>INTELLECTUAL MERIT: Twelve of the students attending are presenting talks exploring various technical approaches to architecture of a future internet, including fault&#1049458;tolerance, malware detection, new forms of multicast, and new approaches to debugging distributed software systems. <br\/><br\/>BROADER IMPACT: The Internet has impacts far beyond the technologies, and the values inherent in the design of a Future Internet will strongly affect the uses and applications of that Internet. An example would be healthcare data, a kind of information that many people are uncomfortable putting on the Internet today, and may be even less comfortable with in a cloud-based future Internet. Approaches to security and availability and other technical issues must be consistent with societal, governing and economic issues. The workshop has technology leaders from the Internet service providers as invited speakers and provides students (a population from which future technology leaders will be drawn) with an opportunity to hear these speakers and interact with them to understand issues beyond the technology.","title":"Support For Future INTERNET Workshop June 9-10th AT University of Pennsylvania","awardID":"1142321","effectiveDate":"2011-07-01","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["527253"],"PO":["565090"]},"185320":{"abstract":"Proposal #: CNS 11-39364<br\/>PI(s): Nelson, Tracy W.<br\/>Institution: Brigham Young University<br\/>Title: RAPID\/IUCRC: An International University Collaborative Research Program between NSF\/IUCRC CFSP Center and Tohoku University <br\/>Project Proposed:<br\/>This RAPID project, enabling graduate students from the Kokawa Laboratory from Tohoku University in Japan to go to Brigham Young University (BYU) to use the facilities and equipment available within the CFSP (Center for Friction Stir Processing, an NSF ENG I\/UCRC) to continue performing essential research for short periods of time (2-6 weeks), responds to the disaster in the labs caused by the March 11 earthquake in Japan. Sensitive Optical and electron microscopy equipment were rendered unusable and are currently not on the priority list for immediate repair. Thus, the project aims to satisfy the following objectives:<br\/>- Enable students to continue their research;<br\/>- Engage graduate, undergraduate, and underrepresented students and faculty from five different countries in an international collaborative effort; <br\/>- Expose students to a wider breadth of research and development in FSW&P;<br\/>- Expand international networking opportunities; and<br\/>- Enlarge the international database of Friction Stir Welding (FSW) processes and practices.<br\/>The research work at both universities has a long history of producing outstanding intellectual results, as evidenced by more than 60 publications in Tohoku and more than 30 at BYU in this area in recent years. While at BYU, the students will be performing joint research in:<br\/>- FSW of steels, stainless steels, and titanium alloys,<br\/>- Tooling for FRW&P, and<br\/>- Grain boundary engineering.<br\/>The USA researchers collaborate with the Japanese investigators from Tohuku University, Dr. Hiroyuki Kokawa and Dr. Yutaka Sato. The former has co-authored a relevant book in the area, while the latter has spent a sabbatical year at BYU. Thus, a useful collaboration already exists. A letter of support and biographical sketch of the Japanese collaborator is included in the supplementary document. <br\/>Broader Impacts: <br\/>Students representing five different countries, along with faculty from Japan and USA will be engaged in this international collaborative research effort. Both undergraduate and graduate students will be invited to present their work to more than 20 industrial sponsors from around the world during annual and semi-annual CFSP meetings. The research will be broadly disseminated in the technical community in the form of journal papers and technical presentations.<br\/>Obviously, contributing equipment to help in the completion of on-going research should enhance the USA students while enabling the exchange of ideas. In general, the project also contributes to train graduate and undergraduate students exposing them to high-impact application areas.","title":"RAPID\/IUCRC: An International University Collaborative Research Program between the Center for Friction Stir Processing (an NSF I\/UCRC) and Tohoku University","awardID":"1139364","effectiveDate":"2011-07-15","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7369","name":"INTERNATIONAL RES NET CONNECT"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"1504","name":"GRANT OPP FOR ACAD LIA W\/INDUS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":[497252],"PO":["557609"]},"181096":{"abstract":"The advance of VLSI technology has reached to 32nm feature size and below. For such a nano-scale process, lithography no longer produces the ideal shape\/dimension of circuit components in a silicon wafer, and the corresponding electrical parameters may vary as large as 1\/3 or more. A major concern in VLSI design is how to evaluate the circuits\/systems performance made in such nano-scale process. In the other words, we want to know how much the performance specs will change due to variation in circuit parameters from their nominal values caused by the process uncertainties. The current research on performance robustness analysis is developed mainly along the line of the Monte-Carlo sampling method, or stochastic and statistical analysis methods. They all require a high level of computation complexity to achieve the required accuracy and one would like to avoid the evaluation of large number of samples to validate the performance range of a VLSI circuit\/system. <br\/><br\/>In this research the PIs propose a novel method for VLSI circuit performance robustness analysis which does not require evaluation of large numbers of samples. Instead, it computes only a few critical polynomials in frequency domain, or critical systems in time domain. It is a fundamentally new way to analyze VLSI circuit performance robustness. The consequent leap of computation efficiency would make nano-scale VLSI circuit design and its performance robustness analysis practically possible. The objectives of this project are: (i) to develop a solid theoretical basis for the performance robustness analysis of VLSI circuits in both frequency and time domains; (ii) to develop an efficient, novel method for computing VLSI circuit performance variation bounds and distribution (due to the process variation) without using the Monte-Carlo method.<br\/><br\/>The broad impact of this project will be its potentially transformative effect on the robust analysis methods for VLSI circuits. This research will be integrated into the graduate education of the Ph.D. students at the two universities involved, and disseminated by publications in journals and presentations at conferences, a workshop and collaboration with industry, and will hopefully contribute to broad thinking across multiple disciplines.","title":"SHF: Small: Collaborative Research: A Novel Method for the Performance Analysis of VLSI Circuits with Severe Parameter Value Variations due to Nano-scale Process","awardID":"1115556","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485314],"PO":["562984"]},"185210":{"abstract":"Proposal #: CNS 11-38733<br\/>PI(s): Lee, Eva K <br\/>Institution: Georgia Tech Research Corporation<br\/>Title: Population Protection and Monitoring in Response to Radiological Incidents <br\/>Project Proposed:<br\/>This RAPID project, collecting rare and real-life data pertaining to radiological emergency response in Japan, builds on previous work that collects and processes a large amount of time-motion study data in the public health emergency response system planning and usage at Georgia Tech. The team will work with Japanese collaborators in their efforts in performing assessment of the recent series of disasters in Japan, as well as in assisting in the recovery. This team is experienced in using a real-time information-decision support system for emergency preparedness. The collection and the analysis of scarce data in the so-called Knowledge Data Bank for Radiological Responses, speaks to the importance and uniqueness of the proposed system. The Japanese-USA academic research team will be engaged in some of the following activities: <br\/>- Establish a knowledge data bank for radiological response: emergency data collection and resource assessment. <br\/>- Process mapping and time motion study.<br\/>- Interview individuals (emergency workers, affected individuals, etc.)<br\/>- Incorporate the radiological knowledge data bank into a real-time simulator and decision support system. <br\/>- Analyze and assess the effects of the disaster in collaboration with NanZan University.<br\/>The final system will facilitate assessment of current operations performance versus pre-disaster preparedness. It will allow for the study, training, and enhancement of emergency response, as well as future planning for radiological incidents. The work provides a unique opportunity to collect on-the-ground emergency response data. <br\/>The researchers collaborate with the Japanese investigators from NanZam University, Dr. Suzuki and Dr. Sasaki, whose work is funded by the Japan Society of Promotion of Science. The Japanese team will arrange trips for the US team to visit the various shelters, distribution, medical, and\/or health-registering sites where they will conduct the time-motion studies, interviews, and operations\/performance observations, and evaluations. A letter of support and biographical sketches of the Japanese collaborators are included in the supplementary document. <br\/>Broader Impacts: <br\/>This project promises an immediate benefit to society by supporting economic recovery efforts in Japan through a participatory research paradigm. The data bank is critical to our national medical preparedness, emergency response, and homeland security. Moreover, the work is urgent for population protection from nuclear plant accidents. Long-term benefits for future disasters are in evidence. Obviously, the emergency response and disaster mitigation research will be enabled with the developed simulation and decision support system along with the knowledge data base. In general, the project also contributes to train graduate students exposing them to high-impact application areas.","title":"RAPID: Population Protection and Monitoring in Response to Radiological Incidents","awardID":"1138733","effectiveDate":"2011-07-01","expirationDate":"2014-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":[496947],"PO":["557609"]},"185100":{"abstract":"Proposal #: CNS 11-38110<br\/>PI(s): Kumar, R. Vijay; Michael, Nathan D<br\/>Institution: University of Pennsylvania<br\/>Title: RAPID: Aerial Robots for Rapid Response: Remote Autonomous Exploration and Mapping<br\/>Project Proposed:<br\/>This RAPID project, developing and deploying a team of autonomous aerial robots that can enter an unstructured, hazardous environment to explore and map a facility, provides information to human operators in safe, remote locations. The work brings together research groups with complementary expertise in robotics to address the challenging problem of acquiring imagery and three-dimensional maps for post-disaster assessment. Autonomous robots will be deployed without a direct communication link enabling access to areas in the Fukushima that are currently inaccessible.<br\/>Addressing an urgent need, the work consists of redesigning aerial robotic systems to perform mapping, localization, and exploration functions in indoor and outdoor environments without prior knowledge of the environment or GPS (Global Positioning System). The system, to be deployed in highly contaminated environments such as the area of Fukushima disasters in Japan, expands the present robotic systems developed by the team. It should be able to build maps and localize, plan, and control autonomously in that map, but requires interactions with a base-station to communicate the relevant information. The paradigm shifting capabilities of aerial robots to act independently and be deployed in critical contaminated areas exhibit novelty. The Japanese-American academic research team will be engaged in some of the following activities: <br\/>- Develop methods to acquire information from highly contaminated environments, such as in case of radiation contamination. <br\/>- Engineer and deploy one or more autonomous robots (i.e., without the link to the base station) equipped with cameras and laser range finders as well as potentially carrying sensors that might reveal new insights about the degree of contamination. <br\/>- Develop algorithms and methods for information gathering and map building. <br\/>- Deploy the robots in Japan through Japanese colleagues.<br\/>Most UAVs (Unmanned Aerial Vehicles) are teleoperated with several human operators engaged in the deployment of each UAV. The cross fertilization of technologies for robotics and UAVs has potential to create new small to medium scale autonomous UAVs with a wide range of civilian and defense applications. This project will explore the use of autonomous UAVs for acquiring information from environments that are impossible to access because of radiation contamination. One or more autonomous quad rotor robots equipped with cameras and laser range finders will be deployed to explore the partially-known environment and build 3-D maps of the structure and potentially carrying sensors that might reveal new insights about the degree of contamination. These robots will have to operate without any communication link to the base station. Thus this will represent the first deployment of a truly autonomous robot of its kind.<br\/>This work involves a collaboration with Dr. Satoshi Tadokoro, a researcher in search and rescue robotics, from Tohuku University in Sendai, Japan. A support letter has been submitted by Dr. Tadokoro for the proposed joint research. Another collaborator from the same University, Dr. Kazuya Yoshida, leads the project entitled ?Robotics in Extreme Environment? and brings the ?Extreme Robotics? background to this collaborative research endeavor. The project, expected to lose robots in extreme environment tests, consequently requires building additional autonomous aerial robots for the purpose of the experiments. Funding is also requested to travel to Japan for collaborative research and experimentation. <br\/>Broader Impacts: <br\/>The tragic sequence of events in Sendai and the Fukushima I and II nuclear power plants has resulted in significant contamination due to radioactive iodine, cesium and stronium, making it nearly impossible for humans to enter many areas in the power plants to assess damage. First, the use of robots to acquire information from currently inaccessible areas will have a significant impact on post-disaster recovery operations. Second, the use of autonomous aerial robots will establish an important milestone in robotics and will allow the nuclear power industry to be better positioned to rapidly respond to disasters in the future. This proposal promises an immediate benefit to society by supporting economic recovery efforts in Japan through a participatory research paradigm. Moreover, long term benefits for future disasters are in evidence since emergency response and unmanned systems are both formative domains and the data collected will advance the discovery and understanding of intelligent, human-centered systems in unpredictable situations. Furthermore, the use of autonomous aerial robots will establish an important milestone in roboti","title":"RAPID: Aerial Robots for Remote Autonomous Exploration and Mapping","awardID":"1138110","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["553286",496631],"PO":["565303"]},"177312":{"abstract":"The field of human genetics has undergone a revolution in the past 10 years with the advent of high-throughput genomic technologies which can measure human variation at low cost. The flagship application of these technologies has been the genome-wide association study (GWAS) where genetic variation information is collected from hundreds of thousands of individuals, a portion of which have a specific disease and a portion of which are healthy individuals. Identification of correlation between genetic variants with disease status has led to the identification of hundreds of new genes involved in dozens of human diseases. All applications of these technologies, including GWAS, require individuals to \"share\" their genetic data. In today's typical GWAS, thousands of individuals must consent to have their genetic information collected and incorporated into a database which also contains information on their disease status. Unfortunately, an individual's genetic data is extremely sensitive as it is considered medical information about an individual. In this proposal, the team addresses the natural tension between privacy and the application of personal genomics technologies by capitalizing on recent breakthroughs in cryptography. They present a novel technological approach to keep one's genetic data private, yet taking full advantage of genetic information - in a privacy-preserving way, by taking advantage of several techniques that have been recently developed in an area broadly referred to as secure computing, which address the problem of allowing a collection of individuals to compute some output that depends on all their inputs, without having to reveal their individual inputs to each other. The core of this proposal focuses on the application of secure computing to two specific problems in personal genomics: The first is the problem of identification of relatives from genetic variation information while preserving privacy of genetic material. The second, is the identification of disease causing variants without sacrificing individual patient's genetic privacy.<br\/><br\/>The development of the techniques presented in this proposal will have a profound impact on personal genomics and the field of genetics in general for several reasons. First, the easing of privacy fears will drop a major barrier to participation in personal genomics likely increasing the utilization of recent advances in genetic and genomic technologies for the public. This increased utilization will accelerate the medical benefits of these technologies. Second, the current thinking is that it is impossible to protect privacy in personal genomics and the results of this project will surprise many in the field, leading to a rethinking of the how to handle privacy in genetic studies. Finally, this research direction will likely lead to new problems and research directions for the cryptography research community and foster new collaborations between genetics researchers, cryptographers and mathematicians.<br\/><br\/>This project also contributes to training the next generation of interdisciplinary scientists. The investigators all teach advanced undergraduate courses in both genetics and cryptography and it is likely that the topics developed in this proposal will be included in the curriculum of the courses. In addition, the graduate students involved in this proposal will obtain interdisciplinary training in both genetics and computer science theory.","title":"III: Medium: Private Identification of Relatives and Private GWAS: First Steps in the New Field of CryptoGenomics","awardID":"1065276","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["533831","521734","554197"],"PO":["565136"]},"177334":{"abstract":"The advent of new microprocessor chip architectures (e.g., GPUs, multi-core\/many-core chips architectures), next generation large scale integrated problems such as datacenter applications points to the need for a comprehensive rethinking of the approach to architecture design, programming, and software design. The research proposed in this document will be concerned with the characteristic behavior of future applications, highlighted by the performance, scalability, and how computer systems (execution and architecture models, and compilers\/runtime software technologies) can be appropriately targeted. This work will result in the characterization of the features and requirements of such applications, in the development of suitable execution and architecture models that match their needs, and in the development of a compilation technology and its associated tools that will work in coordination with the proposed architecture models for the targeted application domains. The outcome of the proposed research will be an in-depth understanding of the needs of future applications, the demonstration of new architecture ideas, as well as the design of compilation methods and tools.","title":"SHF: MEDIUM: Collaborative Research: Architecture, Programmability, and Performance of Large Scale Parallel Systems","awardID":"1065448","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[475422],"PO":["366560"]},"175178":{"abstract":"Modern managed programming languages, from Java and C# to Javascript and Ruby, provide a compelling set of software engineering advantages over traditional languages like C and C++. Unfortunately, the continue to suffer from a range of well-documented performance problems. Their inefficient use of memory, in particular, imposes a significant penalty, with debilitating consequences for the quality and capacity of critical server software built in these languages. In spite of intensive research and development, these problems have remained stubbornly unsolved. As a result programmers face a difficult dilemma: choose a safe and secure managed language, but take a major performance hit, or continue taking their chances with C and C++.<br\/><br\/>This project explores a new approach, called cooperative virtual machines, which attacks the problem by improving communication and cooperation between the programmer and the managed language runtime system (the virtual machine). The key idea is that with extra information, virtual machines can provide much more efficient services because they are customized to each application's needs. The project involves building new tools for exploring and quantifying memory performance, designing a configurable garbage collector for large server applications, and developing techniques to give programmers<br\/>more control over the low-level representation and management of data structures. Significant improvements in memory utilization and performance will allow existing computing infrastructure (hardware and software) to deliver higher quality services to more users. A crucial component of this project is improved pedagogical tools and techniques to help new programmers reason about the performance of these complex systems.","title":"CAREER: Cooperative Virtual Machines: Mechanisms and Policies for Application-Aware Runtime Services","awardID":"1053862","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["553399"],"PO":["564588"]},"177246":{"abstract":"Spatial diversity has proved to be very effective in increasing wireless network capacity and reliability. However, equipping a wireless node with multiple antennas may not always be practical. This project explores Cooperative Communications (CC), where each node is only equipped with a single antenna and spatial diversity is achieved by exploiting the antennas on other nodes in the network. Although CC at the physical layer has been under intensive research in recent years, fundamental understanding of CC in ad hoc networks remains limited. The goal of this project is to optimize network level throughput by exploiting cooperative relaying at the physical layer. This project focuses on designing network level algorithms based on analytical models for cooperative relaying such that network level throughput can be maximized. Specifically, there are three inter-dependent research thrusts in this project: (1) optimal use of network coding in cooperative relay networks; (2) relay node selection for throughput maximization; and (3) performance limits of cooperation in multi-hop relay networks. This research serves a critical need in advancing cooperative relay network research by developing new mathematical tools to study some open and important problem areas. The success of this project offers a major step forward in establishing its theoretical foundation. The research on throughput optimization also leads to the development of new algorithmic and optimization tools that are beyond the traditional domain of convex optimization. An important educational objective of this project is to develop new cross-disciplinary course materials for wireless networking.","title":"NeTS:Medium: Throughput Optimization of Cooperative Relaying in Wireless Networks","awardID":"1064953","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["560139","296385","564848"],"PO":["557315"]},"181371":{"abstract":"The objective of this project is to develop efficient distributed algorithms for joint information processing, optimization and coordination of static, pervasive sensor nodes and mobile, autonomous agents that altogether monitor and act on the environment. The application scenarios include tracking, searching and planning mobile agents with the underlying sensor network as a supporting communication infrastructure, and online resource management and allocation guided by real-time sensor detections.<br\/><br\/>This project focuses on three research problems: distributed min-cost bipartite matching, kinetic minimum Steiner tree, and facility location for mobile nodes, by using two non-trivial technical approaches, namely, embedding of the network metric into tree metrics, and distributed primal dual framework. There are two central intellectual questions investigated in this project.<br\/><br\/>1) How to make use of the sensor data to best serve user requests? This involves developing communication efficient schemes for sensors detecting interesting local events and the mobile users seeking such information to find each other.<br\/><br\/>2) How to best make use of the continuity and coherence in mobility, either in the presence of mobility enabled sensor data (e.g., detections of continuously moving targets), as well as the locations of mobile users? The project provides solutions that adapt to the system configuration with low update cost, avoiding drastic sudden changes or any level of reconstruction.<br\/><br\/>This project helps to extend the current Internet to the physical world, encouraging seamless integrations of sensing and control of the physical environment. The PI integrates the research agenda with new and existing curriculum development for both undergraduate and graduate education, and continues her efforts in improving female presence in computer science and exposure of research for high school students.","title":"NeTS: Small: Algorithmic Foundations for Joint Information Processing and Optimization in a Hybrid Mobile Sensor Network","awardID":"1116881","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["519259"],"PO":["565303"]},"181492":{"abstract":"Over the past decades, the field of algorithms has developed a toolbox of theoretical techniques that let computer systems to do more with less---to solve harder problems more quickly, using less memory, less communication with other computers, and less of a human user's assistance. One measure of algorithms' research success has been the large number of implemented systems whose designs have been impacted by contributions from the algorithms community. There are many more such successes waiting in the wings for individuals or groups who can draw the connection between an existing algorithmic technique and an existing applied problem. Often, the biggest challenge is recognizing the connection between the practitioner's problem statement and the proper algorithmic solution techniques. This research addresses the process of ``technology transfer'' from the algorithmic toolbox to other computer science domains.<br\/><br\/>The investigator is working closely with practitioners in various areas of computer science to identify computational problems whose efficient solution would advance their research agendas, dig through the theory toolbox to find techniques that, properly adapted, can be used to efficiently solve those problems, and assist in such adaptation. Domains being addressed include natural language processing, detection of influence pathways in biological networks, traffic route planning that accounts for uncertain delays, network coding for efficient use of communication bandwidth, and efficient use of crowdsourced computation. But rather than being driven by a particular problem domain, the investigator is interested in the overall process for applying theoretical work in algorithms to problems in the practical domain, and is always seeking new applied problems that can benefit from this approach.<br\/><br\/>Successful completion of the proposed work will contribute advancement to many different branches of computer science. The contributions to other branches of computer science will, in turn, allow them to achieve their goals of broad impact on society. The investigator also hopes to increase the general sense of connection between theoreticians and practitioners, yielding increased collaborations and successful applications of algorithms to theory beyond those made directly during this project. The project will contribute to research training by continuing to employ large numbers of students with attention given to gender diversity.","title":"AF: Small: Applied Algorithims: Tech Transfer from the Algorithims Toolbox II","awardID":"1117381","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[486265],"PO":["565251"]},"181261":{"abstract":"A key enabler of Nanoscience and Nanotechnology is the Atomic Force Microscope (AFM) that has opened up new realms, of interrogation and manipulation of matter at the atomic scale. It has resulted in breakthroughs in understanding sub-atomic molecular structure, protein folding dynamics and materials characterization. The potential impact of significantly faster AFM based imaging is immense, e.g., it will allow the study of dynamics of material at the nanoscale that was hitherto not accessible. The aim of this research is to study modern signal processing techniques that achieve gains in imaging speeds by an order of magnitude. The investigators will push the synergistic transfer of knowhow between the engineering and the AFM communities through student visits and specialized workshops. The findings will be integrated at appropriate levels into the undergraduate and graduate curriculum.<br\/><br\/>The main component of an AFM is a cantilever that deflects in response to forces at the pico-Newton scale. The focus of this research is the dynamic mode AFM operation, where the cantilever gently taps the sample being imaged; the mode of choice for imaging biological samples. Even though forces do form a good indicator of sample topography, the system memory and the nonlinearities of the AFM system dynamics preclude a direct mapping of the cantilever forces into a finer description of topography, especially at high imaging speeds. The investigators model the complex nanoscale interactions in a mathematically tractable manner that facilitates the development of maximum a posteriori (MAP) sequence detection of the sample features. Factor graph representations and the development of appropriate inference schemes will be studied. The algorithms will be implemented on FPGAs and tested exhaustively with experimental data.","title":"CIF: Small: Collaborative Research: Signal processing for enabling high speed probe based nanoimaging","awardID":"1116322","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["550838"],"PO":["564898"]},"181097":{"abstract":"The advance of VLSI technology has reached to 32nm feature size and below. For such a nano-scale process, lithography no longer produces the ideal shape\/dimension of circuit components in a silicon wafer, and the corresponding electrical parameters may vary as large as 1\/3 or more. A major concern in VLSI design is how to evaluate the circuits\/systems performance made in such nano-scale process. In the other words, we want to know how much the performance specs will change due to variation in circuit parameters from their nominal values caused by the process uncertainties. The current research on performance robustness analysis is developed mainly along the line of the Monte-Carlo sampling method, or stochastic and statistical analysis methods. They all require a high level of computation complexity to achieve the required accuracy and one would like to avoid the evaluation of large number of samples to validate the performance range of a VLSI circuit\/system? <br\/><br\/>In this research the PIs propose a novel method for VLSI circuit performance robustness analysis which does not require evaluation of large numbers of samples. Instead, it computes only a few critical polynomials in frequency domain, or critical systems in time domain. It is a fundamentally new way to analyze VLSI circuit performance robustness. The consequent leap of computation efficiency would make nano-scale VLSI circuit design and its performance robustness analysis practically possible. The objectives of this project are: (i) to develop a solid theoretical basis for the performance robustness analysis of VLSI circuits in both frequency and time domains; (ii) to develop an efficient, novel method for computing VLSI circuit performance variation bounds and distribution (due to the process variation) without using the Monte-Carlo method.<br\/><br\/>The broad impact of this project will be its potentially transformative effect on the robust analysis methods for VLSI circuits. This research will be integrated into the graduate education of the Ph.D. students at the two universities involved, and disseminated by publications in journals and presentations at conferences, a workshop and collaboration with industry, and will hopefully contribute to broad thinking across multiple disciplines.","title":"SHF: Small: Collaborative Research: A Novel Method for the Performance Analysis of VLSI Circuits with Severe Parameter Value Variations due to Nano-scale Process","awardID":"1115564","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485316],"PO":["562984"]},"185013":{"abstract":"This award supports student participation in tutorials on security issues at the hardware design layer, in order to expand the pool of scientists qualified to pursue research topics in this area.","title":"Enhancing Interdisciplinary Research in Security and Computer Architecture Via Tutorial at FCRC","awardID":"1137656","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["518483","541924"],"PO":["565264"]},"175234":{"abstract":"As transistor count has increased exponentially with Moore?s Law, power has become the number one problem in microprocessor design. Traditional scaling theory relies on reducing supply voltages in proportion to the reduction in device dimensions to keep dynamic power in check. In addition, leakage power also increases exponentially with decreasing supply voltages. This project proposes a novel micro-architectural design technique whose goal is to avoid the power wall by migrating most of the functionality of a modern microprocessor to spin-torque transfer magneto-resistive random-access memory (STT-MRAM) ? a leakage resistant, non-volatile, resistive memory technology. The central idea is to implement most of the on-chip storage and combinational logic using scalable, leakage-resistant STT-MRAM arrays and lookup tables to lower power dissipation, thereby allowing many more active cores under a fixed power budget than a conventional implementation could afford. To accomplish this, the investigator addresses challenges all the way from the circuit-level implementation of fundamental hardware building blocks to the development of larger-scale micro-architectural resources, control policies, and management approaches. At the circuit level, the project explores the design of content-addressable memories, registers, hybrid cells, and lookup tables based on STT-MRAM. At the architecture level, novel cache architectures, latency-hiding techniques, throughput optimizations, and write-power mitigation mechanisms are employed throughout the memory hierarchy. Novel hybrid memory cells are explored to eliminate write throughput and latency problems in heavily written queues and register files, while adaptive write policies, loop stream detectors, and micro-architectural resource allocation mechanisms limit write power to a small fraction of what is possible under a na\u00efve implementation. The project also addresses lookup-table based design of functional units and other combinational logic, as well as design- and run-time reconfiguration of these units.<br\/><br\/>Leveraging STT-MRAM in processor design holds the potential to induce a significant leap in the performance and scalability of computer systems, with tremendous positive fallout to science, technology, and society at large. The project is expected to pave the way towards efficient power-aware many-core processors that can scale to hundreds of active cores under a fixed power envelope. The educational component of the project involves (1) training both graduate and undergraduate students in computer architecture, (2) the introduction of a a new memory systems course that integrates resistive memories into the syllabus, and (3) an improved computer architecture curriculum that includes design experience for undergraduates. The investigator plans involvement in local outreach programs promoting the participation of women and underrepresented minorities in computer science and engineering, and initiating a new effort to increase the enrollment of minorities in University of Rochester's computer science and electrical and computer engineering programs.","title":"CAREER: Overcoming the Many-Core Power Wall with Resistive Computation","awardID":"1054179","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["517764"],"PO":["366560"]},"177302":{"abstract":"Correlation of seemingly innocuous information can create inference chains that tell much more about individuals than they are aware of revealing. However, while media coverage occasionally draws attention to privacy leaks on individual web sites, there is still no comprehensive analysis of the fundamental risks that users face in their online worlds. This project pursues such a study, focusing in particular on the threat of personal, yet publicly available information that can be correlated with modern multimedia retrieval and content analysis technologies. One thrust of the work is informing users about potential risks by exposing the broader possibilities that arise with more sophisticated privacy attacks. A second thrust concerns understanding the control that users can exercise over their privacy in the light of such potential. When analyzing the impact of \"global inference\" on privacy, the primary conceptual challenge concerns understanding the trade-off between the benefits that providing personal information to web services offers, versus the risks that doing so entails. By combining expertise from two traditionally separate communities---network security and multimedia---this work advances the start of privacy protection in an area that is poised to raise in importance as more information moves into public spaces. The project enables users to better understand the threats they are facing by developing scenarios that intuitively demonstrate the relevant effects; and it develops novel tools supporting them in better protecting their privacy. The research results will benefit the user community by promoting risk awareness and empowering control. The project also includes outreach activities including working with Berkeley Foundation for Opportunities in Information Technology (BFOIT) to recruit minority students in this research, organizing summer schools, and raising public risk awareness by working with Identity Theft 911.","title":"TC: Medium: Understanding and Managing the Impact of Global Inference on Online Privacy","awardID":"1065240","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1668","name":"FED CYBER SERV: SCHLAR FOR SER"}}],"PIcoPI":["562329","558409"],"PO":["562974"]},"173089":{"abstract":"The Gordon Research Conference (GRC) on Visualization in Science and Education is a premier meeting for active researchers from a variety of disciplines including scientific visualization, use of visualization in education, visualization design, virtual reality, cognitive research, etc. The conference provides a framework to exchange ideas and build a broadening community around a crucial cross-and trans-disciplinary field that supports scientific research and science education. This three-phase project builds on the previous meetings and activities of the interdisciplinary community of the Gordon Research Conference on Visualization in Science and Education, dating back to 1994. Phase 1 includes three pre-conference workshops (on Design Principles for the New Media, Assessing Visualizations: Design & Effectiveness, and Forging Creative Communities with Web-based Tools) and a new interactive Sci-Viz website informs and engages past and potential future members of the community with information on the pre-workshops and the conference itself, and serves as an organizing tool for dissemination and the design of the 2013 conference. Phase 2 (the conference itself) follows traditional GRC format and is held at Bryant University in July 2011 (see http:\/\/bit.ly\/grc-viz-2011). Phase 3 includes collaborative efforts supported via mini-grants, post-conference activities which include a meeting to produce a follow-up report on results and lessons learned from the 8 meetings of the Visualization GRC conference series plus follow-up online activities that involve the wider community in ongoing revisions of the report as well as contributing shared resources. <br\/><br\/>The broader impacts of the conference and its associated activities are to strengthen the professional interdisciplinary community around science visualization. Interactive community website (http:\/\/taxane.chem.unb.ca\/GRC-SciViz) will provide information on the series of activities and include a report outlining future research directions and collaborative activities.","title":"Visualization: Transforming Communities and Practice: Gordon Research Conference, Workshops, and Mini-Grants to Advance Visualization Research in Science & Education","awardID":"1042738","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7625","name":"REESE"}}],"PIcoPI":["181971",464254],"PO":["563751"]},"186069":{"abstract":"Organisms adapt to external perturbations through the optimized structure of their gene regulatory networks (GRNs). In the long-term, the state transition network of a GRN converges to a set of attractors that make the organism resilient to removal or functional impairment of genes. In wireless sensor networks (WSN), such attractors refer to a group of sensors serving as sink nodes for packets sent over multiple hops. This project maps such attractor based genomic robustness onto WSNs to infer optimal topologies and routing strategies that mitigate both sensor failure and a noisy wireless channel. This is being achieved by conducting in silico gene ?knock-down? experiments by simulating the functional removal of a gene from sample GRNs, to understand the dynamics of the attractor state space. This information is next used to design WSN topologies and routing protocols that are resilient to network uncertainty, node breakdown and compromise. <br\/>This project pursues the design of optimal wiring rules between sensors in a robust WSN that guarantees maximum probability of successful packet transmission under a given routing strategy. The guiding principle is to follow nature?s foot-steps in designing simple rules (i.e., routing algorithms) that guarantee maximum efficiency over an optimized WSN topology. It also develops innovative network-science based tools, and provides insights into the interplay of GRNs and WSNs that inspire new designs for engineered systems (i.e. fault-tolerant topologies for WSNs). Validation and testing are accomplished on real life WSN testbeds. Research results will be disseminated through publications, besides allowing for the design of new graduate-level courses.","title":"EAGER: Collaborative Research: Improving the efficiency of Wireless Sensor Networks using principles of Genomic Robustness","awardID":"1143737","effectiveDate":"2011-07-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["563362"],"PO":["565303"]},"182560":{"abstract":"This award provides travel and subsistence to graduates students (and some undergraduates) who will attend the Oregon Programming Languages Summer School (OPLSS) on \"Types, Semantics and Verification.\" The program is available at http:\/\/www.cs.uoregon.edu\/Activities\/summerschool\/summer11\/. The summer school is a two-week event that brings together top international researchers and students to engage in lectures and discussions on leading-edge foundational topics in programming languages and software verification, much of which is not yet in textbooks. The NSF support ensures participation of US students while paying attention to underrepresented groups. The summer school will help build the next generation of researchers and a global workforce with expertise in these important areas of research and education. 100 students have registered for the 2011 summer school. The organizers will make video of lectures and other educational materials available to the public immediately after the event.","title":"Oregon Programming Languages Summer School (OPLSS) on \"Types, Semantics and Verification\"","awardID":"1123479","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[489107],"PO":["564388"]},"181240":{"abstract":"Increasingly, real-world data has many dimensions or features but rather than filling out all dimensions equally, the data distributed on or near a surface of much lower intrinsic dimension. Examples include fMRI data and natural image and video data sets. This research will push the boundary of what is currently possible in the analysis of such large data sets by incorporating hierarchical structure into what is known as a \"sparse dictionary representation\" of the data. The results will include both basic intellectual contributions to machine learning methods and computational advancements that will aid the investigation of complex real world data. <br\/><br\/>A fundamental problem in learning the structure of complex data is how to effectively extract a set of features that reflects the underlying structure of the data. In many applications, including face recognition and object recognition, sparse dictionary representations have proved effective for this purpose. However, since solving large-scale sparse representation problems is very expensive, the method is generally limited to problems of moderate scale. This research reformulates the method into an incremental, multi-stage, hierarchical dictionary learning process. This approach incrementally extracts information from the data and uses this to refine the data representation in an organized hierarchical fashion. This enables the building of large-scale dictionaries in a computationally efficient way. The method hence extends the power of sparse dictionary representation methods to a wider variety of real world applications. It also has the flexibility to incorporate an existing state-of-the-art sparse coding algorithm as the basic solver and hence can extend the functionality of existing sparse coding algorithms to multi-stage, hierarchical dictionary learning.","title":"CIF: Small: Fast Stagewise Learning of Sparse Hierarchical Data Representations","awardID":"1116208","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["492896"],"PO":["564898"]},"181284":{"abstract":"Blind source separation (BSS) has found wide use in many disciplines including signal processing as it starts from a simple generative model minimizing assumptions on the data generation mechanism and achieves useful decompositions of the observed data. In particular, independent component analysis (ICA) has been the most commonly used approach to achieve BSS since statistical independence of the underlying components is plausible in many applications. Besides independence, sample correlation is another inherent property of many signals of interest. Traditionally, these two properties are addressed separately when developing methods for source separation. Entropy rate, on the other hand, is a natural cost that allows one to account for independence and sample correlation jointly, and hence promises to result in a new class of powerful solutions with wide applicability. In addition, it enables one to easily incorporate model selection---another key problem complementing the power of BSS---into the problem through the use of information theoretic criteria.<br\/><br\/>The focus of this research is the development of a class of powerful methods for source separation and model selection using entropy rate so that one can take both the higher-order-statistical information and sample correlation into account to achieve significant performance gains in more challenging problems. The main application domain is one that can truly take advantage of this fully combined approach: the analysis of functional magnetic resonance (fMRI) data and the rejection of gradient and pulse artifacts in electroencephalography (EEG) in concurrent EEG-fMRI data. Both are applications that have proven challenging for the traditional model-based approach due to the unique nature of the noise and artifacts in these problems. Hence, they provide a unique testbed for the performance evaluation of the new class of methods developed under this study. Since independence and sample correlation are intrinsic properties of many other types of data, the new set of methods will be attractive solutions for many other problems as well.","title":"CIF: Small: Collaborative Research: Compressed Sensing for Coherent Designs under Gaussian\/Non-Gaussian Noise","awardID":"1116447","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["563221"],"PO":["564898"]},"181075":{"abstract":"Aggressive technology scaling has brought new challenges to the design and manufacturing of Integrated Circuits (ICs). A mounting challenge is the modeling and characterization of power consumption in face of possible operating and manufacturing variabilities. Accurate power characterization leads to extended battery life for mobile devices and to enhanced device reliability. A second challenge arises because manufacturing is increasingly outsourced to external foundries. Verifying that the manufacturer has not inserted any malware or \"Trojan\" circuitry that compromises the security of the final product is essential. The proposed research provides a unifying framework for these two challenges by detailed algorithmic analysis of infrared emissions from the backside of ICs. We propose techniques to convert the infrared emissions into accurate spatial and temporal power estimates, and to use the power and infrared ?fingerprints? to verify that the chip does not contain extra circuits inserted by the manufacturer, unbeknownst to the designer, that could cause a security breach. The successful completion of this project will lead to improved power modeling and characterization tools and increased confidence that there are no added malware in manufactured chips. The project will also lead to design prototypes and a large volume of valuable data and benchmarks that will be openly disseminated via NSF-funded Trust-Hub and other web-based portals. <br\/><br\/><br\/>The methods and tools will find broad usage in the industry, government, and in academia. They will also impact the daily lives by increasing the battery lifetime and by improving the system's reliability and integrity. Education plan includes research experience for undergraduates, curriculum development, integration of material into graduate courses and emphasizing entrepreneurship within education. Recruitment and development of under-represented groups of students will be targeted.","title":"SHF: Small: Collaborative Research: Algorithmic Techniques for Post-Silicon Characterization Using Infrared Emissions","awardID":"1115424","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["543459"],"PO":["562984"]},"185168":{"abstract":"The 20-th International Conference on Parallel Architectures and Compilation Techniques (PACT) takes place in Galveston Island, Texas ? October 10?14, 2011.<br\/><br\/>PACT seeks to increase student participation in conference and the field. The proposed funding would support the travel of eligible US students to the conference. Recipients would be able to attend the main conference, workshops, and tutorials. Travel grants will encourage the research interests and the involvement of students in the field who are not well funded and those who are just beginning their participation in the field or are interested in entering it. A special effort will be made to reach out to women and under-represented minorities.","title":"Student Travel Support for The 20-th International Conference on Parallel Architectures and Compilation Techniques (PACT)","awardID":"1138543","effectiveDate":"2011-07-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["561154",496841],"PO":["565272"]},"175136":{"abstract":"The Internet has undergone tremendous growth recently, fueled by the emergence of user-generated video, TV- and movie-on-demand services, peer-to-peer file sharing, social networking, and rapid proliferation of smart phones. These new application mixes and the resulting characteristics of network traffic have raised numerous research challenges for practitioners working on various aspects of networks including construction, routing, and staged deployment. As a result, \"network design\" or more generally \"networking,\" with its many variants, is one of the most active research areas in computer science.<br\/><br\/>Theoretical modeling of networks plays a vital role in understanding computer and communication networks. Examples of such theoretical models that will be addressed further in this project are cost and efficiency optimizations with applications in Fiber Optic Networks, Content Distribution Networks (CDNs), Virtual Private Networks (VPNs), Cellular Networks, and Social Networks. The Principal Investigator will invent and use a variety of techniques in approximation and randomized algorithms, fixed parameter algorithms, algorithmic game theory, and his own Bidimensionality theory, to optimize cost and performance of these strategic networks.<br\/><br\/>As a broader impact, the PI seeks to leverage his network of collaborators to apply these theoretical developments to the design of practical algorithms and mechanisms, to deploy them in real-world settings, and to perform experiments to characterize and explain their behavior and performance. The PI believes that variations of the algorithms designed in this project will be used in real networks, enabling faster networks and cheaper Internet access in practice. This is an especially important goal since the Internet in the U.S. is currently not as fast or cheap as in several other developed or even developing countries.<br\/><br\/>The wealth of attractive open problems in foundations of network design provide, at one extreme, challenging research topics, and at the other extreme, intuitive and accessible problems to inspire students to enter research in computer science, mathematics, and economics. The educational thrust of this project is to actively engage undergraduate and graduate students in study of and research into network design foundations.","title":"CAREER: Foundations of Network Design: Real-World Networks, Special Topologies, and Game Theory","awardID":"1053605","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["508020"],"PO":["565251"]},"178216":{"abstract":"This project analyzes strategic manipulation of learning players in repeated games. People are not born with optimal strategies, but instead learn to interact in markets and other social situations. Increasingly, humans interact with computerized agents who may be programmed to use some adaptive learning heuristic. Especially when economic success is at stake, resources may be invested to manipulate learning players. First, the Principle Investigator (PI) seeks to find simple adaptive heuristics that are essentially unbeatable by any opponent in generic and economically relevant classes of games. Second, the PI seeks to discover simple adaptive heuristics whose long run outcome can not be manipulated by sophisticated opponents. Such heuristics are of interest to robustly implement outcomes in mechanisms with learning players. Third, the PI will investigate the existence of simple adaptive heuristics whose long run outcome is not only close to Nash equilibrium but which can also not be manipulated by sophisticated opponents in generic and economically relevant classes of games. Prior work on learning in games focuses on simple heuristics that lead in all games to Nash equilibrium. The objective is to settle the important open question whether such learning heuristics themselves can be robust to strategic manipulation. Finally, the PI aims to find dynamically optimal strategies against well-known adaptive heuristics such as myopic best reply, fictitious play, reinforcement learning, imitation, trail & error learning, and regret matching in generic and economically relevant classes of games.<br\/><br\/>The findings developed in this project are not only relevant for the theory of learning in games in economics but they are foremost relevant for the understanding of real-life strategic interaction and the design of interacting learning machines. In reality, players almost always have to learn how to interact and are almost always heterogeneous with respect to knowledge, strategic sophistication and learning abilities. This becomes obvious for instance in the increasing interaction of humans with machines such as calling robots and automated trading. Especially for automated trading in financial markets, one expects simple learning players to be manipulated by other more sophisticated players if the latter can achieve a strategic advantage. The results of this project are expected to influence the design of interacting learning machines robust to manipulation in many environments.","title":"ICES: Small: Manipulation of Learning Heuristics in Strategic Interaction","awardID":"1101226","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[477826],"PO":["565251"]},"177248":{"abstract":"American Sign Language (ASL) animations have the potential to make information accessible to many deaf adults in the United States who possess only limited English literacy. In this research, which involves collaboration across three institutions, the PIs' goal is to gain a better understanding of ASL linguistics through computational techniques while advancing the state of the art in the generation of ASL animations for accessibility applications for people who are deaf. To these ends, the PIs will develop linguistically based models of two aspects of ASL production: movements required for head gestures and facial expressions that carry essential grammatical information and frequently extend over domains larger than a single sign, and the timing and coordination of manual and non-manual elements of ASL signing. Preliminary work has shown that these issues significantly affect how well signers understand ASL animations, and that these aspects of current ASL animation technologies require improvement. How should the face of a human or animated character be articulated to perform, with accuracy, the linguistically meaningful facial expressions that are part of ASL grammar? How should the onsets, offsets, and transitions of these movements be produced? How should the facial expressions and hand movements be temporally coordinated so that the ASL production is as grammatically correct and understandable as possible? To answer open questions such as these, the PIs' novel approach will apply techniques from computer vision to linguistically annotated video data collected from human signers, in order to produce models for use in animation-production. The PIs will expand their existing annotated video ASL corpora through new data collection and annotation, and will analyze these data to study the use, timing, and synchronization of manual and non-manual components of ASL production. The annotated videos will be used to train high quality computer vision models for recognition of linguistically significant facial expressions and timing subtleties. Parameters of these computer vision models will be used to hypothesize computational models of ASL timing and facial movements, to be incorporated into ASL-animation generation software and evaluated by native signers. The models will be iteratively refined in cycles of user-based studies and incorporated into ASL animation technologies to more accurately mimic human signing. Project outcomes will include high quality models of the movement of virtual human characters for animations of ASL performance. The analysis of video corpora of ASL will produce new linguistic insights into the micro-facial expressions and the temporal coordination of the face and hands in ASL production, while advances in the analysis of ASL prosody will contribute to an understanding of the fundamental commonalities and modality-specific differences between signed and spoken languages that is essential to a full understanding of the human language faculty. The creation of new modeling approaches and recognition techniques will advance the field of computer vision, by benefiting the identification and tracking of the human face and body in video during the rapid and complex movements of ASL (and other forms of human movement).<br\/><br\/>Broader Impacts: This research will lead to significant improvements to technology for generating linguistically accurate ASL animations, which will make information, applications, websites, and services more accessible to the large number of deaf individuals with relatively low English literacy. Advances in computer vision techniques for recognizing ASL in videos of humans will have general applicability in human-computer interaction, recognition and animation of facial expressions, and computer vision. The corpora created in this project will enable students and researchers in both linguistics and computer science (including those without access to the requisite technological and human resources to carry out their own data collection from native signers and time-intensive linguistic annotations) to engage in research on ASL. The techniques to be developed will also enable partial automation of the time-consuming creation of annotated ASL video corpora. As in the PIs' earlier work, the proposed research will create opportunities for people who are deaf and members of other underrepresented groups to participate in scientific research.","title":"HCC: Medium: Collaborative Research: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing","awardID":"1064965","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["522465"],"PO":["565227"]},"181461":{"abstract":"The electromagnetic spectrum is becoming scarce as the usage of wireless communications devices becomes ubiquitous. Current research has shown that by using multiple antennas at each wireless device, significantly higher densities of users can share the electromagnetic spectrum without reducing the performance of individual users. Most existing research in this area however is based on idealized system models that are often unrealistic. This project studies the extent to which the promising performance gains of multi-antenna systems predicted using idealized network models apply to networks models with more realistic assumptions, and is an important bridge between existing theoretical understanding and future practical implementation of a technology that could potentially alleviate the spectral crowding problem significantly.<br\/><br\/>Most existing research on spatially-distributed wireless networks with multi-antenna nodes is based on idealized models such as uniformly random spatial distribution of users, uncorrelated channels between antennas, and the availability of accurate channel-state-information (CSI); assumptions which often do not hold in practice. The investigator is studying the impact of non-uniform node distributions, channel correlations, and inaccurate CSI on the performance of wireless networks with multi-antenna nodes. Specific questions addressed include 1) What spectral efficiencies can be achieved in such networks? 2) To what extent can increasing the number of antennas per user with user density maintain constant perlink data rates in such networks? 3) What are appropriate models for spatial node distribution, channel correlation, and CSI uncertainty for spatially distributed networks? Answers to these questions help improve existing understanding of the performance benefits of multiple-antenna systems in realistic systems and in particular, their potential to alleviate spectral crowding in practice.","title":"CIF: Small: RUI: Multiple-Antenna Systems in Spatially Distributed Networks with Non-Idealized Assumptions","awardID":"1117218","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[486187],"PO":["564924"]},"181373":{"abstract":"The widespread adoption of multicore processors requires multithreaded<br\/>software to exploit these hardware resources. Unfortunately, the<br\/>construction and validation of reliable concurrent software currently<br\/>requires extraordinary effort, due to unanticipated interactions<br\/>between concurrent threads. Thus, developing better programming<br\/>techniques and tools for concurrent programming is essential. This<br\/>research develops a cooperative programming methodology for<br\/>multithreaded software, based on the philosophy that all thread<br\/>interference must be explicitly documented via source-level \"yield\"<br\/>annotations by the programmer.<br\/><br\/>The project will investigate both static and dynamic checking<br\/>techniques to verify the correctness of yield annotations. Once<br\/>verified, these annotations guarantee that code executed between<br\/>successive yields is serializable and thus amenable to sequential<br\/>reasoning. Moreover, yield-free code is deterministic. Despite<br\/>provided these strong safety guarantees, this methodology does not<br\/>impact program performance. The cooperative methodology provides a<br\/>robust foundation for multithreaded software and can potentially<br\/>transform the principles and practices of multithreaded software<br\/>engineering. This work on cooperability will also provide research<br\/>opportunities for graduate and undergraduate students, and it will<br\/>support endeavors to provide access to science education for all<br\/>students.","title":"SHF: Small: Collaborative Research: Static and Dynamic Analysis for Cooperative Concurrency","awardID":"1116883","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["556722"],"PO":["565264"]},"181384":{"abstract":"Learning theory studies the extent to which meaningful patterns can be extracted from data. Two popular frameworks for the analysis of learning algorithms are statistical learning and worst-case online learning. Recent developments suggest that these two seemingly disparate frameworks are, in fact, two endpoints of a spectrum of problems which can be studied in a unified manner. The goals of this project are (a) to understand learnability and to develop efficient algorithms for a spectrum of problems corresponding to various probabilistic and non-probabilistic assumptions on the data; (b) to extend learnability results to encompass performance criteria beyond the classical notion of regret; (c) to understand the inherent complexity of reinforcement learning and to develop novel algorithms inspired by the learnability analysis; (d) to study learnability in settings with imperfect or partial information, and to understand algorithmic implications of dealing with uncertainty.<br\/><br\/>Algorithms that extract patterns from data are becoming increasingly important in the information age. However, classical methods that assume a \"static\" nature of the world are unable to capture the evolving character of data. Recent advances in learning theory have been on the dynamic interaction between the world and the learner. Being able to tailor the algorithms to particular assumptions on data is arguably a central goal of learning theory. The intellectual merit of this proposal includes the development of a unified theoretical framework, increasing our understanding of what is learnable by computers. Advances in this direction will likely facilitate the development of more intelligent systems, having a positive impact on technology. The interdisciplinary nature of the project will likely increase collaboration between Computer Science and Statistics.","title":"AF: Small: From Statistical to Worst-Case Learning: A Unified Framework","awardID":"1116928","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[486003,"559900"],"PO":["565251"]},"181395":{"abstract":"A key enabler of Nanoscience and Nanotechnology is the Atomic Force Microscope (AFM) that has opened up new realms, of interrogation and manipulation of matter at the atomic scale. It has resulted in breakthroughs in understanding sub-atomic molecular structure, protein folding dynamics and materials characterization. The potential impact of significantly faster AFM based imaging is immense, e.g., it will allow the study of dynamics of material at the nanoscale that was hitherto not accessible. The aim of this research is to study modern signal processing techniques that achieve gains in imaging speeds by an order of magnitude. The investigators will push the synergistic transfer of knowhow between the engineering and the AFM communities through student visits and specialized workshops. The findings will be integrated at appropriate levels into the undergraduate and graduate curriculum.<br\/><br\/>The main component of an AFM is a cantilever that deflects in response to forces at the pico-Newton scale. The focus of this research is the dynamic mode AFM operation, where the cantilever gently taps the sample being imaged; the mode of choice for imaging biological samples. Even though forces do form a good indicator of sample topography, the system memory and the nonlinearities of the AFM system dynamics preclude a direct mapping of the cantilever forces into a finer description of topography, especially at high imaging speeds. The investigators model the complex nanoscale interactions in a mathematically tractable manner that facilitates the development of maximum a posteriori (MAP) sequence detection of the sample features. Factor graph representations and the development of appropriate inference schemes will be studied. The algorithms will be implemented on FPGAs and tested exhaustively with experimental data.","title":"CIF: Small: Collaborative Research: Signal processing for enabling high speed probe based nanoimaging","awardID":"1116971","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["509594"],"PO":["564898"]},"185224":{"abstract":"This grant provides international travel support for U.S. based graduate student participants to attend the 2011 International Conference on Data Mining (ICDM 2011), which will be held in Vancouver, Canada, on December 11-14, 2011 (http:\/\/icdm2011.cs.ualberta.ca\/). ICDM has established itself as the world's premier research conference in data mining. It provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative, practical development experiences. The conference covers all aspects of data mining, including algorithms, software and systems, and applications, as well as related areas such as data management, machine learning and bioinformatics. The conference proceedings are published by IEEE. The conference seeks to continuously advance the state-of-the-art in data mining. With the growth of the Web, the Internet, and data intensive technologies such as Sensor Networks, and Bioinformatics, Data Mining is an extremely important area in Information Technology. Besides the technical program, the conference features workshops, tutorials, panels, data mining contest, and starting this year, the Ph.D. forum.<br\/><br\/>A strong representation of U.S. researchers at the Conference is useful in maintaining U.S. competitiveness in this important area. The total number of ICDM participants in the past has been in excess of 500, with a majority of the participants from the U.S., then Europe and Asia. It is expected to provide scholarships to 16 U.S. based graduate student participants. This grant will partially support the travel costs for the U.S. based graduate student participants. <br\/><br\/>More information of the Ph.D. forum can be found at http:\/\/icdm2011.cs.ualberta.ca\/phd-forum.php. The award results will be announced at http:\/\/icdm2011.cs.ualberta.ca\/travel-grants.php.","title":"Supporting US-Based Students to Attend the 2011 IEEE International Conference on Data Mining (ICDM 2011)","awardID":"1138800","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["234536","476108","547675"],"PO":["563751"]},"185114":{"abstract":"We are faced with unprecedented challenges stemming from global climate change, rising energy cost, and their impact on national competitiveness and security. To reduce greenhouse gas emissions and dependency on imported fossil fuels, it is imperative to harvest as much renewable energy as possible, which, in turn, can benefit from efficient large-scale energy storage systems that can buffer variable energy supply. Recent progress in battery technology has made it possible to use batteries to store energy, and then power platforms that incur a significant energy load, such as transportation vehicles, homes, and industrial buildings. However, the slow pace of improvement is insufficient to make the performance of rechargeable batteries competitive with, and an attractive alternative to (for example) conventional powertrains, including gasoline combustion engines. In particular, when a large number of battery cells (e.g., a 6800-cell pack for Tesla S model and a 300-cell pack for GM Volt) are put together as a pack, their electrochemical interaction and reaction can shorten the pack?s life significantly despite the high quality of individual cells. This research project explores how efficient battery management (BM) can extend the pack?s life for as long as the constituent cells can last (e.g., 10?15 years). This project is developing a holistic architecture, SMARTGREEN, based on active monitoring and control mechanisms. SMARTGREEN maximizes the synergy between battery management algorithms, software (cyber), and reconfigurable battery hardware (physical), that are tightly-coupled. SMARTGREEN focuses on: intelligent monitoring, active computation, proactive prognostics, and dynamic reconfiguration, operating in tandem to dramatically extend battery life and operation-time.","title":"EAGER: SmartGreen: An Adaptive Architecture for Management of Large Energy Storage Systems","awardID":"1138200","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["553551"],"PO":["565255"]},"175324":{"abstract":"This project explores an integrative approach to embedded hardware security, where efficient design solutions complement appropriate test\/validation steps and security analysis. It creates technology to protect embedded systems at different stages of life-cycle against hardware intellectual property (IP) piracy and reverse engineering, hardware Trojan attacks in untrusted design and fabrication facilities, and malicious modifications of hardware IP. It investigates a design approach based on scalable key-based hardware obfuscation to protect hardware IP from illegal usage and malicious modifications. The design technique is combined with a \"self-referencing\" based validation approach, which compares side-channel signature of a chip with itself - in both spatial and temporal manner, to reliably detect hardware Trojans. Security analysis is performed to measure effectiveness of the proposed framework using appropriate trust metrics and to identify emerging security threats. The approaches are validated using software automation tools and a custom hardware emulation platform. <br\/><br\/>The research improves understanding of technological issues related to embedded hardware security and will enable technologies for secure and trustworthy hardware at low cost. The project integrates education through training students various aspects of hardware security using hands-on experiments, involving undergraduate and high-school students in research, and disseminating research results, course modules, software tools and the hardware test bed to local schools and peer research community. It will create awareness about the security issues and countermeasures through the \"Hardware Security\" group in Facebook created by the PI and through organizing special sessions in major IEEE\/ACM conferences.","title":"CAREER: An Integrative and Scalable Approach to Embedded Hardware Protection","awardID":"1054744","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["530582"],"PO":["565264"]},"175214":{"abstract":"Increases in the number of cores in multicore processors have lead to<br\/>increases in the architectural and environmental diversity present in<br\/>computer systems: the number, complexity, and mix of available cores<br\/>vary greatly and the resources allocated to an application differ from<br\/>run-to-run and within runs.<br\/><br\/>Diversity has a significant impact on application performance;<br\/>applications must adapt to differences in their architecture and<br\/>environment to achieve good performance. However, developing adaptive<br\/>applications greatly increases the difficulty of writing efficient<br\/>parallel programs while increasing the level of skill required to<br\/>write a parallel application, and may therefore limit programmers'<br\/>ability to write parallel applications and take full advantage of<br\/>multicore processors.<br\/><br\/>This project develops and disseminates new compilation techniques and<br\/>runtime adaptation strategies in which a compiler analyzes the<br\/>concurrency and locality features of an application, selects a runtime<br\/>adaptation strategy based on these features, and adds adaptation to<br\/>the application. Key contributions address the challenges of<br\/>discovering and representing concurrency and locality and selecting<br\/>adaptation strategies based upon application characteristics.<br\/><br\/>These new compilation techniques and adaptation strategies will free<br\/>programmers from the need to concern themselves with architectural and<br\/>environmental diversity when writing parallel applications. This<br\/>freedom will then enable a wide variety of applications to benefit<br\/>from multicore processors, thus ensuring that multicore processor<br\/>systems will be able to live up to users' increased performance<br\/>expectations.","title":"CAREER: Compiler-Inserted Runtime Adaptation for Multicore Processors","awardID":"1054075","effectiveDate":"2011-07-15","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[469901],"PO":["565255"]},"181550":{"abstract":"Obtaining information from data is one of the most fundamental problems of modern science and technology. The aim of machine learning is to develop algorithms to automatically extract useful information from complex, high-dimensional data. Making progress toward this aim requires developing an understanding of the aspects of data, which are amenable to analysis and can be learned using computationally efficient methods. In particular, modeling non-linear structures in high-dimensional data has become one of the very challenging and active lines of research, which has seen significant progress over the last ten years.<br\/><br\/>The goal of this project is to develop and analyze new mathematical representations for data, based on spectral and algebraic methods. We will explore how different structures in the data, such as cluster, manifold or parametric model structures, are reflected in their spectral and algebraic properties and how they can be extracted algorithmically from data, paying particular attention to the issues of high dimensionality and non-linearity. These insights will be used to build better and more adaptive algorithms for inference and data analysis tasks. <br\/><br\/>We will also analyze experimentally and theoretically properties of these algorithms, when data deviates from the posited model structure. This is a key issue in practical applications, which nearly always involve uncertainty and noise.","title":"RI: Small: Algebraic and Spectral Structure of Data in High Dimension","awardID":"1117707","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["562318"],"PO":["562760"]},"181572":{"abstract":"The computing revolution of the last few decades has been driven in large part by a rapid growth in the performance of microprocessor chips. Unfortunately, this growth is now severely restricted by hard limits on power consumption. To ensure that computing technology will continue to evolve, solutions that dramatically increase the energy efficiency of computation must be created. This proposal focuses on developing techniques for building microprocessors that are an order of magnitude more efficient than the current state of the art. This will be achieved by designing and testing technologies for making chips more intelligent in the way they manage power. These systems will dynamically monitor their power consumption, allocate power intelligently to critical tasks and coordinate application execution with the power regulation mechanism, all with the goal of reducing energy waste. The proposed solutions span multiple technology layers, bringing together experts from different areas of chip design from both academia and industry.<br\/><br\/><br\/>Beyond its technological and commercial potential, this research will further strengthen multidisciplinary teaching and research in energy efficient design at Ohio State University. The PIs will pilot a joint graduate-level course focusing on ultra-low power microprocessor design. This proposal will also help foster a long-term collaboration between Ohio State and Mentor Graphics that is committed to sending engineers and researchers to visit the Ohio State campus, advice graduate students involved in this project and be intellectually involved. This will encourage students to seek careers in computer aided VLSI design and microprocessor technology, thus helping fuel an industry that is vital to the growth of the US economy.","title":"SHF: Small: GOALI: Addressing the Challenges of Parameter Variation in the Design of Ultra-Low Power Chip Multiprocessors Using Near-Threshold Technology","awardID":"1117799","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"1504","name":"GRANT OPP FOR ACAD LIA W\/INDUS"}}],"PIcoPI":["534480","494690"],"PO":["562984"]},"181341":{"abstract":"Today's software systems are increasingly built from flexible combinations of components that can be configured in a multitude of different ways. For example, the popular Firefox web browser has more than a thousand configuration options. While a high degree of configurability has many benefits, it also makes testing and reasoning about configurable software a major challenge. The goals of this proposed work are 1) to develop new techniques to efficiently discover the structure of software systems' configuration spaces, and 2) to exploit that information to improve common software engineering tasks, specifically testing and program understanding. In particular, this work will allow software developers to quickly and accurately answer questions such as, What are the \"right\" configurations to test a system under? How can we avoid testing a system under \"unnecessary\" configurations? How does software configuration change as systems evolve? We expect that, as much of the software used today is configurable, the advances made in this project in understanding and testing configurable systems will have widespread benefits in the reliability and trustworthiness of critical software.<br\/><br\/>The proposed technical approach will pursue four main directions. First, the PIs will develop a representation of \"effective\" configuration spaces---the configurations needed to achieve a specific goal---that is both usable by software engineering tools and understandable to developers. Second, the PIs will develop novel techniques that compute the effective configuration space of a system. Third, the PIs will explore a number of software engineering applications that will make use of effective configuration space information, including configuration-aware test case selection, configuration-aware regression testing, and several program understanding tasks for configurable systems. Finally, the PIs will conduct a wide range of fundamental empirical studies that will test the research hypotheses and evaluate the proposed approach on large scale subject systems. Among others outcomes, these studies will examine the effective configurations of a range of systems; investigate how those configurations evolve over time; and determine how failures relate to system configurations. The results will impact both practice and education.","title":"SHF: Small: Empirical Studies, Principles and Techniques for Software Systems with Complex Configuration Spaces","awardID":"1116740","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["556281",485900],"PO":["564388"]},"175325":{"abstract":"When data is outsourced at a cloud storage provider, data owners lose control over the integrity of their data and must trust the storage provider unconditionally. Coupled with numerous data loss incidents, this prevents organizations from assessing the risk posed by outsourcing data to untrusted clouds, making cloud storage unsuitable for applications that require long-term security and reliability guarantees. This project establishes a practical remote data checking (RDC) framework as a mechanism to provide long-term integrity and reliability for remotely stored data. At the same time, the project seeks to develop new functionality for remote data checking that overcomes limitations of early RDC protocols and improves the usability and deployability of RDC on existing cloud storage infrastructures. Unlike previous work, this research takes a holistic approach and considers RDC protocols that minimize the combined security costs of all data management phases over the lifetime of a distributed storage system. This includes prevention, repair, and retrieval. Maintaining the health of the data in a distributed storage system requires various transformations to be applied on the data and requires data to migrate among storage servers. This project develops novel RDC protocols that are compatible with the full range of replication, erasure coding and network coding operations employed by distributed storage systems, thus enabling owners to maintain better control over their data. This project increases the transparency of cloud storage platforms and improves the security dimension of storage outsourcing enabling wider adoption of cloud storage technologies. To disseminate these ideas, the project's educational activities include curriculum development, mentoring undergraduate and graduate students and engaging them into research, and outreach to high-school teachers.","title":"CAREER: Secure and Reliable Outsourced Storage Systems Using Remote Data Checking","awardID":"1054754","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["528629"],"PO":["562974"]},"183641":{"abstract":"The 2011 AAAI Robotics Exhibition and Workshop (San Francisco, CA August 7-11, 2011) continues a focus on key research problems in manipulation and learning through challenges in: (1) humanoid robotics, (2) learning by demonstration, and (3) samll-scale manipulation (robot chess). The teams selected for each challenge define research problems and repeatable experiments in areas that drive autonomous assistance in both military and domestic needs. The workshop enhances the challenge goals by (1) creating awareness in the larger AI community of available software tools, and (2) crafts a roadmap for development platforms that are more accessible to the general computer science research community.<br\/><br\/>A significant number of hands-on exhibits complement the workshop discussions and panel. Here, research teams showcase working demonstrations that support the challenge themes of learning, teaming and manipulation. Exhibits are on display for 2 full days during the AAAI Conference, providing an excellent opportunity to engage a broad technical audience. The exhibits are open to the general public to raise awareness of the state-of-the-art in robotics. Students from local schools and summer camps visit. Access to the exhibits provides an opportunity for students and leaders to learn how robotics and AI play important roles in society.","title":"AAAI 2011 Robotics Program","awardID":"1129177","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550868"],"PO":["543539"]},"181155":{"abstract":"In large scale prediction problems that arise in many application areas, data is plentiful, and it is computational resources that constrain the performance of prediction methods. The broad goal of this research project is the design and analysis of methods for large scale prediction problems that make effective use of limited computational resources. The main aims are: to improve our understanding of the tradeoff between the accuracy of a prediction method and its computational requirements; to develop model selection methods that adaptively choose the model complexity to give the best predictive accuracy for the available computational resources; to improve our understanding of the difficulty of solving large scale prediction problems using distributed computational resources; to develop analysis techniques and methods for asynchronous online prediction, which exploit the flexibility to respond to queries out of order; and hence to develop effective methods for large scale prediction problems.<br\/><br\/>As data acquisition and storage has become cheaper, enormous data sets have become available in many areas, including web information retrieval, the biological, medical, and physical sciences, manufacturing, finance and retail. Consequently, for many statistical prediction problems, the amount of data available is so huge that we can treat it as unlimited. For instance, in using image and caption data to train a prediction rule that can automatically choose appropriate labels for images, the web provides an effectively unlimited supply of training data. Similar situations arise in using click stream data to predict the choices of visitors to a popular web site, or in using customers' ratings of movies to make useful recommendations. For these large scale prediction problems, the bottleneck to performance is not the amount of data, rather it is the computational resources that are available. Many modern prediction methods have been designed and analyzed from the perspective that data is precious: they aim for optimal predictive accuracy for a given sample size. But for large scale problems, this is the wrong perspective; computation is the precious resource that must be used wisely. This shift in perspective introduces some novel tradeoffs. One of the most important tradeoffs arises in choosing the complexity of a prediction rule. Should we use our computational resources trying to optimize over a very complex family of prediction rules, which would not allow us to gather much data? Or should we save computation by using simpler prediction rules, and instead spend this computation on gathering more data? This research project is aimed at improving our understanding of these tradeoffs, and hence developing strategies for large scale prediction problems that best exploit the available computational resources.","title":"MCS: AF: Small: Algorithms for Large Scale Prediction Problems","awardID":"1115788","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"1269","name":"STATISTICS"}}],"PIcoPI":[485442],"PO":["565251"]},"181276":{"abstract":"Modern information processing circuits found in devices such as cell phones and laptop computers run a fixed speed, known as a clock speed. As clock speeds increase, more data can be processed in a shorter time allowing for intensive applications such as streaming video and audio to be used. As clock speeds increase, the amount of power required to run the circuits also increases. A new approach would require no clock signal at all. In this approach, called asynchronous processing, data would be processed as soon as it is available. Asynchronous circuits can provide very fast data processing capabilities while also reducing the amount of power consumed. Another advantage is that recently methods have been devised that monitor signals radiated from clocked circuits that can allow thieves to obtain copies of the data being processed. Asynchronous circuits make this form of data theft much harder and are therefore more secure circuits. Unfortunately, the methods used to design asynchronous circuits are very immature and several important research problems must be solved to enable this technology to become widely used. This project has proposed solutions to these problems and the research will allow asynchronous design methods to be more fully developed and will ultimately enable asynchronous circuits to be commonly used. <br\/><br\/>The impact of having asynchronous data processing circuits in devices such as notebook computers and cell phones is that these devices will consume less power allowing them to operate much longer before recharging their batteries. For some applications, the performance of asynchronous circuits will increase since processing will occur on the data as soon as it is available. In contrast, a clocked circuit must wait until the next clock cycle before further data processing can occur. Finally, asynchronous circuits offer more security against certain data stealing attacks known as \"side channel\" attacks. This will prevent data thieves from accessing your data by listening to the signals emanating from","title":"SHF: Small: A Register Transfer Level Toolset for Low Power Asynchronous Design Using Null Convention Logic","awardID":"1116405","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485742],"PO":["562984"]},"181541":{"abstract":"As many application domains continue to generate data at exponentially increasing rates, much of the data that is gathered is stored in a compressed format. However, very few classic data processing algorithms have been updated to handle compressed data. This project aims to address this gap by developing (i) algorithms for massive data sets that can directly operate on compressed data; and (ii) compression schemes that are aware of the algorithms that would operate on the data. <br\/><br\/>In many settings, algorithms that manipulate very large composite objects while interacting only with their succinct descriptions can substantially reduce the time and memory requirements relative to their counterparts that have to work with uncompressed representations of the same data. These performance gains are realized by leveraging highly repetitive or parametrically specified input structures, to enable algorithms to manipulate very large composite objects while interacting only with their compressed descriptions. Anticipated results of the project include new geometric algorithms that solve problems such as convex hull, Voronoi diagrams, nearest points and earth-mover distances when the inputs are in compressed format; new graph algorithms that compute minimum spanning trees, shortest paths, and network flows on compressed input graphs; and new compression-aware data structures that support efficient storing, querying and processing of compressed data. All the algorithmic contributions will be validated with experiments on real and synthetic massive data sets. The resulting algorithms are likely to find application in many different domains including networks, genomics, databases, computer graphics, artificial intelligence, geographic information systems, integrated circuit design, and computer-aided engineering. <br\/><br\/>Broader Impacts: Compression-aware data processing algorithms and algorithm-aware data compression schemes have applications across a wide range of tasks that involve processing of massive data sets consisting of large data objects (e.g., images, sequences, graphs). The formulations, algorithms, codes, and theories <br\/>that will be developed and disseminated by this project are likely to contribute to the development of efficient and practical algorithms and data structures that could impact the way in which organizations collect, store, process, such data. The project offers enhanced research based training opportunities for students in an area of considerable theoretical as well as practical significance. Additional information about the project can be found at: http:\/\/www.cs.virginia.edu\/robins","title":"III: Small: Compression-Aware Algorithms for Massive Datasets","awardID":"1117684","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[486395,486396],"PO":["565136"]},"181310":{"abstract":"Computers permeate all socio-economic activities of our times. The limitations of our present silicon-based computational power are no longer residing in the far future. The topic of this award is one that is relevant to finding new directions for future computation, while at the same time enlarging our understanding of physical systems suitable for quantum computing. The research will address fundamental science questions, such as whether topological states of matter can store, at non-zero temperatures, quantum (qubit) or classical (bit) elements of information. The research activities will be accompanied by educational efforts at three different levels: The PIs will provide solid education and training to graduate students in the field of quantum information; they will develop undergraduate courses at the interface between physics and computer science; and they will use their expertise to help develop the \"Future of Information\" module for techCAMP, a program directed towards middle-school and high-school teachers. <br\/><br\/>Encoding information in topological states of certain many-particle systems has been proposed as robust way to store quantum information. In these systems, the ground state is not unique and its multiplicity is not altered by local perturbations. As a result, when used to encode information, topological qubits are less susceptible to errors than standard qubits. Although protected from static perturbations, it is still uncertain how effective topological quantum memories are in the presence of dynamical perturbations. The proposed research addresses two important aspects of this issue. The first concerns the study of loss of coherence in realistic setups, where both equilibrium and non-equilibrium noise must be considered. The second is whether topological quantum memories in the presence of thermal fluctuations are attainable in physically realizable systems. Building on these investigations, the goal is to find ways to improve fault tolerance in topological quantum information processing.","title":"AF: Collaborative Research: Robustness of Topological Quantum Memories","awardID":"1116590","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[485821],"PO":["565157"]},"181464":{"abstract":"Computers permeate all socio-economic activities of our times. The limitations of our present silicon-based computational power are no longer residing in the far future. The topic of this award is one that is relevant to finding new directions for future computation, while at the same time enlarging our understanding of physical systems suitable for quantum computing. The research will address fundamental science questions, such as whether topological states of matter can store, at non-zero temperatures, quantum (qubit) or classical (bit) elements of information. The research activities will be accompanied by educational efforts at three different levels: The PIs will provide solid education and training to graduate students in the field of quantum information; they will develop undergraduate courses at the interface between physics and computer science; and they will use their expertise to help develop the \"Future of Information\" module for techCAMP, a program directed towards middle-school and high-school teachers.<br\/><br\/>Encoding information in topological states of certain many-particle systems has been proposed as robust way to store quantum information. In these systems, the ground state is not unique and its multiplicity is not altered by local perturbations. As a result, when used to encode information, topological qubits are less susceptible to errors than standard qubits. Although protected from static perturbations, it is still uncertain how effective topological quantum memories are in the presence of dynamical perturbations. The proposed research addresses two important aspects of this issue. The first concerns the study of loss of coherence in realistic setups, where both equilibrium and non-equilibrium noise must be considered. The second is whether topological quantum memories in the presence of thermal fluctuations are attainable in physically realizable systems. Building on these investigations, the goal is to find ways to improve fault tolerance in topological quantum information processing.","title":"AF: Collaborative Research: Robustness of Topological Quantum Memories","awardID":"1117241","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[486193],"PO":["565157"]},"181112":{"abstract":"Continuous optimization is a mathematical discipline with extensive<br\/>applications in engineering design and business\/logistical planning.<br\/>Its currently most common solution techniques are difficult to adapt<br\/>to newly evolving computer architectures comprising dozens to<br\/>thousands of processing elements working in parallel. Combining<br\/>several existing techniques with some recent results of the principal<br\/>investigator, this project explores a means of solving continuous<br\/>optimization problems that should adapt more readily to parallel<br\/>computer architectures than present standard solvers, allowing the<br\/>architectures' full power to be brought to bear on large,<br\/>time-consuming problems. Without such new solution approaches,<br\/>solution of critical design and planning problems may not benefit from<br\/>most of the advances in computing power anticipated for the next<br\/>decade. The project will also involve cooperative work with the<br\/>Brazilian research community.<br\/><br\/>The technical approach is to capitalize on recent advances in<br\/>augmented Lagrangian and conjugate gradient algorithms to produce a<br\/>new kind of modular parallel continuous constrained optimization<br\/>solver. The solver consists of a classical augmented Lagrangian outer<br\/>loop, with subproblems solved by the a state-of-the art<br\/>box-constrained conjugate gradient method terminated by a recently<br\/>developed relative error criterion. The research consists of three<br\/>stages: the goal of stage one is to create an object-oriented, modular<br\/>serial implementation, test it extensively, and address some<br\/>theoretical issues. Stage two aims to evolve the stage-one substrate<br\/>into a parallel solver for which the user explicitly specifies how to<br\/>map the problem structure to multiple processing elements. Stage<br\/>three's goal is to automate the structure detection and mapping<br\/>process. Stages two and three will use stochastic programming<br\/>problems as test cases.","title":"AF: Small: Approximate Augmented Lagrangians: First-Order and Parallel Optimization Methods, with Applications to Stochastic Programming","awardID":"1115638","effectiveDate":"2011-07-15","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[485348],"PO":["565251"]},"185853":{"abstract":"ABSTRACT<br\/> This international venture aims to coordinate the US Science of Learning Center scientists with those from the Australian Science of Learning Centre through an exciting two day workshop and extended visits to the Australian laboratories for the purpose of planning collaborations and exchanging information. The two day workshop will bring together scientists, policy makers, and government officials for the purpose of introducing the scientific goals and progress from the Science of Learning communities and engaging in a rigorous day of scientific talks and discussions aimed towards gaining interdisciplinary perspectives on the role of attention in learning and formulating research topics designed to move this science into the educational setting. Finally, US trainees and scientists will spend extended time furthering their discussions and plans with Australian scientists through laboratory visits, meetings over data, and extended information exchange on topics relevant to their specific lines of work. Hence the intellectual merit of this activity rests in the exchange of ideas between the US scientists and the Australian scientists, focused on particular issues in the Science of Learning. Cyberinfrastructure will be developed to support the ongoing exchange of information and sharing of data from this workshop. The Australian Science of Learning Centre has graciously offered to incorporate the US community in their existing symposium on Attention and Learning, in addition to providing all facilities and organizational services. The broader impacts of this workshop will be in developing synergies between the scientists in the two countries as well as within the Science of Learning Centers itself. The workshop will serve as a foundation for developing an International Science of Learning Community. Gaining cross-cultural perspectives will enrich our science and be excellent an excellent experience for our trainees. A major challenge that faces every nation is how to provide effective and high quality education. This workshop will contribute to solving this problem and is likely to have an impact on the future of the global workforce and the global nature of science in general. The grass-root efforts of international collaborations can be transformational both to the education of the centers' trainees and to the future quality of the education this country provides children. These grass-root efforts are essential in this era of global science.","title":"The United States and Australian Collaborative Workshop on the New Science of Learning","awardID":"1142181","effectiveDate":"2011-07-01","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0401","name":"Division of SBE Off of Multidisciplinary A","abbr":"SMA"},"pgm":{"id":"7704","name":"Science of Learning Activities"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["539251",498816],"PO":["560906"]},"181167":{"abstract":"Networks, of all kinds, surround us in our modern existence. Telecommunications networks, road networks, social networks, flight connections, and even the circuits in the microprocessors in our computers and cellphones are just a few examples. Some fundamental network problems appear again and again in a variety of these settings, and better algorithms - and more importantly, keener insights - into these problems are needed. Many of these problems are algorithmically hard optimization problems, and we cannot expect optimal solutions in reasonable time. Instead, approximations must be accepted, and the aim is to provide the best possible guarantee efficiently.<br\/><br\/>The goal of this project is to explore new approaches and techniques for fundamental optimization problems, both new and old, in network design. The problems to be investigated include the classical traveling salesman problem, the problem of finding a tour of minimum length visiting all the nodes in a network, and the Steiner tree problem, which asks for the cheapest tree connecting some collection of terminals within a network. These problems have been intensively studied, but recently there has been some exciting progress. Two main techniques underlying this progress are the study of \"thin\" spanning trees, and advances in the design of strong rounding algorithms. In this project, the PIs aim to continue investigation in these directions with the hope of further breakthroughs. Another class of problems we consider relates to the quite practical issue of provisioning a network to handle varied and uncertain demands. This context has already demonstrated a combination of beautiful mathematical structure and important practical implications, and one of the goals of this project is to expand the class of such network design problems that can be efficiently solved.<br\/><br\/>Due to their fundamental nature, progress on these problems would have an immediate impact on the field, driving even further progress. There would also be a broader impact, with theoretical advancements aiding and guiding practical improvements in industry. In particular, part of the project will focus on recent models of demand patterns and capacity requirements in telecommunication networks. As bandwidth requirements increase rapidly, with the dramatic increase in video and multimedia, ways need to be found to more efficiently utilize network resources. Another central aspect of this project with broader impact is the training and mentoring of graduate students and junior researchers.","title":"AF: Small: New Approaches to Fundamental Problems in Network Design","awardID":"1115849","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[485468,485469,485470],"PO":["565251"]},"181288":{"abstract":"Clustering or organization of data into groups is a fundamental problem that forms the basis of exploratory data analysis and aids in data management. However, there is often a significant resource and computational cost associated with obtaining and analyzing large-scale datasets that routinely arise in modern systems, such as the Internet, biological and social networks. The ability to discover meaningful clusters in high-dimensional data that is plagued with high noise, outliers and missing observations, will have a significant impact on understanding these systems. <br\/><br\/>This project aims to develop robust clustering methods that can identify clusters very efficiently by selectively querying for the most informative data measurements. Spectral clustering is a popular technique that identifies clusters by analyzing the eigenvectors of a matrix of similarity values between the data points. This project investigates the effect of missing and erroneous data on the eigenvector structure, and leverages this understanding to develop active methods that intelligently guide subsequent data queries.<br\/><br\/>Robust and efficient clustering methods are crucial for identifying groups of proteins and drugs that interact with each other, paving the way for transformative health technologies. These methods are also important for learning and maintaining the organization of computer and social networks, thus promoting seamless exchange of ideas and technology. This PI is involved in disseminating the research through collaborations with the CMU Lane Center for Computational Biology, publishing results and software online (http:\/\/www.cs.cmu.edu\/~aarti\/research_projects), developing and teaching inter-disciplinary courses, as well as the Opportunities for undergraduate women research in Computer Science (OurCS) program at Carnegie Mellon University.","title":"III: Small: Spectral Methods for Active Clustering and Bi-Clustering","awardID":"1116458","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["541682"],"PO":["565136"]},"184335":{"abstract":"The project develops a high data rate optical communication system and a localization algorithm that combines optical with acoustic information as well as two robot capabilities that use the optical communication system. The first explores new methods for acoustic and optical underwater coordination that lead to target identification and efficient data muling using autonomous underwater vehicles. The second explores the use of optical communication as a remote control of an underwater robot as well as for live video transmission from the robot.<br\/><br\/>The systems contribute a new approach to search and rescue at sea and also enhance the tools available to marine sciences. The proposed underwater systems enhance the scope of observations and communication of the ocean?s physical properties which are essential for advancing knowledge on a wide variety of multi-disciplinary, societally-relevant concerns such as climate variability, gaseous sequestration (e.g., CO2), biogeochemical cycles, and ecosystem dynamics for small scale (e.g., phytoplankton) to large scale (e.g., mammals and commercial fisheries) biota. Furthermore, there is an international collaboration with Prof Hirose from the Tokyo Institute of Technology (TIT) to create an optically-enabled version of AnchorDiver III and demonstrate its ability to localize, communicate, and transmit video imaging. This involves student and faculty exchanges in the international research community.","title":"EAGER: Underwater Optical Communication and Perception","awardID":"1133224","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["527733"],"PO":["564069"]},"188944":{"abstract":"EAGER: Molecular-Level Stochastic Simulation to predict the dynamics of Protein Misfolding and Aggregation<br\/><br\/>Summary: Proteins are biochemical workhorses that are needed in many important biological functions. Lately one aspect of these macromolecules have gained significant attention, which is their ability to \u00a1\u00a5stick to each other\u00a1\u00a6 to from \u00a1\u00a5protein aggregates\u00a1\u00a6 or \u00a1\u00a5amyloids\u00a1\u00a6. This behavior is more common when proteins fail to adopt a \u00a1\u00a5correct\u00a1\u00a6 three dimensional shape, commonly known as a \u00a1\u00a5misfolded\u00a1\u00a6 form. These aggregates can be both beneficial and toxic for cellular processes. Although seems simple, this process is extremely complicated and no precise molecular understanding has emerged. Also, the ability of the proteins to misfold and aggregate via multiple pathways leading to various forms of aggregates has never been explored. Such molecular-level details of the process are important to know since functional aspects of these aggregates are related to the molecular size, shapes, stability and the rates of the formation. Since many of these parameters of this stochastic process are extremely difficult to analyze via conventional biochemical means, molecular-level computational simulations can be valuable. A precise understanding of the protein aggregation phenomenon would broaden the fundamental knowledge of both pathological and functional aspects of biomolecular science besides throwing newer insights. <br\/> In this proposal, we will use amyloid-?\u00d2 (A?\u00d2) peptide as a model protein that is known to form misfolded aggregates to accomplish our goals. Our main objective is to establish a fundamental framework for stochastic molecular-level simulation of the \u00a1\u00a5on-pathway\u00a1\u00a6 fibril formation process that will serve as a basis for analyzing more realistic models with competing pathways to precisely predict the dynamics and mechanisms of protein aggregation. We have initiated a collaborative effort between two PIs at University of Southern Mississippi, (USM) with expertise in computational and biophysical analysis, to achieve our truly inter-disciplinary objectives. <br\/><br\/>Intellectual Merit: Protein aggregation is a nucleation-dependent process, however, precise understanding of its kinetics is not yet known. Aggregation and fiber formation is often considered to be a stochastic process with large variations in macroscopic molecule behavior and hence, stochastic molecular-level simulations would be essential to understand their dynamics. Furthermore, it is not realistic to consider aggregation as an isolated event as there are many different factors that influence protein aggregation in a physiological environment. Broadly, these include molecules that may \u00a1\u00a5interact\u00a1\u00a6 with the protein besides others such as ionic strength, temperature etc. Hence in this proposal, we are focused on developing a fundamental framework of modeling protein aggregation and amyloid formation phenomenon via molecular-level modeling and stochastic simulation methodologies. The biophysical experiments can show the cumulative effects of the aggregates whereas the simulation will be able to predict the concentration change dynamics with respect to time for every aggregate involved in the pathway. This will allow us to study the exact nature of each aggregate and their sensitivity to the over-all pathway dynamics. <br\/><br\/>Broader Impact: USM is an excellent place to conduct this research from a scientist training perspective; MS being among the states with the highest levels of poverty besides providing a truly diverse student population. The research will provide a broader impact to the scientific community in the form of a fundamental mechanistic knowledge about protein aggregation systems. Our educational outreach mechanisms will involve giving seminars at participating undergraduate institutions in MS, recruiting economically disadvantaged students (including women and minorities) to perform summer research in the PI and Co-PI laboratories at USM. It will also enhance our graduate program through the design of new inter-disciplinary courses (e.g. \u00a1\u00a7Systems Biology\u00a1\u00a8 and \u00a1\u00a7Computational biophysics\u00a1\u00a8).","title":"EAGER: Molecular-Level Stochastic Simulation To Predict The Dynamics of Protein Misfolding and Aggregation","awardID":"1158608","effectiveDate":"2011-07-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":["563362"],"PO":["565223"]},"185238":{"abstract":"The February 22, 2011 earthquake in Christchurch New Zealand, an aftershock of a larger earthquake in September 2010, caused significant infrastructure and economic damage, and life loss, to a modern city with similar population characteristics as US metropolitan communities. In the days and weeks following the earthquake, various risk communication strategies were utilized to reach individuals affected by the ongoing aftershocks, including online networked communications. By collecting data on access to and use of online information in this critical period following the earthquake, this project will advance knowledge about information and communication capacities as they affect coping and resiliency in the aftermath of disaster. Specifically, the project examines the effects of reliance on online communications on individual coping ability and community recovery, and on the role of networked online communication among those directly affected by disaster. These questions will be examined through a series of focus groups and a household survey in the disaster-affected area. <br\/><br\/>INTELLECTUAL MERIT: This research will address key questions about information access in disaster; the effectiveness of crisis communications using networked online technology; and links between information access and resiliency among disaster-affected populations. It will make substantial contributions to the literature on \"crisis informatics\" due to the fact that this study will include a representative sample of a disaster affected community rather than just technology users who are utilizing social media. It will also contribute knowledge to the effects of information access on perceptions of community resiliency, in a metropolitan area, linked to shifting communication capacities in disaster. <br\/><br\/>BROADER IMPACTS: The project will address the effect of networked communications, and its absence, on coping post disaster and has the potential to inform policy at the local, state, and national levels and to improve resiliency in U.S. communities. Outcomes from this project will include written reports, peer-reviewed articles, and presentations to a number of audiences including the U.S. Natural Hazards Workshop. This research will also provide support for one researcher from an underrepresented STEM group.","title":"RAPID: When Online is Off: Communicating in Disaster Following the February 22, 2011 Christchurch, NZ Earthquake","awardID":"1138901","effectiveDate":"2011-07-15","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[497022],"PO":["565342"]},"180552":{"abstract":"Biophysics simulation helps biomedical researchers understand the physical constraints on biological systems as they engineer novel drugs, synthetic tissues, medical devices, and surgical interventions. However, writing high-performance biophysics simulation software for modern parallel computer hardware is a challenging problem. This research project will solve this problem by developing a new generation of biophysics simulation software that is optimized for complex high-performance computer hardware. The project will develop this software using a family of domain specific languages (DSLs). A DSL is a concise programming language with a syntax that is designed to naturally express the semantics of a narrow problem domain. Biophysics simulation DSLs will be used to improve the productivity of simulation software developers and the efficiency and performance of the resulting software by enabling the DSL implementation to take advantage of high-level domain-specific optimizations that are inaccessible to general-purpose compilers and general-purpose languages. The simulation technology built from the family of biophysics simulation DSLs will be used to solve the important biological problems of developing new neuroprosthetics, combating viral infections, effective drug discovery, and understanding drug side effects. In addition, this research will expose students at the graduate and undergraduate level to the role that domain-specific languages play in computing in general and biophysics simulation in particular.<br\/><br\/>The biophysics simulation DSLs will be developed with a general DSL infrastructure. This infrastructure will make use of polymorphic embeddings, multi-stage compilation, and parallel execution patterns to implement the high-level, implicitly parallel DSLs in a common host language. The DSL infrastructure will simplify DSL development by providing a reusable framework for parallelism and domain-specific optimization. When completed, this infrastructure will allow scientists in other application domains to create and use their own high-performance DSLs, in the same manner that this research uses the infrastructure to develop DSLs for biophysics simulation. The result will be a new generation of DSLs in a number of domains that provide high-productivity application development and high-performance on modern heterogeneous parallel hardware such as multicore microprocessors, GPUs and distributed systems.","title":"SHF: Large: Domain Specific Language Infrastructure for Biological Simulation Software","awardID":"1111943","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[483870,483871,483872,"507674","556786"],"PO":["564588"]},"181333":{"abstract":"The work will focus on models of computing with brain cells, more specifically the neurons and their major support cells in the brain, the glia. The major novelty of the project is the consideration of the glia cells in these models of computation. The interplay between glia and neurons has not been considered in the domain literature until now. Thus the project will not only look at how to perform computation taking inspiration from the organization of the neurons and glia into networks, but also explore topics such as investigating the power related to sequentiality\/parallelism of such devices, synchronization, data processing and manipulation such as considering different types of output encoding for the computation. Several other topics considered already for neuronal systems such as universality\/non-universality of networks with various restrictions, normal forms and consideration of the refractory period of neurons will also be investigated for the new devices.<br\/><br\/>The project will incorporate an experimental part in the second year of the timeline when with the help of the senior researcher on the project, Dr. Mark DeCoster (professor of Biomedical Engineering) the results of the first phase of the work will be validated in the lab. The work holds promise to significantly impact several research areas as well as the emerging area of computing with cells. In Computer Science, the research may yield new paradigms and new computing techniques (as has happened in the discovery of genetic algorithms and neural networks). In Biology, the project is likely to provide insight into the organization of brain cells networks, including the support cells for neurons, the glia. <br\/><br\/>One of the major objectives of this project is to gain more knowledge and insight into the extraordinary biological systems ? the cells and also to understand in a systemic way how the sub-cellular processes work and how the cell-cell communications can be useful for performing distributed computations. Algorithms, source codes and results of the project will be freely disseminated to the public through the project's website.","title":"SHF: Small: Collaborative Research: Computing with cells-the neuron case","awardID":"1116707","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[485879,485880],"PO":["565223"]},"181586":{"abstract":"Protein-ligand binding affinity is the principal determinant of many vital processes, such as cellular signaling, gene regulation, metabolism, and immunity, that depend upon proteins binding to some substrate molecule. Consequently, it has a central role in drug design. Due to prohibitive costs and delays associated with experimental drug discovery, academia and pharmaceutical and biotechnology companies rely on virtual screening using computational molecular docking. Typically, this involves docking of tens of thousands to millions of ligand candidates into a target protein receptor?s binding site and using a suitable scoring function to evaluate the binding affinity of each candidate to identify the top candidates as leads or promising protein inhibitors. Since a scoring function (SF) is used to score, rank, and identify drug leads, the fidelity with which it predicts the affinity of a ligand candidate for a protein?s binding site and its computational complexity have a significant bearing on the accuracy and throughput of virtual screening. However, current state-of-the-art scoring functions have a number of deficiencies, including either mediocre accuracy for affinity prediction or low throughput, inconsistent accuracy, inflexibility in accuracy-throughput trade-off provided, and reliance on only a single category of scoring function.<br\/><br\/>INTELLECTUAL MERIT: Accurately predicting the binding affinities of large sets of diverse protein-ligand complexes remains one of the most challenging problems in computational biomolecular science, with applications in drug discovery, chemical biology, and structural biology. We seek to address this problem by developing efficient discrete optimization algorithms that facilitate: (1) the design of accurate, high-throughput single and multi SF methods with provable optimality for a given protein-ligand complex dataset; (2) determination of biochemically-relevant SFs through novel biochemical rule filters that suitably constrain the protein-ligand complex features selected; (3) prediction robustness through a novel multi-SF approach that reduces the variance in accuracy associated with relying on only a single SF; and (4) flexibility in accuracy-throughput tradeoff provided through a new integrated dynamic multi-SF approach. <br\/><br\/>BROADER IMPACTS: This project will have a number of broader impacts: (1) public health benefits by facilitating efficient and cost-effective drug discovery, which in turn helps lower drug costs and improves affordability; (2) impact on other domains where scoring function type approaches are used; (3) interdisciplinary training of students in an important application area; (4) dissemination of research and software artifacts developed during the project; and (5) participation and training of underrepresented groups and K-12 outreach.","title":"AF: Small: Accurate, Biochemically-Relevant, and Robust Scoring Functions for Protein-Ligand Binding Affinity Prediction","awardID":"1117900","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[486517],"PO":["565223"]},"181234":{"abstract":"This research will develop new computational methods for simulating quantum mechanical systems, while identifying fundamental obstacles preventing efficient algorithms. These methods will be applied to simulating quantum computers and other devices, which in turn will provide new algorithmic paradigms for hard computational problems of practical importance. <br\/><br\/>Quantum computers are known to solve problems that are believed to be intractable by any conventional computer. These include cryptographic problems such as factoring large numbers, as well as simulating quantum systems with many degrees of freedom. This research will utilize recently discovered connections between quantum mechanics and the matrix permanent. Like the determinant, the permanent of a square matrix is also defined as a sum over permutations, only without signs. But it is fundamentally different. While the determinant can be efficiently computed using basic linear algebra, computing the permanent is among the hardest computational problems, and is even believed to be intractable on a quantum computer. Nevertheless, it is known that any procedure for approximating permanents to a certain accuracy will immediately give a method for efficiently simulating any quantum algorithm. This project will thus develop new classical randomized methods for simulating quantum mechanics through the study of the algorithmic complexity of the approximations of permanents, while determining complexity-theoretic obstacles to such approximations. This research will uncover fundamental truths about matrix permanents and their relation to computer science and computational physics. It will provide new lower and upper bounds on the permanent and related quantities, while establishing new hardness results for computing and approximating permanents. The new bounds on permanents will provide deeper theoretical and algorithmic understandings of measurement probabilities in quantum optics. The mathematical part of this research will have impact on many areas such as quantum information theory, combinatorics, semidefinite programming, multilinear algebra, operator theory. In turn, this research will contribute to our theoretical understanding of the capabilities and limitations of computers, both classical and quantum.","title":"AF: Small: Classical Simulation of Quantum Algorithms and Approximation of Permanents","awardID":"1116143","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[485638,485639],"PO":["565157"]},"181124":{"abstract":"Scientific inquiries generate many different kinds of shapes. Protein backbones are represented as curves, MRI scans give us a three-dimensional representation of the brain, and stress tensors in materials can be represented by point sets in a curved manifold. Because of improvements in data gathering and sensing technologies, we now have large databases of shapes, and have the chance to gain a deeper understanding of scientific processes by performing detailed analysis of these shape collections.<br\/><br\/>Shape databases are large and complex. Thus, a core challenge when analyzing shape collections is finding succinct descriptions, or synopses, of these collections. Such synopses give us snapshots of the data, describe local and global variations in a population, and provide the building blocks for advanced and rigorous data analysis. However, shapes have complex representations, and the spaces that shapes inhabit are mathematically intricate. Computing compact synopses with provable quality requires new geometric and algorithmic ideas.<br\/><br\/>In this project, the PI, using experience in the areas of shape matching, data analysis, synopsis data structures, and the geometry of non-Euclidean spaces, will develop a suite of algorithmic techniques for producing synopsis structures on shapes, including generalized means and medians, clusterings, subspace representations, and approximate histograms and distributions. This research lays the foundations for rigorous data analysis in shape spaces. It develops the algorithmic and geometric building blocks that will yield a deeper understanding of these spaces, and in doing so will yield many new ideas for computations in the non-Euclidean spaces that shapes inhabit. More broadly, the computational study of shape as a pathway to deeper scientific inquiry takes advantage of the power of computational methods as well as growing stores of data, and creates new opportunities for the emerging area of data-driven science.","title":"AF: Small: Synopsis Data Structures for Data Analysis in Shape Spaces","awardID":"1115677","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["542076"],"PO":["565157"]},"181366":{"abstract":"Aggressive technology scaling has brought new challenges to the design and manufacturing of Integrated Circuits (ICs). A mounting challenge is the modeling and characterization of power consumption in face of possible operating and manufacturing variabilities. Accurate power characterization leads to extended battery life for mobile devices and to enhanced device reliability. A second challenge arises because manufacturing is increasingly outsourced to external foundries. Verifying that the manufacturer has not inserted any malware or \"Trojan\" circuitry that compromises the security of the final product is essential. The proposed research provides a unifying framework for these two challenges by detailed algorithmic analysis of infrared emissions from the backside of ICs. We propose techniques to convert the infrared emissions into accurate spatial and temporal power estimates, and to use the power and infrared ?fingerprints? to verify that the chip does not contain extra circuits inserted by the manufacturer, unbeknownst to the designer, that could cause a security breach. The successful completion of this project will lead to improved power modeling and characterization tools and increased confidence that there are no added malware in manufactured chips. The project will also lead to design prototypes and a large volume of valuable data and benchmarks that will be openly disseminated via NSF-funded Trust-Hub and other web-based portals. <br\/><br\/><br\/>The methods and tools will find broad usage in the industry, government, and in academia. They will also impact the daily lives by increasing the battery lifetime and by improving the system's reliability and integrity. Education plan includes research experience for undergraduates, curriculum development, integration of material into graduate courses and emphasizing entrepreneurship within education. Recruitment and development of under-represented groups of students will be targeted.","title":"SHF:Small:Collaborative Research: Algorithmic Techniques for Post-Silicon Characterization Using Infrared Emissions","awardID":"1116858","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485957,485958],"PO":["562984"]},"181377":{"abstract":"Future computing chips inside mobile devices such as smart phones, and servers used in data centers will contain many processors, memories and specialized functional units, connected by a sophisticated on-chip network. The on-chip network is a critical design element that influences the performance, power consumption, and cost of the chip. Hence, there is a compelling need for tools and techniques to explore the design space of an on-chip network quickly to create networks that are optimized for a given application and\/or a market segment. Trace-based simulation is used widely to design and optimize on-chip networks. However, trace-based simulation can result in incorrect and misleading conclusions about network behavior because, a trace does not model the packet injection rate of the application accurately. In this project, the investigators develop techniques to overcome this limitation of trace-based simulation, which allows for rapid design space exploration of on-chip networks with accuracy approaching that of full-system simulation but with simulation time similar to trace-based simulation.<br\/>Dependencies between packets are inferred by sampling multiple runs of an application on a fully connected network topology with different link latencies. Traces augmented with dependency information, model the packet injection rate of an application more accurately. FPGA-based acceleration is used to collect and analyze traces and a fast multithreaded network simulator that is capable of processing traces augmented with packet dependency information, is developed.<br\/>The broader impact of the work will be through a validated repository of benchmark traces augmented with the packet dependency information and a multi-threaded network simulator that can be used by the research community to design and optimize on-chip networks with hundreds of processors.","title":"CCF: Small: Improving Trace Based Simulation of On-Chip Networks","awardID":"1116897","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485983,485984],"PO":["366560"]},"181289":{"abstract":"Voice conversion (VC) systems transform segments of speech from a given source speaker so that it<br\/>can be identified as spoken by a specified target speaker. Currently, standard VC systems require<br\/>parallel training on extensively labeled sets of speech data where the source and target speaker share<br\/>equivalent content for building direct mapping models. This project builds on the concepts of nonparallel<br\/>VC systems reducing the need for labeled and shared speech content between source and target<br\/>speakers as well as allowing for both intra-lingual and cross-lingual conversion scenarios. This project<br\/>focuses on two main areas: (1) Building a framework for non-parallel VC without explicit phonetic,<br\/>sound, word, or sentence level labels, and (2) Providing effective target speaker mapping to obtain<br\/>converted speech with as good as or better quality compared to current VC systems. The VC<br\/>framework consists of three main components: (1) A speaker independent language model; (2) An<br\/>algorithm for model adaptation to target speaker; (3) A speech synthesis block to generate converted<br\/>speech from a target-adapted language model.<br\/><br\/>This project will provide a broad framework for applications such as personalization of assistive textto-<br\/>speech (TTS) systems, foreign language learning, and as a possible component in speech-to-speech<br\/>translation systems. This project will support graduate student research and provide results for<br\/>community distribution through conference and journal submission. Additionally, an open-source<br\/>software toolset will be developed and freely distributed. The project will also be used in outreach for<br\/>underrepresented groups in Science Technology Engineering and Mathematics (STEM) disciplines.","title":"RI: SMALL: Modeling Voice Source Transformation in Monolingual and Crosslingual Non-parallel Voice Conversion Applications","awardID":"1116475","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[485769],"PO":["565215"]},"184249":{"abstract":"This award provides funding for a new Research Experiences for Teachers (RET) Site focused on Emerging and Novel Engineering Technologies at the University of Texas-Pan American. Each year 12 middle and high school teachers from school districts in the Rio Grande Valley will participate in research projects at the university. The projects span a large array of topics in engineering and computer science including control systems, power and energy, wireless networks, algorithms, and robotics. The teachers will also develop modules related to their research which they will implement in their classes in the following school year. The teachers will showcase their work and build communities of practice with other educators at an annual workshop. Through participation in the RET Site the teachers will have an enhanced knowledge base in engineering and computer science and the skills to translate this into their classroom practices, thus impacting their students and motivating them towards science, technology, computing, and engineering disciplines.<br\/><br\/>The intellectual merit of this project revolves around the expertise of the research team and the clearly-defined projects that are of current interest and relevant to the teachers. K-12 curriculum units that are developed are tied to state and national standards. The evaluation plan is outstanding. Overall the project is conceptually sound with authentic research experiences tied to research goals that are compelling and of interest as models for others.<br\/><br\/>The broader impacts of the project lie in the potential to impact STEM education in the Rio-Grande Valley, a region with a population that is 86% Hispanic. Teachers will learn about engineering and computer science through active participation in research combined with professional development and sustained follow-up during the academic year. This should motivate and prepare an untapped student talent pool to careers in engineering and computer science. Materials will be disseminated to a large number of teachers in the participating schools and broadly through the national TeachEngineering.org Digital Library. The goal of the project is to build a long-term collaborative partnership between the university and the area schools. The project should enable the partners to work together to build a sustainable foundation of outstanding computing and engineering education in the region.","title":"RET Site: Research Experiences for Teachers in Emerging and Novel Engineering Technologies (RET-ENET) in the Rio Grande Valley","awardID":"1132609","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1359","name":"RES EXP FOR TEACHERS(RET)-SITE"}}],"PIcoPI":[494088,494089],"PO":["564181"]},"188858":{"abstract":"A prosthetic robot hand is designed and developed with programmable passive dynamics modeled after human hands. The goals include the development of mathematical models of variable passive dynamics in human hands, the study of the role of passive dynamics in hand operations, the design of novel joint mechanisms, development of a hand prototype and an EMG control interface, and the development of pedagogical laboratory modules for education.<br\/><br\/>The project develops technology to substantially improve rehabilitation and quality of life for persons with hand disabilities, including amputees returning from ongoing wars. This work addresses the acute need for substantial improvement in the functionality and control mechanisms for prosthetic hands. Most current prosthetic hands do not address the underlying design constraints of a useful prosthetic hand, namely, weight and size limitations, limited control, ease of control and suitable aesthetics. In addition, students learn this new subject and contribute to the advance of science and technology in this domain. Finally, programs are in place to recruit women, individuals with disabilities, and Native American students in engineering research and education.","title":"CAREER: Development of a Robotic Prosthetic Hand with Programmable Passive Dynamics","awardID":"1157954","effectiveDate":"2011-07-20","expirationDate":"2016-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[506396],"PO":["534411"]},"181411":{"abstract":"Today, simulation is the de facto method for studying multicore cache hierarchies. But simulation is costly due to the combinatorial design spaces involved, especially as multicore processors scale to 100s of cores and 100+ MB of on-chip cache. Reuse distance (RD) analysis can help architects evaluate multicore memory performance more efficiently. Unfortunately, locality in multicore processors depends on how per-thread memory reference streams interleave. Reliance on memory interleaving makes multicore locality profiles architecture dependent, limiting their ability to analyze different configurations. For loop-based parallel programs, however, threads are typically symmetric and exhibit similar locality characteristics. Such thread symmetry makes multicore RD analysis tractable: locality profiles remain stable with respect to cache capacity scaling, and change systematically with core count and problem size scaling.<br\/><br\/>This project is exploring several research directions related to multicore RD analysis for loop-based parallel programs. First, it is characterizing how Concurrent RD and per-thread RD profiles for symmetric threads vary with processor and problem scaling. Second, it is developing techniques to predict these profile variations. Simple prediction techniques such as reference groups, as well as more sophisticated parametric and non-parametric learning approaches, are being studied. Finally, it is applying the new RD analysis to explore large-scale multicore design spaces, identifying good cache hierarchy organizations. It is also using the RD analyses to improve existing memory performance enhancement techniques such as multithreading and locality optimization.","title":"SHF: Small: Developing and Applying Reuse Distance Analysis Techniques for Large-Scale Multicore Processors","awardID":"1117042","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[486066,"541967"],"PO":["565272"]},"183842":{"abstract":"The purpose of this grant is to support US based graduate students for traveling to and attending the ACM MobiCom 2011 conference to be held in Las Vegas, Nevada on September 19-23, 2011. MobiCom is a premier conference that serves as a meeting point of researchers from academia and industry, as well as practitioners in different fields of wireless networking and mobile computing. In conjunction with the main conference, MobiCom 2011 will also host several workshops and tutorials on topics of emerging interest. The NSF travel grant will allow a diverse set of students from US institutions to participate in this forum and gain valuable experience by interacting with senior researchers as well as peers in this field. Such interactions and exposure can positively influence the quality of the students' individual research and professional growth, in turn shaping the future of wireless networking and mobile computing technology.","title":"NeTS: Small: Student Travel Grants for Attending MobiCom 2011","awardID":"1130085","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["508426"],"PO":["557315"]},"181598":{"abstract":"A new class of estimation methods are developed that can optimally utilize minima resources for high-precision pose tracking. This project develops the first formal methodologies for providing selections of parameters (e.g., camera frame rate, image resolution, number and type of detected features) for small portable devices to delay depletion of the battery. The approach is based on a rigorous study of the properties of the pose tracking problem with visual and inertial sensors and (1) identifies the fundamental limits of the attainable estimation accuracy, and (2) allows the analytical prediction of the accuracy as a function of the use of the sensing and processing resources. This makes possible the development of algorithms that optimally allocate system resources whose design relies on an optimization framework where the estimation errors constitute the cost function to be minimized, the resource limitations are explicitly modeled as constraints, and all the relevant design parameters (e.g., camera frame rate, number of features used) are the optimization variables. The immediate impact of this research effort is the increased cost efficiency and accuracy for navigation tasks in diverse applications.<br\/><br\/>The developed technology is available to the wider community through open-source position-tracking software for mobile phone devices and provides useful assistive technology. We engage K-12 students at local outreach events and introduce them to engineering as well collaborate with MESA, a long-standing program at the University of California at Riverside with a proven record of attracting underrepresented minority students to science and engineering.","title":"RI: Small: Minimalistic Estimators for Navigation of Miniature Mobile Platforms","awardID":"1117957","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["549258"],"PO":["564069"]},"181125":{"abstract":"This project develops and realizes efficient and large scale mapping and 3D reconstruction on mobile robots. We develop a new optimization paradigm which combines the advantages of both direct and iterative methods by (1) investigating a novel class of optimization methods for robot mapping problems: subgraph-preconditioned conjugate gradients (SPCG) that combine the advantages of direct and iterative methods while minimizing the disadvantages, (2) investigating subgraph preconditioner selection and quality analysis, (3) applying the above techniques to large-scale 3D reconstruction problem mobile robots, and (4) investigating on-line versions of these algorithms. We adapt the SPCG for this setting by incrementally building the graph sparsifier that gives us a good preconditioner.<br\/><br\/>Beyond robotics and vision, we show that similar bounds can be derived for the general problem of approximating distributions. A concrete deliverable of the proposed work is a software package that embeds the new hybrid approach to solving the mapping\/reconstruction non-linear optimization problem, and is easily deployable to a wide range of mobile robotic platforms: terrestrial, aerial, underwater, or underground, acting individually or in teams. The robotics research community has access to this technology, which provides great improvement over the capabilities of current mapping\/reconstruction software, both in terms of the size of the problem, as well as in terms of speed and online applicability. Finally, at a more local level, this research impacts education of both graduate and undergraduate students at Georgia Tech.","title":"RI: Small: Ultra-Sparsifiers for Fast and Scalable Mapping and 3D Reconstruction on Mobile Robots","awardID":"1115678","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[485377],"PO":["564069"]},"181279":{"abstract":"This proposal addresses the grand challenge of reducing power consumption of digital electronic circuits leveraging off of a promising new automated design flow for asynchronous circuits. This novel flow actively manages data-flow within the circuit and is capable of disabling computational blocks that would otherwise perform unnecessary calculations and waste power. The proposed research is to develop mathematical software algorithms that will optimize the partitioning of the circuit into computational blocks to maximize the blocks that are disabled and thereby minimize power consumption.<br\/><br\/><br\/>The impact of this research will span engineering, scientific, and social values. First, it will enable synthesized asynchronous designs to be lower-power and faster and thus more compelling. Moreover, because the research is based on a formal framework of communicating processes it is generally applicable to many computational systems, including next-generation beyond-CMOS computational platforms. At the social level it will enable the enrichment of students at USC and other institutions as the tool flow is made more widely available. Lastly, it will provide a rich context for a pre-college outreach program in which the principal investigator teaches high-school students about the world of electrical engineering, integrated chip design, and in a greener economy and helping form a sustainable society","title":"SHF: Small: Reconditioning: Optimizing Conditional Communication in Asynchronous Design","awardID":"1116416","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485748],"PO":["562984"]},"187956":{"abstract":"This project considers the unification of two view of language: that<br\/>from natural language processing and that from linguistic typology.<br\/>Our view is that typological information is both useful for solving<br\/>real-world natural language processing thats and automatically<br\/>derivable from language data. This research first explores how to use<br\/>typological knowledge to improve performance on problems such as<br\/>dependency parsing and machine translation for low density langauges.<br\/>Intuitively, our statistical models waste time exploring a hypothesis<br\/>space that is too big: the space of realistic grammars is much smaller<br\/>than the space of all grammars. The second part of this research<br\/>considers the automatic acquisition and boostrapping of typological<br\/>knowledge from raw text. The outcome of this research is: (a)<br\/>improved statistical models for hard natural language processing<br\/>problems; and (b) a larger library of typological universals that have<br\/>been derived automatically from data. Our outcomes are empirically<br\/>evaluated on the raw language processing tasks and in terms of the<br\/>quality of the universal implications mined from data, but comparing<br\/>them with known repositories of universals. <br\/><br\/>Our results will impact the fields of natural language processing and linguistics. From the research side, this research will find applications in a wider variety of problems than the ones we intend to study; in particular, the use of linguistic universals in natural language processing technology<br\/>will fundamentally change the way multilinguality is addressed in this<br\/>field. From a linguistics perspective, the goal of this project is to<br\/>shed new light on linguistic universals. This should impact not only<br\/>the area of typology, but also the study and preservation of<br\/>endangered languages. By automatically identifying typological<br\/>features and implications from data, the process of documenting<br\/>endangered languages could be made more efficient: leading to a<br\/>smaller loss of knowledge of these languages.","title":"RI: SMALL: Statistical Linguistic Typology","awardID":"1153487","effectiveDate":"2011-07-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550878"],"PO":["565215"]},"181610":{"abstract":"Precisely controlling the physical parameters of an integrated circuit (IC) fabrication process is becoming more challenging in advanced technology nodes. This lack of control manifests as increasing levels of variations in power and delay in product chips. Unfortunately, regional or within-die variations have increased sharply in recent technologies, and many are dependent on the context of the design layout region. The traditional methods of tracking process variations, e.g., those that utilize scribe line test structures, have become increasingly less effective for predicting power and delay variations within chips that they are adjacent to. This has driven the need for embedded test structures, i.e., those incorporated directly into the product IC. This research is designed to address the deficiencies in the current state-of-the-art of embedded test structure design by focusing on designs that are truly embedded and leverage as much of the existing chip infrastructure as possible. For example, power islands will be investigated as a means of obtaining regional leakage variations and non-invasive modifications to the scan chain architecture will be investigated as a means of measuring regional delay and power variations.<br\/><br\/>A key emphasis of the proposed work is to improve the correlation of embedded test structure measurements with actual chip parameters, e.g., by developing test and measurements techniques that are carried out under actual operating conditions. Both graduate and undergraduate students will participate in the design and test of fabricated chips, which will enhance their educational experience thus preparing them for a more technologically educated workforce for the IC industry.","title":"SHF: Small: Measurement and Analysis of Regional Process Variations using Existing and Minimally Invasive On-Chip Embedded Resources","awardID":"1118025","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[486576],"PO":["562984"]},"191587":{"abstract":"CCF - 0746850 <br\/>PI name: KArtik Mohanram<br\/>Title: CAREER: Design Optimization for Robustness to Single-Event Effects<br\/>Institution: William Marsh Rice University<br\/><br\/><br\/>ABSTRACT:<br\/>Technology trends and economic factors are driving forces behind the widespread move to mainstream computing and communication systems based entirely on commodity hardware and operating systems. Yet, the premium that we as a society place on the reliability of such systems has increased commensurate with our reliance on them for the smooth operation of our lives. Soft errors resulting from single-event effects (SEEs) are an important-and possibly dominant-failure mode that impact the reliability of such mainstream commodity systems.<br\/><br\/>This research will develop low-cost SEE-reliability-aware and SEE-reliability-driven design solutions based on optimization to maximize robustness to SEEs, commonly termed SEE-hardening. SEE-hardening is an attractive low-cost solution to increase reliability since it does not require any runtime support from either the hardware or the operating system. SEE-hardening can also be used to complement and reduce the overhead cost of traditional fault detection and tolerance techniques. The optimization algorithms for SEE-hardening resulting from this work provide seamless tradeoffs between SEE-hardness and area-delay-power, enabling cost-effective solutions commensurate with the criticality and reliability requirements over the lifetime of the target application. A major impact of this research is to enable ubiquitous low-cost highly reliable computing, by expanding its reach to domains that lack the financial resources to acquire custom solutions. Through academic and industry collaborations, this project will develop an integrated testbed and web-based resources to facilitate broad research in reliable system design, an area that is rapidly gaining in importance and interest.","title":"CAREER: Design Optimization for Robustness to Single-Event Effects","awardID":"1208933","effectiveDate":"2011-07-01","expirationDate":"2015-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":["517951"],"PO":["562984"]},"181434":{"abstract":"Abstract<br\/><br\/>Programmed molecular self-assembly could be used for the massively parallel construction of nanoscale devices. For example,\"Smart drugs\" that target drug activity to disease cells and activate in response to specific molecular clues would have minimal side effects and improve therapeutic outcomes. Such tasks require molecular systems that operate autonomously in complex environments, sensing and responding to molecular events. <br\/><br\/>This project proposes an approach for the automated construction of programmable molecular systems using DNA. DNA is not used to store genetic information but as a nanoscale engineering material. Interactions between single-stranded DNA molecules are determined by the linear sequence of these molecules and follow the rules of Watson Crick base pairing. The relatively low cost of synthesis and the predictability of interactions set DNA apart from other (bio) polymers such as proteins, and make DNA an ideal substrate for an engineering approach. <br\/><br\/>In this method, a desired chemical system is first specified using the language of chemical reaction networks. Next, this formal description is compiled into an experimentally testable DNA implementation. Auxiliary multi-stranded DNA complexes mediate the interactions between these signal strands. Because the language of chemical reactions can be used to specify a large number of behaviors -- including chemical oscillations, chaos, digital logic and even algorithmic responses -- this work suggests a powerful approach for generating complex molecular behaviors.<br\/><br\/>The proposed research is tightly integrated with an outreach program with two main aims. The first aim is to develop an educational framework that teaches the interdisciplinary skills required to succeed in molecular programming research. The second aim is to leverage this framework to attract and engage students who are not traditionally involved in electrical engineering or computer science research. A strong educational focus on molecular programming could be an important recruiting tool for attracting more women undergraduates to electrical engineering and computer science.<br\/><br\/>To achieve their aims, the PI and co-PI are engaged in developing and teaching a new interdepartmental curriculum on synthetic biology: The departments of Electrical Engineering, Computer Science & Engineering and BioEngineering are offering a joint sequence of classes on synthetic biology. These courses form an important first step towards developing a broad new educational program on \"molecular programming\". The PI also participates in the College of Engineering BRIDGE program at the University of Washington, which is designed to increase the participation of underrepresented minorities and women in engineering.","title":"SHF: Small: Programming Networks of Molecular Interactions Using DNA Strand-Displacement Cascades","awardID":"1117143","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["549568",486124],"PO":["565223"]},"181445":{"abstract":"The automatic identification in images of people, places, objects, and especially object categories is a central and ongoing challenge within computer vision. This project addresses this problem using low-level image features to learn intermediate representations, ones in which objects in images are labeled with an extensive list of highly descriptive visual attributes. This work demonstrates this approach in three domains: faces, plant species, and architecture. In each domain, it develops techniques for deriving visual attribute vocabularies, training attribute detectors, and building compositional models to automatically label attributes in images.<br\/><br\/>The project is making four fundamental contributions to the use of visual attributes. 1) It is developing new methods by which automatic systems and humans can interact to select domain-appropriate attribute vocabularies and label large image collections. 2) It is developing compositional models that capture dependencies between attributes. This provides more accurate attribute detection and enables inference of global properties of objects. 3) Using compositional models, the project is developing new, localizable attributes that capture the geometric relations between object parts and landmarks. 4) The project is designing algorithms that combine attributes to identify objects, search through image vast collections, and automatically annotate image databases.<br\/><br\/>Not only is this research generating large datasets of labeled images that should help catalyze new research, it is also demonstrating the feasibility of new systems for analyzing images in specialized domains such as faces, plants, and architecture. For example, the project develops new software applications for analyzing and searching images of faces as well as free mobile apps for plant species identification.","title":"RI: Small: Collaborative Research: Visual Attributes for Identification and Search in Images","awardID":"1117170","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["519250"],"PO":["564316"]},"181456":{"abstract":"Current microprocessor systems are based on semiconductor logic gates, which employ electronic input and output signals and power supplies. Each type of logic gates has a specific input?output signal correlation pattern. Voltages can be simply high or low: digital 1 or 0, respectively. A critical feature, which contributes to the success of modern computers, is input-output signal uniformity: the same voltage value emerging as output of one gate can be admitted as input of another gate. Very large scale integration is a crucial component of modern silicon processors. The development of even more powerful processors depends on continued progress in miniaturizing their components. However, if current trends continue, conventional silicon chips will soon reach their physical limits. By then, their transistors will be so small that current leakage will become an insurmountable problem. It is believed that constructing computers in which computations are performed by individual molecules is the inevitable wave of the future (P. Ball, Nature 2000, 406, 118-120). <br\/><br\/>The long-term goal of this project is the development of a first DNA-based nanocomputer, a biocompatible and smaller counterpart of the modern silicon-based processor. This project aims at the solution of the two major problems of molecular computation: the universal large scale connectivity of molecular logic gates and precise localization of logic gates in a nanoscale environment. A basic set of connectable DNA logic gates (NOT, AND OR) will be created. The DNA gates will be organized in a network that corresponds to an EX-OR logic function both in solution and on a two-dimensional DNA platform. Accomplishing the project will deliver the first connectable nanoscale logic units, a basis for the future DNA nanoprocessor. <br\/><br\/>This research will be integrated with education by introducing research topics into undergraduate teaching, students, research training at undergraduate and graduate levels, postdoctoral training.","title":"Connectable nanoscale DNA logic gates","awardID":"1117205","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7946","name":"BIO COMPUTING"}}],"PIcoPI":[486176],"PO":["565223"]},"181346":{"abstract":"The Internet backbone, including both core and edge routers, is becoming more flexible, scalable and programmable to enable future innovations in the next generation Internet. While the functionality of Internet routers evolves, the performance remains a major concern for real-life deployment. Traditionally, core routers have been designed using throughput as a key performance metric. While the throughput requirements continue to grow, peak power and total energy dissipated have emerged as additional critical considerations in the design of core routers as well as in other network equipment. Although ternary content addressable memories (TCAMs) have been widely used for packet forwarding, they have poor power performance. This work studies the use of low-power memory technology such as the static random access memory (SRAM) combined with field-programmable gate arrays (FPGAs) \/ application-specific integrated circuits (ASICs) to develop high-throughput and power-efficient solutions for various packet forwarding<br\/>engines including IP lookup, router virtualization, packet classification and flexible flow processing (e.g., OpenFlow). Packet forwarding engines in next generation routers and switches are designed using a hardware-software co-design framework. Based on this framework, novel architectures and algorithms are developed using power (including energy) as a key performance metric in addition to throughput. Specifically, to bridge the gap between software and hardware development, high-level power-performance models for hardware implementations of packet forwarding engines are developed and validated. These models facilitate design of various heuristics for power-efficient algorithms and architectures for virtualized IP lookup, multi-field packet classification and flexible flow processing. Instead of the highly popular TCAM based solutions, this work focuses on SRAM-based parallel and pipeline architectures. Novel techniques including partitioning, clock gating, power-aware data structure design and power-aware load balancing are studied to simultaneously increase throughput and reduce power and\/or energy dissipation","title":"SHF: Small: Hardware-Software Co-Design for Next Generation Packet Forwarding Engines","awardID":"1116781","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["563658"],"PO":["366560"]},"183216":{"abstract":"All branches of modern science demand data, and modern data acquisition technologies in biomedical research have adapted to increase the volume of data by at least three orders of magnitude in the past three decades. Much of this data is rather complex. Complex data has three properties: high volume (e.g., genome data, gigabyte per person), multimodal (e.g., Time x Genes x Control), heterogeneous (e.g., different time scales, data feeds).How these data are analyzed now presents a major challenge in scientific discovery. Although a series of powerful algorithms for statistical learning can yield accurate predictions for pattern recognition problems, the increased power of these learning algorithms has come at the expense of a considerable increase in complexity for adjusting a large number of parameters in a high dimensional space. Yet, such mathematically sound models that are computationally stable and statistically meaningful still demand more data than most biology laboratories and clinics can provide. This workshop focus fits into the initiative between NSF and NIH to investigate developing science and technology for merging computational, physics based models with biology and medicine. This two day workshop at RPI fits into the initiative between NSF and NIH to investigate developing science and technology for merging computational, physics based models with biology and medicine. There will be a panel at the end of each day on a particular topic, e.g., how to integrate scales, how to deal with uncertainty and missing data, how to use data semantics in multi-scale modeling. The panel discussions will be organized into a report and will be made publicly available. The seminars and panel discussions will be recorded and made it available to public.","title":"Workshop on Addressing Complexity in Multiscale Modeling and Analysis of Complex Data","awardID":"1127047","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["541892","541893"],"PO":["565136"]},"185768":{"abstract":"This project investigates how interactions on social media such as Facebook (FB) impacted the behavior of people affected by the tornado that hit Tuscaloosa, AL on April, 27, 2011. It will examine how interactions on FB influenced preparations prior to the disaster (e.g., by influencing people to take cover), the mobilization of recovery efforts (e.g., by spreading information on recovery efforts) and the psychological functioning of victims (e.g., by communicating with friends and family and providing social support). FB posts will be qualitatively analyzed to determine (a) the spread of information (contagiousness); (b) normative expectations; and (c) emotional functioning and support provided at each time point. A subsequent survey will examine how FB usage (past and present) impacts on psychological functioning three and six months after the storm.<br\/><br\/>Intellectual merit: The project will provide new scientific knowledge about how social influence processes affect perceived social support and psychological well being after disasters. Specifically, the project will extend the prior literature on social networks by (a) examining the impact of social influence processes as well as the spread of images on coping in online social networks such as FB; (b) focusing on the warning phase of an impending disaster in addition to the aftermath; (c) examine both the victims of the disaster as well as members of their social networks and how appeals for social and monetary support spread; and (d) assessing individual coping at multiple time points.<br\/><br\/>Broader impacts. The results will provide new insights into how to optimize communication after disaster, leading to better preparation, better mobilization of recovery efforts, and better psychological coping. Results will be disseminated widely to disaster planners, researchers and practitioners through conference presentations, journal publications, data sharing, and via a dedicated website disseminating the methodology and results of the project. In addition, the project will help train new investigators in the area of crisis communication, will fuel student theses and dissertations, and will provide mentorship opportunities for undergraduate students from underrepresented groups.","title":"RAPID: Social Influence and Social Networking in the Wake of the Tornadoes of April 27, 2011: An Examination of the Social Psychological Processes","awardID":"1141918","effectiveDate":"2011-07-01","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["564308","498601",498601],"PO":["564456"]},"175319":{"abstract":"The driving vision of this NSF CAREER project is a smartphone that can see, hear, and feel and therefore serve the user continuously. Smartphones have embraced a variety of sensors, such as cameras, microphones, accelerometers, GPS, and more. An emerging, important category of smartphone applications requires the use of sensors to learn about the physical world and the human user, often in the background without user engagement. However, existing and emerging smartphone platforms are fundamentally flawed for such sensing applications: they adopt a centralized processing model that always uses the increasingly powerful central processor, even for very simple tasks, in particular sensor data processing. This model leads to unacceptable battery lifetime when a smartphone senses frequently.<br\/><br\/>To address this fundamental flaw, this project targets at reinventing the smartphone platform by adopting a heterogeneous, distributed processing model that incorporates weak processors for simple, frequent tasks. The research has three objectives: (i) Relieve developers from dealing with the heterogeneous, distributed processing model with runtime and compiler support; (ii) Support efficient and secure execution for third-party applications that use heterogeneous, distributed resources; (iii) Provide optimized design and realization of the envisioned heterogeneous smartphone hardware, including both board and chip integrations. While the project has a focus on smartphone-like systems, the research results are expected to support general embedded systems with heterogeneous, distributed resources. The research project also provides a multidisciplinary platform to realize the educational objectives of developing system and experimental components for mobile embedded computing curriculum, involving undergraduate students in publishable research, and promoting science and engineering studies to high-school students and to groups underrepresented in these areas.","title":"CAREER: Reinventing Smartphones for Sensing","awardID":"1054693","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["548312"],"PO":["565255"]},"181611":{"abstract":"III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families<br\/>Swaminathan Vishwanathan, Purdue University; Manfred Warmuth, University of California, Santa Cruz<br\/><br\/>Machine learning is currently indispensible for building predictive models from massive data sets. A large majority of widely used machine learning algorithms are based on minimizing a convex loss function. A fundamental problem with all such models is that they are not robust to outliers. To address this limitation, this project develops probabilistic models based on a parametric family of distributions, namely, the t-exponential family, that lead to quasi-convex loss functions and yield models that are robust to outliers. <br\/><br\/>The key challenge when working with the t-exponential family of distributions, as in the case of the exponential family, is to compute the log-partition function and perform inference efficiently. The project addresses this challenge in two specific cases. For problems with small number of classes exact iterative schemes are being developed. For problems where the number of classes is exponentially large, approximate inference techniques are being developed by extending variational methods. <br\/><br\/>In partnership with Google, some of the data mining algorithms resulting from this project are being applied to a challenging real-world problem of recognizing text in photos (the PhotoOCR problem). The project offers opportunities for research-based advanced training of graduate students as well as research opportuinities for undergraduates in machine learning and data mining. Algorithms for constructing predictive models from data that are robust in the presence of outliers are likely to find use in a broad range of applications. Open source implementions of algorithms, publications, and data sets resulting from the project are being made available through the project web page at: http:\/\/learning.stat.purdue.edu\/wiki\/tentropy\/start","title":"III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families","awardID":"1118028","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["527386"],"PO":["565136"]},"193766":{"abstract":"ABSTRACT<br\/>0546574<br\/>Wanpracha Chaovalitwongse<br\/>Rutgers University New Brunswick<br\/><br\/>CAREER: NOVEL OPTIMIZATION METHODS FOR COOPERATIVE DATA MINING WITH HEALTH-CARE AND BIOTECHNOLOGY APPLICATIONS<br\/><br\/>There is an urgent need to advance and apply quantitative and qualitative approaches to the study of<br\/>epilepsy and brain disorders. As uncontrolled epilepsy poses a significant burden to society due to as-<br\/>sociated healthcare cost, this project is aimed at the development of an automated seizure prediction<br\/>system and brain abnormal activity classifier. To achieve this goal, optimization-based data mining<br\/>(DM) approaches will be developed to quantitatively analyze the brain activity through electroen-<br\/>cephalogram (EEG) data. The proposed DM techniques will excavate hidden patterns\/relationships<br\/>in EEGs, which will give a greater understanding of brain functions (as well as other complex sys-<br\/>tems) from a system perspective. Specifically, a new DM paradigm for the seizure prediction and<br\/>brain activity classification will be developed based on novel optimization-based DM techniques for<br\/>feature selection, clustering, and classification. The proposed research will contribute to the computer<br\/>science, engineering and medical communities along the following four lines: (1) the development of<br\/>novel mathematical models and optimization techniques for DM problems and time series analysis,<br\/>(2) the implementation of statistical techniques to detect patterns from selected features\/clusters<br\/>for predicting seizures and classifying normal and epileptic EEG activity, (3) the utility of detection<br\/>theory and the experimental designs to assess and validate the efficacy, robustness, and uncertainty<br\/>of the proposed DM paradigm as well as fine-tune the optimal parameter setting, (4) the extension<br\/>of the fundamental research findings in optimization and DM to other cross-disciplinary research,<br\/>which will constitute a new avenue of research in optimization-based DM and time series analysis.<br\/>The proposed research is very crucial to decision making processes in real world problems. Success<br\/>of this research will advance the state-of-the-art in the field of optimization in DM, and have a<br\/>greatly significant impact on medical research. The research scope in this proposal touches upon<br\/>several emerging optimization and DM problems, which are driven by ever growing computational<br\/>power. The proposed research has shown a broad impact on many research fields including computer<br\/>science, operations research, computational biology, and logistics. The scope of this project itself<br\/>will broaden opportunities and enable the participation of all citizens women and men, underrep-<br\/>resented minorities, and especically persons disabled by epilepsy. Success of this proposal in seizure<br\/>prediction research will relieve the anguish from this life-threatening disease and improve the life<br\/>quality of at least 2 million Americans (14 millions worldwide), who are currently suffering from<br\/>epilepsy regardless of race, age, or gender.","title":"CAREER: Novel Optimization Methods for Cooperative Data Mining with Healthcare and Biotechnology Applications","awardID":"1219639","effectiveDate":"2011-07-31","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["555420"],"PO":["565251"]},"191588":{"abstract":"Carbon-based nano-materials such as carbon nanotubes (CNTs) and, more recently, graphene layers and graphene nanoribbons (GNRs), have attracted strong interest as alternative device technologies for future nanoelectronics applications. This collaborative research project will potentially result in transformative advances required to harness the early science of these nano-materials into practical design technologies.<br\/><br\/>Specifically, PIs will develop a multi-scale simulation framework that integrates quantum simulations with compact model development for CNT and GNR field-effect transistors (CNTFETs and GNRFETs). They will develop ambipolar logic circuits and ultra-steep sub-threshold logic circuits as two promising candidate solutions with applications to both CNTFETs and GNRFETs. PIs will identify, model, and explore the effect of different variability and defect mechanisms in these devices to provide expedient means to systematically understand and predict their effects on the performance and reliability of practical carbon-based circuits.<br\/><br\/>Results will be disseminated through an integrated testbed for research and education in beyond-silicon computing, with an emphasis on carbon-based electronics. Through collaborations with a broad range of academic investigators as well as government and industry affiliates, this collaborative effort will strengthen ties between the device and CAD communities, help create links among them, and accelerate convergence to key design parameters essential for large scale integration of carbon-based electronics. Additionally, the development of learning modules, inter-disciplinary courses, and outreach efforts such as the Design Automation Summer School will bring the architectures, design tools and methodologies -- alongside fabrication and basic physics -- that will most likely define the first generation of nano-computing systems into the mainstream academic curriculum.","title":"SHF: Small: Collaborative Research: Modeling, Simulation, and Design for Performance and Reliability in Carbon-based Electronics","awardID":"1208934","effectiveDate":"2011-07-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7947","name":"NANOCOMPUTING"}}],"PIcoPI":["517951"],"PO":["565157"]},"181358":{"abstract":"The widespread adoption of multicore processors requires multithreaded<br\/>software to exploit these hardware resources. Unfortunately, the<br\/>construction and validation of reliable concurrent software currently<br\/>requires extraordinary effort, due to unanticipated interactions<br\/>between concurrent threads. Thus, developing better programming<br\/>techniques and tools for concurrent programming is essential. This<br\/>research develops a cooperative programming methodology for<br\/>multithreaded software, based on the philosophy that all thread<br\/>interference must be explicitly documented via source-level \"yield\"<br\/>annotations by the programmer.<br\/><br\/>The project will investigate both static and dynamic checking<br\/>techniques to verify the correctness of yield annotations. Once<br\/>verified, these annotations guarantee that code executed between<br\/>successive yields is serializable and thus amenable to sequential<br\/>reasoning. Moreover, yield-free code is deterministic. Despite<br\/>provided these strong safety guarantees, this methodology does not<br\/>impact program performance. The cooperative methodology provides a<br\/>robust foundation for multithreaded software and can potentially<br\/>transform the principles and practices of multithreaded software<br\/>engineering. This work on cooperability will also provide research<br\/>opportunities for graduate and undergraduate students, and it will<br\/>support endeavors to provide access to science education for all<br\/>students.","title":"SHF: Small: Collaborative Research and RUI: Static and Dynamic Analysis for Cooperative Concurrency","awardID":"1116825","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485938],"PO":["565264"]},"184889":{"abstract":"The International Society for Computational Biology is awarded a grant to support student and select presenter participation in the annual meeting of the International Society for Computational Biology on \"Intelligent Systems for Molecular Biology\". The conference will be held in Vienna, Austria, July 17-13, 2011. The conference will hold thematic sessions for invited presentations, tutorial programs on related subjects, and special interest group workshops. Biological areas presented at the conference will include bio-imaging and visualization, databases and ontologies, evolution and comparative genomics, gene regulation and transcriptomics, mass spectrometry and proteomics, population genomics, protein interaction, and molecular networks and structure. <br\/><br\/>Results will be disseminated as published proceedings indexed in MEDLINE and Current Contents, and video recordings of selected presentations will be available online. This award will support participation of US students and under-represented minorities in an important forum for presenting research and ideas at the intersection of biology and computer science.","title":"CONFERENCE: International Society for Computational Biology on Intelligent Systems for Molecular Biology to be held July 13-17th, 2011 in Vienna Austria","awardID":"1137140","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"1165","name":"ADVANCES IN BIO INFORMATICS"}}],"PIcoPI":["558784"],"PO":[496031]},"184207":{"abstract":"This is funding to support a doctoral consortium (workshop) of approximately 10 promising graduate students from the United States and abroad, along with 5 distinguished research faculty. The event will take place on Sunday, October 23, immediately preceding and in conjunction with the 13th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS 2011), to be held Monday-Wednesday, October 24-26, in Dundee, Scotland. The ASSETS conferences are the premier forum for presenting innovative research on the design and use of both mainstream and specialized assistive technologies. This includes the use of technology by and in support of: individuals with hearing, sight and other sensory impairments; individuals with motor impairments; individuals with memory, learning and cognitive impairments; individuals with multiple impairments; older adults; and professionals who work with these populations. Researchers and developers from around the world in both academia and industry will meet to exchange ideas and present their latest work . More information about the conference may be found at http:\/\/www.sigaccess.org\/assets11. <br\/><br\/>A key component of building this community is through its youth. The ASSETS 2011 doctoral consortium will provide an opportunity for graduate students from diverse backgrounds (computing, engineering, psychology, architecture, etc.) to come together and explore their research interests in an interdisciplinary workshop, under the guidance of the PI and a panel of other distinguished experts in the field, so that they can appreciate the broader spectrum of research and development approaches to assistive technologies and universal usability, and also experience the community in which they can pursue their endeavors. Student participants will make formal presentations of their work during the consortium, and will receive constructive feedback from the faculty panel. The feedback is designed to help students understand and articulate how their work is positioned relative to related research, whether their topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are appropriately analyzed and presented. Thus, the consortium will help shape ongoing and future research projects aimed at assistive technologies and universal access, will promote scholarship and networking among new researchers in this emerging interdisciplinary area, and will also expose these promising young researchers to a larger community. In an effort to further integrate doctoral consortium participants into the conference itself, a poster session has been set aside in the technical program to allow all doctoral consortium participants to present their research to the full conference. In addition, one student from the doctoral consortium will be selected to deliver the closing plenary presentation. An evaluation of the consortium will be conducted and the results made available to the organizers of future such events. <br\/><br\/>Broader Impacts: The doctoral consortium will help expand the participation of young researchers pursuing graduate studies in this field, by providing them an opportunity to gain wider exposure in the community for their innovative work and to obtain feedback and guidance from senior members of the research community. It will further help foster a sense of community among these young researchers, by allowing them to create a social network both among themselves and with senior researchers at a critical stage in their professional development. Because the students and faculty constitute a diverse group across a variety of dimensions, including nationality\/cultural and scientific discipline, the students' horizons are broadened to the future benefit of the field. The organizers will also take special steps to promote participation from institutions with relatively large numbers of students from under-represented groups.","title":"WORKSHOP: Doctoral Consortium at the 2011 ACM Conference on Computers and Accessibility","awardID":"1132409","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["494943"],"PO":["565227"]},"174209":{"abstract":"Type 1 - LOIL02170097: Decadal predictability of extreme events:<br\/>Impact of a model error representation and numerical resolution<br\/>(collaborative research)<br\/>The investigators will implement a stochastic backscatter scheme into the Community<br\/>Atmosphere Model and explore how to improve the internal variability and, in particular,<br\/>the prediction of extreme events on decadal and regional scales. Only a small number of<br\/>publications apply Extreme Value Theory to climate models and many questions remain<br\/>open. While some work has been conducted comparing model and reanalysis with constant<br\/>greenhouse forcing, the major body of published work in this area focuses on the occurrence<br\/>of extreme events in a changing climate and on the robustness of climate trends of extreme<br\/>events across di erent low-resolution models.<br\/>The emphasis of the investigators is very di erent: they will look at the internal variability<br\/>of models under a constant greenhouse forcing. The investigators focus both on the ability of<br\/>low-resolution climate models to realistically predict extreme events on decadal time-scales<br\/>and global spatial scales, and on the feasibility of replacing the missing variability due to<br\/>low-resolution with a stochastic model error scheme and if such a scheme can improve the<br\/>decadal prediction of extreme events. Model integrations with and without a stochastic<br\/>backscatter scheme would be conducted and extreme value statistics be used to determine<br\/>the impact of the scheme onto the occurrence of extreme events. For comparison, the same<br\/>statistic would be computed using the the ERA40, the ERA-Interim analysis and\/or the<br\/>NCEP\/NCAR reanalysis as best proxy for multi-decadal observations.","title":"Collaborative Research: Type 1 - LOIL02170097: Decadal Predictability of Extreme Events: Impact of a Model Error Representation and Numerical Resolution","awardID":"1048915","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7552","name":"COFFES"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8012","name":"CR, Earth System Models"}}],"PIcoPI":[467354,"540225","477247"],"PO":["564898"]},"181546":{"abstract":"Maintaining a sustainable society via technological innovations has been a major challenge for computing system design. In this project, we focus on the energy efficiency of an important type of computer applications, the database management systems (DBMS), which often consume a large portion of the computing resources and energy in modern data centers. The goal of this project is to design and implement a DBMS that enables significant energy conservation with graceful degradation of query processing performance. The project achieves its goal using the following approaches: (1) Dynamically exploit the energy-performance tradeoffs in DBMS as well as low-power modes of hardware systems for improved energy efficiency with performance guarantees; (2) Formulate the energy-efficient DBMS design as a feedback control problem, and adopt appropriate formal control techniques to achieve the desired performance and energy efficiency with theoretical analysis and guarantees; (3) Apply advanced multi-stage optimization methods to solve complex energy-aware storage management problems; and (4) Coordinate various control and optimization loops in different layers of the DBMS for maximized energy savings and global system stability. The broader impacts of the project are at two levels. At the society level, the reduction of energy consumption and CO2 emission by implementing the proposed energy-aware DBMS can be substantial. The project can benefit a large number of industrial sectors, the operations of which depend heavily on database-supported software such as online retailing systems, financial management software, and social networking platforms, by significantly lowering their electricity cost. At the education level, the project trains Ph.D. students in an interdisciplinary environment and enhances several courses taught at the University of Florida and the University of Tennessee at Knoxville by providing a rich set of application examples, software tools, and project opportunities. Publications, technical reports, software and experimental data from this project can be found at www.cse.usf.edu\/~ytu\/EDBMS.","title":"III: Small: Collaborative Research: Making Databases Green - An Energy-Aware DBMS Approach","awardID":"1117699","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["534498","524877"],"PO":["563727"]},"181315":{"abstract":"In his 1948 paper that founded information theory, Shannon introduced a notion of \"entropy\" to measure the amount of \"randomness\" in a process. However, to a computer with bounded resources, the amount of randomness can appear to be very different from the Shannon entropy. Indeed, various measures of \"computational entropy\" have been very useful in computational complexity and the foundations of cryptography. Recent work by the PI and others have introduced new measures of computational entropy, increased our understanding the new and old computational entropy measures, and found greater applicability of the these measures in cryptography and complexity theory.<br\/><br\/>This project aims to refine our understanding of computational entropy, use computational entropy to seek unified and optimal constructions of cryptographic primitives, bring us closer to resolving the power of randomness in space-bounded computation, and identify new applications of computational entropy in the theory of computation.<br\/><br\/>This research is closely integrated with the PI's educational efforts. The PI continues to develop courses and online lecture notes related to the project (on topics such as Pseudorandomness, Cryptography, and Applied Algebra). Graduate students are involved in all aspects of the research, and undergraduates participate regularly as well. The PI is also very active in service to the scientific community, including outreach efforts such as \"vision nuggets\" that convey research directions in theoretical computer science to a broad audience.","title":"AF: Small: Computational Entropy","awardID":"1116616","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["525639"],"PO":["565157"]},"181557":{"abstract":"This project applies statistical sampling techniques to detect deviation from normal or desired behavior for a variety of applications, ranging from security problems such as phase-change memory (PCM) wear leveling and intrusion detection to performance problems such as multicore communication, data migration, and locality optimizations. Existing performance monitoring and measurement techniques are insuf&#64257;cient for our applications for a variety of reasons. Many depend upon the OS which (1) may be compromised and hence be unsuitable for security-related monitoring, and (2) tracks data at page granularity whereas memory hierarchy performance often needs monitoring at the block granularity. To avoid these limitations, this project designs a monitoring architecture for statistically sampling memory access patterns. Brute-force monitoring would require large, frequently-searched hardware structures that increase complexity and power, whereas sampling enables much smaller structures that are searched at low rates, incurring far less overhead. In general, sampling loses accuracy or requires a large number of samples (and large hardware structures) if the monitored behavior exhibits high standard deviation. Our key insight is that we can bound the standard deviation of the behavior within the region of interest to the application, thereby allowing accurate and low-overhead sampling. Our key intellectual merit is to show that statistical sampling and performance monitoring can unify the aims of disparate applications and enable monitoring with high accuracy and low overhead. The broader impacts include paving the way for robust statistics-based performance monitoring in future computer systems and for research and education efforts that combine statistics and computer architecture.","title":"CSR: Small: Statistical Memory Monitoring in Hardware for Security and Performance","awardID":"1117726","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["517389","550778"],"PO":["565255"]},"181348":{"abstract":"As semiconductor technology scales to make smaller circuits in order to reduce circuit fabrication costs, a by-product is that it is very hard to make circuit components that match specifications exactly. Displacement of a single atom can make the circuit behave differently. The diversity of devices being produced from the same process with the same design makes it un-economical to test them with a \"one size fits all\" test program. Because each component is unique, we also need a test strategy that is fine-tuned to the circuit being tested. The proposed work involves adjusting the test program to devices under test iteratively as we gather information about each device throughout the testing process. <br\/><br\/>Keeping a leading edge in information technology is essential for the economical and social well being of the nation. Our most advanced technologies produce statistically diverse devices from the same design and manufacturing specification. Future technologies will produce even more diverse devices. Thorough testing of these increasingly complex devices is a key component in maintaining an edge in information technology. This project will explore new testing methodologies by developing methods to enable a shift from static to dynamic test procedures. In addition, this project will pilot a set of undergraduate design projects involving collaboration between Georgia Tech and Arizona State to mimic the prevalent need to collaborate over long distances in industry.","title":"SHF: Small: Collaborative Research: Unified Framework for Adaptive Analog and Digital Performance Characterization Using Learned Information from the Circuit Under Test","awardID":"1116786","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485916],"PO":["562984"]},"181249":{"abstract":"As semiconductor technology scales to make smaller circuits in order to reduce circuit fabrication costs, a by-product is that it is very hard to make circuit components that match specifications exactly. Displacement of a single atom can make the circuit behave differently. The diversity of devices being produced from the same process with the same design makes it un-economical to test them with a \"one size fits all\" test program. Because each component is unique, we also need a test strategy that is fine-tuned to the circuit being tested. The proposed work involves adjusting the test program to devices under test iteratively as we gather information about each device throughout the testing process. <br\/><br\/>Keeping a leading edge in information technology is essential for the economical and social well being of the nation. Our most advanced technologies produce statistically diverse devices from the same design and manufacturing specification. Future technologies will produce even more diverse devices. Thorough testing of these increasingly complex devices is a key component in maintaining an edge in information technology. This project will explore new testing methodologies by developing methods to enable a shift from static to dynamic test procedures. In addition, this project will pilot a set of undergraduate design projects involving collaboration between Georgia Tech and Arizona State to mimic the prevalent need to collaborate over long distances in industry.","title":"SHF: Small: Collaborative Research: Unified Framework for Adaptive Analog and Digital Performance Characterization using Learned Information from the Circuit Under Test","awardID":"1116252","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["525702"],"PO":["562984"]},"185407":{"abstract":"Recovering signals from incomplete, noisy measurements is one of the foundational problems in signals and systems theory. In a variety of applications, from RADAR to spectroscopy, signals are combinations of a few basic sine waves. But, to extract information from the system, one must first identify the frequencies of these sine waves from a noisy, incomplete collection of acquired samples. Unfortunately, most popular techniques for signal analysis provide few guarantees in the presence of noise and require a great deal of prior knowledge about the structure of the signal to be estimated. This research addresses these problems by applying key insights from contemporary applied mathematics, analyzing signals as part of a unified approach to decompose systems into simple building blocks.<br\/><br\/>Motivated by the investigator's recent work on recovering signals from highly incomplete information, this project revisits these fundamental, algebraic problems in signal processing with a modern perspective based on convex optimization. This research applies the theory of atomic norm minimization to the practical problems of denoising mixtures of moments in signals, systems and controls. This study develops an abstract theory of atomic norm denoising and a general program for computing mean-square-estimation rates. This work focuses on spectrum estimation problems and will be evaluated in terms of the shortcomings of previous subspace-based approaches. Finally, this research program applies the atomic norm framework to Hankel operator problems in control theory, investigating new approaches to open problems in system identification and model-order reduction.","title":"Denoising, Decomposition, and Deconvolution of Moment Sequences by Convex Optimization","awardID":"1139953","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["564275"],"PO":["564898"]},"181602":{"abstract":"Many experiments in molecular biology designed for discovering cellular functions and organization, or estimating the efficiency of drug treatments, are prohibitively expensive to be systematically conducted on large scale systems. It is therefore of paramount importance to develop analytical methods for predicting which experiments are likely to provide the most informative outcomes of biological tests, based only on a small sample of incomplete and noisy measurements. Frequently, the incomplete data are random or pseudo-random samples of a matrix, or more generally, a multidimensional array (tensor). This suggests the use of a new algorithmic and analytic paradigm, termed low-rank matrix and tensor completion, for solving the inference problems at hand. <br\/><br\/>This research involves the development of novel tensor and nonlinear completion methods for inference of protein-protein interaction networks and design of synthetic lethality experiments. The methods used represent a combination of information-theoretic and algorithmic approaches centered around constrained optimization on Grassman manifolds. This research program has the potential to benefit many branches of molecular biology, neuroscience, computer vision, control, economics and signal processing in terms of answering fundamental inference questions for sparse systems and reducing the cost and time associated with system testing and experimental design. It will also provide unprecedented opportunities to graduate students in the electrical engineering and mathematics departments at UIUC to explore state-of-the art technologies and research problems at the intersection of molecular biology, bio-informatics, and signal processing.","title":"CIF: Small: Nonlinear Matrix and Tensor Completion with Applications in Systems Biology","awardID":"1117980","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["518625",486556,"518457"],"PO":["564898"]},"181503":{"abstract":"Clustering is a crucial component of exploratory data analysis. This project aims to develop novel algorithms that address various shortcomings of k-means, the most widely used clustering algorithm. Specific objectives include (a) the development of initialization methods to address the sensitivity of k-means to the initial cluster centers; (b) the investigation of alternative distance measures to address the sensitivity of k-means to outliers; and (c) the development of practical acceleration methods. Innovations developed during this project should be readily applicable to a wide range of clustering algorithms. For example, initialization is crucial for most clustering algorithms. Furthermore, an effective initialization method can be used independently of k-means as a standalone clustering algorithm. K-means is often used as a subroutine in other learning algorithms. Therefore, development of acceleration methods for k-means is of great practical interest.<br\/><br\/>The project is expected to make broader impacts on several fronts. At the international level, we intend to make significant contributions to the data mining literature by publishing in top-ranked journals. At the national level, we aim to enhance the competitiveness of the US by seeding the next generation of scientists. At the regional level, we hope to improve the quality of education in an EPSCoR state and contribute to the development of a diverse and skilled workforce. At the institutional level, we intend to improve the research environment and the curriculum of our Computer Science program. Finally, at the individual level, we hope to increase the participation of students from underrepresented groups in research and equip them with valuable skills including self-confidence, independent thinking\/problem solving, and effective communication.","title":"RUI: Novel Enhancements to the K-Means Clustering Algorithm","awardID":"1117457","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[486290],"PO":["562760"]},"181404":{"abstract":"Blind source separation (BSS) has found wide use in many disciplines including signal processing as it starts from a simple generative model minimizing assumptions on the data generation mechanism and achieves useful decompositions of the observed data. In particular, independent component analysis (ICA) has been the most commonly used approach to achieve BSS since statistical independence of the underlying components is plausible in many applications. Besides independence, sample correlation is another inherent property of many signals of interest. Traditionally, these two properties are addressed separately when developing methods for source separation. Entropy rate, on the other hand, is a natural cost that allows one to account for independence and sample correlation jointly, and hence promises to result in a new class of powerful solutions with wide applicability. In addition, it enables one to easily incorporate model selection---another key problem complementing the power of BSS---into the problem through the use of information theoretic criteria.<br\/><br\/>The focus of this research is the development of a class of powerful methods for source separation and model selection using entropy rate so that one can take both the higher-order-statistical information and sample correlation into account to achieve significant performance gains in more challenging problems. The main application domain is one that can truly take advantage of this fully combined approach: the analysis of functional magnetic resonance (fMRI) data and the rejection of gradient and pulse artifacts in electroencephalography (EEG) in concurrent EEG-fMRI data. Both are applications that have proven challenging for the traditional model-based approach due to the unique nature of the noise and artifacts in these problems. Hence, they provide a unique testbed for the performance evaluation of the new class of methods developed under this study. Since independence and sample correlation are intrinsic properties of many other types of data, the new set of methods will be attractive solutions for many other problems as well.","title":"CIF: Small: Collaborative Research: Compressed Sensing for Coherent Designs under Gaussian\/Non-Gaussian Noise","awardID":"1117012","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[486052],"PO":["564898"]},"181305":{"abstract":"The software marketplace is striving to accommodate the impending shift from traditional desktop applications to a model dominated by cloud computing. This paradigm shift will enable users to access computing resources from any location using an increasingly diverse set of clients, a growing number of which are mobile devices. Because traditional software development is tailored toward centralized execution, many centralized applications need to be adapted to serve remote users efficiently and reliably. Traditionally such adaptations require modifying application source code by hand, which is difficult, costly, and error-prone. To address this problem, this project develops automated program transformations that programmers can use to achieve efficient and reliable distributed execution. <br\/><br\/>This project will help answer the following fundamental research questions: (1) How can one distribute Object Oriented software even though objects do not distribute naturally? (2) How can distributed applications effectively leverage the latency\/bandwidth trade-offs of modern networks? How can one alleviate inefficiencies imposed by use of remote pointers? (3) How can one effectively harden a distributed application against network volatility? This project will develop automated refactoring techniques, semantics preserving transformations performed under the programmer\u00b9s control. These techniques will (1) remodularize monolithic applications into service modules to exploit natural distribution boundaries, (2) optimize distribution middleware as guided by program analysis to reduce the aggregate latency of remote calls and efficiently transfer object graphs as parameters, and (3) enhance distributed applications with the ability to cope with network volatility. These techniques will help achieve efficient and reliable distributed execution with reduced software development costs and improved programmer productivity, thus benefiting enterprises, researchers, and the general public alike.","title":"SHF: CSR: Small: Automated Refactoring Techniques for Efficient and Reliable Distributed Execution","awardID":"1116565","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["563450"],"PO":["564388"]},"181569":{"abstract":"Device physics, manufacturing, and engineering challenges in process scaling are providing signi?cant challenges in producing reliable transistors for future technologies. Many academic experts, industry consortia, and research panels have warned that future generations of silicon technology are likely to be much less reliable with multi-core chips with cores failing in the ?eld due to faults in silicon are around the corner. Concurrently with the reducing reliability, the individual energy ef?ciency of transistors is not keeping up with increase in transistor density. These two trends portend a perfect storm: as the energy ef?ciency of transistors is slowing down, they are becoming highly unpredictable which will force further inef?ciencies. Addressing hardware reliability is a fundamental problem for microprocessors and hence for sustaining the IT revolution. This project looks at mechanisms for allowing chips and the higher levels of software to continue working even when devices fail. The basic idea the project looks at is how to detect when chips fail.<br\/><br\/>The core idea that this projrct builds upon is the principle of Sampling. Instead of checking for failures all the time, the idea is to use a periodic sampling window for checking for device failures. The project investigates formal models, hardware implementation, and evaluation to understand the effect of device failures and the impact of the detection techniques.","title":"CSR: SMALL: Formal Models, Processor Architecture, and Evaluation of Sampling for Hardware Reliability","awardID":"1117782","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["521594"],"PO":["565255"]},"192118":{"abstract":"Timing errors resulting from process variability, manufacturing defects, and aging effects in scaled CMOS technologies are an important failure mode that impact the reliability of multi-level control logic in commodity mainstream computer systems. This project makes a case for lookahead logic circuits based on the principles of prefix computation to address performance, reliability, and power challenges posed by timing errors in multi-level control logic. Lookahead logic circuits promise low-cost logic-only solutions that can be used to (i) increase performance and\/or yield by reducing logic delay, (ii) improve logic circuit robustness to timing errors by masking them, (ii) lower power consumption by increasing the scope of aggressive dynamic voltage and frequency scaling, and (iv) enable more effective and targeted post-silicon debug and online wearout prediction. The research will formalize the principles of lookahead-based function decomposition and prefix-based computation for multi-level logic circuits, develop logic synthesis and design solutions for lookahead logic, and investigate its applications in the context of superscalar and multi-threaded processor design.<br\/><br\/>A major impact of this research is to enable ubiquitous low-cost highly reliable computing, by expanding its reach to domains, where custom solutions are economically infeasible. An integrated outcome of this project is the development of a testbed and web-based resources to facilitate research in reliable system design. Through course development and collaborations with industrial partners, the research will contribute to education, community resource development and technology transfer to industry.","title":"SHF: Small: Lookahead Logic Circuits for Performance, Power, and Reliability","awardID":"1211099","effectiveDate":"2011-07-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["517951"],"PO":["366560"]},"181118":{"abstract":"Modern computer chips contain billions of tiny transistors connected by wires. The massively integrated circuits (ICs) are prone to wear out and degradation during their lifetimes. When electric current flows in a microscopic wire, the moving electrons collide into diffusing metal atoms and cause a gradual ion movement. This current-flow-induced material transport is called electromigration (EM). The electromigration phenomenon is a major source of interconnect degradation. Over time EM may cause wire breaks or shorts leading to circuit malfunction. Greater current densities, wire geometry or material imperfections, and increased temperature worsen EM. In the past, improvements in interconnect manufacturing kept pace with interconnect scaling and allowed for relatively simple ways of keeping EM at bay by capping current densities. Interconnect scaling is now approaching the point where the existing models and assumptions are no longer valid. At the same time chip?s thermal conductivity decreases while the density of currents carried by the on-chip wires and the operating power steadily increase causing significant self-heating. These effects bring again EM to the forefront and cause that it now poses a serious reliability threat for working chips. There is an urgent need to develop a comprehensive chip-level EM analysis tool. The PI proposes to develop a physical EM simulator to understand failure mechanisms in various interconnect configurations. The PI will use the simulator to develop metrics for wire time-to-failure estimates. The PI proposes to develop statistical models of interconnect failure and a chip-level analyzer of electromigration degradation effects. Interconnects whose expected lifetimes are shorter than desired can be modified. The will develop such modification strategies as well.<br\/><br\/>This work supports the US semiconductor industry by addressing a vital research problem that may affect the integrated circuits scaling trends. The proposed techniques would be useful for designers because the worst circuit degradation conditions occur in small regions of the chip that can be identified and resolved. In addition, the tools to be developed may also help process engineers to study the effects of new materials on circuit level properties. The proposed research activities will also serve as a platform for training PhD students who will be well prepared to work in modern industry or academia.","title":"SHF: Small: Modeling and Preventing Electromigration-Caused Degradation in Cu Dual Damascene Scaled Interconnects","awardID":"1115663","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["550832"],"PO":["562984"]},"185639":{"abstract":"This joint program reviews the state-of-the-art of the field of model-based assistive robotic technologies for medicine and rehabilitation in the USA and Japan, outlines the fundamental science and technologies in this area, and proposes models for future collaborative research between Japan and USA by building on the mutual strengths. In particular, gait training for stroke patients using exoskeletons is studied.<br\/><br\/>The field of medical and rehabilitation robotics offers much in terms of broad societal impact. In addition, the project exposes graduate students and faculty to the culture of research in Japan, and more broadly, Asia. Today, Japan leads in many areas of robotics and automation. The students in the NSF-JST program interact with researchers in Japan and visit prominent universities and research laboratories. This exposure provides excellent training for the students and prepares them for careers in industry and academia globally.","title":"EAGER: NSF-JUST Program on Robotic for Rehabilitaion and Medicine","awardID":"1141010","effectiveDate":"2011-07-01","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":["558467"],"PO":["543539"]},"181603":{"abstract":"Testing microprocessors and other chips to verify their clock rate is known as delay testing. If the test is wrong, and a 3 GHz chip is really a 2.99 GHz chip, the customer system may randomly crash. But if the test is wrong and a 2.8 GHz chip will really run at 3 GHz, the manufacturer loses money by selling the 3 GHz chip as a lower-priced 2.8 GHz chip. Today it is expensive to accurately determine the clock rate of high-speed chips. This project is developing new test techniques to lower the cost of accurate manufacturing tests, so that chip prices can be reduced. Accurate delay testing becomes even more challenging when chips are stacked together in products such as cell phones and tablets, since noise from one chip can influence the speed of its neighbors.<br\/><br\/><br\/>The broader impact of this project is in educating students to do advanced development and research in the semiconductor and electronics industries, both by directly working on the project, and through courses that use the project results. The results of this research will be distributed to academia and industry. A longer-term broader impact is acceleration of cost reductions in stacked chips in advanced consumer products, providing more functionality at an affordable price with desirable weight, size, and battery life.","title":"SHF: Small: High Quality Test and Post-Silicon Validation of System-On-Chip Integrated Circuits","awardID":"1117982","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[486559],"PO":["562984"]},"181548":{"abstract":"III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families<br\/>Swaminathan Vishwanathan, Purdue University; Manfred Warmuth, University of California, Santa Cruz<br\/><br\/>Machine learning is currently indispensible for building predictive models from massive data sets. A large majority of widely used machine learning algorithms are based on minimizing a convex loss function. A fundamental problem with all such models is that they are not robust to outliers. To address this limitation, this project develops probabilistic models based on a parametric family of distributions, namely, the t-exponential family, that lead to quasi-convex loss functions and yield models that are robust to outliers. <br\/><br\/>The key challenge when working with the t-exponential family of distributions, as in the case of the exponential family, is to compute the log-partition function and perform inference efficiently. The project addresses this challenge in two specific cases. For problems with small number of classes exact iterative schemes are being developed. For problems where the number of classes is exponentially large, approximate inference techniques are being developed by extending variational methods. <br\/><br\/>In partnership with Google, some of the data mining algorithms resulting from this project are being applied to a challenging real-world problem of recognizing text in photos (the PhotoOCR problem). The project offers opportunities for research-based advanced training of graduate students as well as research opportuinities for undergraduates in machine learning and data mining. Algorithms for constructing predictive models from data that are robust in the presence of outliers are likely to find use in a broad range of applications. Open source implementions of algorithms, publications, and data sets resulting from the project are being made available through the project web page at: http:\/\/learning.stat.purdue.edu\/wiki\/tentropy\/start","title":"III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families","awardID":"1117705","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["559403"],"PO":["565136"]},"181339":{"abstract":"Matrix and polynomial computations are the backbone of modern computing in sciences, engineering and signal and image processing. This project shall advance the known algorithms in two central subject areas of this <br\/>field, namely the solution of linear systems of equations and polynomial root-finding. <br\/><br\/>Solving linear systems will be advanced by the development of novel preconditioning techniques, which will enable faster and more accurate solutions. The proposed novel methods of randomized preconditioning are highly promising for the important class of input matrices that have small numbers of small singular values. Known methods are substantially more costly for this class. The same randomization techniques, as well as alternative methods using homotopic continuation, promise substantial advance in solving structured (e.g. Hankel and Toeplitz) linear systems of equations. The cited promise relies on the results of the initial but quite extensive formal and experimental study that motivated the project. Immediate applications of the proposed methods include the computation of polynomial greatest common divisors (GCDs) and approximate GCDs. These are themselves highly important subjects of symbolic and symbolic-numerical computing having applications to control, image and signal processing and the computation of algebraic curves and surfaces.<br\/><br\/>The classical problem of polynomial root-finding has been intensively studied for four millennia (since the Sumerian times) but is still the subject of intensive research, motivated by important applications to algebraic and geometric computations and signal processing. Besides the task of advancing complex root-finding, a well known challenge, motivated in particular by problems in algebraic geometric optimization, is the approximation of the R real roots of a polynomial having C complex roots, when the ratio C\/R is large. Interestingly, the leading numerical polynomial root-finding packages and programs MPSolve and Eigensolve can save at most 10% of their running time by restricting the task to real root-finding. <br\/><br\/>The recent progress in polynomial root-finding largely relied on matrix methods. Some of them can be advanced by employing new preprocessing techniques for linear systems of equations. Such preprocessing enables parallel acceleration of root-finding. Another direction, also based on matrix methods, enables the design of numerical techniques that approximate just the real roots of univariate polynomials. This novel algorithmic feature implied the acceleration of the known numerical methods by the cited large factor C\/R. Further research directions include the design of new matrix algorithms for polynomial roots as well as matrix-free variations of the recent novel algorithms, their extension to root-finding for the systems of multivariate polynomials, and the implementation work.<br\/><br\/>The expected progress in two central areas of modern computing will have interdisciplinary impact; it will combine numerical and symbolic methods, thus promoting their symbolic-numerical combination; the project will demonstrate the power of some important general techniques of algorithm design, such as randomization and homotopic continuation, and will bring together the energy and resources of a geographically diverse group of scientists, who are working in various subject areas but are interested in participation in the project. Last but not the least, the project assumes participation of students from Lehman College of CUNY and the Graduate Center of CUNY; for such students this project will be an excellent research experience. Participation of students from minorities and underrepresented groups is also expected and they will be supported both by the NSF and CUNY.","title":"AF: Novel Methods for Fundamental Matrix and Polynomial Computations","awardID":"1116736","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[485895],"PO":["565251"]},"190844":{"abstract":"This project establishes a fundamental analytical and design framework for highly resilient wireless networks based on the study of the qualitative and quantitative properties of the largest connected component. The essential mathematical basis for this study comes from the theory of percolation. Building on recent results in continuum percolation, the project designs network structures to greatly enhance the resilience of large-scale wireless networks to node and link failures resulting from attacks, natural hazards or resource depletion. Expected results from the project include (1) deeper understanding of percolation processes and resilience in large-scale wireless networks with multiple transmission power levels and channel fading, (2) analysis of network resilience to degree-dependent and cascading node failures, (3) design of wireless networking structures which maximize resilience to random node and link failures, (4) understanding of percolation and resilience in wireless networks described by signal-to-interference-plus-noise-ratio models and directed graphs, and (5) percolation and resilience in mobile wireless networks. This project will (1) have direct and long-term impact on the reliability and security of wireless network architectures used in national<br\/>security, commercial enterprise, scientific exploration and research, health services, and other important social projects, (2) impact undergraduate and graduate education through a planned course, (3) enhance research and education infrastructure through partnering with other university departments, government research institutions, and industry, and (4) enhance scientific and technological understanding through publications as well as participation in multi-disciplinary conferences and workshops.","title":"CT-ISG: Percolation Processes and the Design of Highly Resilient Wireless Networks","awardID":"1205560","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["524620"],"PO":["565327"]},"181505":{"abstract":"To maximize performance, modern processors attempt to execute as many instructions as possible in parallel. This requires the use of branch predictors to guess which instructions will be executed next in an effort to keep the processor busy. However, the penalty associated with incorrectly guessed branch outcomes has a great impact on performance. Despite being studied extensively and improved steadily over the past two decades, branch prediction remains to be critical for achieving high performance and low power. <br\/><br\/>This project introduces a new direction in the design of branch predictors by providing insights into why branch predictors fail in predicting specific instances of dynamic branches. The research focuses on developing Complementary Branch Predictors (CBP), which can be added to any conventional branch predictor (BP). CBPs intelligently track the branches that are incorrectly predicted by a BP. A CBP complements the BP by providing corrective predictions for when and where a BP fails to predict the correct path. Given that modern BPs are already highly accurate, CBPs end up providing corrective predictions for the hardest-to-predict branches. Consequently, tightly-coupled BPs and CBPs form a new class of branch predictors that are fast, highly-accurate and power-efficient. This research explores the design space of this new class of branch predictors to achieve higher performance, lower power, and lower hardware budget.<br\/><br\/>The findings from this project have the potential to significantly change the design of future branch predictors. The PI will collaborate with industry partners to further increase the broader impact. The research will engage graduate and undergraduate students and promote the participation of underrepresented groups.","title":"SHF: Small: Complementary Branch Predictors","awardID":"1117467","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[486295],"PO":["366560"]},"181538":{"abstract":"Project Abstract<br\/><br\/><br\/>In this proposal the PIs present plans to investigate a number of fundamental problems within theoretical models of self-assembly, as well as plans to directly engage a large number of students in research activities. Self-assembly is the process by which simple components autonomously come together by way of local interactions to form complex structures. Self-assembling systems are abundant in nature and are the key mechanisms for the formation of biological organisms. Further, self-assembly technology is emerging as a powerful tool for manipulating matter at the nanoscale. The development of a mature mathematical and computational understanding of self-assembly theory is an important step towards harnessing the power of self-assembly for the large scale fabrication of complex nanoscale devices such as circuits, molecular motors, nanoscale computers, and nano-biomedical devices.<br\/><br\/>The focus of this project is theoretical studies which involve mathematical analyses and also the development of simulation software. Such theoretical work well complements ongoing experimental research and a key portion of this project involves regular visits to top experimental research labs such as Erik Winfree?s DNA and Natural Algorithms Group at Caltech and Russell Deaton?s DNA and Biomolecular Computing group at the University of Arkansas to engage in mutually beneficial discussions regarding the development of self-assembling models and systems. Such involvement can help to guide their long range experimental paths as well as provide the PIs with invaluable insight into the laboratory realities.<br\/><br\/>In this proposal the PIs present a diverse array of projects which they will pursue with heavy student involvement. The following are brief descriptions of a few: the PIs will study the ability of self-assembling systems to self-replicate and evolve. These abilities are the fundamental cornerstones of living systems, and such studies will provide insights into how living systems originated and function. The PIs will extend, along several directions, a powerful model of fault tolerance which they recently introduced, fuzzy temperature fault tolerance. New fault tolerance techniques are of extreme importance if substantial products are to be realized from artificial self-assembling systems. The PIs will pursue studies in the power of randomization techniques for reducing tile complexity while generating close approximations of self-assembled shapes. The PIs will greatly extend the self-assembly software available for both teachers and researchers. The PIs simulator is currently used by several instructors and research groups and has great promise to help draw more students into the field, and research in general.<br\/><br\/>The emphasis on undergraduate research of this project has broader impacts that are of particular interest to the NSF. The University of Texas-Pan American (UTPA), located in the South Texas Rio Grande Valley area, has more than 18,000 students, with an 89% Hispanic and 59% female student population. A primary goal of this proposal is to increase the research activity of UTPA Computer Science students by developing a large portion of the research within the environment of student-centered research seminars. The PIs plan to further develop current research seminars into full courses to involve a larger number of students in original research. Because of the nature of the UTPA student body, this plan will help broaden participation in post graduate Computer Science among under-represented minority groups. The PIs also propose to support the broader research community by providing much needed educational and research software.","title":"AF: Small: Explorations of Theoretical Models of Self-Assembly","awardID":"1117672","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":["486384",486384,"486386",486386],"PO":["565223"]},"195717":{"abstract":"This project studies the performance of complex large-scale spatial networks that use distributed communication algorithms by applying different random network models. While much of the previous work on random spatial networks has focused on topological aspects and on centralized algorithm design, the emphasis of this project is on developing distributed algorithms and on characterizing desired invariant network properties that such networks must satisfy to allow for the efficient implementation of distributed algorithms. Expected results from the project include (1) decentralized algorithms for ef&#64257;cient path navigation, (2) decentralized algorithms for information &#64258;ow in random networks, (3) decentralized algorithms for information dissemination, (4) distributed energy management in random networks, and (5) distributed topology control for maintaining percolation-based connectivity. This project will (1) increase the understanding of complex information systems upon which society increasingly depends, (2) lead to design guidelines for practical network architectures and protocols, (3) disseminate the scientific and technological findings through publications, (4) further the development of related disciplines such as mathematics (probability) and statistical physics, (5) impact undergraduate and graduate education through a planned joint course on random networks, and a summer lecture series, (6) enhance research experiences for undergraduates through summer research projects, and (7) encourage the participation of women and minorities in networking research.","title":"NeTS: Small: Collaborative Research: Large Scale Networks and Information Flow: From Emergent Behavior to Algorithm Design","awardID":"1234410","effectiveDate":"2011-07-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[524620],"PO":["564993"]},"190845":{"abstract":"While the Internet has far exceeded expectations, it has also stretched initial assumptions, often creating tussles that challenge its underlying communication model. Users and applications operate in terms of content, making it increasingly limiting and difficult to conform to IP's requirement to communicate by discovering and specifying location. To carry the Internet into the future, a conceptually simple yet transformational architectural shift is required, from today's focus on where ? addresses and hosts ? to what ? the content that users and applications care about.<br\/>This project investigates a potential new Internet architecture called Named Data Networking (NDN). NDN capitalizes on strengths ? and addresses weaknesses ? of the Internet's current host-based, point-to-point communication architecture in order to naturally accommodate emerging patterns of communication. By naming data instead of their location, NDN transforms data into a first-class entity. The current Internet secures the data container. NDN secures the contents, a design choice that decouples trust in data from trust in hosts, enabling several radically scalable communication mechanisms such as automatic caching to optimize bandwidth. The project studies the technical challenges that must be addressed to validate NDN as a future Internet architecture: routing scalability, fast forwarding, trust models, network security, content protection and privacy, and fundamental communication theory. The project uses end-to-end testbed deployments, simulation, and theoretical analysis to evaluate the proposed architecture, and is developing specifications and prototype implementations of NDN protocols and applications.","title":"FIA: Collaborative Research: Named Data Networking (NDN)","awardID":"1205562","effectiveDate":"2011-07-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["524620"],"PO":["565090"]},"181319":{"abstract":"The automatic identification in images of people, places, objects, and especially object categories is a central and ongoing challenge within computer vision. This project addresses this problem using low-level image features to learn intermediate representations, ones in which objects in images are labeled with an extensive list of highly descriptive visual attributes. This work demonstrates this approach in three domains: faces, plant species, and architecture. In each domain, it develops techniques for deriving visual attribute vocabularies, training attribute detectors, and building compositional models to automatically label attributes in images.<br\/><br\/>The project is making four fundamental contributions to the use of visual attributes. 1) It is developing new methods by which automatic systems and humans can interact to select domain-appropriate attribute vocabularies and label large image collections. 2) It is developing compositional models that capture dependencies between attributes. This provides more accurate attribute detection and enables inference of global properties of objects. 3) Using compositional models, the project is developing new, localizable attributes that capture the geometric relations between object parts and landmarks. 4) The project is designing algorithms that combine attributes to identify objects, search through image vast collections, and automatically annotate image databases.<br\/><br\/>Not only is this research generating large datasets of labeled images that should help catalyze new research, it is also demonstrating the feasibility of new systems for analyzing images in specialized domains such as faces, plants, and architecture. For example, the project develops new software applications for analyzing and searching images of faces as well as free mobile apps for plant species identification.","title":"RI: Small: Collaborative Research: Visual Attributes for Identification and Search in Images","awardID":"1116631","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["541955"],"PO":["564316"]},"184719":{"abstract":"As scientists continue to produce data from experiments, sensors, and simulations, the next<br\/>challenge is in analyzing and visualizing the results. Addressing this issue is not just a<br\/>problem of technology nor of simply deploying larger computers and data storage devices; (future)<br\/>scientists need to develop the skills to effectively take advantage of the latest resources<br\/>for advancing scientific discoveries.<br\/><br\/>The Remote Data Analysis and Visualization Center (RDAV) is well-situated to address<br\/>essential issues in workforce development by developing and providing a robust learning opportunity for undergraduate students. RDAV students work on developing software to address computational problems stemming from active projects in science and engineering and proposed by researchers who have been allocated computation time on RDAV?s Nautilus hardware. Experience with these hardware and software resources are each, separately, uncommon skills that undergraduates seldom encounter; the combination is extremely rare. These projects integrate experience working with high performance computing hardware as well as the specialized software tools used in scientific domains. Students gain important applicable skills in computational science.","title":"Undergraduate Training at NSF Teragrid XD RDAV Center","awardID":"1136246","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}}],"PIcoPI":["513541",495443],"PO":["565292"]},"180914":{"abstract":"Virtualization typically involves remapping physical resources (e.g., storage, memory, processors) into logical resources so that these computational resources can be shared in dynamic environments. The remapping process has an online component, in that requests arrive and the system responds. In traditional online problems, resources are thought of as irrevocably assigned. However, in virtualized environments, previously allocated resources can be reassigned or reallocated. This project formalizes how computational resources can be reallocated efficiently when there is some cost for the reallocation.<br\/><br\/>The project investigates the algorithmic complexity of reallocation problems arising from large-scale systems design, and especially reallocation algorithms that are universal with respect to reallocation-cost functions. The research plan addresses problems in memory and storage reallocation, scaleout and sharding in storage systems, reallocation for dynamic combinatorial and geometric structures (finite-element meshes, metric spaces), and classical online allocation problems (scheduling and bin packing). The aim of the project is three-fold: (1) to study the asymptotic performance of reallocation, (2) to obtain results that are universal with respect to a wide class of reallocation cost functions, and (3) to quantify the performance of known heuristics in widespread use.<br\/><br\/>The way computation works today, on laptops and desktops, in supercomputers and data centers, and in cloud computing, critically depends upon virtualization. Hardware trends (e.g., multicore, solid state disks) and software trends (e.g., data-centric computing) mean that virtualization is becoming even more prevalent. Reallocation algorithms have a large impact on the performance of virtualized systems. The algorithmic approach outlined in this project promises to improve the performance, scalability, and predictably of large systems. As such, this research show great promise for improving the general infrastructure of computing in the next decade.","title":"AF: Small: Collaborative Research: Algorithms for Reallocation Problems","awardID":"1114809","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["533341"],"PO":["565251"]},"181618":{"abstract":"The performance growth in uniprocessor (single core) performance resulting from improvements in semiconductor technology has recently slowed down significantly. Sequential applications or sequential portions of parallel applications require further advances to improve their performance. Today's processors complete instructions in their program order, which is a major performance bottleneck because any long-latency instruction, such as access to memory, delays completion of all subsequent instructions. This project aims to achieve higher single core performance by defining a new, compiler assisted mechanism for out of order instruction completion. It investigates how the use of compile-time program knowledge can be passed to the hardware and be used to simplify the architectural checks required for such out of order completion. The architecture of a standard processor is fully preserved and legacy software can execute without modification.<br\/>The out-of-order instruction commit mechanism will use a novel compiler\/architecture interface. The compiler provides information about instruction ``blocks'' and the processor uses the block information to decide which instructions can be committed out of order and when. Some blocks are guaranteed to be data independent blocks which allows instructions from different such blocks be committed simultaneously and out of order. Other blocks have data or control dependencies and require in-order execution and in-order commit. Micro-architectural support for the new commit mode is driven by the block information, which significantly simplifies the hardware. Exception handling is also simplified with compiler assistance. The new commit mechanism will effectively increase the size of the instruction window allowing more cache misses to be overlapped for both L1 and L2 caches. The project will also investigate additional compiler and architecture optimizations to further improve performance.","title":"SHF: Small: Improving single core performance via compiler-assisted out-of-order commit","awardID":"1118047","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["532654"],"PO":["565272"]},"180948":{"abstract":"Virtualization typically involves remapping physical resources (e.g., storage, memory, processors) into logical resources so that these computational resources can be shared in dynamic environments. The remapping process has an online component, in that requests arrive and the system responds. In traditional online problems, resources are thought of as irrevocably assigned. However, in virtualized environments, previously allocated resources can be reassigned or reallocated. This project formalizes how computational resources can be reallocated efficiently when there is some cost for the reallocation.<br\/><br\/>The project investigates the algorithmic complexity of reallocation problems arising from large-scale systems design, and especially reallocation algorithms that are universal with respect to reallocation-cost functions. The research plan addresses problems in memory and storage reallocation, scaleout and sharding in storage systems, reallocation for dynamic combinatorial and geometric structures (finite-element meshes, metric spaces), and classical online allocation problems (scheduling and bin packing). The aim of the project is three-fold: (1) to study the asymptotic performance of reallocation, (2) to obtain results that are universal with respect to a wide class of reallocation cost functions, and (3) to quantify the performance of known heuristics in widespread use.<br\/><br\/>The way computation works today, on laptops and desktops, in supercomputers and data centers, and in cloud computing, critically depends upon virtualization. Hardware trends (e.g., multicore, solid state disks) and software trends (e.g., data-centric computing) mean that virtualization is becoming even more prevalent. Reallocation algorithms have a large impact on the performance of virtualized systems. The algorithmic approach outlined in this project promises to improve the performance, scalability, and predictably of large systems. As such, this research show great promise for improving the general infrastructure of computing in the next decade.","title":"AF: Small: Collaborative Research: Algorithms for Reallocation Problems","awardID":"1114930","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["531670",484898],"PO":["565251"]},"181619":{"abstract":"With the rapid proliferation of knowledge bases and their use in automated inference in a variety of applications, there is a growing need for effective approaches for (i) knowledge translation, i.e., the task of applying knowledge learned or developed in one domain to another semantically different domain, and (ii) knowledge integration, i.e., the task of building a unified knowledge base from disparate sources. While the problem of data translation and integration has received a great deal of attention in the literature, the problem of knowledge integration remains relatively under-explored. Existing work on this topic has focused on primarily logical frameworks which make it difficult to take into account uncertainty in knowledge and in semantic mappings used for knowledge translation and integration. <br\/><br\/>This project aims to use Markov Logic Networks to express both knowledge and semantic mappings to obtain a unified probabilistic model for jointly translating knowledge and refining semantic mappings. This provides a basis for addressing the challenges of integrating heterogeneous knowledge with uncertain mappings, assessing the correctness of translated knowledge, simplifying the resulting models to obtain more compact approximate translations, and evaluating the methods in realistic application scenarios. <br\/><br\/>The results of the proposed work are likely to be applicable in a number of application domains that require integration or translation of knowledge across disparate knowledge sources. The work strengthens and facilitates interdisciplinary collaborations, provides enhanced research-based training opportunities for graduate and undergraduate students. The knowledge translation and integration software, and the benchmark data will be made available to the broader community through the project web site: http:\/\/aimlab.cs.uoregon.edu\/SKTI\/","title":"III: Small: Statistical Knowledge Translation and Knowledge Integration Using Markov Logic","awardID":"1118050","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[486594,486595],"PO":["565136"]},"178192":{"abstract":"The proposed workshop brings together graduate students, junior faculty, and senior faculty to explore and transform theoretical concepts and research methodologies central to the field of Science and Technology Studies. The late Dr. Leigh Star's theoretical and empirical work will provide the workshop's central focus. Drawing from pragmatism and the Chicago school's sociology of work, Star's scholarship highlighted the social dimensions of scientific practice and innovation. Where others conducted ethnographies of the work of laboratory scientists' manipulating tools and data to construct epistemic insights, Star focused upon the countless, taken-for-granted and often dismissed practices of assistants, technicians, and students that make scientific breakthroughs possible. Digging even deeper into the conditions that make science possible, Star foregrounded the infrastructure of classifications, computer technologies, paperwork, and regulations that constrained and constituted scientific work. Her work has impacted numerous fields including but not limited to Science and Technology Studies, Sociology, Anthropology, and Computer Science. <br\/><br\/>The workshop, co-funded by the Science and Technology Studies program and the Information and Intelligent Systems Division, provides an in-depth examination and transformation of Star's conceptual, empirical, and methodological contributions. Drawing from a broad set of theoretical concepts, Star examined themes of ecology, infrastructures, invisible work, articulation work, boundary objects, and classifications. Star's intellectual innovations opened up new areas of inquiry, brought social and behavioral scientists to Science and Technology Studies [STS], strengthened methodology in STS, and emphasized an unwavering focus on scientific work often rendered invisible in publication, award, and promotion practices. Scholars in North America, Europe, and Japan have further expanded upon and transformed Dr. Star's approaches and methodologies. The workshop aims to situate Star's ideas in the formation of Science and Technology Studies as well as move these theoretical frameworks into new scientific terrain.","title":"Workshop: Theoretical and Methodological Innovations in Science and Technology Studies","awardID":"1101154","effectiveDate":"2011-07-01","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"7603","name":"SCIENCE, TECH & SOCIETY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["535783"],"PO":["563704"]},"185090":{"abstract":"Proposal #: CNS 11-38020<br\/>PI(s): Papanikolopoulos, Nikolaos<br\/>Institution: University of Minnesota<br\/>Title: RAPID: Robotic Systems for Disaster Assessment and Disaster Relief<br\/>Project Proposed:<br\/>This RAPID project proposal, consisting of a development effort and dispatching of improved robotic inspection tools to colleagues in Japan, will expand on existing, successful robotic development efforts to create a new robot with specific capabilities needed to assist response to the Sendai earthquake and tsunami. The novelty of the proposed robot may be found in its locomotion capabilities and its resistance to dirt and water. Its ease of use make it stand out among research artifacts. The researchers will be engaged in some of the following activities: <br\/>- Engineer a treaded version of the Aquapod submersible robot<br\/>- Develop a sensor suite for the Aquapod<br\/>- Develop a simulator tool for first responders<br\/>- Deploy the robots in Japan through Japanese colleagues<br\/>The investigators collaborate with Dr. Kazuhiro Kosuge from Tohuku University and Satoshi Takoro from Tohohu University. Their letters and other appropriate invitations have been secured from the Japanese researchers and responders, including the International Rescue Systems Institute, to ship a small team of robots to Sendai, Japan to assist in the inspection of critical infrastructure and other recovery operations as well as perform research on the efficacy of emergency response methods and practices. (The search and rescue phase has been terminated.) These new, improved robots combine treads and limbs for novel locomotion capabilities for their sensor payloads. New sensor payloads enable expanded sensing of location, environmental data, and radiation. With these enhanced capabilities, a team of robots will be deployed for victim and economic recovery decision-making while simultaneously collecting ephemeral data not possible with existing devices. The Sendai disaster is unique in its large geographical and economic scale and types of damage. These robots deployed in Sendai will provide an additional modality of unmanned systems to be tested along with conventional robotic types. <br\/>Broader Impacts: <br\/>This proposal promises an immediate benefit to society by supporting economic recovery efforts in Japan through a participatory research paradigm. Moreover, long term benefits for future disasters are in evidence since emergency response and unmanned systems are both formative domains and the data collected will advance the discovery and understanding of intelligent, human-centered systems in unpredictable situations. The PI has a history of outreach to museum, K-12, and general public education events. In addition, the simulator tool promises to enhance the safety of emergency responders, which has multiplicative benefits.","title":"RAPID: Robotic Systems for Disaster Assessment and Disaster Relief","awardID":"1138020","effectiveDate":"2011-07-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["561562","557449","523124"],"PO":["557315"]},"188390":{"abstract":"This research program will develop mathematical techniques to automatically prove the robustness of computer programs operating under uncertainty. The aim is to bridge the gap between control-theoretic studies of robustness, on the one hand, and program semantics and analysis on the other. In cyberphysical systems, where sensor-derived data from the physical world is intertwined with computations, uncertainty arises from volatile or erroneous sensor data. Such systems demand predictable responses to perturbations of the system's initial conditions, otherwise they will be unreliable. The project has three themes: (1) Theory: It will create comprehensive semantics of robustness properties such as continuity and stability in the setting of imperative programs over complex data types. (2) Algorithms: Analysis algorithms are needed to prove robustness of programs, or alternately find violations of robustness. Scalable, lightweight analyses will be developed along with precise but heavyweight methods. (3) Tools: These analyses will be implemented in tools applicable to software for real-world cyber-physical systems.","title":"CAREER: Robustness Analysis of Uncertain Programs: Theory, Algorithms, and Tools","awardID":"1156059","effectiveDate":"2011-07-01","expirationDate":"2015-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["551025"],"PO":["564388"]},"177181":{"abstract":"Wireless networking assumes that radios are half-duplex. On a given frequency, a half-duplex radio can either transmit or receive, but not both at the same time. The project disproves this long-held assumption; it shows how a radio that can transmit and receive simultaneously on the same frequency can be built using commodity off-the-shelf components. The design is based on two key ideas. First, is the design of analog circuits that can perform adaptive signal inversion, i.e. take an input RF signal and produce its exact inverse, and programmatically adapt the attenuation and delay on the inverted signal to match the self-interference experienced by the received signal. This enables the design of wideband full duplex radios that can handle transmit powers upto 20dBm. Second, the project exploits the full duplex primitive to design a real-time bidirectional channel for control and data, as well as more complex patterns such as chains. By interspersing control and data information in a message, nodes can dynamically react to channel changes in real-time.<br\/><br\/>The above two primitives - full-duplex operation and a real-time bidirectional control channel - can help solve many long-standing fundamental problems in wireless networks, including hidden terminals, bitrate adaptation, network congestion, resource allocation and unfairness. The project will produce a prototype full duplex radio for WiFi style networks and show experimentally how it can improve their performance, further all designs will also be made public through research publications and open-source hardware designs.","title":"NeTS: Medium: Full Duplex Wireless","awardID":"1064504","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["502394",475015],"PO":["565303"]},"177192":{"abstract":"Sampling continuous-time signals constitutes a fundamental process in the modern digital technology era. This research investigates a novel class of event-triggered samplers, in which the sampling times are dictated in a dynamic way by the actual signals to be sampled. The richness of the event-triggered sampling schemes allows for the mathematical formulation of meaningful optimization problems that can lead to sampling strategies with optimal characteristics and resulting in potentially significant performance gains. The theories and techniques to be developed in this project can potentially make significant contributions to important application areas such as sensor network systems for event surveillance and monitoring, and networked control systems for smart-grid-based power generation and distribution.<br\/><br\/>This research focuses on three major problems in the general areas of decentralized statistical signal processing, namely, sequential detection (hypothesis testing and changepoint detection), parameter estimation, and signal estimation and control. In all three problems, optimal event-triggered mechanisms will be investigated that can optimize the corresponding detection or estimation performance. In addition, a special class of event-triggered samplers will be analyzed and the corresponding design methods for digital signal processing (DSP) will be developed .This research will have transformative impacts on the fields of statistical signal processing and digital signal processing. In particular, the use of event-triggered sampling in detection, estimation and control can significantly improve the performance in a decentralized setup and simplify considerably the communication requirements in terms of power and bandwidth.","title":"CIF: Medium Projects: Event-Triggered Sampling: Application to Decentralized Detection and Estimation","awardID":"1064575","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[475039,475040],"PO":["564898"]},"185080":{"abstract":"The City of Takoma Park, MD made election history in November 2009 by holding the first independently-verifiable secret-ballot public election. Voters could, for the first time anywhere in the world, independently audit a secret-ballot governmental election using the Scantegrity voting system. This project improves on two aspects of the system used by Takoma Park in 2009. First, it provides an absentee voting protocol, Remotegrity, which enables absentee voters to benefit from the system?s verifiability properties. The protocol is designed to be simple enough for use in a real-world public election. Protocol properties are defined and proven rigorously, using the techniques of cryptographic proofs. Second, it improves upon the security of the election website by using the Composite component-based OS for a secure foundation. The website adapts to faults and compromises by dynamically decreasing the exposure of different clients to each other, thus preventing data corruption and resource starvation. The research provides a detailed design of a secure bulletin board for elections that goes beyond an abstract description of the cryptographic protocols used, decreasing the security vulnerabilities of the bulletin board. In particular, it makes the bulletin board less vulnerable to attempts by malicious clients to change the posted digital election audit trail or to launch massive denial-of-service attacks. The research is urgent because it is being considered for use in the November 2011 municipal election of the City of Takoma Park, MD, by its Board of Elections and City Council.","title":"RAPID: Secure Bulletin Boards and Absentee Voting in Real-World Independently-Verifiable Elections","awardID":"1137973","effectiveDate":"2011-07-01","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[496577,"502189"],"PO":["564388"]},"176170":{"abstract":"II-EN: Computing research infrastructure for constraint optimization, machine learning, and dynamical models for Computational Sustainability<br\/><br\/>High performance computing infrastructure will be acquired to support the research, outreach, and educational activities of researchers engaged by the Institute for Computational Sustainability (ICS) in a number of research projects which have significant computational needs. Computational sustainability is a new interdisciplinary field that aims to apply techniques from computer science, applied mathematics and related disciplines to help the balancing of environmental, economic, and societal needs for sustainable development. Computational sustainability research brings together computational sciences and a variety of other disciplines to focus on developing computational models, methods, and tools for supporting the design of sustainable policies, practices, products, and tools. ICS has established a number of significant research projects in areas ranging from biodiversity conservation, to natural resource management, poverty mapping, and material discovery for fuel cell technology. <br\/><br\/>Computational sustainability research is already leading to foundational contributions in several areas of Computer Science. Computational Sustainability presents decision and optimization problems with a mixture of continuous and discrete variables in highly dynamic and uncertain environments, pushing the boundaries of the current state-of-the art of computer science. ICS research focuses on integrating techniques from constraint reasoning, optimization, dynamical systems, machine learning, and data mining, in order to obtain effective dynamic decision theoretic models to address sustainability problems. The computing infrastructure requested would allow ICS researchers including 21 faculty, 53 students (including 24 undergraduates), and over a 100 collaborators to scale up their work to larger problems than they would have been otherwise be able to solve. <br\/><br\/>ICS research has direct impacts on policy makers and practitioners engaged in sustainability work. For example, working in collaboration with the Laboratory of Ornithology at Cornell, ICS researchers have provided computational analysis for the U.S. Department of the Interior's 2011 State of the Birds report. In collaboration with the U.S. Forest Service, ICS research is improving the design of wildlife corridors for species such as grizzly bears, wolverines, and lynx. ICS is working with The Conservation Fund to develop conservation plans that will be used by government and conservation agencies. In collaboration with the Cornell Fuel Cell Institute, ICS members are developing automated tools to support the discovery of new materials for fuel cell technology. ICS is also building a vibrant computational sustainability research community through conferences, workshops, lectures, and online discussions. Combined with ICS education and outreach activities, these research efforts promote teaching, training, and the advancement of women and underrepresented minorities in computer science. Additional information regarding the ICS and its efforts to meet the critical societal, environmental, and economic needs for knowledge, methods, and tools that advance computational sustainability efforts can be found at the ICS website: www.cis.cornell.edu\/ics","title":"II-EN: Computing research infrastructure for constraint optimization, machine learning, and dynamical models for computational sustainability","awardID":"1059284","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[472103,"562312",472105,"560670"],"PO":["560586"]},"177292":{"abstract":"Because the safety, reliability, and performance of our computing infrastructure rests on the quality of its software, improving software quality is of prime importance for continuing the technological and social advances made possible by computers. Compilers, the primary tools used in constructing software, are therefore crucial--their correctness is essential if developers are to create new, usable software that is free from flaws that lead to crashes and susceptibility to malware. This goal of this project is to provide a methodology for verifying the correctness of compiler transformations for modern computing platforms, emphasizing software designed to work on multicore architectures.<br\/><br\/>This research investigates techniques for building program transformation validators for the LLVM (Low-Level Virtual Machine) infrastructure, an open-source intermediate language used in industrial compilers. The researchers will define denotational semantics for symbolic evaluation of LLVM programs, and prove (in the interactive theorem prover Coq) that the interpretations of symbolic evaluation results are consistent with operational semantics. To account for multi-core, shared-memory computer architectures, the project will define a concurrent memory model, parameterized by target architecture configurations, which promises sequential consistency for data race free programs. This model's semantics will be expressive enough to represent program behaviors, and suitable for mechanized proofs. If successful, this research will decrease the cost of developing and testing compilers, and improve our understanding of the programming language implementations, particularly on multi-core processors, thereby leading to a more reliable, secure, and cost-effective computing ecosystem.","title":"CCF: Medium: Validating Program Transformations in a Mechanized LLVM","awardID":"1065166","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["556654"],"PO":["564388"]},"178293":{"abstract":"This interdisciplinary project investigates the detailed structure of information sources and processing that underly strategic interactions in games of incomplete information. Traditional economic models typically treat private information, or signals, as generated from some underlying state. However, multiagent interactions often pivot around situations where signals are actually based on alternative interpretations of available information, and such processes often produce qualitatively different results for the game. By developing a common language using probabilistic graphical models, the effort seeks insights about alternative models of signals, and computationally effective representations in support of strategic analysis. Specific technical developments will include extension of qualitative probabilistic reasoning methods, tailored to capture important distinctions in signal structure. The project further develops an expanded view of signals, that encompasses deliberate choices about what information to consider in complex decisions under uncertainty.<br\/><br\/>Results from this project will have applications to many situations of economic significance. An example is the domain of auctions, which encompasses markets in electronic commerce (e.g., eBay and B2B exchange), internet advertising (keyword search, social media), financial securities (equities, commodities), and energy (electricity, fuels). Auctions are conventionally modeled in the game-theoretic framework employed in this research, but typically using much more restricted models than necessary. By bringing to bear new computational methods and extending the scope of analysis, the project may lead to innovative new market designs, and a better understanding of human and automated market behavior. The techniques employed bridge across computational and economic disciplines, and open the door to more comprehensive and operational models of decision making.","title":"ICES: Small: The Structure of Signals: Causal Interdependence Models and Bayesian Inference","awardID":"1101465","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":["563733",478010,"488583"],"PO":["565251"]},"176160":{"abstract":"The Manually Annotated Sub-Corpus (MASC) is a shared corpus that supports research across<br\/>several disciplines: linguistics, computational linguistics, psycholinguistics, sociolinguistics and<br\/>machine learning. It includes a wide variety of present-day American English texts annotated for<br\/>several linguistic phenomena. Because MASC provides a unique resource, considerable<br\/>community momentum has grown up around it. This project builds upon this momentum to<br\/>enable the corpus to grow on its own, and to address the need for additional annotations. The<br\/>major activities are to : (1) provide web-based mechanisms to facilitate community contribution<br\/>and use of MASC annotations; (2) develop means to more fully automate the annotation<br\/>validation process; (3) extend the WordNet annotations to cover adjectives, to support research<br\/>on evaluation of ?subjective? annotations and harmonization of WordNet with other resources;<br\/>(3) promote use of MASC and new annotations by diverse groups, by sponsoring shared tasks<br\/>that exploit the corpus? unique characteristics and supporting beta-testers of software, data, and<br\/>annotations; and (4) aggressively develop an ?Open Language Data? community around MASC<br\/>through workshops, tutorials, and active participation in relevant community activities.<br\/><br\/>MASC provides an unparalleled resource for training and testing of tools for natural language<br\/>processing, which can enable a major leap in the productivity of NLP research and ultimately<br\/>impact the way people use and interact with computers. It is the first fully open, communitydriven<br\/>resource in the field. All data and annotations are freely distributed in a manner that<br\/>permits immediate and easy accessibility for users around the globe.","title":"RUI: CRI: CI-ADDO-EN: Collaborative Research: MASC: A Community Resource For and By the People","awardID":"1059246","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[472076],"PO":["565215"]},"177161":{"abstract":"Over the last decade, scripting languages have assumed a huge role. Initially web developers used Perl and Python to enrich the content of web servers; later Ruby on Rails took the scene by storm. Over the same period, JavaScript has become the dominant language on the client side of the web. Additionally, the popularity of scripting language has inspired developers to use them for the construction of many other kinds of systems, including mission-critical real-time systems. <br\/><br\/>While scripting languages are productive tools for the exploration of design ideas, their use also introduces several new kinds of problems into the software cycle. Most basically, scripting languages tend to lack a type system, which tends to raise the debugging and maintenance costs for systems. Worse, even though scripting languages tend to be safe, their flexible primitive operations induce difficult-to-predict behavior in programs and thus creates novel kinds of security holes. At the same time, scripting languages do not come with a well-defined semantics, making it nearly impossible to validate the soundness of a program analysis for safety or security properties.<br\/><br\/>In response to these observations, this proposal promises to re-engineer the semantics of scripting languages. Specifically, the PIs propose to investigate the construction of executable semantics for three scripting languages: JavaScript, Python, and Racket. They will use the language-level test suites to check that the semantics model the implementations adequately. In addition, the PIs will use the semantics to design and validate type systems and program analyses for these scripting languages. <br\/><br\/>Over the long run, the proposal should impact the world at large in three ways. First, the type systems and analyses for scripting languages should help software developers improve the safety of their software and reduce their maintenance cost. Second, the semantics for the scripting languages will help researchers validate their ideas concerning program analyses. Finally, the PIs will develop a process for semantic re-engineering that should be useful to many additional scripting language communities.","title":"SHF: Medium: Collaborative Research: Semantics Engineering for Scripting Languages","awardID":"1064418","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["519555"],"PO":["564588"]},"177282":{"abstract":"There is a rising demand to scale application performance by distributing <br\/>computation across many machines in a data-center. It is difficult to write <br\/>efficient and robust parallel programs in the data-center setting because <br\/>programmers need to worry about reducing communication overhead while handling <br\/>possible machine failures. <br\/><br\/>This project investigates a new data-centric parallel programming <br\/>model, called Piccolo, that can simplify the construction of in-memory <br\/>data-center applications such as PageRank, neural network training etc. <br\/><br\/>In-memory applications can hold all their intermediate states in the aggregate <br\/>memory of many machines and benefit from sharing these intermediate states <br\/>between machines during computation. Traditionally, these applications <br\/>have been built using low-level communication-centric primitives such as MPI, <br\/>resulting in significant programming complexity. The recently popular <br\/>MapReduce and Dryad also do not fit well with these applications <br\/>because their data flow programming model lacks support for shared states. <br\/><br\/>Unlike data flow models, Piccolo explicitly supports the sharing of mutable, <br\/>distributed states via a key\/value table interface. Piccolo makes sharing <br\/>efficient by optimizing for locality of access to shared tables and <br\/>automatically resolving write-write conflicts using user-defined accumulation <br\/>functions. As a result, Piccolo is easy to program for, enables applications <br\/>that do not fit into MapReduce, and achieves good scalable performance.","title":"CSR: Medium: Collaborative Research: Programming parallel in-memory data-center applications with Piccolo","awardID":"1065114","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["562011"],"PO":["565255"]},"177293":{"abstract":"There is a rising demand to scale application performance by distributing<br\/>computation across many machines in a data-center. It is difficult to write<br\/>efficient and robust parallel programs in the data-center setting because <br\/>programmers need to worry about reducing communication overhead while handling <br\/>possible machine failures. <br\/><br\/>This project investigates a new data-centric parallel programming<br\/>model, called Piccolo, that can simplify the construction of in-memory<br\/>data-center applications such as PageRank, neural network training etc. <br\/><br\/>In-memory applications can hold all their intermediate states in the aggregate<br\/>memory of many machines and benefit from sharing these intermediate states<br\/>between machines during computation. Traditionally, these applications<br\/>have been built using low-level communication-centric primitives such as MPI,<br\/>resulting in significant programming complexity. The recently popular <br\/>MapReduce and Dryad also do not fit well with these applications<br\/>because their data flow programming model lacks support for shared states.<br\/><br\/>Unlike data flow models, Piccolo explicitly supports the sharing of mutable,<br\/>distributed states via a key\/value table interface. Piccolo makes sharing<br\/>efficient by optimizing for locality of access to shared tables and<br\/>automatically resolving write-write conflicts using user-defined accumulation<br\/>functions. As a result, Piccolo is easy to program for, enables applications<br\/>that do not fit into MapReduce, and achieves good scalable performance.","title":"CSR: Medium: Collaborative Research: Programming parallel in-memory data-center applications with Piccolo","awardID":"1065169","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["518136"],"PO":["565255"]},"178372":{"abstract":"The project is aimed at modeling the diffusion of information online and empirically discriminating among models of mechanisms driving the spread of memes. We explore why some ideas cause viral explosions while others are quickly forgotten. Our analysis goes beyond the traditional approach of applied epidemic diffusion processes and focuses on cascade size distributions and popularity time series in order to model the agents and processes driving the online diffusion of information, including: users and their topical interests, competition for user attention, and the chronological age of information. Completion of our project will result in a better understanding of information flow and could assist in elucidating the complex mechanisms that underlie a variety of human dynamics and organizations. The analysis will involve studying meme diffusion in large-scale social media by collecting and analyzing massive streams of public micro-blogging data.<br\/><br\/>The project stands to benefit both the research community and the public significantly. Our data will be made available via APIs and include information on meme propagation networks, statistical data, and relevant user and content features. The open-source platform we develop will be made publicly available and will be extensible to ever more research areas as a greater preponderance of human activities are replicated online. Additionally, we will create a web service open to the public for monitoring trends, bursts, and suspicious memes. This service could mitigate the diffusion of false and misleading ideas, detect hate speech and subversive propaganda, and assist in the preservation of open debate.","title":"ICES: Large: Meme Diffusion Through Mass Social Media","awardID":"1101743","effectiveDate":"2011-07-01","expirationDate":"2015-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[478203,"489924",478205,478206],"PO":["565251"]},"176194":{"abstract":"This project focuses on the upgrade of an existing computing cluster, Virgo-2, with Graphics Processing Unit (GPU) capability. The upgraded cluster, Virgo-3, will enhance the computational infrastructure of the Distributed Computing Laboratory (DCL) at the University of Texas at El Paso (UTEP), as well as increasing access to HPC resources by UTEP researchers. <br\/><br\/>Virgo-3 will be utilized by researchers to exploit the potential parallelism that exists in their computational workloads. The GPUs in Virgo-3 will significantly increase the research productivity of UTEP researchers, which will lead to more rapid discoveries in areas such as cardio-visualization and communication networks. A finite element model of the heart, as well as the human torso, can be analyzed more efficiently using GPUs as a result of the inherent data-level parallelism. This more efficient finite element analysis will accelerate the work of UTEP researchers to advance the field of electrocardiography. GPUs can also be used to provide cost effective speedup for discrete event simulation to improve the productivity of communication networks research, where forty percent of computation time for discrete event simulation is consumed by updates to the future event list. This research will explore the utilization of GPUs to speedup future event list manipulation. Additionally, Virgo-3 will be available to other researchers, so that they can utilize the benefits of distributed computing in their research activities.<br\/><br\/>Finally, this project will have broad-based impact through summer computer camps that are targeted to underrepresented minority pre-college students in the El Paso, Texas region.","title":"II-EN: Virgo 3 -- GPU Upgrade to the UTEP Distributed Computing Lab","awardID":"1059430","effectiveDate":"2011-07-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["527334",472182],"PO":["563661"]},"185193":{"abstract":"NSF: RAPID: Collaborative Research: Using Lessons from the Disaster in Japan to Develop Communications for Emergency Situations<br\/><br\/><br\/>During disasters, the telecommunication infrastructure are usually heavily damaged or overloaded, which leads to serious disruptions in the warning and rescue operations. Similarly, part of the Japanese cellular Early Earthquake Warning (EEW) system were damaged during the March 11th earthquake and tsunami. This collaborative project proposes to study the disruption of emergency communications during the last disaster in Japan and investigate corresponding solutions. In particular, the project has the following three integrated objectives: 1) To study the cellular EEW system of Japan and its use in the March 11th earthquake in Japan; 2) To study the communication problems that were encountered leading to disruptions in warning and rescue operations; and 3) To explore tower-less phone-to-phone direct communication mode that can make the cellular phone communications much more resilient during disasters. <br\/><br\/>This project supports collection of data about communication disruptions in Japan; treatment of such data to better understand the impact of telecommunication failures; and finally, solutions how to enhance the cellular system with ad hoc communications. The project is a close collaboration among PIs inthe US, their collaborators in Japanese universities and cellular service providers in Japan. <br\/><br\/>The outcome of this project will lead to a deeper understanding of tradeoffs among robustness, simplicity, scalability, self-organization and adaptivity in designing a cellular emergency broadcast system for USA and beyond. The results of this project will have a direct and practical impact on developing an effective emergency warning system using the latest communication devices.","title":"RAPID: Collaborative Research: Using Lessons from the Disaster in Japan to Develop Communications for Emergency Situations","awardID":"1138659","effectiveDate":"2011-07-15","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["532760"],"PO":["565303"]},"177174":{"abstract":"Over the last decade, scripting languages have assumed a huge role. Initially web developers used Perl and Python to enrich the content of web servers; later Ruby on Rails took the scene by storm. Over the same period, JavaScript has become the dominant language on the client side of the web. Additionally, the popularity of scripting language has inspired developers to use them for the construction of many other kinds of systems, including mission-critical real-time systems. <br\/><br\/>While scripting languages are productive tools for the exploration of design ideas, their use also introduces several new kinds of problems into the software cycle. Most basically, scripting languages tend to lack a type system, which tends to raise the debugging and maintenance costs for systems. Worse, even though scripting languages tend to be safe, their flexible primitive operations induce difficult-to-predict behavior in programs and thus creates novel kinds of security holes. At the same time, scripting languages do not come with a well-defined semantics, making it nearly impossible to validate the soundness of a program analysis for safety or security properties.<br\/><br\/>In response to these observations, this proposal promises to re-engineer the semantics of scripting languages. Specifically, the PIs propose to investigate the construction of executable semantics for three scripting languages: JavaScript, Python, and Racket. They will use the language-level test suites to check that the semantics model the implementations adequately. In addition, the PIs will use the semantics to design and validate type systems and program analyses for these scripting languages. <br\/><br\/>Over the long run, the proposal should impact the world at large in three ways. First, the type systems and analyses for scripting languages should help software developers improve the safety of their software and reduce their maintenance cost. Second, the semantics for the scripting languages will help researchers validate their ideas concerning program analyses. Finally, the PIs will develop a process for semantic re-engineering that should be useful to many additional scripting language communities.","title":"SHF: Medium: Collaborative Research: Semantics Engineering for Scripting Languages","awardID":"1064474","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["518591"],"PO":["564588"]},"178186":{"abstract":"Classical economic theory models humans as acting rationally through the optimization of their expected utilities. The paradigm of bounded rationality takes a step toward greater realism by placing computational limitations on agents' abilities to determine optimal decisions. Behavioral and cognitive studies reveal that humans are also categorically bounded, meaning that they use a finite categorization of the set of decision problems that may be posed, with a small number of categories. This project will focus on the use of quantization theory and information theory to establish foundations for the interplay between categorization and decision making. The researchers aim to understand the impact of categorization on individual decision making, team decision making through majority voting, and sequential decision making. The theory developed in the project will include both analysis of situations in which the categorization is fixed and optimal design of categorizations. In addition to the behavioral justification for the study of categorization, informational limitations on learning suggest that categorization into classes of decision problems has ramifications for engineering design.<br\/><br\/>This project has the potential to influence economic theory and the understanding of certain social and organizational phenomena. Specifically, the project offers a way to understand the decision making performance of teams, with the incorporation of certain human limitations and the potential for differing preferences (e.g., between Type I and Type II errors) among teammates. Differences in preferences lead to a quantifiable penalty of team discord, even when the team shares the common goal of making correct decisions. There is also a quantifiable advantage from team diversity in the sense of obtaining better performance when teammates apply different categorizations. These new concepts could contribute to principles for team formation and for how data gathering policies can be optimized with the goal of fair decision making.","title":"ICES: Small: Decision Making with Bounded Categorization","awardID":"1101147","effectiveDate":"2011-07-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":["508043"],"PO":["565251"]},"175261":{"abstract":"Within medical robotics, this research advances telerobotics, and continuum robot architectures, models, sensing, and control, as well as medical science by enabling new diagnostic procedures. Scientific understanding of continuum robots is entering an exciting era where early assumptions implying constant curvature are giving way to more descriptive models of how actuators and external loads combine to produce variable curvatures. Infusing theory from mechanics into robotics science provides a bright path forward. Outside its own field, this also enhances human understanding of biology ? mechanics-based models provide insight on how elephant trunks, octopus tentacles, and other biological continuum structures perform so elegantly.<br\/><br\/>These systems are minimally invasive and are used to combat the most deadly form of cancer (lung), surgically access one of the most difficult to reach locations (the skull base), and increase the impact of one of the most underutilized curative treatments available (cochlear implantation). Within and beyond these first three applications, these less invasive, more accurate, information-guided continuum robots improve public health by reducing patient recovery times, infection rates, and treatment costs, and enabling entirely new surgical approaches to diseases that are untreatable (and in many cases terminal) today. The proposed curriculum infuses research results into the classroom, and involves undergraduates, high school girls, and high school teachers which enhances the research while promoting learning. The tactile haptic device increases the accessibility of engineering to the blind. The testbeds enhance the research infrastructure of the PI?s lab, and the haptic devices enhance the educational infrastructure in universities, in high schools, and in schools for the blind.","title":"CAREER: Lifesaving Robotic Tentacles","awardID":"1054331","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["527013"],"PO":["564069"]},"175151":{"abstract":"The past decade has proven that variational approaches to approximate Bayesian inference hold<br\/>the key to pragmatic solutions to many signal processing problems previously thought to be fundamentally difficult. Important signal processing problems where the application of appropriate<br\/>variational approximate Bayesian inference techniques led to significant breakthroughs include the decoding of capacity approaching codes for noisy channels, underdetermined speech source separation, and distributed estimation over networks. These applications provide evidence that variational inference techniques have revolutionized the state of the art in signal processing because of their ability to provide high performance estimates at reasonable complexity and communication (equivalently, energy consumption) costs. However, the tradeoff between their performance and required complexity and communication, the very phenomenon leading to their widespread and growing adoption, remains incompletely understood.<br\/><br\/>The underlying thesis of this project is that entropy & information geometry lies at the heart<br\/>of the tradeo ff between performance, complexity, and communication in variational inference based<br\/>signal processing. The research being performed improves our understanding of entropy geometry,<br\/>which primarily dictates the fundamental relationship between the performance of a collaborative<br\/>estimation algorithm and its communication cost. This research also develops the information geometric relationship between a variational inference signal processing technique's performance and<br\/>complexity. Together, these insights allow algorithms for robust speech processing and collaborative estimation over networks to be developed that provide optimal performance at a tunable complexity and communication cost. In addition to its benefits in each of these areas, the work has a synergistic aspect in that it contributes directly to the creation of an overall design science for<br\/>efficient distributed variational inference signal processing algorithms.","title":"CAREER: Entropy Geometry in Variational Inference Signal Processing","awardID":"1053702","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["539473"],"PO":["564898"]},"185194":{"abstract":"NSF: RAPID: Collaborative Research: Using Lessons from the Disaster in Japan to Develop Communications for Emergency Situations<br\/><br\/><br\/>During disasters, the telecommunication infrastructure are usually heavily damaged or overloaded, which leads to serious disruptions in the warning and rescue operations. Similarly, part of the Japanese cellular Early Earthquake Warning (EEW) system were damaged during the March 11th earthquake and tsunami. This collaborative project proposes to study the disruption of emergency communications during the last disaster in Japan and investigate corresponding solutions. In particular, the project has the following three integrated objectives: 1) To study the cellular EEW system of Japan and its use in the March 11th earthquake in Japan; 2) To study the communication problems that were encountered leading to disruptions in warning and rescue operations; and 3) To explore tower-less phone-to-phone direct communication mode that can make the cellular phone communications much more resilient during disasters. <br\/><br\/>This project supports collection of data about communication disruptions in Japan; treatment of such data to better understand the impact of telecommunication failures; and finally, solutions how to enhance the cellular system with ad hoc communications. The project is a close collaboration among PIs inthe US, their collaborators in Japanese universities and cellular service providers in Japan. <br\/><br\/>The outcome of this project will lead to a deeper understanding of tradeoffs among robustness, simplicity, scalability, self-organization and adaptivity in designing a cellular emergency broadcast system for USA and beyond. The results of this project will have a direct and practical impact on developing an effective emergency warning system using the latest communication devices.","title":"RAPID: Collaborative Research:Using Lessons from the Disaster in Japan to Develop Communications for Emergency Situations","awardID":"1138665","effectiveDate":"2011-07-15","expirationDate":"2013-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["532762"],"PO":["565303"]},"186173":{"abstract":"Sketching in Hardware is a three-day workshop from July 28-31, 2011 on the design, use and teaching of physical computing toolkits. It is the only annual gathering devoted exclusively to the discussion of digital hardware tools for non-specialists.<br\/><br\/>The development of digital hardware toolkits has motivated significant academic research in the past decade. However, though powerful, these toolkits are still in early stages of development, akin to the earliest Web authoring programs. This annual workshop brings together key researchers, designers, developers, manufacturers, users and educators for an intimate, hands-on discussion of tools for physical computing prototyping. Its goal is to create a dialogue between disciplines and thereby lower the barrier to entry to working with digital hardware for people without a formal engineering education into.<br\/><br\/>The 2011 workshop will focus on how tools build on each other, and how they work together to spawn new ideas. More specifically, it will aim to:<br\/><br\/>* Encourage development of new tools and expansion of existing ones;<br\/>* Support the interconnection of existing toolkit technologies;<br\/>* Create software that support explorations of electronics hardware exploration; by non-specialists through interfacing with familiar software creative tools;<br\/>* Identify common challenges and create opportunities for collaboration in solving those challenges.<br\/><br\/>Every participant will give a 20 minute presentation, with an emphasis on topics that have not been presented publicly before. In the past these have introduced new and improved tools, demonstrated new techniques for teaching physical computing, and identified non-traditional areas where physical computing prototyping is important. In addition, there will be several guided discussions of major issues (determined in advance through email discussions). Several hours will be spent in breakout groups to introduce new toolkits and techniques.<br\/><br\/>Broader Impacts: The devices, software, techniques, and social connections presented and created during the workshop are highly influential across a broad range of disciplines and audiences. By bringing together individuals from a wide range of disciplines and career stages and emphasizing the relevance of work over seniority or background, the meeting brings talented young people and people outside the usual social networks into those networks. The published talk slides are downloaded thousands of times and passed around the Internet, while connections made at the event create lasting bonds and collaborations between researchers in disparate fields.","title":"Workshop: Sketching in Hardware 2011","awardID":"1144173","effectiveDate":"2011-07-15","expirationDate":"2011-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[499583],"PO":["565227"]},"185140":{"abstract":"Most of our Nation's systems have as components operating systems that are replete with vulnerabilities despite the best efforts of the designers to discover and fix the vulnerabilities. Building on the accomplishments of the research community, industry is attempting to create new hardware structures that can help improve security. Particularly noteworthy of these is the Trusted Platform Module (TPM) chip developed by the Trusted Computing Group (TCG), a large industry consortium that involves over 100 members. Now, many delivered processors contain TPM chips, with the goal being to provide a place to store keys, protect storage, provide code isolation attestation about the code running on a system -- all functions that can improve security. <br\/><br\/>Although the hardware provides extremely useful and powerful security properties, researchers have been slow at adopting these techniques towards the design of secure systems. One reason is that users are not familiar with this new technology. <br\/><br\/>To remedy this situation, the PI is running annual workshops in which the developers of the TPM technology and users who have made effective use of it provided tutorial material and hands-on experiences for the workshop's participants which included students, faculty, and potential industry users. <br\/><br\/>The ultimate goal of the workshops is to define a research program for the next decade focused around hardware support for security. This is the second workshop in the series. <br\/><br\/>These funds are used to reimburse the expenses of graduate and undergraduate students participating in the workshop, which will take place in June, 2011 in Pittsburgh.","title":"Trusted Infrastructure Workshop","awardID":"1138302","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[496764],"PO":["565327"]},"185041":{"abstract":"Student travel support for DNA17<br\/>Conference title: 17th International Conference on DNA Computing and<br\/>Molecular Programming (DNA17)<br\/>Location: California Institute of Technology, Pasadena, California, USA<br\/>Dates: September 19?23, 2011<br\/>Website: http:\/\/dna17.caltech.edu<br\/><br\/><br\/><br\/>Summary<br\/>This is an unsolicited proposal for 12,000 USD for student travel support for the 17th International Conference on DNA Computing and Molecular Programming (DNA17). The primary purpose of the proposal is to give travel assistance to students who are giving oral or poster presentations at the conference. The selection procedure for travel awards is described in detail below and is designed to give priority to women and underrepresented minorities, and to research quality. Approximately 20 successful student applicants, from US institutions (excluding Caltech), will be funded for travel and accommodation, with an expected average award of 600 USD per student.<br\/><br\/><br\/>Intellectual Merit<br\/>The annual International Conference on DNA Computing and Molecular Programming (DNAx) is the premier forum where scientists with diverse backgrounds come together with the common purpose of advancing the engineering and science of biology and chemistry from the point of view of computer science, physics, and mathematics. Continuing this tradition, the 17th International Conference on DNA Computing and Molecular Programming (DNA17), under the auspices of the International Society for Nanoscale Science, Computation and Engineering (ISNSCE), will focus on the most recent experimental and theoretical results that promise the greatest impact. A steady stream of papers in the field appear in Nature and Science, as well as other top journals such as Nature Nanotechnology, Nature Chemistry, Nature Biotechnology, Angewandte Chemie, JACS, Physical Review Letters, Physical Review Letters, SIAM Journal on Computing ? and the authors of these papers regularly attend DNAx conferences to present their work in a preliminary form.<br\/><br\/><br\/>Broader Impact<br\/>By funding travel to students we are actively encouraging and incentivizing a new generation of researchers to attend the conference. We are giving special priority to women and minority applicants whose papers or posters were accepted at the conference. Students who attend DNA17 will get exposure to early versions of work that will go on to be published in top venues and, furthermore, they will have the opportunity to interact with, and potentially collaborate with, researchers who are producing work of the highest quality.","title":"Student travel support for DNA17","awardID":"1137770","effectiveDate":"2011-07-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["549585","558957","518688","518689"],"PO":["565223"]},"175130":{"abstract":"1053575<br\/>Towns<br\/> XSEDE: Enabling New Digital Science<br\/>The eXtreme Science and Engineering Discovery Environment (XSEDE) partnership will develop an unprecedented, comprehensive advanced digital services cyberinfrastructure (CI) to enable transformative open science and engineering research and innovative training and educational programs. The goal of XSEDE is to offer users tremendous capabilities with maximum productivity, enabling them to advance and share knowledge across domains. The XSEDE architecture, engineering, operations, support, and education activities are co-designed by an unparalleled team to achieve this goal, far surpassing TeraGrid in usability, reliability, capability, performance, and security?and ultimately, in user productivity and science impact. XSEDE will enable scientists, engineers, and educators to exploit powerful digital services and social networking environments to support knowledge exchange and advance understanding across domains. Just a few examples of the advances to science and society include: accurately predicting earthquake damage to urban structures; modeling of protein and nucleic acid folding and structure prediction to understand how drugs interact with target macromolecules to improve health care; developing novel designs for nanoscale microprocessors; advancing scientific understanding of plants to provide a safe and sustainable food supply, as well as benefits in renewable energy; and simulating pandemic spread to create a virtual laboratory where policy decisions such as school closure, vaccine deployment, and quarantine can be explored. The XSEDE partnership will fulfill this vision by creating the most advanced, capable, and robust advanced digital cyberinfrastructure in the world?and supporting it with the most expert and experienced team of CI professionals. XSEDE will accelerate open scientific discovery and enable researchers, educators, and students across disciplines and across campuses to conduct transformational research efforts and innovative education programs. XSEDE will create strong ties with campus personnel spanning technology, workforce development, and policy issues to enhance CI for research and education. Researchers will use XSEDE directly, from campus and personal systems, from other high-end centers and cyberinfrastructure resources, and via science gateways and discovery environments. XSEDE users will be backed by an integrated national user support program offering an array of services from experts in the application of technology to advance science and engineering, including extensive training and advanced user support and collaboration. XSEDE?s governance model will include participation by these users as stakeholders, while providing centralized management to ensure robustness and to facilitate rapid responses to new issues and opportunities. XSEDE will carry out a multifaceted Training, Education, and Outreach Services (TEOS) program to raise the competency of the present and future scientific community. XSEDE will work proactively with the nation?s educational institutions to create a significantly larger and more diverse STEM workforce. TEOS will broaden participation by working with under-represented faculty and students to engage larger numbers of under-represented individuals from among minority-serving and EPSCoR institutions, women, and people with disabilities. TEOS will disseminate best practices, lessons learned, and quality materials and will leverage external partnerships to scale-up successful practices. XSEDE will leverage the XD Technology Insertion Service (TIS) activities?already awarded to the XSEDE team?into continuously advancing CI and will work closely with the Technology Audit Service (TAS) team to ensure that XSEDE can be effectively evaluated and improved. These activities will ensure that XSEDE is robust, easy to use, performing as designed, and evolving constantly to meet the growing demands of scientific research and researchers. Advancing science with the most powerful, diverse, and integrated set of advanced digital services ever?and linking that to other CI projects and to campuses and local research infrastructure?is unprecedented. No engineering and technology plan can anticipate all contingencies and future opportunities. The successful realization of NSF?s vision for XD will require deep expertise and vast experience, as well as focused and passionate effort. The XSEDE team is uniquely experienced and qualified for this incredible opportunity.","title":"XSEDE: eXtreme Science and Engineering Discovery Environment","awardID":"1053575","effectiveDate":"2011-07-01","expirationDate":"2016-06-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7619","name":"EQUIPMENT ACQUISITIONS"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7781","name":"PETASCALE - TRACK 1"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7476","name":"ETF"}}],"PIcoPI":["559499",469704,"496016","538215","559172",469708,"494781"],"PO":["558595"]}}