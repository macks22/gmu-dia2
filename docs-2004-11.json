{"104203":{"abstract":"Growing pressure to collect information on the Internet has created a need for<br\/>more sophisticated ways to characterize privacy rights and balance them against<br\/>legitimate commercial and law enforcement objectives. A wide range of businesses<br\/>now rely on their ability to collect information about online customers as a<br\/>foundation of their added value in the market place. The use or potential use of<br\/>the Internet for purposes such as child pornography and planning of hate crimes<br\/>and terrorism have heighted interest in (and loosened regulations on) network<br\/>monitoring. Theft of intellectual property has led to the development of<br\/>numerous protection mechanisms; these often involve registrations and involuntary<br\/>monitoring of various kinds. In order to make security easier (and explore a<br\/>lucrative business model), a number of companies seek to act as third-party<br\/>caretakers of private information such as keys, authentication secrets, and<br\/>credit card numbers. Arrayed against these trends are many citizens incensed by<br\/>aggressive means used to collect information from them and a variety of groups<br\/>that champion privacy rights.<br\/><br\/>There have been many advances in technologies both to aid information gathering<br\/>and limit it. One important trend is toward more advanced systems for creating<br\/>and managing digital credentials and authorization databases. In current<br\/>practice credentials are sometimes `pushed' (like presenting a ticket to get into<br\/>a movie theater) sometimes `pulled' (like getting access to an airplane seat with<br\/>a `paperless' ticket) and sometimes both (like making a purchase at a store with<br\/>a credit card whose validity is confirmed online). These approaches have<br\/>Internet-based analogs, and technical advances have increased the range of<br\/>options considerably in recent years. For example, work on public key systems<br\/>has advanced techniques for delegation based on chains of `pushed' credentials<br\/>and increased the automation of credential collection. Another important trend<br\/>is toward more advanced systems for protecting privacy using anonymizing<br\/>techniques. Tools such as onion routers and anonymous web publication servers<br\/>provide some support but other techniques directly aid fine-grained mechanisms<br\/>for obtaining privileges without exposing information unnecessarily. <br\/><br\/>This work aims to develop formal support for characterizing privacy in the<br\/>context of these advances. Efforts to improve access control systems and<br\/>credential distribution have paid little attention to privacy mechanisms so far,<br\/>resulting in systems that are good at propagating credentials reliably, but not<br\/>tuned to do so within well-understood privacy constraints. This collaboration<br\/>will build on our work in credential distribution and anonymity to create an<br\/>integrated architecture and protocols to provide advanced access control within<br\/>the limitations of privacy constraints.","title":"Collaborative Research: Formal Privacy","awardID":"0506546","effectiveDate":"2004-11-01","expirationDate":"2006-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2802","name":"TRUSTED COMPUTING"}}],"PIcoPI":["553855"],"PO":["521752"]},"94968":{"abstract":"The goal of this project is to develop robust algorithms for reconstructing the 3D shape and reflectance properties of real world scenes. The distinguishing feature of the proposed approach is the modeling of global light transport, taking into account both realistic reflection and global interreflection of light with surfaces in the unknown scene. Modeling realistic light transport is a open problem with major importance in computer vision, due to the fact that everyday materials reflect light in complex ways (e.g., wood, hair, velvet), and because the appearance of geometrically complex scenes is strongly affected by interreflection. This project seeks to model reflection in a very general setting, where the shape and the reflectance properties of the scene are both unknown and unconstrained. Modeling of interreflection will focus on diffuse scattering, and fully account for global light propagation through the scene. Toward this objective, a new computational framework is introduced that uses data-driven models of light transport that can be captured directly from photographs, and recovers scene structure without the need for complex simulations or optimizations of the underlying physical process. A key advantage of such data-driven models is that they work robustly in very general conditions.<br\/><br\/>The proposed work opens up new avenues in 3D shape sensing technology, a problem with widespread applications in robotics, visualization, mapping, aerial imaging, and virtual reality. The ability to capture material models on real objects will improve realism in computer graphics, and impacts applications such as entertainment, visualization, and the communication of visual media. The project will involve undergraduate and graduate research projects and the outcome will be a set of results and tools that will be broadly disseminated and incorporated into research projects and educational initiatives at the University of Washington.","title":"Data-Driven Modeling of Shape, Reflection, and Interreflection","awardID":"0413198","effectiveDate":"2004-11-15","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["533242"],"PO":["564316"]},"100343":{"abstract":"Current high-end applications usually exploit just a fraction of the theoretical performance of <br\/>large platforms. Interactions among the hardware, system software, programming interface, and <br\/>algorithm are extremely complex, and the implications for development of hybrid MPI+OpenMP <br\/>Fortran\/C\/C++ applications are challenging. The emerging generation of machines will be even <br\/>more complex, as will the applications that exploit them. Typical application development and <br\/>tuning scenarios involve the manual and separate use of compilers and performance tools, and <br\/>program modifications based upon insights laboriously gleaned from their output. <br\/>In this proposal, we intend to raise the quality of the application development and tuning <br\/>process by creating an integrated environment for program optimization that reduces the manual <br\/>labor and guesswork of existing approaches. We will develop strategies and the corresponding <br\/>interfaces that enable the application developer, compiler and performance tools to collaborate to <br\/>generate optimized code based upon a variety of sources of feedback, including performance <br\/>data from .offline. development runs as well as from .online. production runs. We will build <br\/>and deploy a flexible, working system that combines robust, existing, open source software . a <br\/>compiler, a program analysis tool and two performance tools with complementary features - into <br\/>a single, coherent environment for collaborative static and dynamic application tuning. <br\/>Application codes of varying complexity supplied by our application partner will motivate our <br\/>development work as well as test and demonstrate our results. The result will be a powerful, <br\/>integrated environment that can be used to obtain traditional performance data via program <br\/>monitoring, event tracing and\/or the extraction of hardware counter information, and to obtain <br\/>support for the static or dynamic tuning of an application code. <br\/> <br\/>Intellectual Merit <br\/>The proposed environment integrates several different existing technologies to provide a new <br\/>level of support for optimizing hybrid MPI+OpenMP codes. Support for experimentation with <br\/>the hybrid programming model is provided. The range of information that may be exploited by <br\/>the compiler to optimize code is expanded to cover many system and application-level <br\/>phenomena and a variety of optimization scenarios. Interactions between tools will facilitate the <br\/>provision of an approach that is able to handle extreme-scale computations. Integration issues <br\/>will be addressed with the goal of creating a deployable, extensible system. <br\/> <br\/>Broader Impacts <br\/>The tools, ideas, and results of this project will be freely distributed and made available to the <br\/>HPC community, nationally and internationally. Besides the general dissemination of results, the <br\/>project has strong ties to performance engineering experts at NCSA. The tools will be installed <br\/>on NCSA systems for testing and evaluation and will be made available to other users. The <br\/>research brings together compiler, performance tool, and application developers, enriching the <br\/>research experience of graduate students to create a well-rounded IT workforce. The PI is active <br\/>in the OpenMP community and the project team has close working relationships with DOE and <br\/>DOD. New knowledge generated will be integrated into advanced graduate coursework.","title":"Collaborative Research: Performance Toolset for Dynamic Optimization of High-End Hybrid Applications","awardID":"0444319","effectiveDate":"2004-11-01","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":["529083"],"PO":["565272"]},"100255":{"abstract":"There is a general vision that in the future, natural language interfaces will provide access to information and services with the simplicity and flexibility required for virtually anyone to participate, eliminating both intellectual and physical barriers, and impacting the way products interface with consumers as well as creating whole new product types. In this project, the PI will test a promising new, innovative natural language processing approach called tridbit technology that may provide a practical way for humans to talk to computers and other devices. Tridbit technology uses unique knowledge structures (called tridbits) and rules for discriminating and combining different types of information to represent the meaning of natural language. The PI's goal is to test whether tridbit technology will hold up in more demanding and realistic environments than previously explored. Various aspects of an existing prototype system will be enhanced and evaluated, including performance in subject areas with varying levels of familiarity and handling of ambiguity and contradictions. Grounding of concepts and constructing natural language to express arbitrary knowledge will also be explored. The technology will be applied to a simulated household environment to control devices such as TVs, VCRs and alarm clocks, with the goal of enabling people to say in their own words what they want a device to do. The PI hopes to demonstrate that natural language interfaces are feasible within a household environment.<br\/><br\/>Broader Impacts: The technology being tested identifies key constraints within the representation of meaning, potentially simplifying many aspects of the natural language understanding problem. A departure from current paradigm, its underlying theories speak to the way information is structured. If validated, it will enhance our understanding of how natural language works, and could influence a wide range of disciplines. A positive outcome to this project could have tremendous impact for people with physical or cognitive disabilities that prevent them from using standard device controls, and would also prove to manufacturers that they need to prepare for these interfaces of the future.","title":"SGER: Exploration of a new apporach to natural language understanding within a controlled application area","awardID":"0443831","effectiveDate":"2004-11-15","expirationDate":"2005-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":[264535],"PO":["565227"]},"94958":{"abstract":"This project is a multidisciplinary effort of computer scientist, mathematicians, and engineers at Rensselaer Polytechnic Institute (RPI) and University of Pennsylvania (U Penn). The focus of the research will be on the development of new planning techniques for tasks that cannot be performed successfully without intermittent contacts. Such tasks are referred to as manipulation tasks. Manipulations tasks, particularly those in which the effects of dynamics influence the system performance and outcome, are exceedingly important in robotics. The ability of a robot to perform such tasks completely changes its character from passive observer to active participant capable of performing useful work, whether it be assisting a first- responder on the scene of a building collapse or assembling parts in micro-scale devices.<br\/><br\/>Intellectual Merit<br\/>This research requires the simultaneous development of new algorithms and mathematical models for analyzing and predicting the motion of multibody systems with frictional, compliant, intermittent contacts. These developments are being done with attention paid to practical engineering concerns (verified by experiments) that drive the form of mathematical models, and in turn, the algorithms and mathematics.<br\/><br\/>Broader Impacts<br\/>The results of this project are directly applicable to a large class of problems in locomotion and manipulation that is central to ongoing research on humanoid robots, and on fixturing and assembly in manufacturing settings. On the educational front, the research team is developing a new graduate-level course on Manipulation Planning Analysis, across UPenn and RPI, that will be co-taught by the co-PIs.","title":"Grasp and Manipulation Planning in the Presence of Dynamics and Uncertainty","awardID":"0413138","effectiveDate":"2004-11-15","expirationDate":"2009-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[249077,"553286"],"PO":["387198"]},"101510":{"abstract":"This paper explores the complexity of the privacy problem. In particular the computational complexity properties of the privacy problem are examined. Prior work in this area explored the recursion theoretic properties and viewed the privacy problem as a form of the inference problem. This work showed that the privacy problem in general was unsolvable. The PIs follow up on this prior work and propose to explore the more important aspects of the privacy problem and that is the complexity of the privacy problem. They also view the privacy problem as a form of inference problem in databases and subsequently propose to develop a complexity theory for privacy problem based on deductive databases and then prove some properties of the problem. <br\/><br\/>Privacy is becoming a very important research area and we are funding a number of efforts on privacy. It is critical that the complexity aspects of the privacy problem be examined. The PIs are top researchers in complexity theory and databases. They are highly qualified to do this research. They have published in top journals such as the Journal of the ACM and Journal of Computer and Systems Sciences. This research could produce some breakthrough results.","title":"Computational Complexity Issues for Privacy in Data Access Mechanisms","awardID":"0451097","effectiveDate":"2004-11-15","expirationDate":"2005-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7228","name":"DATA AND APPLICATIONS SECURITY"}}],"PIcoPI":["499391","389215"],"PO":["469867"]},"102522":{"abstract":"The workshop took place November 19-20, 2004 at MIT. It served as the 14th in the ongoing series of Fall Workshops on Computational Geometry, which originated at Stony Brook in 1991. The workshop on computational geometry is a forum for reporting ongoing state-of-the art research in many areas of theoretical and applied computational geometry. The focus of this particular workshop is to highlight open problems and ongoing challenges to the field, with the intention that formalized statements of open problems will help form a catalyst for new investigations and innovative solutions, while motivating new researchers, particularly students, to engage in attacking some of the outstanding questions in discrete and computational geometry. A key focus of the workshop will be an Open Problem Forum in order to promote a free exchange of questions and research challenges.<br\/><br\/>The results of the workshop will be widely disseminated by means of the internet, with talk abstracts and a detailed list of open problems raised. Many of the open problems will be highlighted in The Open Problem Project (TOPP) web pages, a recently established open problem forum moderated by three of the workshop organizers (Demaine, Mitchell, and O'Rourke).","title":"Workshop on Computational Geometry with a Focus on Open Problems","awardID":"0456026","effectiveDate":"2004-11-15","expirationDate":"2005-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["471875","527736"],"PO":["321058"]},"100366":{"abstract":"Current high-end applications usually exploit just a fraction of the theoretical performance of large platforms. Interactions among the hardware, system software, programming interface, and algorithm are extremely complex, and the implications for development of hybrid MPI+OpenMP Fortran\/C\/C++ applications are challenging. The emerging generation of machines will be even more complex, as will the applications that exploit them. Typical application development and tuning scenarios involve the manual and separate use of compilers and performance tools, and program modifications based upon insights laboriously gleaned from their output. In this proposal, we intend to raise the quality of the application development and tuning process by creating an integrated environment for program optimization that reduces the manual <br\/>labor and guesswork of existing approaches. We will develop strategies and the corresponding interfaces that enable the application developer, compiler and performance tools to collaborate to generate optimized code based upon a variety of sources of feedback, including performance data from .offline. development runs as well as from .online. production runs. We will build and deploy a flexible, working system that combines robust, existing, open source software . a compiler, a program analysis tool and two performance tools with complementary features - into a single, coherent environment for collaborative static and dynamic application tuning. Application codes of varying complexity supplied by our application partner will motivate our development work as well as test and demonstrate our results. The result will be a powerful, integrated environment that can be used to obtain traditional performance data via program <br\/>monitoring, event tracing and\/or the extraction of hardware counter information, and to obtain <br\/>support for the static or dynamic tuning of an application code. <br\/> <br\/>Intellectual Merit <br\/>The proposed environment integrates several different existing technologies to provide a new level of support for optimizing hybrid MPI+OpenMP codes. Support for experimentation with the hybrid programming model is provided. The range of information that may be exploited by the compiler to optimize code is expanded to cover many system and application-level phenomena and a variety of optimization scenarios. Interactions between tools will facilitate the provision of an approach that is able to handle extreme-scale computations. Integration issues will be addressed with the goal of creating a deployable, extensible system. <br\/>Broader Impacts <br\/>The tools, ideas, and results of this project will be freely distributed and made available to the HPC community, nationally and internationally. Besides the general dissemination of results, the project has strong ties to performance engineering experts at NCSA. The tools will be installed on NCSA systems for testing and evaluation and will be made available to other users. The research brings together compiler, performance tool, and application developers, enriching the research experience of graduate students to create a well-rounded IT workforce. The PI is active in the OpenMP community and the project team has close working relationships with DOE and DOD. New knowledge generated will be integrated into advanced graduate coursework.","title":"Collaborative Research: Performance Toolset for Dynamic Optimization of High-End Hybrid Applications","awardID":"0444468","effectiveDate":"2004-11-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["506602"],"PO":["565272"]},"100323":{"abstract":"Extremely large scale systems offer a new challenge to application designers. Current software development techniques do not scale well in execution efficiency on these systems or, more importantly, in the amount of time the programmer spends writing, debugging, and tuning the software. To realize extreme-scale computing, we must increase programmer productivity. To that end, we require three advances in the programming paradigm for these systems. First, the application programmer must interact with the development environment at a level higher than processes<br\/>or execution threads. Tools must support these interaction modes and more abstract application views. Second, system monitoring functions must exist to provide feedback to the application programmer on overall system performance. Monitoring an extreme-scale system must include some degree of automation and must be able to infer overall performance from a small set of monitoring points. Feedback must be compressed to highlight performance issues at the high abstraction level the programmer requires. Finally, many low-level optimization decisions must be automated by incorporating a new generation of compiler optimizations targeting global program behavior, and<br\/>these must be intimately integrated with the monitoring system. These advances are described collectively as autonomic performance optimization.<br\/><br\/>Our proposed research addresses these requirements by developing new tools and extending current tools to manage large software projects. We will extend our prior work on the Tau framework of performance instrumentation and analysis tools to the scale used by these HEC applications. To do this, we will incorporate a new framework for monitoring representative \"skeletons\" that can provide information to the programmer about total system performance by using a simpler model that matches the execution profile of the full application. Performance modeling of the skeleton is achieved by placement of profile monitors at strategic points in the system. We will utilize advanced<br\/>machine learning techniques to determine the placement of these monitor points, as well as to synthesize the resulting large quantity of performance information into the proper form for the application designer. Finally, we will automate some critical low-level design decisions by feeding profile data directly to the compiler and dynamic code translator. The optimizations developed target data layout, data duplication throughout the system, and dynamic data movement. Optimizing data management will decrease average access latency for memory references, reducing congestion on the inter-processor and inter-cluster networks while freeing the programmer from making detailed data placement decisions.<br\/><br\/>The intellectual merit of this proposal is in the new paradigm of autonomic performance optimization as a framework for the integration of performance methods and tools for HEC systems. The broader impact is both technical and societal. Ultimately, we strive to enhance the computational tool infrastructure used to solve Grand Challenge scientific computing problems. However, we believe this must be done in association with the evolution of large-scale computing to use introspective, autonomic platforms and systems. Our work will enable practitioners to more easily build efficient, scalable applications, to solve very large and complex problems, and to do so more quickly<br\/>than is currently feasible. The significant increase in the productivity of applications writers will not only enhance the development of scientific applications important to our national infrastructure, but will also open HEC to important economic and societal applications where computing is advancing science and technology.","title":"Collaborative Research: ST-HEC: Scalable, Interoperable Tools to Support Autonomic Optimization of High-End Applications","awardID":"0444207","effectiveDate":"2004-11-01","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["537232"],"PO":["565272"]},"100367":{"abstract":"The objective of this research is to substantially improve the productivity of programmers writing<br\/>applications for petaflop-scale systems by using programmer defined light-weight transactions as the<br\/>single abstraction for expressing parallelism, delineating communication, reasoning about memory<br\/>consistency, providing failure recovery, and allowing performance optimization. Transactions as the<br\/>central abstraction for designing and programming parallel systems leads to a shared memory<br\/>programming and memory coherence model called Transaction Coherence and Consistency (TCC).<br\/>Transactions simplify parallel programming by providing a way of writing correct shared-memory<br\/>programs without threads, locks and semaphores. TCC systems provide high performance communication<br\/>and synchronization with support for hardware mechanisms that can keep memory coherent and<br\/>consistent based on programmer-defined transactions.<br\/>To achieve the research objective, this research program will focus on five activities. First, the researchers<br\/>will develop new abstractions that use transactions to provide a shared memory programming model that<br\/>makes it much easier to analyze and optimize application performance. Second, the researchers will<br\/>develop performance monitoring systems that make use of transactions to detect performance bottlenecks<br\/>and to provide intuitive feedback to programmers. Third, the researchers will use the transaction based<br\/>programming model to implement compiler-based static and dynamic feedback-directed optimizations<br\/>that automatically detect and eliminate performance bottlenecks and extend the scalability of transaction<br\/>coherency to 105 processors. Fourth, the researchers will use transactions to optimize the performance of<br\/>parallel storage I\/O. Finally, the researchers will develop simulation and emulation technology that will<br\/>enable us to experiment with petaflop-scale systems that support light-weight transactions before they are<br\/>available.<br\/>Broader Impacts<br\/>The broad impact of this research is to use transaction-based parallel programming to educate and enable<br\/>a new class of parallel software developers who can implement parallel software with the same facility<br\/>that sequential software is written today. Enabling parallel software development will be critical to<br\/>advancing computing performance from desktop applications to large-scale scientific and commercial<br\/>applications. While parallel processing has been essential for large-scale machines for a while, recent<br\/>announcements by Intel, AMD and IBM demonstrate that it will soon be critical for desktop applications<br\/>as well. To educate students, other researchers, and industry about the benefits of transaction-based<br\/>parallel programming, we will incorporate transactional programming concepts in the parallel<br\/>programming curriculum and make transaction-based applications available to the wider scientific<br\/>community. The researchers expect that releasing a suite of optimized transaction-based applications<br\/>along with simulation technology will be instrumental in encouraging other researchers to experiment<br\/>with and explore the benefits of transactions. To further promote the use of transaction-based parallel<br\/>programming we will organize a tutorial or workshop at a major scientific computing conference that will<br\/>cover the principles and experience of programming with transactions.","title":"Extending the Limits of Large-Scale Shared Memory Multiprocessors","awardID":"0444470","effectiveDate":"2004-11-01","expirationDate":"2007-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["438513","556786","556787"],"PO":["565272"]},"97909":{"abstract":"Americans are living longer and more fulfilled lives, and they desire to live as independently as possible. But independent lifestyles come with risks. To address these issues, researchers are developing \"smart home\" technologies to help older adults remain independent at home while controlling costs. Smart homes enhance residents' safety and monitor health conditions using sensors and other devices. Such technology can help keep older adults independent while controlling costs. The PIs have proposed a model of decline in older adults in which mobility and cognitive impairments lead to functional decline, thereby reducing independence. Appropriate interventions, if offered in a timely manner, can improve functional ability. The key is early identification of changing conditions that indicate impairments. In this project, the PIs will establish a partnership among faculty, students, and researchers across diverse schools within two universities, in order to develop and evaluate intervention technology for elders. They will utilize a unique eldercare facility in Columbia, called TigerPlace, to study technology targeting mobility and cognitive impairments. Project objectives include: development of an integrated monitoring system that reliably captures data about the elder residents and their environment in a noninvasive manner and balances the needs of health safety and privacy; collection of data in typical independent living, elder settings, using the integrated monitoring system; development of algorithms to extract patterns of activity from the collected sensor data; and evaluation of the usability of the technology and investigation of fundamental issues in human-computer interaction for the population of older adults. To these ends, a monitoring system developed by one of the participants and incorporating gait, physiological, and environmental sensors will be extended. An event-driven video sensor network will be developed to generate an \"anonymized\" video stream that hides identifying features of the residents, and sensor and video data will be fused and processed for identifying patterns of behavioral activity. Fundamental human-computer interaction (HCI) issues as related to elders will also be investigated. Deliberative evaluation and usability studies will be conducted.<br\/><br\/>Broader Impacts: This project will have broad impact on our society, by offering a model for eldercare technology and by providing policy makers with answers to complex questions of cost effectiveness and outcomes to help guide decisions about services for older people, such as those funded by Medicare and Medicaid (it is estimated that 12.4% of the U.S. population was over 65 in 2000, and this percentage is projected to grow by 2030 is 20%). From the technical point of view, the research will advance the state of the art in HCI for elders, in anonymized and compressed video, in extracting human motion and behaviors from video data, in extracting human activity from sensory events, and in fusing sensor and video data for identifying behavioral patterns. In addition, the project will train both graduate and undergraduate students and help establish an infrastructure in conducting future research projects; faculty and administrators from Lincoln University in Jefferson City, MO will assist the PIs in recruiting traditionally underrepresented nursing and CIS students for multi-disciplinary teams and summer internships.","title":"ITR - (EVS) - (int+soc+dmc): Technology Interventions for Elders with Mobility and Cognitive Impairments","awardID":"0428420","effectiveDate":"2004-11-15","expirationDate":"2009-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7314","name":"ITR FOR NATIONAL PRIORITIES"}}],"PIcoPI":["537126","561794",257744,"561792",257746],"PO":["565227"]},"100357":{"abstract":"Current high-end applications usually exploit just a fraction of the theoretical performance of <br\/>large platforms. Interactions among the hardware, system software, programming interface, and <br\/>algorithm are extremely complex, and the implications for development of hybrid MPI+OpenMP <br\/>Fortran\/C\/C++ applications are challenging. The emerging generation of machines will be even <br\/>more complex, as will the applications that exploit them. Typical application development and <br\/>tuning scenarios involve the manual and separate use of compilers and performance tools, and <br\/>program modifications based upon insights laboriously gleaned from their output. <br\/>In this proposal, we intend to raise the quality of the application development and tuning <br\/>process by creating an integrated environment for program optimization that reduces the manual <br\/>labor and guesswork of existing approaches. We will develop strategies and the corresponding <br\/>interfaces that enable the application developer, compiler and performance tools to collaborate to <br\/>generate optimized code based upon a variety of sources of feedback, including performance <br\/>data from .offline. development runs as well as from .online. production runs. We will build <br\/>and deploy a flexible, working system that combines robust, existing, open source software . a <br\/>compiler, a program analysis tool and two performance tools with complementary features - into <br\/>a single, coherent environment for collaborative static and dynamic application tuning. <br\/>Application codes of varying complexity supplied by our application partner will motivate our <br\/>development work as well as test and demonstrate our results. The result will be a powerful, <br\/>integrated environment that can be used to obtain traditional performance data via program <br\/>monitoring, event tracing and\/or the extraction of hardware counter information, and to obtain <br\/>support for the static or dynamic tuning of an application code. <br\/><br\/>Intellectual Merit <br\/>The proposed environment integrates several different existing technologies to provide a new <br\/>level of support for optimizing hybrid MPI+OpenMP codes. Support for experimentation with <br\/>the hybrid programming model is provided. The range of information that may be exploited by <br\/>the compiler to optimize code is expanded to cover many system and application-level <br\/>phenomena and a variety of optimization scenarios. Interactions between tools will facilitate the <br\/>provision of an approach that is able to handle extreme-scale computations. Integration issues <br\/>will be addressed with the goal of creating a deployable, extensible system. <br\/><br\/>Broader Impacts <br\/>The tools, ideas, and results of this project will be freely distributed and made available to the <br\/>HPC community, nationally and internationally. Besides the general dissemination of results, the <br\/>project has strong ties to performance engineering experts at NCSA. The tools will be installed <br\/>on NCSA systems for testing and evaluation and will be made available to other users. The <br\/>research brings together compiler, performance tool, and application developers, enriching the <br\/>research experience of graduate students to create a well-rounded IT workforce. The PI is active <br\/>in the OpenMP community and the project team has close working relationships with DOE and <br\/>DOD. New knowledge generated will be integrated into advanced graduate coursework.","title":"Collaborative Research: Performance Toolset for Dynamic Optimization of High-End Hybrid Applications","awardID":"0444407","effectiveDate":"2004-11-01","expirationDate":"2008-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["343259"],"PO":["565272"]},"102525":{"abstract":"The PI and his colleagues, as members of the Cognitive Levers (CLever) research team within the Center for Lifelong Learning and Design at the University of Colorado at Boulder, have suggested that a suitable mobile architecture could support activities for daily living for people with cognitive disabilities. The PI argues that in order to design cost-effective, intelligent technologies to connect caregivers and members of the target community, we must leverage existing handheld and data network technologies and identify critical technologies necessary to complement and address gaps in current mobile, ubiquitous computing environments and platforms. The PI notes that the recent commercial introduction of a new generation of mobile handheld devices that combine Wi-Fi and wide-area cellular data technology provides a potentially cost-effective platform upon which to build and evaluate prototype components. In this exploratory project, the PI and his team will investigate the feasibility of their ideas. They will design mobile architectures based on theories of distributed intelligence ranging from \"thin client\" architectures to peer-to-peer networks. They will obtain performance data from real-world systems while carrying out authentic tasks, thereby obtaining specific content specifications. They will investigate the limits of the targeted technologies with respect to: connectivity between base and mobile stations, data accessibility, and throughput; location-awareness; technical sufficiency of commercial handheld devices; and overall architecture design. The PI expects that project outcomes will enable him to develop grounded theories and effective architectures for the use of new technologies in this field.<br\/><br\/>Broader Impacts: The target community for this research has traditionally been under-served by society. This exploratory \"stress test\" of commercial off-the-shelf (COTS) mobile technologies with laboratory prototype components (MfA, MAPS, LifeLine), while performing real-world tasks, in order to understand current capabilities, constraints, and technical requirements, should catalyze rapid and innovative advances in the design of a robust, mobile socio-technical environment to effectively support people with cognitive disabilities and their caregivers in authentic everyday living activities.","title":"SGER: Designing and developing mobile computing infrastructures and architectures to support people with cognitive disabilities and caregivers in authentic everyday tasks","awardID":"0456053","effectiveDate":"2004-11-15","expirationDate":"2006-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":["483372","483373","547843",270341],"PO":["565227"]},"100369":{"abstract":"Extremely large scale systems offer a new challenge to application designers. Current software development techniques do not scale well in execution efficiency on these systems or, more importantly, in the amount of time the programmer spends writing, debugging, and tuning the software. To realize extreme-scale computing, we must increase programmer productivity. To that end, we require three advances in the programming paradigm for these systems. First, the application programmer must interact with the development environment at a level higher than processes<br\/>or execution threads. Tools must support these interaction modes and more abstract application views. Second, system monitoring functions must exist to provide feedback to the application programmer on overall system performance. Monitoring an extreme-scale system must include some degree of automation and must be able to infer overall performance from a small set of monitoring points. Feedback must be compressed to highlight performance issues at the high abstraction level the programmer requires. Finally, many low-level optimization decisions must be automated by incorporating a new generation of compiler optimizations targeting global program behavior, and<br\/>these must be intimately integrated with the monitoring system. These advances are described collectively as autonomic performance optimization.<br\/><br\/>Our proposed research addresses these requirements by developing new tools and extending current tools to manage large software projects. We will extend our prior work on the Tau framework of performance instrumentation and analysis tools to the scale used by these HEC applications. To do this, we will incorporate a new framework for monitoring representative \"skeletons\" that can provide information to the programmer about total system performance by using a simpler model that matches the execution profile of the full application. Performance modeling of the skeleton is achieved by placement of profile monitors at strategic points in the system. We will utilize advanced<br\/>machine learning techniques to determine the placement of these monitor points, as well as to synthesize the resulting large quantity of performance information into the proper form for the application designer. Finally, we will automate some critical low-level design decisions by feeding profile data directly to the compiler and dynamic code translator. The optimizations developed target data layout, data duplication throughout the system, and dynamic data movement. Optimizing data management will decrease average access latency for memory references, reducing congestion on the inter-processor and inter-cluster networks while freeing the programmer from making detailed data placement decisions.<br\/><br\/>The intellectual merit of this proposal is in the new paradigm of autonomic performance optimization as a framework for the integration of performance methods and tools for HEC systems. The broader impact is both technical and societal. Ultimately, we strive to enhance the computational tool infrastructure used to solve Grand Challenge scientific computing problems. However, we believe this must be done in association with the evolution of large-scale computing to use introspective, autonomic platforms and systems. Our work will enable practitioners to more easily build efficient, scalable applications, to solve very large and complex problems, and to do so more quickly<br\/>than is currently feasible. The significant increase in the productivity of applications writers will not only enhance the development of scientific applications important to our national infrastructure, but will also open HEC to important economic and societal applications where computing is advancing science and technology.","title":"ST-HEC: Collaborative Research: Scalable, Interoperable Tools to Support Autonomic Optimization of High-End Applications","awardID":"0444475","effectiveDate":"2004-11-01","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["530888"],"PO":["565272"]},"100359":{"abstract":"Extremely large scale systems offer a new challenge to application designers. Current software development techniques do not scale well in execution efficiency on these systems or, more importantly, in the amount of time the programmer spends writing, debugging, and tuning the software. To realize extreme-scale computing, we must increase programmer productivity. To that end, we require three advances in the programming paradigm for these systems. First, the application programmer must interact with the development environment at a level higher than processes or execution threads. Tools must support these interaction modes and more abstract application views. Second, system monitoring functions must exist to provide feedback to the application programmer on overall system performance. Monitoring an extreme-scale system must include some degree of automation and must be able to infer overall performance from a small set of monitoring points. Feedback must be compressed to highlight performance issues at the high abstraction level the programmer requires. Finally, many low-level optimization decisions must be automated by incorporating a new generation of compiler optimizations targeting global program behavior, and these must be intimately integrated with the monitoring system. These advances are described collectively as autonomic performance optimization.<br\/><br\/>Our proposed research addresses these requirements by developing new tools and extending current tools to manage large software projects. We will extend our prior work on the Tau framework of performance instrumentation and analysis tools to the scale used by these HEC applications. To do this, we will incorporate a new framework for monitoring representative \"skeletons\" that can provide information to the programmer about total system performance by using a simpler model that matches the execution profile of the full application. Performance modeling of the skeleton is achieved by placement of profile monitors at strategic points in the system. We will utilize advanced machine learning techniques to determine the placement of these monitor points, as well as to synthesize the resulting large quantity of performance information into the proper form for the application designer. Finally, we will automate some critical low-level design decisions by feeding profile data directly to the compiler and dynamic code translator. The optimizations developed target data layout, data duplication throughout the system, and dynamic data movement. Optimizing data management will decrease average access latency for memory references, reducing congestion on the inter-processor and inter-cluster networks while freeing the programmer from making detailed data placement decisions.<br\/><br\/>The intellectual merit of this proposal is in the new paradigm of autonomic performance optimization as a framework for the integration of performance methods and tools for HEC systems. The broader impact is both technical and societal. Ultimately, we strive to enhance the computational tool infrastructure used to solve Grand Challenge scientific computing problems. However, we believe this must be done in association with the evolution of large-scale computing to use introspective, autonomic platforms and systems. Our work will enable practitioners to more easily build efficient, scalable applications, to solve very large and complex problems, and to do so more quickly than is currently feasible. The significant increase in the productivity of applications writers will not only enhance the development of scientific applications important to our national infrastructure, but will also open HEC to important economic and societal applications where computing is advancing science and technology.","title":"Collaborative Research: ST-HEC: Scalable, Interoperable Tools to Support Autonomic Optimization of High-End Applications","awardID":"0444413","effectiveDate":"2004-11-01","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["359520"],"PO":["565272"]},"98030":{"abstract":"Dispute resolution is a fundamental and pervasive activity of government, requring efficiency, effectiveness and fairness. This project proposes applying process technology to develop and evaluate dispute resolution processes through online delivery. The intention is to improve dispute resolution, and deepen understanding of how to be more successful in developing and evaluating processes with the stringent requirements of public governance. The project focus is on the dispute resolution processes and approaches used by the National Mediation Board (NMB). <br\/><br\/>Intellectual Merit: The project's intellectual focus is on the applicability of process technology to processes having particularly stringent efficiency, effectiveness and fairness requirements. These, in turn, rest upon the need for strong management of communications and information flow, and strong assurances about security, privacy, and accuracy. The establishment of transparently fair, validated, processes designed by multi-stakeholder collaboration can meet these requirements, and lead to increased trust in government. The research will include evaluation of the success of innovative process definition, analysis, and collaboration technologies in meeting these stringent requirements and in increasing trust. These technologies have succeeded in such domains as software development, medicine, and scientific data processing.; they will now be evaluated in the complex domain of online dispute resolution, with its particularly challenging goal of trust enhancement in an environment where the parties are in dispute. The research is interdisciplinary in nature, involving researchers from computer science, management, and legal studies. <br\/><br\/>Broader Impacts: Most federal agencies must respond to grievances from citizens and groups, and resolve disputes between the agency and individuals and organizations. The Federal government has recently mandated that its agencies use alternative dispute resolution (ADR) methods to resolve disputes. As a result of both Congressional and Presidential action during the 1990s, federal administrative agencies must use non-adversarial dispute resolution whenever possible. Over eighty Federal agencies have such mediation programs. The NMB's clients and sibling organizations will be prepared to adopt these technologies in applying Online Dispute Resolution to their own needs. Results will be distributed through traditional publication venues and also by the UMass Center for Information Technology and Dispute Resolution, the UMass Electronic Enterprise Institute, and the NMB's participation in the US Government Interagency Alternative Dispute Resolution (ADR) Working Group of over 25 agencies.","title":"Process Technology for Achieving Government Online Dispute Resolution","awardID":"0429297","effectiveDate":"2004-11-15","expirationDate":"2007-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["440692","536737","387450"],"PO":["371077"]},"98085":{"abstract":"PolyFlow: An Architectural Model for Highly Concurrent Instruction Execution<br\/><br\/>Abstract<br\/><br\/>The PolyFlow project will investigate new dynamic techniques for using control-dependence information to find and exploit global instruction-level concurrency within a single thread of execution. PolyFlow builds on compiler-assisted thread level parallelization mechanisms that concurrently fetch, rename and execute multiple, widely separated, portions of the program. Based on the insight that the control dependence graph provides concrete information about the instructions that will be fetched in the future, the PolyFlow compiler provides this graph, rather than thread boundary information, directly to the micro-architecture, allowing the micro-architecture to make feedback-directed dynamic and speculative instruction prioritization and resource utilization decisions. The project will investigate the specific architectural, feedback, compilation, and resource management mechanisms necessary for PolyFlow to attain high levels of concurrency on a single thread. PolyFlow may fundamentally change how we approach single-threaded computation from a parallelism perspective, potentially opening up fresh avenues of research in architecture and compilation. PolyFlow's performance gains can be molded by system designers throughout the information technology sector into better system attributes such as reliability, security, innovative human-computer interfaces, system autonomy or application intelligence.","title":"PolyFlow: An Architectural Model for Highly Concurrent Instruction Execution","awardID":"0429711","effectiveDate":"2004-11-01","expirationDate":"2007-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":["322427","526238"],"PO":["325495"]},"98134":{"abstract":"Software programs executing on a broad range of internet systems are constantly subject to malicious<br\/>attacks in various forms. Program execution behavior might be altered as a result causing substantial damage, data may become corrupted and privacy can be greatly compromised.<br\/><br\/>The objective is to develop a secure processor model which secure applications can easily be built on. The proposed designs can protect the privacy of the processor owner through diversifying the representation of its identity.<br\/><br\/>Intellectual Merit<br\/>To achieve our goals, we will augment the existing microprocessor architecture to incorporate new features. The proposed architectural components address a broad range of attacks on uniprocessor and multiprocessor architectures. In particular, we will develop novel techniques as follows.<br\/> Architecture Support for Enhancing Uniprocessor Security. The confidentiality and integrity of such a microarchitecture are maintained through encryption and decryption of the code and data transferred across the chip.<br\/>While the efficiency of encryption has been solved successfully by PIs and others, the computation intensive nature of crypto operations has led the verification of inbound traffic being delayed. Such delay imposes vulnerability as one can peek critical data on-chip during this time. This project designs a strong verification engine with which information leakage of on-chip data is prevented,<br\/><br\/>Broader Impact<br\/>Accomplishing the proposed research objectives can significantly impact the research community, processor industry, and academic education. The secure processor model in this proposal is a reliable computing base on which higher levels of secure systems may be built. Since applications are encrypted differently, OS security and user application security are separated. Compromising the OS does not necessarily weaken the user program or data. Higher levels security protocols can thus take advantage of the secure architectural components. Techniques proposed in this proposal are also very practical. Many of them are ready to be synthesized and integrated into real processors.<br\/><br\/>The education component in this proposal seeks to better prepare the students for the coming challenges in computer system design. The research projects included in this proposal offer the students great opportunities in terms of class projects, Master or Doctoral theses, and other valuable practical experience.","title":"Collaborative Research: Architectural Support for Security and Privacy Protection on Uni- and Multi- Processors","awardID":"0430021","effectiveDate":"2004-11-01","expirationDate":"2006-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":["508314"],"PO":["565272"]},"98311":{"abstract":"Designing practically efficient parallel algorithms with rigorously <br\/>analyzed run time guarantees seems to be difficult for a large class <br\/>of problems, commonly known as irregularly structured problems. The <br\/>solutions for such problems typically depend on run time workload <br\/>monitoring and dynamic load balancing, and the run time is often <br\/>critically dependent on the particular instance of the problem being <br\/>solved with no easy way to guarantee optimal, worst-case run times. <br\/>The goal of the proposed research is to design theoretically rigorous <br\/>and practically efficient parallel algorithms for selected irregular <br\/>applications and to develop software to demonstrate their effectiveness. <br\/>Research will also be conducted in developing distributed tree data <br\/>structures and software libraries to support these and other irregular <br\/>applications. Three important classes of applications will be studied:<br\/>grid and particle based methods in scientific computing, sequence <br\/>alignment problems in computational biology, and computing aggregates <br\/>on multidimensional data cubes in data mining. The work in scientific <br\/>computing will be focused on providing spatial data structure support <br\/>for problems such as molecular dynamics, adaptive mesh refinement <br\/>and multigrid methods. Several computationally intensive irregular <br\/>problems in sequence homology detection will be studied including<br\/>spliced sequence alignments for gene discovery, multiple sequence <br\/>alignments to discover protein motifs and full genome comparisons for <br\/>comparative genomics. The third application area is computation of<br\/>aggregates on multidimensional data. Using techniques from computational<br\/>geometry, efficient parallel algorithms for computing aggregates and <br\/>answering queries on sparse, multidimensional data sets will be developed.<br\/>The goal is to discover common underlying themes to provide a unifying <br\/>way to address seemingly disparate problem areas. For example, spatial <br\/>distribution of the data or the geometrical structure of the problem <br\/>domain is the source of irregularity for most of the problems considered. <br\/>Similarly, tree data structures are often the backbone of parallel <br\/>algorithms for many irregularly structured problems.<br\/><br\/>The broader impacts of the proposed research include fundamental<br\/>contributions to solving irregularly structured problems, a major <br\/>theme of research in parallel processing. Student participation in<br\/>research is pursued using multiple strategies - graduate students <br\/>through direct funding under the project, undergraduate students <br\/>via senior design projects, female students through Iowa State's Women<br\/>in Science and Engineering program, and minority students through<br\/>collaboration with a minority serving institution.","title":"Techniques and Tools for Parallel Solution of Irregular Problems","awardID":"0431140","effectiveDate":"2004-11-15","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":["565135"],"PO":["399214"]},"93571":{"abstract":"Within the next five years, it will be possible to provide enough transistors on a single chip for on the order of a hundred ARM-equivalent processors. At that point, individual, programmable processors will be like registers were to early VLSI design - building blocks within a larger organizing framework. The investigators refer to this class of devices as Programmable Heterogeneous Multiprocessors (PHMs), emphasizing that not only will individual processors on the chip be programmable, but collections of processors on the chip as well as the chip as a whole must be considered programmable. Applications that might appear on such devices include that of current cell phones, current personal digital assistant (PDA) applications, global positioning system (GPS) sensing, Bluetooth, motion sensing, ad hoc networking, 3-D image processing,<br\/>compression\/decompression, security, multimedia and a broad set of human computer interaction<br\/>(HCI) software. Ubiquitous and pervasive computing will produce newer scenarios with even more complex functionality. But these possibilities are limited by the ability to design and technically realize these scenarios in the individual space and power-constrained computing devices. <br\/>The class of single-chip computing that will leave the desktop in meeting the needs of future computing poses distinctly different programming and design challenges from board and network-level heterogeneous multiprocessors because a single chip is a finite resource, the hardware design will be semi-custom, and coordination of system resources will be required. All of these speak to levels of optimization that will be required for consumer electronic devices because of their physical limitations.<br\/><br\/>The project will investigate new design strategies that leverage the next level of design that is enabled as system performance can be evaluated with reduced modeling detail. The approach will define novel views of the proposed functionality and multiprocessor architecture, enabling simulation and optimization of the whole system's performance. Novel, proposed strategies include the development of design scenarios, which are the answer to the currently separate views of testbenches that exercise computer systems and benchmarks that are used to evaluate programmable designs, and the enabling of novel system-level strategies to save power by using a larger number of simpler processors that execute at lower clock rates or with sophisticated multiprocessor-level scheduling strategies. While ambitious, the project's vision is<br\/>enabled because of prior work in developing a novel simulator that capture the performance interactions of concurrent software executing on concurrent hardware using a layered model.","title":"NGS: A Design Environment for Single Chip Heterogeneous Multiprocessors","awardID":"0406384","effectiveDate":"2004-11-01","expirationDate":"2006-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["507772","464680"],"PO":["301532"]},"97831":{"abstract":"Proposal number: CNS-0427748<br\/><br\/>Title: ITR: Panoply: Enabling Safe Ubiquitous Computing Environments<br\/><br\/>PI: Leonard Kleinrock<br\/><br\/>ABSTRACT<br\/><br\/>Ubiquitous computing offers both powerful possibilities and great risks and challenges. Mobile devices entering ubiquitous environments may bring dangers in or face threats already there, so careful control of the interaction between such devices and a ubiquitous environment is required. The Panoply project will provide safe ubiquitous computing environments by using the innovative spheres-of-influence model to dynamically organize related devices into geographical and semantic groups. Spheres of influence offer both a conceptual model for reasoning about group interactions and an organizing principle for an actual implementation. Devices in a sphere share a common security policy and mechanisms that can prevent importation of malicious code or improper use of local resources. Contaminated devices entering a sphere can be rejected, decontaminated or allowed to operate in limited, safe ways. On the reverse side, devices entering a sphere can control what they expose and offer to other devices there, rather than allowing complete access, thereby providing greater privacy and safety for mobile device users. Panoply will use policy negotiation and automated planning capabilities to provide safety in a sphere of influence. The project will build an implementation of the spheres-of-influence model and will demonstrate its safety and effectiveness.","title":"ITR - (ASE+NHS) - (dmc+int+soc): Panoply: Enabling Safe Ubiquitous Computing Environments","awardID":"0427748","effectiveDate":"2004-11-15","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7314","name":"ITR FOR NATIONAL PRIORITIES"}}],"PIcoPI":[257468,"343177","485932"],"PO":["434241"]},"94982":{"abstract":"The PI proposes to establish a 3 year program to develop non-parametric representations of the behaviors and actions of unknown objects in video sequences. The approach is to use the generative statistical modeling tools of Pattern Theory to optimize non-linear dimensionality reduction tools to be applied to natural image sequences. Applications include registration and de-noising of medical or aerial image data sets, understanding of non-rigid and deformable motion, and content based video indexing. The intellectual merit includes formal tools to indicate under what real world conditions techniques such as Isomap fail. Collaborators whose focus is the study of cardiovascular MRI data, diagnostic human motion analysis, and the mechanics of head injuries facilitate explicit testing of these approaches in real and diverse applications. The broader impacts of this proposal include expanding access to healthcare by allowing new experimental protocols for MRI imaging studies, and gives tools that support human motion analysis in unstructured settings. Extracting general representations from video supports content based indexing which lies at the heart of the multi-media component of the national digital library initiative. A web-based portal (Wumap) will serve as a national resource for automatically generating web-based visualizations of natural patterns and will have portions designed for young K-12 students.","title":"Non-Parametric Representations of Motions and Actions in Video","awardID":"0413291","effectiveDate":"2004-11-15","expirationDate":"2008-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["483609"],"PO":["564316"]},"98129":{"abstract":"Software programs executing on a broad range of internet systems are constantly subject to malicious<br\/>attacks in various forms. Program execution behavior might be altered as a result causing substantial damage, data may become corrupted and privacy can be greatly compromised.<br\/><br\/>The objective is to develop a secure processor model which secure applications can easily be built on. The proposed designs can protect the privacy of the processor owner through diversifying the representation of its identity.<br\/><br\/>Intellectual Merit<br\/>To achieve our goals, we will augment the existing microprocessor architecture to incorporate new features. The proposed architectural components address a broad range of attacks on uniprocessor and multiprocessor architectures. In particular, we will develop novel techniques as follows.<br\/>Architecture Support for Enhancing Uniprocessor Security. The confidentiality and integrity of such a microarchitecture are maintained through encryption and decryption of the code and data transferred across the chip.<br\/>While the efficiency of encryption has been solved successfully by PIs and others, the computation intensive nature of crypto operations has led the verification of inbound traffic being delayed. Such delay imposes vulnerability as one can peek critical data on-chip during this time. This project designs a strong verification engine with which information leakage of on-chip data is prevented,<br\/><br\/>Broader Impact<br\/>Accomplishing the proposed research objectives can significantly impact the research community, processor industry, and academic education. The secure processor model in this proposal is a reliable computing base on which higher levels of secure systems may be built. Since applications are encrypted differently, OS security and user application security are separated. Compromising the OS does not necessarily weaken the user program or data. Higher levels security protocols can thus take advantage of the secure architectural components. Techniques proposed in this proposal are also very practical. Many of them are ready to be synthesized and integrated into real processors.<br\/><br\/>The education component in this proposal seeks to better prepare the students for the coming challenges in computer system design. The research projects included in this proposal offer the students great opportunities in terms of class projects, Master or Doctoral theses, and other valuable practical experience.","title":"Collaborative Research: Architectural Support for Security and Privacy Protection on Uni- and Multi- Processors","awardID":"0429986","effectiveDate":"2004-11-01","expirationDate":"2005-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":["448722"],"PO":["565272"]},"97613":{"abstract":"Achieving High-Performance Reconfigurable Computing in Commodity Devices<br\/>Abstract<br\/><br\/>FPGAs are chips that can be programmed and reprogrammed to implement complex digital logic. They combine the performance of hardware with the flexibility of software. Potential applications range from hardware accelerators for high-performance computers, as well as in everyday electronic devices. Thus, improving their performance is of significant interest.<br\/>Although FPGAs can provide high performance for a wide range of applications, their achieved clock cycles are typically 5x-10x slower than other circuits. This is due to the programmable nature of the underlying hardware, as well as the limitations in the input circuits.<br\/>FPGAs can support much higher theoretical clock rates than currently can be achieved in practice. This research will develop architectural features and tools needed to realize this potential. This approach will combine established techniques with new algorithms for generating and mapping highly pipelined circuits. The key is to allow for very significant levels of circuit pipelining in situations that demand it, while trading area for performance.<br\/>We will also optimize the FPGA architectures to support pipelining, while not adversely affecting general-purpose designs. This will include optimized logic blocks that can support aggressive pipelining, as well as routing designed for interconnect pipelining.<br\/>This proposal contains new approaches to radically increase the speed of FPGAs, a major building block in today's electronics systems. By providing faster hardware, we can provide greater flexibility, capabilities, and speed in many different systems. This may include high-end computers with reconfigurable hardware units, and versatile electronics like multi-network, multi-service cell phones and enhanced multi-media capable PDAs.","title":"Achieving High-Performance Reconfigurable Computing in Commodity Devices","awardID":"0426147","effectiveDate":"2004-11-01","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":["485671"],"PO":["564388"]},"93555":{"abstract":"Grid computing brings geographically dispersed computing power to meet the ever-increasing demands. However, representing new and future computing platforms, Grid is a globally shared, heterogeneous, and autonomous controlled platform. It is far more advanced, powerful, dynamic and complex than computing platforms in he past. Such complexity requires new system software technology to efficiently utilize its computing capacity. Most existing performance technologies are targeted for dedicated platforms. Recent performance facilities, such as NWS, only predict short-term (less than five minutes) resource availability, which is not appropriate for long-term applications. New software technologies are needed for long-term, application-level performance predication and task scheduling to alleviate the complexity of Grid. <br\/>Preliminary results show that GHS is fundamentally better than existing systems for long-term applications and can lead to substantial decrease in computing cost. The PIs will collaborate with researchers at DOE national laboratories and IIT to demonstrate the great potential of GHS with important national interest applications. <br\/>The research approach is based on the observation that Grid environments do not have central control and performance efficiency has to be based on resource availability. GHS will exploit this observation. In particular the project: 1) will design, implement, and validate stochastic and analytical models to predict the computing and communication resource availability and their influence on user applications. 2) will develop, implement, and validate practical and non-intrusive performance measurement technologies. 3) will design, implement, and validate task scheduling and rescheduling algorithms to utilize the prediction given in (1) to reduce user application run-times.","title":"NGS: Grid Harvest Service (GHS): A Performance System for Grid Computing","awardID":"0406328","effectiveDate":"2004-11-01","expirationDate":"2007-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["557487","557489","455144","550730"],"PO":["551712"]},"98109":{"abstract":"This research focuses on the comparison of surfaces which are similar, but not identical. For example, almost all leg bones are similar in shape (a cylinder with knobs on the ends) but may differ greatly in length, diameter, size and shape of the knobs, etc. The goal of this research is to provide robust methods for comparing, categorizing, and analyzing surfaces of this type. Researchers in the bio-medical, physical anthropology, and forensics fields, to name a few, currently rely on human experts to perform these measurements. The types of measurements they can make are also limited to simple ones, such as length and circumference. These measurements are used to answer questions such as: What race\/sex is this bone from? How tall was the person? Is the surface is abnormal? Was there an injury at some time? Why does the joint not function normally? Analytical measurements will not only remove the (sometimes subjective) human component, but will also broaden the scope of surface comparison beyond simple length measurements. <br\/><br\/>The first part of this research is a constructive approach for creating manifolds of arbitrary topology. These manifolds provide a natural mechanism for analyzing specific subsets of the surface via the creation of local maps that cover any desired portion of the surface. Manifolds also provide a parameterization mechanism for identifying corresponding points on surfaces. The research addresses theoretical issues related to manifolds such as improved local map placement, more controllable blend strategies, manifolds with boundary, and using manifolds to parameterize existing meshes or point clouds. The second part of his research, detailed surface comparison, is a relatively novel task, and includes a well-posed statement of the problem in addition to the development of algorithmic solutions. For example: What is a feature? How do we quantitatively measure feature differences in a meaningful way? How do we measure the quality of the correspondence between two surfaces?","title":"Surface Construction and Comparison using Manifolds","awardID":"0429856","effectiveDate":"2004-11-01","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["547864"],"PO":["565157"]},"94974":{"abstract":"This project is a multidisciplinary effort of computer scientist, mathematicians, and engineers at Rensselaer Polytechnic Institute (RPI) and University of Pennsylvania (U Penn). The focus of the research will be on the development of new planning techniques for tasks that cannot be performed successfully without intermittent contacts. Such tasks are referred to as manipulation tasks. Manipulations tasks, particularly those in which the effects of dynamics influence the system performance and outcome, are exceedingly important in robotics. The ability of a robot to perform such tasks completely changes its character from passive observer to active participant capable of performing useful work, whether it be assisting a first- responder on the scene of a building collapse or assembling parts in micro-scale devices.<br\/><br\/>Intellectual Merit<br\/>This research requires the simultaneous development of new algorithms and mathematical models for analyzing and predicting the motion of multibody systems with frictional, compliant, intermittent contacts. These developments are being done with attention paid to practical engineering concerns (verified by experiments) that drive the form of mathematical models, and in turn, the algorithms and mathematics.<br\/><br\/>Broader Impacts<br\/>The results of this project are directly applicable to a large class of problems in locomotion and manipulation that is central to ongoing research on humanoid robots, and on fixturing and assembly in manufacturing settings. On the educational front, the research team is developing a new graduate-level course on Manipulation Planning Analysis, across UPenn and RPI, that will be co-taught by the co-PIs.","title":"Collaborative Research: Grasp and Manipulation Planning in the Presence of Dynamics and Uncertainty","awardID":"0413227","effectiveDate":"2004-11-15","expirationDate":"2009-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["534411","564987"],"PO":["387198"]},"100360":{"abstract":"A Framework for Next-generation Scheduling and Task Management <br\/>for Extreme-scale Computing<br\/><br\/>Kang G. Shin and Abhijit Bose<br\/>The University of Michigan<br\/><br\/><br\/>The objective of this research project is to develop efficient <br\/>algorithms and robust software for scheduling and resource management in <br\/>support of extreme-scale computing. Such computations may involve tens <br\/>of thousands of processors configured as a high-end computing system. <br\/>More efficient scheduling and resource management systems than those <br\/>currently used are needed to address scalability and fault-tolerance for <br\/>such large systems. The same argument also holds for the increasingly <br\/>complex requirements of many emerging applications. The <br\/>current-generation schedulers primarily manage a cluster of CPUs that <br\/>are configured statically for an application. An application is <br\/>allocated a fixed number of processors or nodes (for SMP systems), and <br\/>is not expected to modify its resource requirement during the execution. <br\/>This may result in lower resource utilization. Furthermore, most of the <br\/>current schedulers do not provide transparent fault-tolerance to the <br\/>running workload. One or more processor\/node failures often terminate <br\/>the currently-executing task on these processors, resulting in wasted <br\/>cycles. Fault-tolerant scheduling will be critical to the scalability of <br\/>future HEC systems to an extremely large number of processors. The <br\/>increasing usage of high-end systems for both computation- and <br\/>data-intensive applications requires that future scheduling systems must <br\/>address the problem of co-scheduling CPUs, I\/O and network resources <br\/>when mapping tasks to appropriate resources. In some HEC architectures, <br\/>the placement of processes has an effect on the overall performance of <br\/>the application due to the underlying interconnection topology. Both <br\/>co-scheduling and workload-aware scheduling will be important for future <br\/>high-end computing systems.<br\/><br\/>An integrated software framework will be developed for scheduling <br\/>and resource management for extreme-scale computing systems that <br\/>provides the following capabilities: (i) on-line workload <br\/>characterization, (ii) predictive scheduling based on time-series <br\/>modeling and forecasting of resource utilization and queued workload, <br\/>(iii) transparent fault-tolerant scheduling of applications that are <br\/>interrupted by one or more faults, and (iv) efficient heuristic and <br\/>evolutionary algorithms that consider the workload and resource <br\/>characteristics and forecasts as part of the scheduling decision.<br\/>This work represents a mix of scheduling theory and robust <br\/>software development. These algorithms and the software framework in <br\/>a production HEC environment at the Center <br\/>for Advanced Computing (CAC) at the University of Michigan. The CAC HEC <br\/>facility provides a testbed consisting of over 1400 CPUs representing<br\/>multiple processor families (AMD Athlons, Opterons, Apple Xserve\/G5) and <br\/>several interconnection systems such as Gigabit Ethernet and Myrinet.<br\/><br\/>The intellectual merit of this research will be to advance the <br\/>state-of-the-art in scheduling and resource management for HEC systems. <br\/>By implementing and deploying the proposed framework at CAC, we will be <br\/>able to collect realistic workload traces from a diverse array of <br\/>end-users and applications. This research will serve as a catalyst for <br\/>the development of robust fault-tolerant scheduling algorithms and their <br\/>implementation software for such systems. We specifically address the <br\/>scalability of fault-tolerance mechanisms such as checkpointing\/restart<br\/>and I\/O that can scale across thousands of processors. Furthermore, our <br\/>proposed integration of research, outreach and education activities will <br\/>make broader impacts to other HEC centers and the scheduling research <br\/>community. The framework developed as part of this project and our <br\/>research results will be disseminated to industry and academia through <br\/>open-source software and high-quality publications.","title":"A Framework for Next-Generation Scheduling and Task Management for Extreme-Scale Computing","awardID":"0444417","effectiveDate":"2004-11-01","expirationDate":"2009-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":["553551","284605"],"PO":["565272"]},"94953":{"abstract":"The project concerns high-performance haptic (sense of touch) interaction with three-dimensional (3D) computed (virtual) and real environments. The principal research objective is to quantitatively determine how much reality is achievable in 3D haptic\/visual virtual and remote real environments. The approach is based on six-degree-of-freedom (6-DOF) haptic interaction technology using Lorentz magnetic levitation. Both proprioceptive (kinesthetic) senses of the fingers, hand, and wrist as well as the tactile senses in the skin are involved in the interaction. Lorentz levitation provides higher bandwidths and motion resolutions than are available with traditional technologies. The project includes i) adding direct force-torque sensing, combining favorable aspects of both impedance- and admittance-type devices; ii) the creation of a highly realistic 3D peg-in-hole virtual environment; iii) development of an elastic deformation environment with buckling phenomena; iv) comparison of subjects' interaction with virtual, real, and remote-real environments, and v) performance of a suite of psychophysical experiments to quantitatively measure the degree of reality provided. The quantitative characterization of haptic interaction transparency afforded by this approach contrasts markedly with pure engineering measurements or purely subjective evaluations. The research results provide knowledge for the engineering science of haptic interface design while helping to elucidate the nature of the human haptic interaction process. This could lead to the future widespread use of haptic technology for computer augmented design, medical training, telemanipulation and telepresence systems, vehicle piloting simulation, and the exploration of complex multi-dimensional data sets. The project has an important educational impact which includes the study of haptics in undergraduate and graduate course work.","title":"Quantitative Analysis of 3D Haptic Performance and Perception","awardID":"0413085","effectiveDate":"2004-11-15","expirationDate":"2009-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["524687","438447"],"PO":["387198"]},"94975":{"abstract":"The goal of this research is to improve the ability of computers to recognize objects in the world. That ability is useful for a range of robotic, surveillance, and image interpretation tasks. Procedures will be developed that should allow for a dramatic scale-up in the numbers of different objects a computer can detect, as well as greatly expanding the range of clutter and adverse lighting conditions under which the computer can detect those objects. The key advance to be explored and developed is the sharing of visual features across different objects, as well as other imaging dimensions of viewpoint, lighting, scale, and position. This feature sharing should dramatically improve the speed and efficiency of detecting multiple objects, as well as improve generalization performance. A set of labeled image databases will be developed that will allow researchers to train object recognition algorithms. The techniques developed may be useful in such diverse areas as robotics, manufacturing, and surveillance.","title":"Shared Feature Object Detection","awardID":"0413232","effectiveDate":"2004-11-15","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["515737"],"PO":["564316"]},"98903":{"abstract":"The goal of this project is to create an architecture for sensor network management and use the architecture to develop and evaluate novel network-management tasks. The architecture must be flexible enough to be applicable to a wide range of sensor node types and networking technologies. It must also be lightweight in terms of memory and computational resource requirements. The architecture defines a common interface for manageable components and a new network service for remote interaction with these components. These technologies serve as the basis for new emerging sensor network capabilities such as network bootstrapping, cluster management, and network routing management. This project is developing specifications and implementations of management algorithms, management information models, and sensor network management services. These techniques are being evaluated through in-depth simulations and through execution on a physical testbed composed of low-power, wireless sensor nodes, such as the Crossbow Mica-class Motes. <br\/><br\/>The technologies that are developed through this project will provide a consistent mechanism for management operations in sensor networks, resulting in greater interoperability between technologies developed by different researchers and vendors. The push to standardize descriptions of managed information will provide an example of points for standardization in the sensor network community, encouraging less fragmentation in technology. The specifications, software, and evaluation results from the project will be disseminated via appropriate journals, conferences, workshops, and the Internet.","title":"NeTS-NOSS: Lightweight and Flexible Sensor Network Management","awardID":"0435023","effectiveDate":"2004-11-01","expirationDate":"2009-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["553551",260689],"PO":["564777"]},"94987":{"abstract":"The goal of the project is to develop a formal foundation for the synthesis and analysis of implicit multi-robot coordination mechanisms. Such a formal understanding will allow the multi-robot community to move away from ad-hoc solutions and toward the principled design and analysis of coordinated multi-robot systems. This will be achieved by introducing a formal language to describe the entities interacting in a coordinated multi-robot system, and apply this framework to the principled synthesis of robot controllers using logic-based induction. At the same time, this project will develop a methodology for modeling the coupled robot-environment system and derive the equations describing dynamics of the system. Finally, these procedures will be combined, so that results of analysis can be used to drive performance-enhancing modifications in the robot controller. To validate the formal concepts of the proposed research, these methods will be applied to the design and analysis of multi-robot coordination methods for a real-world sensor\/actuator network deployment and maintenance task. The proposed research is novel in that it combines formal techniques from Computer Science, Mathematics, and Physics. As such, the work will serve as a vehicle for raising the profile of mathematical analysis in the robotics community. Undergraduate and graduate robotics courses will benefit from the inclusion of the developed formal analysis techniques. In addition, twice a year demonstrations will be given at local high schools to teach the students the concepts of collective behavior and give them the skills to better understand and approach complex problems.","title":"Automatic Synthesis and Optimization of Controllers for Multi-Robot Coordination","awardID":"0413321","effectiveDate":"2004-11-15","expirationDate":"2008-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["558935","517887"],"PO":["403839"]},"101540":{"abstract":"Cyber-infrastructure refers to the ability to access and integrate today's hardware, software, and human information technology resources in order to facilitate science, engineering, research and education goals. In the next five years, the challenge for Cyber-infrastructure will be socio-cultural and behavioral dynamics, economics, and policy. The objective of this award is to hold a hold a workshop that will seek to identify for the SBE and CISE Directorates at NSF the key problems in the social sciences, economics, organizational, and policy studies for Cyber-infrastructure. The goal of the workshop will be to stimulate discussion within and between the communities served by the SBE and CISE Directorates about Cyber-infrastructure and to produce a report that outlines some of the key challenges and areas for fruitful research, development and experimentation by SBE and CISE. Specific goals for the workshop are<br\/>1) To produce a report which lays out a Cyber-infrastructure research and development roadmap for the SBE and CISE community and provide a framework for projects and efforts going forward;<br\/>2) To provide a venue for community building within the SBE and IT communities;<br\/>3) To lay out a program for research on the effects of IT on society and the <br\/>dynamics of IT-focused organizations and the cyber-culture<br\/><br\/>Broader Impacts:<br\/>SBE plays a key role in the development of Cyber-infrastructure. Not only is data and computational Cyber-infrastructure an enabler for key efforts for SBE communities, but SBE can play a critical role in the design, development and culture surrounding the deployment of Cyber-infrastructure itself. A strategic approach to the development of a body of research, development, and experimentation that addresses the socio-cultural, economic, and policy challenges of Cyber-infrastructure will be critical to ensure its success. In particular, social and behavioral scientists, humanists, organizational, policy and management researchers, etc. are critical as process builders for Cyber-infrastructure as well as end users of Cyber-infrastructure. Such communities have only sporadically been engaged in the development of Cyber-infrastructure models, social and organizational structures and have a wealth of experience and context to offer.","title":"Social, Cultural, Economic and Policy Challenges for Cyberinfrastructure Workshop; Dec. 1-3, 2004; Washington, DC","awardID":"0451212","effectiveDate":"2004-11-01","expirationDate":"2005-10-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"1320","name":"ECONOMICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}}],"PIcoPI":["562621","563532"],"PO":["352775"]},"100352":{"abstract":"Current high-end applications usually exploit just a fraction of the theoretical performance of <br\/>large platforms. Interactions among the hardware, system software, programming interface, and <br\/>algorithm are extremely complex, and the implications for development of hybrid MPI+OpenMP <br\/>Fortran\/C\/C++ applications are challenging. The emerging generation of machines will be even <br\/>more complex, as will the applications that exploit them. Typical application development and <br\/>tuning scenarios involve the manual and separate use of compilers and performance tools, and <br\/>program modifications based upon insights laboriously gleaned from their output. <br\/>In this proposal, we intend to raise the quality of the application development and tuning <br\/>process by creating an integrated environment for program optimization that reduces the manual <br\/>labor and guesswork of existing approaches. We will develop strategies and the corresponding <br\/>interfaces that enable the application developer, compiler and performance tools to collaborate to <br\/>generate optimized code based upon a variety of sources of feedback, including performance <br\/>data from .offline. development runs as well as from .online. production runs. We will build <br\/>and deploy a flexible, working system that combines robust, existing, open source software . a <br\/>compiler, a program analysis tool and two performance tools with complementary features - into <br\/>a single, coherent environment for collaborative static and dynamic application tuning. <br\/>Application codes of varying complexity supplied by our application partner will motivate our <br\/>development work as well as test and demonstrate our results. The result will be a powerful, <br\/>integrated environment that can be used to obtain traditional performance data via program <br\/>monitoring, event tracing and\/or the extraction of hardware counter information, and to obtain <br\/>support for the static or dynamic tuning of an application code. <br\/> <br\/>Intellectual Merit <br\/>The proposed environment integrates several different existing technologies to provide a new <br\/>level of support for optimizing hybrid MPI+OpenMP codes. Support for experimentation with <br\/>the hybrid programming model is provided. The range of information that may be exploited by <br\/>the compiler to optimize code is expanded to cover many system and application-level <br\/>phenomena and a variety of optimization scenarios. Interactions between tools will facilitate the <br\/>provision of an approach that is able to handle extreme-scale computations. Integration issues <br\/>will be addressed with the goal of creating a deployable, extensible system. <br\/> <br\/>Broader Impacts <br\/>The tools, ideas, and results of this project will be freely distributed and made available to the <br\/>HPC community, nationally and internationally. Besides the general dissemination of results, the <br\/>project has strong ties to performance engineering experts at NCSA. The tools will be installed <br\/>on NCSA systems for testing and evaluation and will be made available to other users. The <br\/>research brings together compiler, performance tool, and application developers, enriching the <br\/>research experience of graduate students to create a well-rounded IT workforce. The PI is active <br\/>in the OpenMP community and the project team has close working relationships with DOE and <br\/>DOD. New knowledge generated will be integrated into advanced graduate coursework.","title":"Collaborative Research: Performance Toolset for Dynamic Optimization of High-End Hybrid Applications","awardID":"0444363","effectiveDate":"2004-11-01","expirationDate":"2008-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7417","name":"S AND T HIGH-END COMPUTING"}}],"PIcoPI":[264783],"PO":["565272"]}}