{"205513":{"abstract":"Computational inefficiency is a common experience: the computer cannot complete a certain task due to lack of resources such as time, memory, or bandwidth. Computational complexity theory classifies -- or aims to classify -- computational tasks according to their inherent inefficiency. Since tasks requiring excessive resources must be avoided, complexity theory is often indispensable in the design of a computer system. Inefficiency can also be harnessed to our advantage. Indeed, most modern cryptography and electronic commerce rely on the (presumed) inefficiency of certain computational tasks.<br\/><br\/>The objective of the proposed research is to make progress on several mutually enriching directions in computational complexity theory, including problems at the intersections with algorithms and cryptography. Building on the principal investigator's (PI's) previous works, the main proposed directions are:<br\/>- computational tasks whose inputs are distributed among several computers which communicate either by broadcast or through a dynamic network,<br\/>- structures that store data compactly while allowing efficient answers to queries,<br\/>- linking the inefficiency of computational tasks by establishing reductions among them, and<br\/>- compiling any cryptographic protocol into a circuit that remains secure even when in the hands of an adversary who observes it during execution.<br\/><br\/>This research is closely integrated with a plan to achieve broad impact through education. The PI is reshaping the theory curriculum at Northeastern on multiple levels. At the undergraduate level, the PI is working on and using in his classes a set of lecture notes aimed towards students lacking mathematical maturity. At the Ph.D. level, the PI is including into core classes current research topics including some of the above. Finally, the PI will continue to do research working closely with students at all levels.","title":"AF: Small: Research in Complexity and Related Areas","awardID":"1319206","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":[550308],"PO":["565157"]},"205755":{"abstract":"An ever-growing number of people and organizations find themselves awash in data. Virtually every domain has been affected by our increasingly powerful ability to monitor and record any kind of data that might be relevant. Unsurprisingly, a strong desire to better understand the data and communicate to others has accompanied this widespread access. Visualization is one technique that can be particularly effective for data analysis and presentation. The ability to represent data and its attributes in a visual manner often allows people to more easily grasp important characteristics of the data and to reach conclusions about its value. Similarly, people frequently prefer to represent data visually when explaining it to others. Unfortunately, to create a data visualization today, a person either needs to be a programmer and use libraries and toolkits developed for that purpose or the person needs to use (typically commercial) systems that provide limited visual representations. Neither solution is appropriate for data scientists who are not programmers but seek to design custom visualizations for the unique aspects of the data they explore. This research is developing new methods of creating visualizations without requiring the use of programming. An initial step of the research is identifying the core primitives of the data visualization process, both with respect to representation and interaction. The research is also formulating a theory and model of how to represent these primitives so that people can easily specify their design intent, thus providing an expressive framework for constructing visualizations. Ultimately, the framework is embodied through the development of a prototype visualization construction system that empowers users to create custom visualizations. Evaluations of the effectiveness of the primitives, the conceptual framework, and system complete the project.<br\/><br\/>The results of this project are expected to have broad impacts in a variety of domains and applications. When people from a variety of fields such as journalism, business, science, and health are able to rapidly develop visualizations of their data, the disciplines themselves will benefit from stronger abilities to analyze data and communicate knowledge from the data. The ability to construct visualizations without programming also enables younger students who have not yet studied computer science to engage in data analysis and visualization design. This research will create learning modules about visualization and data analysis for local high schools and students with a goal to grow interest within the students for areas such as data science, visualization, and human-computer interaction. Additionally, this project provides education and research experience for graduate students. Recent noteworthy national research agendas have identified the need for more students and young researchers trained in STEM-related disciplines. The resulting visualization construction system being created will be freely available on the internet for anyone to use. All results of the research will be communicated through academic venues and the publicly-accessible website (http:\/\/www.cc.gatech.edu\/gvu\/ii\/designvis).","title":"CGV: Small: Creating Information Visualizations without Programming","awardID":"1320537","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[550876],"PO":["563751"]},"205524":{"abstract":"Research in wireless sensor networks has been very successful in creating academic testbeds and short term real deployments for many application areas such as home health care, saving energy in buildings, infrastructure monitoring, agriculture, and various environmental science applications. However, significant new problems arise for long term deployments in uncontrolled environments. It is also important to note that most of these applications perform activity recognition. Yet, these activity recognition solutions are not always robust enough for long term deployments. Consequently, the goal of this work is to develop robust and reliable activity recognition for in-home deployments that address the realism of long term deployments. To accomplish this goal requires new research results in: obtaining labeled ground truth for training activity recognition systems, recognizing overlapping activities, detection of activities that occur across rooms of a home, handling missing sensor events and sensor failures, addressing the issues of multiple person homes and visitors, and handling the evolution of human behaviors. These solutions must be combined in a holistic manner. In addition, the utility of activity recognition often depends on recognizing anomalies from typical human behaviors. Anomaly detection can also suffer from the realisms of long term deployments and, therefore, is also addressed. The basic research approach includes employing data mining, machine learning, and other techniques in robust ways that account for realisms in long term deployments. Demonstration of the utility of the solutions spans from lab experiments to realistic long term deployments for 9 months or longer.<br\/><br\/>The broad significance of this work occurs because developing robust activity recognition schemes for wireless sensor networks that operate for long time periods implies improvement in applications such as home health care and saving energy in homes and buildings. Home health care can save lives, provide improved life style and greater independence for the elderly and chronically ill, lower medical costs, and via longitudinal studies, increase understanding of the causes of diseases. Energy is a scarce resource and improved activity recognition can be used to perform control actions that save energy. This energy savings can save money and lower the impact of global warming.","title":"CSR: Small: Realism in Activity Recognition for Long Term Sensor Network Deployments","awardID":"1319302","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[550342],"PO":["565319"]},"205645":{"abstract":"This project will study the situated practices, human assumptions, and organizational routines that transform \"little data\" into mineable stores of \"big data\" harnessed for measures and metrics. Currently, our knowledge about the origins of big data and what goes into collecting, curating, manipulating, and deploying these huge information resources is limited. We know even less about the social and cultural implications of these activities, particularly in non-academic contexts. A growing group of scholars urge critical interrogation of the methods, analytical assumptions, and underlying biases of big data science. A nuanced understanding of the situated practices through which big datasets are assembled and manipulated is required before we can comprehend their social and political implications, particularly if we are to evaluate the quality of the scientific results based on analysis on the manipulation of such datasets.<br\/><br\/>The research will be carried out through a multi-sited ethnography of obstetrical data production in healthcare, an area where big data and associated metrics are both important and problematic. First, it will examine the situated practice and lived experience of creating the massive amounts of information that come to form the datasets. Second, it will trace how the results emerge through automatized measures and algorithms and affect the very environments they are supposed to reflect. This research spans the lifecycle of data. It will investigate how information is collected by practitioners, clerks, and coders and transformed into local repositories of supposedly \"clean\" data to be manipulated by performance improvement specialists. It will then trace how information is transferred and refined further in a statewide data center and deployed by a major quality improvement organization. Finally, the research will follow the aggregated data back to the local hospitals themselves and assess how data visualizations and performance measures affect local decisions and hospital functioning.<br\/><br\/>The broader impacts of this project include both near and long-term benefits. In the short term, this research will benefit the individuals and organizations struggling with questions about how to organize local resources to produce and deploy big data in service of management and performance improvement goals. In the long term, this research will generate foundational conceptual models that help to create design recommendations and practice guidelines regarding the social, ethical, and political implications of creating and using big data.","title":"HCC: Small: Creating a Data-Driven World: Situated Practices of Collecting, Curating, Manipulating, and Deploying Data in Healthcare","awardID":"1319897","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[550621,550622],"PO":["564456"]},"205766":{"abstract":"Increasingly, location-aware datasets are of a size, variety, and update rate that exceed the capability of spatial computing technologies. This project addresses the emerging challenges posed by such datasets, which sometimes are also referred to as Spatial Big Data (SBD). SBD examples include trajectories of cell-phones and GPS devices, temporally detailed (TD) road maps, vehicle engine measurements, etc. SBD has the potential to transform society. A recent McKinsey Global Institute report estimates that personal location data could save consumers hundreds of billions of dollars annually by 2020 by helping vehicles avoid congestion via next generation routing services such as eco-routing. Eco-routing may leverage various forms of SBD to compare routes by fuel consumption or greenhouse gas (GHG) emissions rather than total distance or travel-time. <br\/><br\/>To develop next-generation eco-routing services, this project innovates in three areas. Frist, Lagrangian Xgraphs, a novel concept in computer science, is explored at conceptual, logical and physical database levels to model traveler's frame of reference, a major departure from traditional binary relationship (e.g., adjacency) graphs. Second, it probes the concept of route-collections, and scalable algorithms for finding route-collections. For example, to identify a route-collection over all possible start-times of a given time-interval, the project explores a critical time point approach which divides a given time-interval into a set of disjoint sub-intervals of stationary-rankings among alternative routes. The approach is not only novel but also very important for the field. Critical time points may become a vital component of dynamic programming (DP) solutions, which would need reconsideration in the face of emerging temporally detailed SBD that violate DP assumptions about stationary ranking of alternate solutions. Third, to address the increasing diversity of SBD methods, algorithm-ensembles and flexible architectures that allow rapid integration of new data sources and routing algorithms are developed. <br\/><br\/>The proposed work serves national goals for energy independence and sustainability by laying the ground work for eco-routing and other travel-related services that reduce fuel consumption and greenhouse gas emissions. By increasing the availability of SBD, the project also enhances the research infrastructure for other researchers. Educational activities include curriculum development and training of students in the emerging area of SBD and Eco-routing. Result dissemination is planned via publication in relevant peer-reviewed conferences and journals. More details are available on the project website (www.spatial.cs.umn.edu\/eco-routing\/).","title":"III: Small: Investigating Spatial Big Data for Next Generation Routing Services","awardID":"1320580","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550903],"PO":["563751"]},"205656":{"abstract":"This project continues the development of the AITopics information portal, a service to the Artificial Intelligence research community and an educational resource for the general public. The portal can be found at www.aitopics.org. This site was originally conceived as a compendium of introductory and historical articles about artificial intelligence for use by the AI community as well as the general public. The initial prototype was a manual effort conducted by volunteers at the Association for the Advancement of Artificial Intelligence (AAAI). Following this, an NSF-funded effort was undertaken in 2012 to reduce the time required of volunteers without reducing the quality of information provided. This project advances the sophistication of the portal by automating the time-consuming process of selecting content from the literature to post on the website, and by enhancing a deployed AI news finder program employed to feed the site with fresh content. The ultimate goal of this work is to build a generally useful content management framework that would be applicable to such outreach effort across all sciences. <br\/><br\/>The project activities include A) development of the NewsFinder program to browse online AI journals as well as current periodicals to find interesting overview articles; B) creation of additional content management technology including tools for user interaction and feedback; creation of meta-data for search engines, creation of summary descriptions; automatically learning criteria for interesting items; and creation of tools to identify new topics as the field changes; C) continued curating of online versions of classic books and papers, including scanning material only available in hard copy; D) extension of NewsFinder to find articles describing new applications in each Applications area; E) as well as to provide useful information to practitioners in a new area of technology, including classification of IAAI papers by industry and type of problem; and F) expansion of the use of social media and mobile devices to refine and deliver information.","title":"EAGER: Aggregating Online Information in Science","awardID":"1319941","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550649],"PO":["565035"]},"205777":{"abstract":"Mechanism design is the science of designing the rules of a game (e.g., auction or election) so that good outcomes ensue despite each participant (human or computational) acting based on self-interest. Intuitively, allowing individuals or organizations to express richer preferences should yield better outcomes. Billions of dollars of annual savings from outcome efficiency improvements due to increased expressiveness have indeed been observed, for example, in combinatorial sourcing auctions by the PI and others. What is missing is a rigorous methodology for designing appropriately expressive mechanisms. This project combines new theoretical results in mechanism design - including computational measures of expressiveness - with custom search algorithms and machine learning techniques. The goal is to create knowledge about mechanism design with varying levels of expressiveness. The work also involves developing an operational methodology to guide the design of appropriately expressive mechanisms across a broad class of combinatorial and multi-attribute domains. Furthermore, the work will yield new theory and computational methodologies for bundling. The objects of study are rational agents and agents with forms of bounded rationality.<br\/><br\/>While many of the results will be general, the work will be validated in qualitatively different applications such as combinatorial auctions, advertising markets, and catalog-offer-based (web) commerce. In the US alone, combinatorial multi-attribute sourcing auctions give rise to tens of billions of dollars in annual trade. Annual volumes associated with advertising markets, spectrum auctions, consumer-to-consumer auctions, and catalog-based commerce are all in hundreds of billions or trillions of dollars. Improvements would thus offer substantial economic and societal benefits.","title":"RI: Small: Expressiveness and Automated Bundling in Mechanism Design: Principles and Computational Methodologies","awardID":"1320620","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["561267"],"PO":["565035"]},"204567":{"abstract":"The Intellectual Merit of this project is the CrossCloud design for a practical, open, federated data base system, based on existing web technologies including Linked Data. CrossCloud's design solves longstanding database federation challenges in a simple and effective way using the Linked Data vision of RDF data sources which use URLs as object identifiers. CrossCloud includes a \"data registration\" mechanism to create a distributed search service for data and a vocabulary mapping system to connect applications developed independently with these components. CrossCloud is decentralized, capable of managing queries and updates from across the internet with no central bottleneck. The Broader Impact of this work begins with making it easier for developers to share users across an industry and then be able to reach a critical mass. Nearly any situation calling for coordinated work among people can benefit from multi-user software and with CrossCloud changing the software market, the needed solutions can finally be put into widespread use. This EAGER grant proposal is being made at a critical time, as the underlying technologies fall into place and enterprising developers begin to settle into working with mobile devices and cloud infrastructure.","title":"EAGER: Demonstrating Decentralized Social Software using Linked Data (Crosscloud)","awardID":"1313789","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}}],"PIcoPI":[547850],"PO":["565292"]},"206514":{"abstract":"In this Cyberlearning: Transforming Education DIP (Development and Implementation) project, the PIs focus on understanding the kinds of tools and help for teachers that will support them as they integrate use of learning technologies into their classroom practices. There are a variety of challenges in creating and managing \"blended learning\" environments where students are learning in the classroom and also using tools and resources online and communicating with distant collaborators online. The PIs posit seven critical research and development needs in learning how to design such environments and help teachers learn to practice in these environments: better understanding of the types of interactions that are uniquely afforded by networked technologies and the ways those interactions contribute to learning; shared datasets that document and link face-to-face and online instructions in ways that enable analysis of learning outcomes; design principles for online learning tools to be integrated into such learning ecosystems; flexible instructional tools for teachers and data that help teachers to adjust classroom orchestration and allow them to realign online interaction structures; sharable frameworks for analyzing the affects of online learning tools; appropriate collaboration tools; and professional development facilities. The PIs in this project take a design research approach to the design of these technologies and approaches and to investigating the ins and outs of socio-technical systems that best support blended learning -- the types and patterns of online interactions that create opportunities for learning, what leads to learning outcomes, how and which data to capture to analyze learning, the kinds of analyses that are needed to inform understanding of interactions within online social learning networks, and ways of supporting practitioners in effectively using the affordances of the socio-technical system to create learning ecosystems that promote digital literacy and learning of 21st century skills along with disciplinary learning. This research is being carried out in the context of the robust ecology of the Digital Youth Network (DYN), an already-existing infrastructure for connecting youth to each other in after-school contexts that includes a variety of tools for digital inquiry and expression.<br\/><br\/>While many teachers understand the utiity of technology in their personal lives, the norms of classroom instruction make it difficult for the majority of teachers to make effective use of networked technologies in their classrooms. However, it is essential to make sure that our educational systems integrate digital and technological learning experiences into daily learning activities to ensure that all youth become technologically literate and learn the skills they need for engaging in the 21st century workforce. This project focuses on designing \"blended learning\" environments and helping teachers take on and learn to productively use digital technologies in their classrooms to help students learn disciplinary content, disciplinary skills, and the many communication, collaboration, and inquiry skills so important to their futures. As well, investigation focuses on how to use the data collected while learners are engaging in learning activities online to help teachers visualize the capabilities and understanding of individual students so as to be able to give appropriate attention to every student's needs. It is expected that analysis of the data collected, along with future analysis by other researchers, will contribute novel forms of online analytic tools to document the relationship between interactions and outcomes related to digital literacy, including evidence of skills and understandings and also other important capacities such as developing interest, spreading knowledge, engaging in creative practices, taking on independent learning, and expanding ideas for future careers.","title":"DIP: Developing Frameworks, Tools and Social Practices to Support Effective Instructor use of Online Social Learning Networks in Blended Learning Models","awardID":"1325004","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":["559581",552902],"PO":["562669"]},"205304":{"abstract":"A distributed algorithm is one that runs simultaneously on multiple processors where pairwise communication is allowed. \u00a0The main goal of this project is to design \"super-fast\" distributed algorithms, i.e., algorithms that run in sub-logarithmic time in a distributed setting. Distributed algorithms that run in logarithmic time (or even poly-logarithmic time) have traditionally been considered fast. A well-known example of a fast distributed algorithm is the 25-year old algorithm for the maximal independent set (MIS) problem due to Luby that requires a logarithmic number of communication rounds. The main premise of this project is that for a variety of fundamental problems in distributed computing, it is possible to design super-fast distributed algorithms. The project aims to develop algorithmic techniques towards this goal and to apply them to classic distributed computing problems such as MIS, node coloring, and a host of other distributed optimization problems.\u00a0<br\/><br\/>As networks increase in size, become more dynamic (e.g., peer-to-peer and mobile networks), and become less reliable (e.g., in wireless settings), there is a greater need for super-fast algorithms. One simple motivation is that if an algorithm is super-fast, it can simply recompute its output in response to a change or a fault in the underlying network.\u00a0The work proposed here will advance the state of the art in distributed algorithms. Some of the problems to be studied are old ones where recent work suggests the possibility of progress. Others are new ones where existing techniques seem inadequate. While the main focus of this project is basic research, applications of super-fast algorithms in wireless, peer-to-peer, and mobile networks will be natural and expected outcomes.","title":"AF: Small: Super-Fast Distributed Algorithms","awardID":"1318166","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[549755],"PO":["565251"]},"205667":{"abstract":"","title":"AF: Small: RUI: Faster Arithmetic for Sparse Polynomials and Integers","awardID":"1319994","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[550673],"PO":["565157"]},"205788":{"abstract":"An increasing amount of software depends on the network, and at the same time networks themselves are becoming more programmable via technologies like OpenFlow, opening a wide range of opportunities both for network efficiencies and in-network computation. While new programming models seek to make network reconfiguration easier, none provide tools to solve the joint problem of software-controlled routing and placement of in-network computational services. This project will design and build a domain specific language (DSL), called Flange, for managing services in networks. The project's approach marshalls configurable forwarding and computing devices in the network, with pervasive monitoring to support service adaptation. The project includes building and releasing a prototype compiler, runtime, system and front-ends that enable Flange programming both with dedicated syntax (standalone DSL) and embedded in host languages. <br\/><br\/>The success criteria for this project is that a network administrator using the prototype is able to set a new service (e.g. video-transcoding) using less than a page of Flange code. Using the same mechanism, a network engineer should be able to launch a persistent function that monitors network conditions and responds to saturation by redirecting traffic to an alternate path. The novelty of the project is twofold. First, the semantic foundation at the root of the project treats both hardware and software aspects of networks uniformly, as constituting a single process network. For example, the project models forwarding as an in-the-network service, while considering it in the same light as other potentially instantiated services in the network such as transcoding or compression. Second, the Flange language will provide novel global view programming abstractions for large networks. Flange programs will be able to inspect network entities, aggregate information, signal and respond to events, all without reference to their own place of execution (which is often physically distributed). <br\/><br\/>The work proposed here stands to impact network operational efficiency and flexibility for national science-focused cyberinfrastructure as well as in commercial applications, where the economic impact of networked computing is clear. In addition, the design decisions the underly Flange are pedagogically relevant and will be presented in courses taught by the PIs.","title":"NeTS: Small: Flange: A Domain Specific Language for Network","awardID":"1320659","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["561820","556710"],"PO":["564993"]},"198781":{"abstract":"Big data today is stored in a distributed fashion across many different machines or data sources. This poses new algorithmic and system challenges to performing efficient analysis on the full data set. To address these difficulties, the PIs are building the MIDDLE (Mergeable and Interactive Distributed Data LayEr) Summarization System and deploying it on large real-world datasets. The MIDDLE system builds and maintains a special class of summaries that can be efficiently constructed and updated while still allowing fine-grained analysis on the heavy tail. Mergeable summaries can represent any data set with a guaranteed tradeoff between size and accuracy, and any two such summaries can be merged to create a new summary with the same size-accuracy tradeoff.<br\/><br\/>Interactive summaries can be quickly adapted to a specified query range of data while maintaining the same size-accuracy tradeoffs relative to the data in that range. This allows accurate efficient analysis to zero-in on small subsets of big data.<br\/>The MIDDLE system enables different big data users to develop a wide spectrum of efficient and scalable data analytic tasks through the use of data summaries. The MIDDLE system is being evaluated and refined with the aid of domain experts. Since the prospect of data-summary-based analytics becoming a part of standard techniques in processing big data is tantalizing, this research generates broader impacts on the nation's government agencies, research institutes, education system, and high-tech industries. Our broad impacts also extend to academia and community outreach, through the design and development big data curriculum and education, and the involvement of general public in understanding and using big data through concise summaries.","title":"BIGDATA: Small: DCM: DA: Building a Mergeable and Interactive Distributed Data Layer for Big Data Summarization Systems","awardID":"1251019","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8083","name":"Big Data Science &Engineering"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"8074","name":"EarthCube"}}],"PIcoPI":["548382","465140"],"PO":["565272"]},"205557":{"abstract":"Classical algorithms for many important graph primitives were designed at a time when the conventional notion of efficiency was polynomial running time. However, many of today's applications involve graphs consisting of millions, or even billions, of nodes. On these massive inputs, practical algorithms must run in time that is as close to linear as possible in the size of the input.<br\/><br\/>The spectral approach to designing graph algorithms views the instance graph as a matrix and makes use of the algebraic properties of the corresponding linear operator. Recently, research in this area has led to the design of faster spectral algorithms for many essential graph problems, such as electrical flow, maximum flow and graph partitioning in undirected graphs. The goal of this project is to develop a novel algorithmic approach by combining spectral methods and the idea of regularization from optimization. Regularization is a mechanism for modifying a given optimization problem to make it more amenable to known algorithms without changing its salient characteristics. Surprisingly, many of the recent breakthroughs in the design of fast spectral algorithms can be viewed as applying different types of regularization. This research aims to exploit this interpretation to design algorithms that are faster, simpler to analyze and easier to implement. Another aim of this research is to integrate different perspectives on regularization from Machine Learning, Statistics and Convex Optimization, to create new bridges between these fields and the design of algorithms.<br\/><br\/>Due to the practical importance of the graph problems under consideration, the work will also focus on empirically evaluating the algorithms designed. These evaluations will be disseminated to the relevant audiences to maximize the impact of the award. Moreover, because this project aims to develop new fundamental techniques in the design of algorithms, a particular effort will be devoted to incorporating material from this research into the PI's teaching activity and to preparing educational material for the public.","title":"AF: Small: New Perspectives on Special Methods for Graph Algorithms","awardID":"1319460","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550419],"PO":["565157"]},"205678":{"abstract":"This Cyberlearning: Transforming Education project brings together leading-edge researchers in computational linguistics and computer-supported collaborative learning to explore feasibility issues in designing an intelligent conversational agent that interacts with groups of learners as they are working together and provides cognitive, meta-cognitive, and social advice to enhance their collaborations. The conversational agent is being designed to enhance group dynamics by providing supportive advice as well as promoting awareness of the dynamics. Design of the agent is informed by theories of meta-cognition, collaborative learning, and accountable talk. In addition to investigating how to design such agents and testing their effectiveness, investigators are exploring how learning develops when a conversational agent is available to help with productive talk, the influences of conversational behaviors on developing understanding and actions, and ways of influencing understanding and behaviors.<br\/><br\/>One of the biggest limitations of online education, especially massive online courses (e.g., MOOCs), is personal contact with instructors and peers. While much of formal education focuses on faculty interactions with large groups of students in lectures, much student learning depends on interactions with teachers and teaching assistants and discussions, study sessions, and project work done with peers. Massive online education will be broadly successful only after we know how to provide these kinds of supports to online learners. This project focuses on designing an interactive agent that can provide conversational support for small groups of collaborating students to help them productively solve problems and learn together. The agent is being designed to provide the kinds of advice that a teacher or teaching assistant would provide if such a human mentor were available. The researchers conjecture that with such support available, online students can provide for each other the kinds of social supports for learning that contribute in integral ways to students' learning experiences in higher education. The agent is being deployed and tested in several different online courses. Results are expected to be applicable to massive online education as well as to blended educational environments such as flipped and project-based classrooms.","title":"EXP: Collaborative Research: Fostering Ecologies of Online Learners through Technology Augmented Human Facilitation","awardID":"1320064","effectiveDate":"2013-09-15","expirationDate":"2016-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7511","name":"TUES-Type 2 Project"}}],"PIcoPI":["554179"],"PO":["562669"]},"205568":{"abstract":"Linear differential equations with rational function coefficients appear in a wide variety of problems in mathematics, science, and engineering. The topic in this project is the study of CIS-equations, which are linear differential equations with a Convergent Integer power Series among their solutions. The investigator has made an unexpected observation: All CIS-equations of order less than 4 appear to be solvable in terms of algebraic and hypergeometric functions. This observation is surprising because CIS-equations are common in many areas of research.<br\/><br\/>At the moment, finding such solutions is time consuming. Computer algebra systems such as Maple and Mathematica often fail to find these closed form solutions, leading users to the incorrect conclusion that closed form solutions are rare. The goal in this project is to develop algorithms that will solve every CIS-equation of order less than 4. To do this, the investigator will build on prior work done with his graduate students, and will use techniques from modular curves, number theory, Belyi maps and dessins d'enfants.<br\/><br\/>The benefit to society is manifold but indirect; computer algorithms do not build bridges, but they are useful for designing bridges, studying ocean waves, fiber optics, quantum mechanics, population dynamics, etc., the list of applications of differential equations is long and diverse. The goal in this project is to develop a complete solver for a class of differential equations that is very common in diverse fields of study such as combinatorics, or the Ising model in physics. Researchers working in such areas will benefit significantly and will save much time when the proposed algorithms have been developed.","title":"AF:Small: Linear Differential Equations with a Convergent Integer Series Solution","awardID":"1319547","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[550445],"PO":["565157"]},"205689":{"abstract":"The award studies the structure of Boolean functions - functions of many inputs whose output is binary. The main goal of the award is to study the influence of individual inputs on the output, as well as the stability of the output to random perturbations applied to the vector of inputs. The study of influences and stability will be used to obtain new algorithms for learning from data and for analyzing markov-chain sampling procedures using generalizations of the Fourier expansion. It will be further used in conjunction with tools from the theory of hardness of approximation to show that for some combinatorial optimization problems it is computationally hard to find (even) an approximately optimal solution. A main theme of the award is to obtain new results on the geometry of the Gaussian measure as a key step for analyzing stability of Boolean functions. This follows the general philosophy of invariance principles which argue that in certain situations functions of bits and functions of Gaussian variables, which are easier to analyze, behave similarly. The award will support the educational efforts and mentoring of new graduate students by introducing to them new connections between pure mathematics, theoretical computer science and their applications.<br\/><br\/>A key feature of modern computation is its binary nature. The award will investigate binary and other computational models motivated by the following questions: How robust are these computational models? How well can they classify data? How well can they be used to solve large optimization problems? The approach is based in parts on deep tools from mathematical analysis including the study of geometric problems in high dimensions.","title":"AF: Small: Boolean Functions: Inequalities, Structure, Algorithms & Hardness","awardID":"1320105","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":[550722],"PO":["565157"]},"209924":{"abstract":"The research objective of the TeraNets project is to establish the theoretical foundations of ultra-broadband communication networks in the terahertz (THz) Band. New channel models for the entire THz Band will be developed. These will account for line-of-sight, non-line-of-sight and multi-path propagation and will capture the peculiarities of molecular absorption and scattering from rough surfaces. Bandwidth-adaptive modulations to exploit the distance-dependent behavior of the bandwidth at THz Band frequencies will be developed. In addition, massive MIMO transmission schemes to increase the communication distance and novel channel coding approaches to efficiently overcome channel errors at THz Band frequencies are to be investigated. Finally, novel Medium Access Control protocols for THz Band communication networks are proposed, which capture the peculiarities of the physical layer and take advantage of large antenna arrays and dielectric mirrors.<br\/><br\/>The TeraNets project is expected to pave the way for research in communication networks in the THz Band. Ultra-broadband communication networks will play a major role in society within the next ten years by drastically increasing the capacity of wireless networks, and enabling long-awaited applications not possible with current wireless technologies. In addition, the THz Band is not yet regulated. The project team is actively involved in IEEE 802.15 WPAN Terahertz Interest Group (IGThz), whose objective is to create the first standard for this paradigm. The research results will be incorporated into the Information and Network Theory and Wireless Communication and Networks within the School of Electrical and Computer Engineering, at the Georgia Institute of Technology, both at undergraduate and graduate levels. This, combined with grauate student resarch mentoring, will produce new experts in this nascent field. The research results will be disseminated in important scientific conferences, journals and premier magazines of the field.","title":"CIF: EAGER: TeraNets: Ultra-Broadband Communication Networks in the Terahertz Band","awardID":"1349828","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[562752,562753,562754],"PO":["564924"]},"205579":{"abstract":"As the computing industry steps into the many-core regime, the main memory system has<br\/>become one of the key bottlenecks that limits both performance and scalability. To address these<br\/>challenges, the memory industry is developing 3D-stacked DRAM technology. Die stacking can<br\/>provide lower latency, much higher bandwidth, and significantly reduced energy dissipation.<br\/>Unfortunately, stacked memory is unlikely to have sufficient capacity to completely replace<br\/>traditional DRAM. Therefore, future memory systems will likely use stacked memory in<br\/>combination with off-chip DRAM, either architecting stacked DRAM as a giga-scale cache or as<br\/>heterogeneous main memory. However, to fully utilize the potential of stacked memory, the<br\/>system architecture must make choices that exploit the unique latency and bandwidth<br\/>characteristics offered by stacked DRAM. For example, simply applying traditional \"well-understood\"<br\/>cache designs and memory designs to stacked DRAM results in low performance<br\/>and poor bandwidth utilization.<br\/><br\/>This project first looks at caching organizations and management strategies for stacked DRAM<br\/>that are tailored to exploit latency and bandwidth properties of 3D stacking. It then looks at<br\/>memory organizations that can incorporate stacked memory as part of memory address space,<br\/>without relying on OS support to exploit temporal locality and still perform memory<br\/>management at fine granularity. Finally, this project investigates morphable architectures that can<br\/>dynamically reconfigure the stacked DRAM between cache structure and main memory, in order<br\/>to conserve power and optimize performance depending on the workload requirements. The<br\/>research solutions in this study will thus help future systems obtain an order of magnitude<br\/>improvement in both bandwidth and energy-efficiency from the effective use of memory<br\/>stacking.","title":"SHF: Small: Architecting Stacked DRAM as Gigascale Cache, or Fast Memory, or Both","awardID":"1319587","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":[550473],"PO":["366560"]},"209814":{"abstract":"The Annual Meeting of the Association for Education Communications and Technology (AECT) provides a forum for interchange of ideas around designing technological support for learning and training. Its Early Career Symposia provide an avenue for early career scholars to receive mentoring from established researchers.<br\/><br\/>This project supports travel for advanced graduate students and new faculty from U.S. universities to attend the Annual Meeting and Early Career Symposium. Sessions during the Symposium are designed to help collaborators imagine forward-looking and viable technology-oriented research agendas, identify the types of collaborators who complement their strengths, and identify the funding agencies and programs that might support their work. An important goal of the Symposium is to add to the community of researchers interested in ways that technology can transform teaching and learning.<br\/><br\/>This activity supports the mission of NSF to train more advanced professionals in science, technology, engineering, and mathematics education. This conference is one among many that researchers in the cyberlearning community regularly attend. Mentors come from the AECT community as well as from the learning sciences community.","title":"CAP - Building a Technology Research Agenda - An Early Career Symposium","awardID":"1348435","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}}],"PIcoPI":[562467],"PO":["562669"]},"209704":{"abstract":"This award will support student travel to The Fourth Annual ACM Conference on Systems, Programming, Languages and Applications: Software for Humanity (SPLASH 2013). The conference is being held in Indianapolis, Indiana during October 26-31, 2013. SPLASH includes OOPSLA, one of the flagship ACM SIGPLAN conferences, as well as many co-located events: Onward!, Wavefront, the Dynamic Languages Symposium, SPLASH-E (Education), the International Conference on Generative Programming: Concepts & Experiences, the International Conference on Software Language Engineering. SPLASH also includes a program of tutorials, posters, demonstrations, a Doctoral Symposium, and an ACM SIGPLAN Student Research Competition. Supporting student travel to attend professional conferences and workshops is a very important mission of the NSF. The funding provided will enable broader student participation, especially by those without adequate support at their home institution, and will help build the next generation of programming language and software engineering researchers.","title":"SPLASH 2013 Travel Support","awardID":"1347630","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[562118],"PO":["564588"]},"209825":{"abstract":"Molecular computation by cells helps them process signals in their environment to make important decisions. Such computation in cells is both analog, which uses signals with shades of grey and operations that are probabilistic or graded, and also digital, which uses \"on\" and \"off\" signals and operations that are logical. The hybrid analog-digital computation in the cell is an extremely important reason for why it is highly efficient in its use of energy and molecular parts (proteins, RNA, and DNA molecules), which are severely limited in number compared with artificial computational systems. For example, current digital microprocessors have nearly a billion electronic parts and a relatively plentiful supply of energy. The cell is several orders of magnitude more energy and part-count efficient than even the best digital computers of the far future are ever expected to be. For advancing the foundations of computer science beyond its boundaries, and for the future of biotechnology and medicine, it is important to understand and to artificially engineer hybrid analog-digital computation in living cells. The proposal focuses on how to engineer a hybrid analog-digital computational automaton in microbial cells that can enable it to sense, amplify, and process analog input molecular signals into pulsatile digital output molecular signals.<br\/><br\/>Intellectual Merit: <br\/>This work combines innovations and knowledge from several disciplines to potentially create a paradigm-changing capability in the fields of synthetic biology, systems biology, molecular programming, biotechnology, computer science, and medicine: 1) Hybrid analog-digital automata can perform complex computations with very few parts and little energy, and the research here can help advance fundamental work in computer science in this area; 2) The highly efficient instantiation of a spiking-neuron-like automaton with only four genes in a microbe creates a fundamental new computational motif that can be ported to a wide range of molecular implementations including in-vitro molecular systems, in-vivo microbial, yeast, and mammalian cells; 3) The microbial fuel cell automaton enables sensitive and continuous molecular sensing to be built in a wide range of hosts without the need for bulky, expensive, non-continuous and toxic photo-bleaching optical systems; 4) By creating molecular cellular automata that serve to implement sophisticated molecular sensing, pattern recognition, and collective computation, the proposal could enable very large scale molecular computational systems to become practical.<br\/><br\/>Broader Impact: <br\/>The hybrid analog-digital automata is a computational motif that could be widely applied to synthetic cell-based treatments in medicine. Microbial sensing and computational systems that are capable of sensing, amplifying, digitizing and classifying minute concentrations of input molecules could have wide application in the food, biotechnology, pharmaceutical, environmental, and security industries. PI participates in the Society of Women Engineers, MIT Summer Research Program, Saturday Engineering Enrichment Discovery Program, and computational and systems biology recruitment programs at MIT, which actively target women and underrepresented minorities.","title":"EAGER: Hybrid Analog-Digital Automata in Microbial Cells","awardID":"1348519","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7946","name":"BIO COMPUTING"}}],"PIcoPI":[562496],"PO":["565223"]},"209858":{"abstract":"This project provides funding support in order to make awards to graduate students to attend the 44th Annual IEEE\/IFIP International Conference on Dependable Systems and Networks (DSN 2014) to be held in Atlanta, Georgia on June 23-26, 2014. The IEEE\/IFIP International Conference on Dependable Systems and Networks (DSN) is a premier international conference for presenting the research results, problem solutions, and insights on new challenges in dependability and security of computer systems and networks.<br\/><br\/>DSN 2014 will provide many and varied opportunities for student submission and participation. These opportunities range from full-length conference and workshop papers to medium-length preliminary contributions considered by the Student Forum to brief descriptions of early-stage research in the Work in Progress category. This project is focused primarily on broader impacts through the education students will receive from attending the technical sessions of DSN, mentoring that they will receive through the Student Forum activities, and networking opportunities they will receive through numerous different conference activities.","title":"IEEE Dependable Systems and Networks Conference: Student Travel Support","awardID":"1348806","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[562577],"PO":["565319"]},"208417":{"abstract":"In the last decade, wireless sensor networking has been one of the most popular research areas for computer engineers and scientists. The use of wireless sensors has also started to gain popularity among researchers other than computer engineers and scientists. Today, sensors are used to enable civil engineers to monitor the structural health of deteriorating infrastructure such as highways and bridges, farmers to develop precision-agriculture techniques, ecologists to observe wildlife in their natural habitat, geophysicists to capture seismic activity of volcanoes, and in many other application areas. However, the task of developing software applications for wireless sensors is challenging for researchers because sensors have limited technical capabilities and software implementations for sensors require meticulous procedures to provide a desired level of services (e.g., reliability, security) for applications. This situation is even further exacerbated for researchers in other fields of engineering and science (e.g., civil engineers and geophysicists) as they may not have a rigorous programming background. Therefore, to facilitate the design, development and implementation of wireless sensor applications, this project provides a new framework called PROVIZ, which integrates visualization and programming functionalities into a common platform. PROVIZ is an open-source, platform independent, modular, and extensible framework for heterogeneous wireless sensor monitoring and application development. It consists of a set of easy-to-use simplified languages (one domain specific scripting language, one icon-based drag-and-drop style visual language) and a simple programming editor for developing wireless sensor applications and a mechanism for (re)programming wireless sensor nodes remotely over-the-air by distributing the generated application image. PROVIZ has the capability to visualize wireless sensor data captured from (1) a packet sniffer, (2) a binary packet trace file (e.g., PSD format), and (3) an external simulator running a wireless sensor application. PROVIZ also has the capability to process data from multiple sniffers simultaneously to visualize a large wireless sensor deployment.<br\/><br\/>PROVIZ will be instrumental to scientists and engineers working with wireless sensors in many disciplines (e.g., civil engineering, ecology, agriculture). PROVIZ will allow these researchers to easily program sensors and to focus more on the tasks in their domains by significantly reducing the overhead of learning how to program sensors. The PROVIZ project will be conducted as an open source project, enabling interested software developers to benefit from and add to it. Further, a variation of PROVIZ will be used to present sensor networking concepts to middle school underrepresented minority students in Georgia. Given the proliferation of wireless sensor utilization in various engineering and science fields, it is envisioned that the success of the PROVIZ project will help contribute to the growth of the future cyber workforce in the U.S.","title":"SI2-SSE: A Sustainable Wireless Sensor Software Development Framework for Science and Engineering Researchers","awardID":"1339781","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"8004","name":"Software Institutes"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["560166",558531],"PO":["565247"]},"210331":{"abstract":"This proposal provides travel support for the NSF's annual Cyber Physical Systems (CPS) Principal Investigators' (PI) meeting, to be held in Arlington, VA, on Oct. 17-18, 2013. The CPS PI meeting is the flagship event for the NSF CPS program. It is conducted annually, and provides an opportunity for all CPS PIs and co-PIs to interact with NSF, other government agencies, and industry. This particular award is to the University of North Carolina at Chapel Hill and will facilitate travel for the PIs and co-PIs of several CPS projects that are nearing completing. The participation of these PIs in the meeting will be of great benefit to NSF and the broader CPS community, as they will be able to describe the successes of the most mature projects in the CPS portfolio, including recent science and technology breakthroughs as well as the broader impacts that are emerging.","title":"Travel Subsidies for 2013 CPS PI Meeting","awardID":"1355325","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7918","name":"CYBER-PHYSICAL SYSTEMS (CPS)"}}],"PIcoPI":["31436"],"PO":["565274"]},"210221":{"abstract":"This project constructs an Omnipresent Vision system - a computational system that allows us to navigate, share, enhance, and understand the visual data captured by a slew of fixed and moving cameras. The society is flooded with various cameras. Almost every cell phone has a video camera and wearable cameras are starting to permeate our lives. These local cameras capture visual experiences from personal perspectives. Static cameras at various outdoor and indoor locations are also constantly capturing videos. These fixed-view cameras offer global, persistent looks into our daily lives. The key idea of this project is to fully leverage the combination of these local and global cameras to enable new visual experiences and facilitate the understanding of the scene and the people within. This is achieved with novel algorithms and computational tools that bring together the local and global views into an integrated platform, model the dynamic scene by joining those two sets of perspectives, and recognize the actions and events in them. <br\/><br\/>The research, at a personal level, enables the spatio-temporal and contextual expansion of the person's view, and at a scene level, it enables the interpretation of the scene at various scales of spatial and temporal resolutions. It also provides new means to understand people and scenes. For instance, it facilitates the understanding of people who cannot communicate their intentions. The research activities also furnish graduate and undergraduate students educational opportunities to take part in spawning this new area of research.","title":"EAGER: A Local-Global Approach Towards Omnipresent Vision","awardID":"1353235","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[563384],"PO":["564316"]},"205810":{"abstract":"Networked wireless systems have significantly affected society, as cellular and WiFi networks are now ubiquitous in our daily life. An important class of networks is that of decentralized networks, which are characterized by a lack of fixed infrastructure, a multi-hop nature, and node mobility; such features provide significant flexibility, but also introduce a variety of design issues. A fundamental problem in decentralized networked systems is the coordination of activities of different nodes, where the availability of sufficient communication resources is critical in order for agents to cooperate and coordinate. This project takes a distinctive approach to develop a fundamental understanding of the interplay between communication and cooperation to achieve coordination and addresses the communication and coordination of probabilistic actions in networked multi-agent systems. This can be seen as an instance of network-based stochastic open loop control where a single communication message is sent to the agents to control their behavior in the absence of feedback. The project investigates two interrelated research tasks. First, the PIs wish to understand both the fundamental information-theoretic limits of coordination in small multi-terminal networks and the dependency of these limits on the communication network topology. The second research goal addresses the design of explicit coordination codes from error correction codes and to assess their performance under stringent delay and complexity constraints.<br\/><br\/>The proposed research has the potential to enable advances in applications where actions must be communicated to ensure a specific behavior. Examples are multi-agent systems for the exploration of an unknown topology, distributed surveillance applications, automatic traffic control applications, and load balancing in large computer networks and in power grids. Additional activities related to the project include the integration of research outcomes in courses at the undergraduate and graduate level and the mentorship of minorities and women in science and technology.","title":"CIF: Small: Collaborative Research: Coordination and Cooperation in Networked Multi-Agent Systems","awardID":"1320785","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[551001],"PO":["564924"]},"205821":{"abstract":"Massive Open Online Courses (MOOCs) are widely regarded as a<br\/>revolutionary innovation; however, traditional instructional<br\/>techniques do not scale to MOOCs with tens of thousands of students.<br\/>This project aims to design a set of algorithmic tools to partially<br\/>automate three of the most basic instructional processes - grading<br\/>assignments, giving students personalized feedback, and creating new<br\/>assignments - in MOOCs that teach computer programming. Such tools<br\/>have the potential to significantly raise the productivity of<br\/>instructors in these courses, and give students a far more effective<br\/>educational experience than what they currently receive. They can also<br\/>guide the development of future MOOCs on programming, and play a role<br\/>in making the MOOC model reach its full potential.<br\/><br\/>The algorithms developed in this project draw on ideas from several<br\/>different areas of computing. They leverage advances in automated<br\/>reasoning about software like automatically finding bugs in<br\/>student code and suggesting fixes, and exploit statistical learning<br\/>techniques that mine databases of previously-completed assignments and<br\/>infer aggregate statistics about a class. Finally, the research has a<br\/>dimension of human computation, for instance seeding logical and<br\/>statistical analysis techniques with data obtained through peer<br\/>evaluation. These methods apply to any programming course; however,<br\/>the investigators will evaluate them by deploying them in a specific<br\/>MOOC on Python programming that they teach.","title":"SHF: Small: Computer-Aided Grading, Feedback, and Assignment Creating in Massive Online Programming Courses","awardID":"1320860","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[551025,551026,551027],"PO":["565264"]},"210298":{"abstract":"This project will develop an initial version of a Programmable and Versatile Backscattering Tag (PVBT) platform that will drive the research on backscatter-based systems of the future. The idea is to develop a modular, extensible and powerful hardware architecture for backscatter-based tags that is programmable at every layer. The PVBT hardware will be extensible in that the RF, analog and digital sections will be modular and exchangeable. In order to demonstrate the power of PVBT the PIs will build a small set of limited prototypes that will drive several core research directions. These will include (1) development and evaluation of protocols in the PHY\/MAC layers providing higher speed and better interference management, (2) exploration of tag-to-tag communication and related networking protocols, (3) sensor networks utilizing backscatter communications. <br\/><br\/>Use of the PVBT will help researchers and manufacturers adopt PVBT-based evaluation in their workflow to develop new designs faster and at a lower cost, thereby helping RFIDs make more inroads in the marketplace. On the educational front, the PIs will initiate development of an online course on backscatter-based systems. The PVBT will also act as a platform for (i) summer projects of local high school students via several programs at SBU, (ii) research and inquiry-based courses in the WISE (Women In Science & Engineering) Program at SBU, (iii) senior projects of engineering undergraduates, and (iv) course\/thesis projects for graduate students at SBU.","title":"EAGER: Designing Programmable and Versatile Tags for Backscatter Networks","awardID":"1354614","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[563567,563568,563569],"PO":["565303"]},"210188":{"abstract":"Discovering and understanding the temporal evolution of events hidden in text corpora is a complex yet critical task for knowledge discovery. Although mining event dynamics has been an important research topic leading to many successful algorithms, researchers, research and development managers, intelligence analysts and the general public are still in dire need of effective tools to explore the evolutionary trends and patterns. This exploratory project focuses on developing and validating a novel idea called narrative animation. Narrative animation uses animated visualizations to narrate, explore, and share event dynamics conveyed in temporally evolving text collections. Film art techniques are employed to leverage the animated visualizations in information organization and change detection, with the goals of enhancing analytical power and user engagement. A prototype system called CityStories is being developed to generate narrative animations of events in cities derived from web-based text. <br\/><br\/>If this novel, risky research is successful, it is expected to yield fundamental results in narrative animation that can advance the current paradigm in information visualization and visual analytics by developing novel techniques in using animations for presenting and analyzing dynamic abstract data at a large scale. The pilot system CityStories system is expected provide a novel network platform for education, entertainment, and data analytics. It will engage general users such as students, teachers, journalists, bloggers, and many others in web information visualization and study. Results of this research will be disseminated through publications, the World Wide Web, and collaborations with researchers and analysts. The project web site (http:\/\/coitweb.uncc.edu\/~jyang13\/narrativeanimation\/narrativeanimation.htm) will include research outcomes, publications, developed software, videos, and datasets for wide dissemination to public.","title":"EAGER: Collaborative Research: Visualizing Event Dynamics with Narrative Animation","awardID":"1352893","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[563306],"PO":["563751"]},"200035":{"abstract":"Materials change their properties in response to the environment in ways that are often detrimental to performance. However, a new generation of \"smart materials\" capitalizes on environmental responsiveness to improve performance in the laboratory. This project investigates how such \"smart materials\" evolve and function in nature, focusing on how adhesives adapt to fluctuating humidity in the environment. Spider capture threads are sophisticated composite structures that generate adhesion through multiple mechanisms that could respond synergistically or independently to the environment. Adhesion starts with the surface contact of sticky glycoproteins that are encased in liquid glue droplets and is then enhanced when those droplets and the underlying axial thread to which they are attached stretch and resist thread \"pull-off\" by forming a broad, suspension bridge-like interface. This whole process is controlled in part by cocktails of salts in the glue droplets that absorb atmospheric water, which then lubricates the glue and controls the extension of the droplets and axial threads. This project combines biology and materials science to compare the molecular compositions of glues, the adhesiveness of capture threads, and how webs capture insects across a community of spiders to understand how biological \"smart\" materials respond to their environments. It will inspire the development of new synthetic adhesive systems and \"smart\", environmentally responsive materials. The project will provide in depth training in interdisciplinary research to PhD, undergraduate and high school students. It will also develop new educational resources for K5-K12 summer camps and teacher workshops.","title":"Collaborative Research: The Performance and Evolution of Environmentally Responsive Biomaterials in a Unique Biological Adhesive System: Spider Orb Web Capture Threads","awardID":"1257719","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7658","name":"PROCESSES STRUCS & INTEGRITY"}}],"PIcoPI":[536246],"PO":["564983"]},"206822":{"abstract":"INTELLECTUAL MERIT<br\/>Perhaps the most important structure for cellular life as we know it is the lipid bilayer. Lipid molecules, consisting of a water-soluble \"head\" and water-insoluble \"tails\", spontaneously assemble into sandwich-like bilayer membranes, which surround all living cells and further compartmentalize the cellular interiors of all eukaryotic organisms the domain of life to which plants, fungi, animals, and humans belong. The network of membranes in a typical eukaryotic cell is very complex and highly dynamic: small compartments bud off from certain membranes like bubbles, carrying cargo from one part of the cell to another, where they can fuse with yet other membranes, including the outer membrane of the cell. Bilayer fusion is therefore a ubiquitous biological process, tightly linked to the transport of material and information, and therefore it is exquisitely controlled by several classes of membrane-associated proteins. These proteins clearly perform work on the fusing membranes, but the intricate sequence of geometric and topological shape transformations they induce on the molecular scale are impossible to observe directly in experiment. In contrast, molecular simulation offers a window onto these details, but until now the relevant length- and time-scales have proven too big to observe even a single fusion event for a realistic system size. This project establishes a collaboration between two investigators with the aim to meet this challenge by combining recent advances in multiscale coarse-grained modeling with enhanced-sampling molecular simulation. Since this strategy allows incorporating important chemical detail while simultaneously representing large-scale membrane deformations, the investigators will be able to elucidate how molecular-level mechanisms drive fusion events across the relevant physiological length- and time-scales. The project proceeds through three phases, namely: (i) modeling the fusion of pristine bilayers with enhanced sampling, (ii) development of coarse-grained models of model fusogenic proteins, the SNARE system, and (iii) combining these two steps into a single methodology. The project will pursue many topics of energetic, morphological, and mechanistic relevance, in particular questions revolving around the so-called hemifusion intermediate state, for which the two outer bilayer leaflets have already fused but a membrane formed by the two inner leaflets still separates the two compartments.<br\/><br\/>BROADER IMPACTS<br\/>This project will impact many topics in the biological sciences due to the central importance of bilayer fusion in a variety of biological processes, including intracellular trafficking, viral entry, neurotransmitter release, fertilization, and more. Beyond the specific questions under study, the computational approach envisioned here takes early steps towards efficient simulation of more complicated multiple-protein\/multiple-membrane phenomena and will therefore benefit future studies of a wider class of molecular biological topics. To broaden applicability of the research outcomes, the simulation framework developed in this project will be made freely available with tutorials that will support efficient learning and facilitate the transformation of existing techniques and modules towards novel applications. This project establishes cross-disciplinary exchange between engineering and (bio)physics, fostering a stimulating interdisciplinary environment for the academic growth of students mentored in this project. It will further the transfer of theoretical and computational methodologies from engineering and physics into the life sciences and their increasingly quantitative set of problems. The ubiquity of bilayer fusion and its connection to a wide class of fascinating themes in biological physics, which is in itself an intriguing cross-disciplinary subject, also present excellent opportunities for the expertise developed in this project to feed outreach specifically tailored towards groups underrepresented in STEM fields for instance through classroom material, lecture demonstrations, and public talks and both investigators will implement such activities, building on both their experience and existing successful programs at their respective institutions.","title":"Collaborative Research: Multiscale molecular simulations of protein-mediated bilayer fusion","awardID":"1330226","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}}],"PIcoPI":[553756],"PO":["565043"]},"205612":{"abstract":"The first focus of this award is the problem of inferring causal relationships. This problem is essential to many statistical applications. Generally speaking, causation can be inferred only by active intervention, through controlled experiment. However such experiments may be impossible or else practically or morally infeasible: for instance, in predicting potential effects regulations or laws on medical, educational and economic outcomes, or, in predicting whole-ecosystem effects of human activity. The starting point for Dr. Schulman's research in this area is Pearl's theory of Structured Causal Models (SCM) which, allows, in special circumstances, \"identification\" of a causal relationship (as opposed to a statistical correlation) from passive observation. The proposed research aims, in the first place, to expand the above special class of circumstances---and thereby make the theory more widely applicable---by using a relaxed but still useful notion of \"weak identification\" of causal relationships. The relaxed notion is more robust, and enables valid inference even if the posited SCM is slightly inaccurate. In the second place, the research aims to provide efficient and numerically stable algorithms for weak identification from empirical data.<br\/><br\/>The second focus of this award, again in computational statistics, is the representation of a large data set (considered as an empirical measure) by a much smaller data set, in such a way that for a specific family of integrals, all integrals of the measure are approximately preserved. This work encompasses two separate application areas. The first concerns clustering and related high dimensional data analysis problems. Here the compressed data set is known as an epsilon-approximation or core-set of the input measure. A particular focus is on \"underclustering\", namely, preparation of core-sets for clustering in a normed space, before the norm has been specified. The technical tools needed in this application have to do with recently developed ideas about the \"total sensitivity\" of the family of integrals, as well as with, on the algorithmic side, bicriteria approximations. The second application area concerns signal processing (or approximation theory) on compact groups. Here the methods draw on representation theory, the classical theory of the moment problem, and convex geometry.<br\/><br\/>This award will be used to train graduate students and postdoctoral fellows in research in algorithms, statistics, and underlying mathematical topics in algebra and geometry.","title":"AF: Small: Algorithms for Inference","awardID":"1319745","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550549],"PO":["565157"]},"205854":{"abstract":"This Cyberlearning Exploration Project explores the potential for social robotics to transform foreign-language learning. The social robot being developed in this project is designed to act as a language partner for students learning a foreign language, in this case those learning Chinese. It augments classroom instruction, providing for the learner a robot companion to converse with. The hypothesis is that social robots can make interactions with language speakers more exciting and more accessible, especially for less commonly taught languages. The embodied robot is designed not only to converse with learners but also to point and nod and gesture at particular people and objects, helping to direct the attention of learners and interact socially with learners in ways that a non-embodied simulation cannot. The PIs take a task-based approach to language learning, helping students learn the language that goes with a variety of real-world situations. The technological innovation is a synthesis of social robots and social simulation; classroom language instruction is augmented by robot interaction. In this Exploration project, robots are being used in classrooms; as the price of such robots decrease, such robots could become available for after-school, library, cafeteria, or even home or public use.<br\/><br\/>This project brings together a leading developer of simulation-based learning solutions for intercultural communication (Alelo, Inc.) with a leading developer of lifelike intelligent robots (Hanson Robokind) to create a prototype social robot for language learners (in this case those learning Chinese) and to investigate the roles such robots might play in language learning and the most effective ways of interacting with learners so as to sustain their engagement in foreign-language conversation. The project meets a critical need in this country for a workforce that can engage well in the international marketplace. The particular focus in this project is on Mandarin Chinese language and culture, but what is learned will be applicable to other foreign languages as well.","title":"EXP: Transforming World Language Education using Social Robotics","awardID":"1321056","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}}],"PIcoPI":[551113,551114],"PO":["562669"]},"206833":{"abstract":"There are at least two key features of the move to cloud computing that introduce the opportunity for significant leaps forward in computer security for tenant services. First, a compute cloud provides a common software, hardware and management basis for rolling out cross-cutting services en masse that have resisted incremental deployment in a one-service-at-a-time fashion. Second, compute clouds offer providers a broad view of activity across an unprecedented diversity of tenant services. This research project leverages these features to develop new approaches to a wide array of fundamental problems in computer security. By convening Cloud Security Horizons summits with industry stakeholders, this project further seeks to both contribute to industry directions in cloud computing and to be informed by them.<br\/><br\/>Particular longstanding security challenges addressed in this project include secure transport, authorization, user and software authentication, security monitoring, and incident analysis. Moreover, since modern clouds are not sufficiently extensible to support the envisioned capabilities, this project is constructing cloud software platforms that enable the flexibility, extensibility and security needed for this research to come to fruition in practice.","title":"TWC: Frontier: Collaborative: Rethinking Security in the Era of Cloud Computing","awardID":"1330308","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553786,"559530",553788],"PO":["565327"]},"204655":{"abstract":"Risk communication is an important part of many cyber security mechanisms. Android's current risk communication mechanism is based on security warnings and has been demonstrated to be ineffective because users become habituated to ignore such warnings and tend to consent to all prompts. This multi-disciplinary research project aims at developing holistic solutions to usable risk communication and control for the Android platform. <br\/><br\/>This project investigates an approach that presents risk information at multiple granularities, including a high-level numerical risk summary, an intermediate-level summary of risk for different dimensions, and detailed risk information. The high-level risk summary is computed by information integration techniques, using information discovered from multiple sources, e.g., user reviews and app source code. This summary enables proactive risk communication (e.g., when the user searches for apps) so that users can take this information into the decision process. <br\/><br\/>This project also introduces a multi-mode approach that, in addition to communicating risks, also controls risks in the sense of discouraging risky applications and ensuring that users truly understand the risks. The project develops mechanisms that aggregate, communicate, and control risks incurred by apps at runtime, and ways to personalize risk integration, communicate, and control techniques to accommodate differences among users.<br\/><br\/>This project is expected to advance the state of the art in principles and techniques to risk communication and control, and has the potential to impact the Android app ecosystem by collaboration with Google researchers.","title":"TWC SBE: Medium: Collaborative: User-Centric Risk Communication and Control on Mobile Devices","awardID":"1314229","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["548078",548078,548079],"PO":["565136"]},"206724":{"abstract":"Intellectual Merit: Translation of the genetic code from triplet codons in mRNA into the amino acid sequences of proteins ultimately determines the protein composition of the cell. Translation affects the amount of each protein, is responsible for errors within the protein, and is regulated in response to energy needs, nutrient availability, and hormonal changes. During translation, sixty-one codons specify insertion of the twenty amino acids into proteins, and thus include many synonymous codons that specify insertion of the same amino acid. There is overwhelming evidence that the efficiency of translation is influenced by the choice of synonymous codons used to encode a polypeptide. However, neither the identity, nor the properties, of codons or codon combinations that reduce translation efficiency are known. The long-term goal of this project is to identify the codons and codon combinations that inhibit translation in the yeast Saccharomyces cerevisiae and to determine how they do so. An initial systematic analysis of the effects of individual codon repeats on translation showed that the CGA codon was translated inefficiently due to I-A wobble interactions with tRNA(Arg-ICG), and that adjacent CGA-CGA codon pairs were far more inhibitory than individual CGA codons. This result implies that there are likely other codon pairs or triplets that inhibit translation. To find inhibitory codon combinations, a robust and sensitive method, called RNAID, was developed, which uses fluorescence activated cell sorting (FACS) to identify GFP variants with altered expression. One aim of the current project is to comprehensively identify all codon pairs that have a significant inhibitory effect on translation and to assess their biological importance. Inhibitory codon combinations will be identified using RNA-ID, coupled with deep sequencing and computational analysis. The biological roles of these inhibitory codon combinations will be examined using bioinformatics approaches to test hypotheses about their location, representation, conservation, and effects on ribosome decoding. The biological consequences of inhibitory codon combinations will then be evaluated, by examining cellular fitness and individual protein amounts in cells expressing tRNA variants that suppress the inhibitory effects of these codon combinations. A second aim of the project is to determine the mechanisms by which these inhibitory codon combinations exert their effects. Cellular components that mediate effects of inhibitory codon combinations will be defined using genetic selections, as well as directed screens to assess the effects of known translational quality control factors and elongation factors. Study of these inhibitory codon pairs will lead to new insights into the translation machinery and its quality control mechanisms, including: the definition of common decoding properties of inhibitory codon combinations, definition of how codon interactions affect elongation, and knowledge of the biological functions of inhibitory codon combinations. <br\/><br\/>Broader Impacts: From the education and training perspective, this project will enable graduate students, undergraduates and high school students to connect biological research to the educational process. These students will employ the cutting edge technology of FACS combined with the simplicity of yeast genetics to participate in investigations of fundamental questions of gene expression. Both the robust methodology of yeast genetics and flow cytometry, and the non-hazardous organism (baker's yeast) are optimal for students' efforts. A program of study will be developed, including an introductory series of exercises, an individual research project and inclusion in scientific discourse. Undergraduates will be recruited from the 50-75 University of Rochester (UR) biochemistry majors for whom the PI is the faculty advisor and from the UR Summer Scholars Program, which recruits many students from primarily undergraduate colleges. High school students will be recruited through direct contact with high school counselors. From an applied scientific perspective, the comprehensive understanding of how codon choice affects translation produced from this project will be immensely useful for scientists interested in engineering and producing large amounts of protein for biochemistry, structural biology or biotechnology.","title":"Codon Choice and Gene Expression in Saccharomyces cerevisiae","awardID":"1329545","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0807","name":"Division of MOLECULAR AND CELLULAR BIOSCIE","abbr":"MCB"},"pgm":{"id":"1112","name":"Genetic Mechanisms"}}],"PIcoPI":[553500],"PO":["558897"]},"205635":{"abstract":"Although myriad extraordinary benefits have resulted from the web and expanding network connectivity, the intertwining of computers with virtually every aspect of life also brings a growing stream of interruptions. These interruptions and the fragmenting of activity they produce are increasingly accepted parts of modern life. A critical research challenge for human-centered computing is how to smooth and mitigate the impact of interruptions as well as assist in resuming interrupted activities. Meeting this challenge will require moving beyond the traditional document-centric view of information to address the complexity of real activity, its often fragmented history, and the need to access information arrayed across digital and paper media.<br\/><br\/>This project will explore the design of systems that will allow people to exploit visual and episodic memory to benefit from the capture of the history and context of their computer-mediated activities. The heart of the project is rethinking the nature of data as stored in computers from being a state to being an inspectable activity history of interaction that can be presented to users. Activity-enriched computing has the potential to reshape how we use computers by creating systems that react to and are augmented by the history of past events, changing the computer environment to one in which history is always available to assist if needed. The primary scientific contributions will be to (a) develop an activity-enriched computing framework for designing a new class of systems in which history is a first-class element, (b) prototype and evaluate activity-enriched applications, and (c) extend theory and methods for representing, visualizing, and analyzing activity histories. Scientific contributions will include a novel activity-enriched computing framework that will (a) capture the rich detail of computer-mediated activity, (b) identify and make meaningful and useful episodes available, (c) link the worlds of paper and digital documents, and (d) exploit summarization and visual access mechanisms to support navigation of activity histories and ease resuming interrupted activities. The overarching objective is to lessen the impact of interruptions and aid reestablishment of context.<br\/><br\/>Broader Impacts: The broader impacts of the proposed activity include the potential to radically improve the efficacy of all computer-mediated activities. The results of the project and the software developed will be widely disseminated and made available on the project website. Additional impact will result from training students in the interdisciplinary approach required to design activity-enriched computing applications, providing research opportunities for both graduate and undergraduate students. A long-term impact will be to crystallize a research community to further develop and evolve activity-enriched computing.","title":"HCC: Small: Activity-Enriched Computing","awardID":"1319829","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[550599],"PO":["565362"]},"206735":{"abstract":"There are at least two key features of the move to cloud computing that introduce the opportunity for significant leaps forward in computer security for tenant services. First, a compute cloud provides a common software, hardware and management basis for rolling out cross-cutting services en masse that have resisted incremental deployment in a one-service-at-a-time fashion. Second, compute clouds offer providers a broad view of activity across an unprecedented diversity of tenant services. This research project leverages these features to develop new approaches to a wide array of fundamental problems in computer security. By convening Cloud Security Horizons summits with industry stakeholders, this project further seeks to both contribute to industry directions in cloud computing and to be informed by them.<br\/><br\/>Particular longstanding security challenges addressed in this project include secure transport, authorization, user and software authentication, security monitoring, and incident analysis. Moreover, since modern clouds are not sufficiently extensible to support the envisioned capabilities, this project is constructing cloud software platforms that enable the flexibility, extensibility and security needed for this research to come to fruition in practice.","title":"TWC: Frontier: Collaborative: Rethinking Security in the Era of Cloud Computing","awardID":"1329641","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["561112"],"PO":["565327"]},"205767":{"abstract":"Heap-allocated pointer structures are a common source of software errors. In particular, pointer safety errors, like memory leaks and dereferencing null or dangling pointers, often cause programs to fail or leave them vulnerable to be exploited by malware. Tools that are able to detect such errors at compile time have long been considered impractical because they scale badly to large programs. This has started to change recently with the advent of verification tools based on separation logic (SL), which scale to software components of industrial size. The aim of this project is to increase the degree of automation, precision, and soundness of today's SL-based verification tools and broaden the scope of tools that could use separation logic. Because heap-allocated data structures are among the most difficult software constructs to reason about, this work has the potential to make a significant impact on the reliability of software.<br\/><br\/>SL-based tools depend on theorem provers for separation logic to automatically discharge proof obligations concerned with properties about pointer structures. Today's tools implement tailor-made provers for this task. However, the analysis of real-world programs involves reasoning about other data types including, for instance, integers, arrays, and bit-vectors. To cope with this, existing separation logic tools make simplifying (and in general unsound) assumptions, rely on interactive help from the user, or implement ad-hoc and incomplete extensions of their tailor-made provers. The PIs will investigate a more systematic approach towards combined reasoning about heap and other data types by integrating an SL theorem prover into a satisfiability modulo theories (SMT) solver. This research is motivated by the observation that reasoning about separation logic fragments can be reduced entirely to reasoning in decidable first-order theories that fit well into the SMT framework. Modern SMT solvers implement decision procedures for many first-order theories that are relevant in program verification, such as linear arithmetic, arrays, and bit-vectors. A reduction to first-order logic enables a seamless combination of separation logic with these theories. Moreover, SMT solvers are already an integral part in the tool chain of many existing verification tools. These tools could directly benefit from an integrated SL prover. In addition, we expect that specific capabilities added to the SMT solver as a result of the project will be useful to a broad set of SMT users.","title":"SHF: Small: Integrating separation logic and SMT for better heap verification","awardID":"1320583","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["562897",550906],"PO":["565264"]},"202258":{"abstract":"This project aims at addressing the fundamental problem of how to maximize the efficiency of access to the wireless medium. In recent years, new information-theoretic solutions have emerged, some of which can now be implemented due to technological advances. Also, long engrained notions, such as half duplex, are being challenged. It has been shown that through a combination of strategies at the physical layer and signal processing, full duplex systems are indeed feasible. These developments have radically altered the very notion of what a spatio-temporal resource is. These developments have necessitated a thorough redefinition and appraisal of the problem of efficient access as addressed in this project.<br\/><br\/>Over the years, developments have taken place in parallel and somewhat in isolation in the physical layer community and in the protocol community. By bringing together a broad-spectrum of expertise from both areas, this project envisages fundamental advances in wireless networking at the access layer. The core challenge is how to use the scarce resources of 'space' and 'time' as efficiently as possible. In addition, distributed operation, channel fading, time-varying channel conditions, and fast time-scales of transmission opportunities and decisions, exacerbate the problem. This project has the potential of making a transformative advancement in the science of medium access.","title":"NeTS: Medium: Collaborative Research: Leveraging Physical Layer Advances for the Next Generation Distributed Wireless Channel Access Protocols","awardID":"1302620","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[542039,542040],"PO":["557315"]},"205415":{"abstract":"Loss of personal data or leakage of corporate data via apps on mobile devices poses a significant risk to users. It can have both a huge personal and financial cost. This work is designing new novel techniques to help reduce the risks for end-users who use a single device for multiple spheres of activity. Getting security right when a single device is used for multiple spheres of activity is a major research challenge, with unforeseen information flows between various subsystems that are currently difficult to control. This project is developing mechanisms to better manage flows between apps on a mobile device so that users are able to compartmentalize different spheres of activity, such as work and personal use.<br\/><br\/>Broader impact: This research benefits both end-users who are concerned about the privacy of their data on mobile devices as well as businesses who wish to permit use of mobile devices for improving efficiency of their operations but are concerned about resulting security risks. Graduate and undergraduate students are trained in the area of security and privacy of information on mobile devices.","title":"TWC: Small: Discovering and Restricting Undesirable Information Flows Between Multiple Spheres of Activities","awardID":"1318722","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550044],"PO":["562974"]},"205657":{"abstract":"Shape analysis is a fundamental problem in computer graphics and vision. A significant body of research over the past few decades has centered around the medial axis, a classical geometric structure introduced by Blum in 1967. Not only does the medial axis provide a low-dimensional representation that captures both the shape and topology of the object, it also creates derivative shape descriptors that measure useful shape properties, such as thickness. This collaborative project that involves two partner institutions builds on that body of research and extends it in two important and related directions. First, the PIs will generalize the definition of medial axis to systemically define a sequence of medial representations of successively lower dimensionality (e.g., medial curve and medial point of a 3D object) that all inherit the essential properties of the medial axis. These representations, called medial forms, are useful in various applications such as shape matching and decomposition. While algorithms for computing low-dimensional skeletal structures are abundant, sound mathematical definitions have not yet been developed. Second, based on the medial forms, the PIs will introduce a set of shape descriptors that can characterize non-uniform (or anisotropic) expansion of shape. These descriptors will not only be able to differentiate plate-like and tubular parts, but also to measure the amount of plate-like stretching and tube-like elongation. Shape anisotropy is common in 3D objects, and is important for deriving knowledge from digital models, particularly in biological research. However, none of the existing descriptors can characterize this important aspect of 3D shape. The PIs will build on their recent study of 2D objects to develop the mathematical foundation and computational algorithms of medial forms of 3D objects, and will explore their use in analyzing shape anisotropy through several derivative shape descriptors. They will formalize the definitions of medial forms of a 3D object, and will examine their properties including their dimensionality, topology, and sensitivity to boundary perturbations. Based on the definitions, the PIs will develop efficient algorithms that create provably good approximations of the medial forms given discrete samples of an object. The PIs will explore how a variety of derivative anisotropy-aware descriptors, in the forms of either geometric skeletons or surface signature functions, complement existing descriptors in shape modeling applications (e.g., segmentation and matching) and in biological shape analysis.<br\/><br\/>Broader Impacts: The new mathematical formulations and shape descriptors will contribute significantly to both the theoretical and applied side of computer graphics. The ability to characterize shape anisotropy will directly translate to more accurate and efficient analysis of shapes in various application domains, and the PIs will particularly explore their use in understanding biological shapes. The team will develop and distributable online software that implements the new algorithms. Students, both at the undergraduate and graduate level, will be actively recruited with an emphasis on diversity, and in addition project outcomes will be used in outreach programs at local middle- and high-schools.","title":"CGV: Small: Collaborative Research: Theories, algorithms, and applications of medial forms for shape analysis","awardID":"1319944","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[550651,550652],"PO":["565227"]},"205305":{"abstract":"The telephony system, which enabled near universal voice communication, has undergone a dramatic change due to technological advances and legal and regulatory changes. Although these changes offer many benefits, including low cost calling and richer functionality, they have introduced new vulnerabilities that can seriously undermine the trust people have in transactions conducted over the telephony channel. In fact, caller impersonation and social engineering over the phone are increasingly being used to commit fraud and steal credentials for online account takeovers. This project focuses on research that can help secure the emerging telephony landscape against these and other emerging threats. To better understand security threats, the project first explores the feasibility of using a telephony honeypot for characterizing the sources and nature of unwanted and malicious calls, and the vulnerabilities exploited by these calls. Based on an understanding of these threats and vulnerabilities, new techniques are investigated to combat and mitigate telephony based attacks. For example, to address call metadata manipulation, features embedded into the call audio itself are investigated, which can potentially be used to establish the source of a call in a more robust manner. The project also seeks to understand cross channel malicious activity in which attackers utilize both the web and telephony channels to devise and mount sophisticated attacks. <br\/><br\/>Telephony has been a trusted channel in the past and the project explores how trustworthy communication can be enabled in the future when voice communication will increasingly be integrated with our other online activities.","title":"TWC: Small: Securing the New Converged Telephony Landscape","awardID":"1318167","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[549757,549758],"PO":["565327"]},"206757":{"abstract":"Cyber-Physical Systems (CPS) encompass a large variety of systems including for example future energy systems (e.g. smart grid), homeland security and emergency response, smart medical technologies, smart cars and air transportation. One of the most important challenges in the design and deployment of Cyber-Physical Systems is how to formally guarantee that they are amenable to effective human control. This is a challenging problem not only because of the operational changes and increasing complexity of future CPS but also because of the nonlinear nature of the human-CPS system under realistic assumptions. Current state of the art has in general produced simplified models and has not fully considered realistic assumptions about system and environmental constraints or human cognitive abilities and limitations. To overcome current state of the art limitations, our overall research goal is to develop a theoretical framework for complex human-CPS that enables formal analysis and verification to ensure stability of the overall system operation as well as avoidance of unsafe operating states. To analyze a human-CPS involving a human operator(s) with bounded rationality three key questions are identified: (a) Are the inputs available to the operator sufficient to generate desirable behaviors for the CPS? (b) If so, how easy is it for the operator with her cognitive limitations to drive the system towards a desired behavior? (c) How can areas of poor system performance and determine appropriate mitigations be formally identified? The overall technical approach will be to (a) develop and appropriately leverage general cognitive models that incorporate human limitations and capabilities, (b) develop methods to abstract cognitive models to yield tractable analytical human models (c) develop innovative techniques to design the abstract interface between the human and underlying system to reflect mutual constraints, and (d) extend current state-of-the-art reachability and verification algorithms for analysis of abstract interfaces, iin which one of the systems in the feedback loop (i.e., the user) is mostly unknown, uncertain, highly variable or poorly modeled.<br\/><br\/>The research will provide contributions with broad significance in the following areas: (1) fundamental principles and algorithms that would serve as a foundation for provably safe robust hybrid control systems for mixed human-CPS (2) methods for the development of analytical human models that incorporate cognitive abilities and limitations and their consequences in human control of CPS, (3) validated techniques for interface design that enables effective human situation awareness through an interface that ensures minimum information necessary for the human to safely control the CPS, (4) new reachability analysis techniques that are scalable and allow rapid determination of different levels of system safety. The research will help to identify problems (such as automation surprises, inadequate or excessive information contained in the user interface) in safety critical, high-risk, or expensive CPS before they are built, tested and deployed. The research will provide the formal foundations for understanding and developing human-CPS and will have a broad range of applications in the domains of healthcare, energy, air traffic control, transportation systems, homeland security and large-scale emergency response. The research will contribute to the advancement of under-represented students in STEM fields through educational innovation and outreach. The code, benchmarks and data will be released via the project website.<br\/><br\/>Formal descriptions of models of human cognition are in general incompatible with formal models of the Cyber Physical System (CPS) the human operator(s) control. Therefore, it is difficult to determine in a rigorous way whether a CPS controlled by a human operator will be safe or stable and under which circumstances. The objective of this research is to develop an analytic framework of human-CPS systems that encompasses engineering compatible formal models of the human operator that preserve the basic architectural features of human cognition. In this project the team will develop methodologies for building such models as well as techniques for formal verification of the human-CPS system so that performance guarantees can be provided. They will validate models in a variety of domains ranging from air traffic control to large scale emergency response to the administration of anesthesia.","title":"CPS: Synergy: Collaborative Research: Formal Models of Human Control and Interaction with Cyber-Physical Systems","awardID":"1329762","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553586],"PO":["565136"]},"205668":{"abstract":"Within many environments, there are hundreds or thousands of microbial species that interact in a community. These communities can be determined from metagenomic sequencing projects in which DNA is collected from an environment (a sample of seawater, a patch of soil, etc.). The communities can be represented using ecological networks where nodes correspond to species and edges represent inferred or known relationships between the microbes (competition, symbiosis, etc.). Microbial communities in different environments or over time can then be compared by comparing their networks. One computational approach for this comparison is to align the networks by finding regions of local similarity, a new application of network alignment. However, network alignment is a computationally challenging problem for which there are no algorithms that have sufficient speed, accuracy, and generality.<br\/><br\/>This project will develop improved algorithms for network alignment to compare microbial ecological networks and for other applications in computational biology. Network alignment has found wide adoption in computational biology to compare biological pathways, to correct errors in networks, and to form hypotheses about the roles of genes with unknown function. <br\/><br\/>Intellectual Merit<br\/>Improved alignments between networks for different environments or time points will help identify conserved patterns of interactions, truly functional relationships between microbes, and bacteria that are performing similar functions within different microbial communities. The algorithms developed by this project will also lead to more accurate prediction of protein function and protein interactions.<br\/><br\/>The approach taken by the project will use two primary algorithmic innovations, which will lead to substantially more useful local network aligners. The first is the development of a new signature for describing the similarities between network nodes based on their connections and attributes and the connections and attributes of other nearby nodes. This signature is based on the eigenvalues of subgraphs representing regions of the network around each node. Preliminary work has shown that such a multiscale spectral signature results in global network alignments that are more accurate, and more efficiently computed, than those using other approaches. The second main algorithmic innovation for this project is the explicit modeling of network alignment as an optimization problem with multiple objectives. This will allow the new aligners to handle the often-competing requirements on alignments so that they can find regions of networks that have, for example, genes with similar sequences and similar interaction partners. These innovations will lead to more accurate, high-quality alignments yielding new biological insight.<br\/><br\/>Broader Impact<br\/>This project has three aspects to its broader impact. An educational tablet application will be developed that will teach graph theory to high-school students. This interactive application will introduce a beautiful, approachable, yet sophisticated, branch of mathematics to a group of students who often would not have the chance to study it. In addition, the project personnel will participate in programs at Carnegie Mellon University that aim to introduce middle-school girls to technical topics by presenting the new techniques. Finally, while the focus is on biological applications, the techniques and software developed as part of this project are expected to be useful in graphics and vision applications, such as object recognition in photos.","title":"AF: Small: Multiscale Spectral Signatures for Local and Multi-objective Biological Network Alignment","awardID":"1319998","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0807","name":"Division of MOLECULAR AND CELLULAR BIOSCIE","abbr":"MCB"},"pgm":{"id":"8011","name":"Networks and Regulation"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1112","name":"Genetic Mechanisms"}}],"PIcoPI":[550675],"PO":["565223"]},"205789":{"abstract":"Virtualized infrastructure is becoming increasingly routine, and so the ability to manage it well is important. Cloud computing is built on virtualized computing infrastructure. Software Defined Networks are virtualized data networks. The combination of virtualized computing and communications constitutes the next wave of innovation in provisioning bespoke infrastructure. This project addresses ways of making such virtualized infrastructure more reliable, by planning and managing the migration of the virtual network structure and associated computation from one set of supporting physical infrastruture to another. Triggers for VN migration include attacks, massive failures, performance collapse or degradation, or changes in the service demands that require an increase or decrease in resource allocation in order to both satisfy users and reduce power consumption. This ability to adapt to network dynamics is referred to as VI migration (VIM). <br\/><br\/>There are many challenges in deploying VIM for the applications above. First, different network agility objectives might require different VIM models and techniques. For example, the trigger to migrate for proactive defense to defeat reconnaissance would need to be time based , whereas, for power management the trigger would be signi&#64257;cant change of network traf&#64257;c load. Second, VN migrations may need to be planned in advance (for example, in the case of proactive defense) but in some cases they cannot be planned in advance and need to be deployed in real time (in response to faults, for example). Third, although most existing theories permit reasoning about correctness of network operation only once as the system stabilizes to a static state, investigating formal frameworks that support migration strategies that are correct-by-construction under continuous movement is a prime research task. To address these challenges this project will develop a VIM architecture that is designed to work in the context of the GENI and PlanetLab virtualization technologies, but which could be used as the basis for other industrial systems. The architecture consists of two main components: a Strategy Synthesizer block and a Migration Mechanism block. The inputs to the Strategy Synthesizer block uses models of the VI and the Migration Mechanism block interacts directly with the substrate to implement the desired migration. <br\/><br\/>VIM is a powerful technique for energy control in Cloud infrastructure and effective tools for managing VIM are expected to facilitate significant savings in IT energy consumption. In addition to the potential commercial and network security impacts of the work, the PIs have identified local and outreach activities that further the participation of women and minorities. The experimental and evaluation aspects of the work will be incorporated as project elements in graduate courses.","title":"NeTS: Small: Collaborative Research: Enabling Network Agility Through Virtualized Infrastructure Migration","awardID":"1320662","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["563217"],"PO":["564993"]},"198793":{"abstract":"The fruits of data mining pervade every aspect of our lives. We have books and movies recommended; we are given differential pricing for insurance; screened for potential terror threats; diagnosed with various diseases; and targeted for political advertising. The ability to sift through massive data sets with sophisticated algorithms has resulted in applications with impressive predictive power. And yet there is still a gap between what such tools can deliver, and what the users of data mining really need. It is often hard to interpret the answers produced by a learning algorithm, due to its sophistication and the use of large data sets to build models. The results of mining are often \"one-size-fits-all\", and convincing a user that results are actually relevant to them is difficult. Finally, there is the important problem of validation. As the results of data mining affect more and more of our lives, the more crucial it is that the user be able to validate decisions made on their behalf and that affect them. The common theme tying these issues together is a user-centric perspective on the problems of data mining. Rather than asking \"What patterns can be found in this mountain of data?\" this work instead asks \"What structures in this data affect me?\" These issues arise precisely because of the vast amounts of data we now have the ability to mine, and the sophisticated methods at our disposal to analyze this data. In this research, the PIs develop a computational framework and key tools for user-centric data mining. A central theme in this research is the idea of interaction. In both machine learning and in the foundations of complexity theory, interaction has been used to allow a (weaker) entity to probe a much more powerful system and determine answers that it lacks the resources to compute directly itself. The PIs use formal interaction mechanisms both from the perspective of a user interacting with a powerful algorithm, as well as a client interacting with a computing source with access to large data, in order to enable the user to interpret and validate the results of data mining. <br\/><br\/>The goal of this project is to develop a computational framework for user-centric data mining that enables existing users to tailor data analysis to their needs and facilitates the use of data mining in new areas where existing The team proposes interactive mechanisms that start with the results of a learning process and, via interaction with the user, produce an explanation expressed in terms of meaningful features, drawing on ideas from active learning, feature selection, and domain adaptation. 2. Locality: Answers that are relevant. Here, the focus is on providing information that depends more on a user?s local neighborhood, achieved via a new local notion of stability. 3. Verifiability: Answers you can check. The team proposes a framework for the validation of computationally-intensive data mining by the computationally-weak user, with ideas from interactive proof theory and stream algorithms. Tools for analyzing patient medical data have become more sophisticated and individual medical profiles play a far more significant role in diagnosis and treatment.The research examines user-centric data mining via three core primitives (classification, regression and clustering), and studies the three problems of interpreting results, providing local explanations, and validating the results of data mining. Firstly, the research draws on ideas from active learning, feature selection and domain adaptation to build interpretable results via interaction with users. Secondly, it introduces local notions of stability as a way of validating predictions for a specific user. Finally, it develops a general framework for validation of an analysis by a computationally-weak user, by drawing on ideas from the theory of interactive proofs and streaming algorithms.","title":"BIGDATA: Small: DA: Collaborative Research: From Data To Users: Providing Interpretable and Verifiable Explanations in Data Mining","awardID":"1251110","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["550976"],"PO":["565136"]},"205327":{"abstract":"The investigator studies mathematical problems that arise in coding and decoding for multiple-input-multiple-out (MIMO) wireless communications systems. Such communication systems promise much higher rates of data transfer with much higher fidelity than traditional single input\/output systems, and the problems that the PI studies are of direct relevance to the design of such multiple input\/output systems. <br\/><br\/>There are three broad classes of problems the research will focus on. The first class deals with issues in fast decoding of space-time block codes. One can speed up decoding by incorporating decoding considerations directly into the design of the code, or, one can also design approximate decoding schemes that will take advantage of the algebraic nature of the codes. Both routes throw up challenging mathematical issues, and the investigator studies various solutions to these. The second class of problems deals with the design of coset codes for slow fading channels. This project studies several problems that arise in this context, including determination of all possible quotients, appropriate choice of codes over finite rings, and importantly, appropriate choice of metric on the code over the quotient ring. The third class of problems deals with the design of codes for the wiretap channel. Here, the investigator studies a lattice conjecture for the Gaussian channel, then moves on to consider the design of codes for the MIMO wiretap channel.","title":"CIF:Small:RUI: Mathematical Problems in Space-Time Block Codes for MIMO Systems","awardID":"1318260","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[549823],"PO":["564898"]},"208968":{"abstract":"We are evaluating a new model of password security in which users place pieces on a game board (e.g., chess pieces on a chessboard). The fact that existing systems are either memorable or secure, but not both, motivated our approach. We are testing 14-15 year old high school students, college students 18-30, and older adults 60-80, and we are conducting two types of experiments. First, we are measuring all groups' memories for passwords of two and four game pieces (after a 20-minute filled delay). Second, we are testing college students' memories for five different passwords over a 12-week period in which the game changes after week 10. The results are expected to reveal a dramatically better authentication method compared to existing systems. More specifically, participants are expected to create unique passwords that they can remember. Furthermore, performance is expected to decline during the first 10 weeks, as their passwords begin to interfere with one another, and increase dramatically during the final two weeks, once the game changes and they are able to use a new type of memory. We will present the results at social science and computer security conferences, and submit for publication to sociology, psychology, and security journals. Our approach offers a radical breakthrough that is mathematically secure and easy to remember. The model is applicable to a wide range of electronic platforms, including smartphones, computers, ATMs, and other high-risk electronic gateways. Consequently, the potential to benefit society with more secure systems should have an enormous impact.","title":"EAGER: The Game Changer: A New Model for Password Security","awardID":"1343141","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["469752",560107,560108],"PO":["565327"]},"209815":{"abstract":"This award will help support student and postdoctoral attendance at the Annual ACM\/SIAM Symposium on Discrete Algorithms Science (SODA) 2014, in Portland, OR on January 5 to 7, 2014. \u00a0SODA is co-sponsored by the Association for Computing Machinery (ACM) and the Society for Industrial and Applied Mathematics (SIAM) . \u00a0SODA is the premier annual research conference in the field of discrete algorithms and one of the three premier conferences in theoretical computer science. SODA has been meeting annually since 1990 and in a typical year has over 300 attendees. \u00a0It is co-located with two smaller workshops, ALENEX (Meeting on Algorithm Engineering and Experimentation) and ANALCO (Meeting on Analysis of Algorithms). \u00a0SODA is attended by researchers from all over the world. \u00a0The field of algorithms is a vibrant one, with high participation rates from young researchers, and many papers with student authors. For these student authors and student attendees, the conference serves as a valuable educational experience, both for the technical content of the talks and for the opportunities for networking that it provides.","title":"SODA 2014 Travel Grant","awardID":"1348439","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["562721"],"PO":["565251"]},"205008":{"abstract":"This project investigates a fundamentally new robotic manipulation approach for minimally-invasive surgical procedures. The approach combines flexible, actively actuated continuum segments with small, rigid-link joints -- retaining the safety advantages of flexible continuum manipulators while achieving the performance advantages of traditional rigid manipulators. The combination of performance and inherent safety is an essential element in the emerging area of co-robotic surgical applications. The overall approach is referred to as interleaved continuum-rigid manipulation. While interleaved continuum-rigid manipulation offers an attractive, intuitive approach, its apparent simplicity belies the obstacles which must be overcome prior to successful realization. This project will address these obstacles though a coordinated effort in design, modeling, and control. The design effort will seek to address the significant technical challenges of the interleaved manipulation approach including rigid-link joint actuation challenges and rigid-link joint to flexible segment drive-train coupling. The modeling investigation will establish a toolset from which the underlying behaviors of the interleaved manipulation approach can be understood. The controls investigation will seek to develop control strategies appropriate for the hybrid flexible-rigid system envisioned. Evaluation will include performance and safety-metric evaluation as well as bench top clinical evaluation, focusing on the execution of simulated clinical tasks.<br\/><br\/>If successful, the results of the research will lead to improvements in manipulation capability for use in highly technical and safety critical minimally-invasive co-robotic surgical procedures. Specifically, interleaved continuum-rigid manipulation will enable levels of performance required for cooperative manipulation tasks in emerging co-robotic surgical techniques while maintaining safety. New classes of interventional techniques for neurological, cardiac, and other high risk procedures will be possible, resulting in improved outcomes and reduced morbidity. In addition, the extension to larger scale manipulators could have significant impact in other co-robotic application areas including search and rescue robotics, light-manufacturing, and home and healthcare assistive robotics. To facilitate wide dissemination of the project results, major findings will be published in both conference and peer-reviewed journals. Additionally, detailed results, including design and analysis data, will be made available online. Finally, the project will have a strong training focus by providing an integrated research and educational environment for graduate and undergraduate student researchers - including those in underrepresented groups through established University fellowship programs.","title":"NRI: Small: Interleaved Continuum-Rigid Manipulation - Enabling High-Performance and Inherent-Safety in Minimally-Invasive Surgical Procedures","awardID":"1316271","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["555024"],"PO":[548925]},"209727":{"abstract":"What is good research? How can it be encouraged and promoted? While these are fundamental questions for any discipline they are also clearly questions that are very very difficult question to answer. How their efforts are measured and rewarded is fundamental to individuals and their actions. Wide distribution of best practices for evaluating research productivity will have significant impact on researchers and, hence, on the research undertaken. Given computing's impact on the entire nation, improving computing research will significantly impact the nation. <br\/><br\/>The Computing Research Association has established a committee to explore the meaning of scholarship within the computing fields and to bring best practices for evaluating and promoting high quality scholarship to departments and individuals. This project will bring together leaders of the computing research community to explore answers to these questions and to provide advice to the community on how best to foster research that has high impact. CRA Best Practices papers have traditionally had high impact on how the community proceeds it is highly likely that this activity will make a significant contribution to improving both the quality and impact of computing research in the future.","title":"Examining the Computing Research Culture","awardID":"1347734","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["209556"],"PO":["565264"]},"210651":{"abstract":"This is a project to develop a tutor that will teach the skills of innovation through invention. The Invention Tutor will simulate the guidance of a well-trained inquiry teacher, who asks critical questions and gives feedback just at the right time, to push students' thinking forward. By tracking student trajectories through the Invention process, the Invention Tutor will respond adaptively to student moves. Most importantly, the tutor will eliminate the constraint of needing a large teacher: student ratio to implement the Invention method successfully. This work will have broader impact by scaling up a successful instructional technique, with the ultimate goal of enhancing deep learning and transfer in science domains, for both high and low-achieving student populations. The project also has significant intellectual merit. The research will identify effective forms of support for Invention tasks and expand our understanding of the Invention process itself. More generally, findings will inform the field's understanding of the process of discovery and how best to guide it. In addition, the Invention Tutor will represent a new kind of tutor that guides open-ended discovery tasks, as opposed to the more typical tutor that guides students to solve well-defined problems.<br\/><br\/>Innovation is an important 21st century skill. The Invention tutor strives to teach that skill without the high teacher:student ratio typically required when teaching such soft skills. Most intelligent tutoring systems rely on a fixed set of rules that guide how the tutor interacts with students that are derived from a study of the tutor's discipline and a cognitive task analysis of what is required to learn the subject. The Invention tutor has no such underlying cognitive task analysis and the challenge of building the tutor will lie in uncovering the rules that guide tutor-student interactions and building upon previous tutor-student successful interactions. The intelligent tutoring system will be disseminated through free web access and the data collected in the project will be available for secondary analyses.","title":"EXP: Developing a tutor to guide students as they invent deep principles with contrasting cases","awardID":"1361062","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}}],"PIcoPI":[564411],"PO":[564412]},"210233":{"abstract":"Often referred to as microblogging, the practice of average citizens reporting on activities \"on-the-ground\" during a disaster is increasingly common. The contents of these message are potentially valuable to responder organizations and victims, but their volume makes it difficult to separate valuable messages from the stream. This project will examine microblogged messages sent during disasters to determine what aspects of the messages (individually and collectively) indicate that they are relevant, verifiable and actionable. Factors to be considered include the content of the messages, the identity of the sender and the overall pattern and spread of messages. The identified factors will then be used to instruct crowdsourced workers who will label messages to create a large corpus of labelled messages. <br\/><br\/>The project is important because microblogging data are seen as increasingly important: they are ubiquitous, rapid and accessible, and they are believed to empower average citizens to become more situationally aware during disasters and to coordinate to help themselves. The result of the project, if it is successful, will be evidence that it is possible to identify relevant, verifiable and actionable messages from a stream of microblogged messages and identification of the evidentiary factors. A further outcome will be a disaster-related, labeled dataset of messages, which will be useful to researchers, e.g., those seeking to automatically classify information within a microblogged data stream.","title":"EAGER: Collaborative Research: Establishing Trustworthy-Citizen-Created Data for Disaster Response and Humanitarian Action","awardID":"1353400","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563413,563414],"PO":["565342"]},"210244":{"abstract":"Computer Vision systems must deal with significant levels of ambiguity - from inter- and intra-object occlusion and varying appearance, lighting, and pose. Probabilistic models provide a principled framework for dealing with uncertainty and for converting evidence into a posteriori belief about the world. Typically, a vision system uses this belief to predict the \"most likely\" or maximum a-posteriori hypothesis. Unfortunately, our current models are inaccurate and this single-best hypothesis is often incorrect. <br\/><br\/>This project explores a novel way to allow vision systems to hedge against uncertainty by producing multiple plausible hypotheses. Specifically, this project develops techniques for finding a diverse set of high-probability solutions from probabilistic models. The project focuses on (a) interactive object cutout (where multiple segmentations are shown to the user to expedite convergence to an acceptable result); (b) semantic segmentation (where multiple plausible scene labelings are propagated to subsequent stages of a cascade for higher-order processing); (c) person\/object tracking (where multiple localization hypotheses on each frame reduce the search space of a sequence tracker). <br\/><br\/>This project is producing new scientific knowledge in the context of probabilistic reasoning and advancing the state of art in computer vision. The techniques developed are useful for other AI domains such as Speech and Natural Language Processing. The PI and his students are broadly disseminating produced work by organizing workshops, tutorials, and journal special issues, and publicly sharing code and results. The project is engaging undergraduate students and women in computer science research.","title":"EAGER: Diverse M-Best Predictions from Probabilistic Models","awardID":"1353694","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[563437],"PO":["564316"]},"205811":{"abstract":"Advances in distributed computing enable the pooling of resources located across the Internet to provide a scalable, and robust solution for many computational needs. Distributed key-value store systems like Google's BigTable and Amazon's Dynamo allow the indexing and retrieval of a large amount of data in parallel, while distributed computing frameworks like MapReduce and Pregel provide a fault-tolerant way to process a large amount of data using distributed computing resources. These distributed computing techniques are applied to the spatial database domain. Specifically, issues involved in storing and retrieving spatial data in a distributed environment, as well as, processing spatial queries in parallel using a distributed computing framework are investigated. All of these methods rely on variants of hashing in order to obtain near constant time behavior in distributing the data and it is preferable that they are as close as possible to being distance-preserving. Specifically, spatial objects in proximity should have similar hash values. In particular, it is desirable to be able to estimate how far apart two objects are (within a given error bound) by just considering their hash values. Such hash functions enable performing an approximate range query using simple hash table lookup operations. Other issues involve the parallel processing of spatial queries. Some easy examples are the distance join query which finds pairs (p,q) of objects (from two different sets) where the distance between p and q is less than a given threshold, or computing the shortest paths from each node to every other node in a road network. More difficult are the spatial problems which can not be easily decomposed into multiple tasks running in parallel, e.g., the distance semi-join query, and network Voronoi diagram construction. This requires developing a generic method to traverse a graph or a tree in parallel to solve these query problems. Ideally, the method should require little or no communication between parallel tasks which will be accomplished by allowing the parallel tasks to produce redundant results which can then be pruned.<br\/><br\/>The developed tools will help improve the robustness and scalability for spatial data management. The parallel query processing results can be useful for query problems which requires traversing a tree or a graph which are often spatially embedded. Having a method to traverse a graph or a tree in parallel that requires little or no communication enables processing of many types of spatial queries using distributed computing resources where currently communication can be very costly. Specifically, it can be expected that the tools will enable spatial applications such as online mapping, computer aided design, online gaming and scientific simulations to handle terabytes of spatial data while it is impossible or inefficient to do with the current technologies. This is of utility to all organizations that process spatial data and attempts will be made to use it in some government agencies. In addition, the project provides educational and research opportunities for graduate and undergraduates. The project web site (http:\/\/www.cs.umd.edu\/~hjs\/distributed-spatial.html) will be used to disseminate results.","title":"III: Small: Managing Spatial Data in a Distributed Environment","awardID":"1320791","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[551003],"PO":["563751"]},"205701":{"abstract":"As the use of elastic cloud architectures and mobile computing systems increases, the need for an additional layer of software based networking arises. The research community has proposed open standards to specify network services without coupling specifications with network interfaces, referred to as Software Defined Networking (SDN). Such a software layer can improve the performance of network routers and switches. As demands due to network security increase, aggregation of traffic from diverse applications including high performance computing will further require novel solutions for the data plane.<br\/><br\/>This project explores hardware as well as software-based solutions to optimize the SDN data plane with respect to latency, throughput, and power efficiency. The work investigates novel algorithms, data structures, and architectures that exploit state-of-the-art technologies including heterogeneous multi-processor system-on-chip architectures, multi\/many-core processors, and Programmable Gate Arrays to realize flexible designs for data plane kernels and understand performance tradeoffs. Novel solutions based on hashing and data structures for large-scale IP lookup as well as parallel solutions for multi-field packet classification to support high performance will be developed. The work also develops new techniques for network virtualization and data aggregation using hybrid trees and virtual engines to achieve high performance on various platforms. The broader impact of the project includes providing a flexible and scalable solution for a high performance Internet backbone to support next generation networking.","title":"SHF: Small: High-performance Data Plane Kernels for Software Defined Networking","awardID":"1320211","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["563658"],"PO":["565272"]},"205822":{"abstract":"Large-scale simulations lead to overwhelming demands on computational resources, the main motivation for model reduction. One seeks to produce a reduced-order model which approximates the original one as accurately as possible, making the simulations much faster and cheaper, while preserving appropriate properties. Model reduction is essential for numerous large scale simulations that require many many runs of the same model at slightly different parameter settings.<br\/><br\/>This project has two major themes that address serious computational issues for linear and nonlinear model reduction. The first problem has to do with the stability and\/or passivity preservation in model reduction of large-scale systems. This takes place in the newly developed Loewner or data-driven framework. The second involves extensions of the DEIM approach to nonlinear model reduction. The techniques developed recently by the investigators have been utilized in a number of areas including: numerous Navier Stokes CFD applications, turbulent flows, shallow water equations; nonlinear elliptic-parabolic systems for modeling lithium-ion batteries; electrical, thermal and micro-electromechanical systems; complex engineering and geophysical flows; finite elastodynamics; nonlinear fracture mechanics, cardiac electrophysiology; reduced order quadrature; neural modeling; production optimization of oil reservoirs; and many others.","title":"AF:Small: Data-Driven Dimension Reduction of Linear and Nonlinear Systems","awardID":"1320866","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[551029,551030],"PO":["565251"]},"210299":{"abstract":"Abstract<br\/><br\/>This Project will support the establishment of a Big Data Consulting Services and Training Center at the University of Georgia (UGA). The goal of this center is to provide faculty and students with basic consultation and training services in order to help them more efficiently meet their data collection, processing, storage, access, sharing, curation, and publications needs. The project proposes to focus on three areas: 1) the use of high performance computing; 2) data management; and 3) cross-disciplinary coordination. Many of the elements are already in place at UGA to support data intensive research. For example, the Georgia Advanced Computing Resource Center (GACRC) and the CUDA Teaching Center already offer resources and some training on the use of high performance computing; the UGA Libraries offer services to help researchers write data management plans; and there are a variety of campus-wide and national resources that researchers can use for their data generation, analysis, and management needs. However, currently there is no clearinghouse or coordinating organization to which any researcher on campus can turn into in order to best incorporate data-intensive approaches into their research. To meet these needs, the project plans to: Form a campus-wide committee comprised of faculty, administrators, information technology (IT) staff, librarians, and students. Survey the campus in order to get a better understanding of data management needs. Visit with individuals who are working with data that are large, long-term, and\/or structurally complex to learn about their needs in more detail and identify the campus and national resources that are available for big data users. The project will then develop a set of web resources that can serve as both a central resource for the campus and a starting point for further efforts to enhance UGA's cyberinfrastructure capabilities. Finally, the project will continue to conduct the popular university-wide \"big data\" events under the auspices of the center to aid in communicating, coordinating, and disseminating activity around data-intensive research.","title":"EAGER: Big Data Consulting Services and Training Center","awardID":"1354626","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[563571],"PO":["565292"]},"204744":{"abstract":"The overarching goals of this work are to understand and to provide socio-computational support for improving the entire cycle of technology-enabled civic engagement: (1) recruitment of people with a stake in the issues; (2) deliberative discussion in which they learn about the policy issues, engage with each other, voice questions and recount experiences; and (3) consensus building in which participants move toward collaborative content-creation, summarization of the knowledge that has emerged in discussion and the development of agreement around key points. In practice, efforts to use social media for citizen policy consultations often fell far short of their knowledge-generating and democracy-reinforcing goals. There thus is a crucial need to discover how to design civic engagement spaces that leverage the potential of social media, so that they support not simply more participation but rather better participation that will benefit both the policymakers seeking input and the citizens who participate in the discussion. <br\/><br\/>To achieve this goal, the project integrates computer science research on natural language learning for social-computational systems, human-computer interaction research on online communities, social media design, and social science research on motivation and individual and group deliberative processes. The research will advance behavioral science understanding of the relationship between individual characteristics and successful e-deliberation; the communicative processes that characterize successful e-deliberation; and the group processes and moderator behaviors that promote a shift from open discussion to consensus building. It will advance the state-of-the-art in natural language processing by developing joint human-computer text analysis techniques to (1) promote on-line civic engagement in policy discussions and (2) facilitate deliberative moderation in this collaborative online setting. It will add to human-computer interaction by advancing recommender systems, online communities, and social media research to support mentoring activities and engagement with alternate points of view. Finally, it will extend scientific understanding of how to motivate and support broader, better citizen participation in public policymaking.<br\/><br\/>The work will have at least five broader impacts: (1) increase understanding of, and infrastructure for, e-participation in policy-making, and provide annotated datasets of civic deliberations for use by other researchers; (2) enhance education through graduate and undergraduate mentoring and development of a new interdisciplinary course on Online Civic Engagement; (3) promote STEM education diversity with programs for middle and high school girls; (4) provide community and government outreach activities; (5) benefit society by improved civic engagement in policymaking in general.","title":"HCC: Large: Social-Computational Support of Civic Engagement in Public Policymaking","awardID":"1314778","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["550147",548295,548296,548297],"PO":["564456"]},"205602":{"abstract":"Android software is widely used in smartphones, tablets, and various<br\/>other devices. In devices with limited resources, an application that<br\/>consumes too many resources can lead to slowdowns, crashes, and<br\/>negative user experience. Software defects related to leaking of<br\/>resources are widespread and varied, but there does not exist a<br\/>comprehensive strategy for testing and debugging of such leaks.<br\/>Another common problem in Android software is when an application<br\/>takes too long to respond to a user action, in which case it is<br\/>perceived as sluggish and of low quality. There do not exist testing<br\/>techniques or tools for exposing the underlying software defects<br\/>leading to poor responsiveness.<br\/><br\/>Resource leaks and poor responsiveness can have severe effects on<br\/>software reliability, performance, and marketplace success. This<br\/>project develops LeakDroid, a novel approach and toolset for testing<br\/>and debugging of such defects. Static code analyses are used to<br\/>extract precise GUI models of Android applications; such analyses are<br\/>essential for many algorithms for program understanding, optimization,<br\/>testing, and debugging. Next, GUI tests are generated based on common<br\/>leak patterns, through sequences of GUI events that should not lead to<br\/>increases in resource usage. Poor responsiveness is exposed with tests<br\/>that introduce delays at chosen code locations. Debugging techniques<br\/>are then employed to find leaking data structures and poor<br\/>responsiveness operations, as well as the corresponding defective<br\/>code. These contributions advance the state of the art in the<br\/>important area of analysis, testing, and debugging for Android<br\/>software. When these advances and tools become part of software<br\/>development practices, they would result in lower costs and increased<br\/>quality applications for millions of users of mobile devices. The<br\/>educational efforts of the project develop the skills of the next<br\/>generation of creators of mobile software","title":"SHF: Small: LeakDroid: Exposing Leaks and Jank in Android Applications","awardID":"1319695","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550527],"PO":["564388"]},"205723":{"abstract":"A 2000 report by the Institute of Medicine indicated that 100,000 patients die in the U.S. each year due to suboptimal treatment decisions made by healthcare professionals who operate under pressure of time while processing overwhelming amounts of data, particularly in the operating rooms and intensive care units. Corrective measures and new regulations have not led to improvements yet. In developed countries, about 6% of the economy is spent on upkeep of infrastructure, and about 50% of the original acquisition cost is spent on maintenance of equipment. Managers of, e.g., fleets of aircraft, are too often unable to maintain the required levels of equipment availability due to unexpected, but identifiable in data, crises in maintenance and logistics: only about 2\/3 of the U.S. military aircraft can be flown at once. Expediting solutions often causes hundreds of millions in avoidable expenses. These two examples, as well as many other societally, economically, and scientifically important domains of human activity, involve large amounts of multi-stream data, which may carry information helpful in mitigating some of the adversities. Existing research efforts produce algorithms that extract useful information from individual sources of data. Substantial new benefits, however, could be realized by exploiting relationships between streams of data. This research program will comprehensively and pragmatically explore that opportunity, and impact communities of healthcare practitioners, equipment managers, as well as users in other domains wherever multiple streams of corroborative evidence are available, it will also benefit students and trainees, and the scientific community at large.<br\/><br\/>This research project will develop and extensively evaluate new algorithms to identify informative correlations between multiple and diverse streams of large, multivariate, numeric and symbolic, potentially sparse data. These algorithms will identify subsets of features and records of data that follow distinct cross-stream relationship patterns, enable descriptive and predictive analytics in static and temporal settings, and allow robust forecasts. The proposed work will build on prior efforts towards detection of complex anomalous patterns in single streams of multidimensional data, and expand it towards cross-stream analysis of multiple data sources. Expected results will allow, e.g., detection of patterns of change in relationships between vital signs measured at the bedside of intensive care patients and their records of treatment or medication, and outcomes. These patterns may be indicative of non-standard responses of a patient to a treatment, or signal emergence of a health crisis. In a broader perspective, this effort will substantially expand capabilities of current techniques of cross-stream analytics such as Canonical Correlation Analysis, it will develop a new variant of Gaussian Processes framework to model inter-stream dynamics, produce new information-theoretical modeling of correlations between streams of symbolic variables (with an extension to handle datasets with a mix of numeric and symbolic features), and it will provide a framework for identifying multimodal structures of cross-stream relationships, including disjunctive and conjunctive-disjunctive patterns. It will take on highly challenging intellectual endeavors while aiming for significant benefits of societal importance, with the primary impact demonstration areas in bed-side informatics and equipment health management. Resulting software implementations of the new algorithms and illustrative examples of their use will be shared with the general research community. The bulk of the proposed work will be performed by graduate students and medical fellows who will immediately use the acquired knowledge in their careers. The PIs will include the results in outreach activities and in their training and teaching course materials.","title":"III: Small: Discovering Complex Anomalous Mappings","awardID":"1320347","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["565056",550804],"PO":["565136"]},"205844":{"abstract":"Secure systems demand theoretical guarantees.\u00a0\u00a0The problem of secure multiparty computation (MPC) abstracts many important problems in distributed security, and thus offers the promise of a general framework for designing distributed algorithms with provable security guarantees.\u00a0\u00a0 Unfortunately, current algorithms for solving MPC are far from efficient.\u00a0\u00a0Thus, system builders, when faced with a problem that falls under the rubric of MPC, are unable to make use of established algorithms with well-known theoretical guarantees. Instead, they are frequently required to design new ad hoc heuristics. The goal of this project is to address this vacuity by designing resource efficient algorithms for MPC.<br\/><br\/>In the MPC problem, there are n players, each with a private input. The goal is to securely compute an n-ary function, f, over all inputs, without revealing anything more about the inputs than can be learned from the output of the function.\u00a0\u00a0We assume that f can be computed by a circuit with m logic gates. In this project, the PI will design algorithms for MPC that are robust in that 1) they can tolerate up to a 1\/3 fraction of the players being controlled by an adversary; and 2) they work even in the case where all non-faulty players are selfish but rational.\u00a0\u00a0Importantly, the algorithms will have the following resource costs: each player sends O(n+m\/n + sqrt(n)) bits, and performs O(n+m\/n+ sqrt(n)) computations. These resource costs are significant improvements over state of the art MPC algorithms, which require each player to send O(nm) bits and perform\u00a0O(nm) computations.<br\/><br\/>MPC generalizes many important problems in distributed computing including classic problems such as: auctions, threshold cryptography, voting and privacy-preserving data mining; and contemporary problems such as: cloud computing, and computing over peer-to-peer networks. Thus, a solution to the secure MPC problem will likely enable progress in many fundamental problems.\u00a0\u00a0A major goal of this project is to apply efficient MPC algorithms to the problem of provably secure and scalable anonymous broadcast.","title":"AF: SMALL: Quorums Quicken Queries - Towards Practical Secure Multiparty Computation","awardID":"1320994","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[551090],"PO":["565251"]},"205965":{"abstract":"Title: Collaborative Research: SAVI- Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field.<br\/><br\/>This award is designated as a Science Across Virtual Institutes (SAVI) award and is co-funded by NSF's Office of International and Integrative Activities. Four U.S. environmental observatories (National Ecological Observatory Network (NEON), Ocean Observatories Initiative (OOI), Advanced Modular Incoherent Scatter Radar (AMISR) and collectively EarthScope, comprising USArray and the Plate Boundary Observatory (PBO), operated respectively by the Incorporated Research Institutions for Seismology (IRIS) and UNAVCO) are working together with their respective counterparts from the European Union on developing common data policies and standards relevant to global research infrastructures in the environment field. <br\/><br\/>The project goals include developing new understanding through broad harmonization of data and constructing multi- discipline, synergistic data products that have wider societal importance. In addition, these activities will help the scientific community to better develop coupled models that better capture critical feedbacks and interactions of the earth system. These models are needed to help make these types of data more accessible to decision-makers at many levels. <br\/><br\/>Linking these existing programs provides a unique opportunity for early career researchers and students to learn in a multi-disciplinary environment and to benefit not only from the U.S. participants but also their European counterparts. The observatories will be developing graduate and post-graduate courses and the workshops will seek to include early career scientists and students. <br\/><br\/>The proposed activities will focus on addressing societally relevant challenges. Through case studies, ways of harmonizing data will be investigated with the goal of making the data from these observatories easier to include in effective decision-making. Disasters, carbon and water will likely be the first set of issues addressed.<br\/><br\/>This program is creating opportunities for enhancing the career trajectories of a new generation of researchers in Europe and the U.S. The virtual institute is providing mechanisms for exposing early-career scientists to interdisciplinary, multi-institutional activities focused on environmental data and cyberinfrastructure; arming them with new scientific tools to address challenging questions in harmonizing environmental data to help in effective decision making; and showing them how international partnerships can help to solve global problems. It is recruiting a diverse set of U.S. and European students to create collaborative networks of environmental data and cyberinfrastructure experts across these countries.","title":"SAVI: Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field","awardID":"1321595","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"8077","name":"Science Across Virtual Instits"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7959","name":"MACROSYSTEM BIOLOGY"}}],"PIcoPI":[551426,551427,551428],"PO":["565235"]},"205613":{"abstract":"Deep packet inspection is at the core of several established and emerging networking applications, such as network intrusion detection and content-aware routing. Due to their expressive power, in recent years regular expressions have been adopted in pattern-sets used for these applications in both industry and academia. Existing high-performance regular expression matching engines are based on finite automata, and are implemented using either logic- or memory-based designs. The former allow peak performance on single packet flows with relatively simple logic, but are not scalable to large numbers of flows; the latter offer scalability in the number of flows at the cost of algorithmic and design complexity. Despite the rich body of work in the area, providing worst-case guarantees is still challenging in the presence of complex regular expressions that include repetitions of wildcards and large character sets. Moreover, existing solutions assume that packets are inspected in-order and after data decompression. <br\/><br\/>This project will develop a language abstraction, data structures, and algorithms for line rate deep packet inspection. In particular, the project will consider open problems in regular expression-based deep packet inspection, namely: (i) handling of complex patterns containing repetitions of wildcards and large character sets, and (ii) inspection of out-of-order packets and compressed traffic. A language-based approach to deep packet inspection will be introduced in order to handle the regular expressions? complexity. This project will integrate concepts from automata theory, practices in data structure and algorithm design, analysis of the requirements of networking applications, and system architecture considerations. <br\/><br\/>The previous work performed by the PI on high speed regular expression matching has attracted the attention of several companies. The PI will leverage these contacts to facilitate the transfer of the proposed research. The PI has added two computer architecture courses to the undergraduate and graduate Electrical and Computer Engineering curriculum at University of Missouri (MU); she will introduce a new networking systems course, which will cover the knowledge generated by this research. The PI will leverage the MU Undergraduate Research Program to involve undergraduate students in the proposed work, which will allow students to work at the intersection of three domains: algorithm and data structure design, system architecture and networking applications. The results of this research will be disseminated through publications and presentations, and by releasing open-source software modules on the PI?s Lab website.","title":"NeTS: Small: A Language-Based Approach to Deep Packet Inspection: from Theory to Practice","awardID":"1319748","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[550551],"PO":["564993"]},"205503":{"abstract":"Cyberattacks are enabled by software vulnerabilities that allow attackers to plant software exploits. As old vulnerabilities are found and fixed, attackers continue to find new ones. As a result, software vendors, system administrators and security professionals have come to rely increasingly on techniques that insert additional code into software for detecting and\/or blocking cyber attacks in progress. This process, called software instrumentation, can be applied to the broadest range of software if it operates on binary format in which most software is distributed, rather than on source code. Moreover, binary based techniques are more general: unlike source-codebased techniques, they are not limited to a particular programming language.<br\/><br\/>One of the major challenges in binary instrumentation is the complexity of modern instruction sets. Accurate instrumentation requires the semantics of all instructions to be captured, since all of the analyses and transformations performed by the instrumentor are based on this semantics. Clearly, this is a daunting task even for a single architecture: the Intel manual describing the x86 instruction set runs to over 1500 pages describing over 1100 instructions. When this task is multiplied across different architectures such as ARM, PowerPC, SPARC, MIPS, etc, the effort involved becomes impractically large. This project will develop a novel approach that avoids the need for modeling instruction sets by leveraging knowledge embedded in retargetable code generators in today's compilers such as GCC. This approach not only simplifies the development of instrumentation, but also makes it applicable to all architectures for which a code generator is available. Using this approach, this project will develop a platform to support efficient, architecture-neutral static instrumentation of commercial off-the-shelf (COTS) binaries. Based on this platform, this project will develop several effective instrumentations for hardening software against common vulnerabilities.","title":"TWC: Small: A platform for enhancing security of binary code","awardID":"1319137","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550275],"PO":["564388"]},"205745":{"abstract":"The project aims to liberate wireless access networks currently constrained by spectral and energy scarcity via a new framework, referred to as FreeNet, by exploiting cognitive networking and green energy. FreeNet will be designed and optimized using a series of novel techniques such as dynamic network architecture optimization, network resource aware traffic scheduling, and spectrum sharing. The network architecture optimization framework will apply advanced probability theory to investigate inherent relationships between the optimal network architecture and the availability of spare spectrum and green energy, and adopt control theory to adapt the network architecture according to the dynamics of the spare spectrum and green energy. The network resource aware traffic scheduling and spectrum sharing algorithms will be designed based on optimization theory. Finally, theoretical analysis will be reduced to practice and translated into communications protocols in enabling and prototyping FreeNet. The theoretical analysis will elicit a series of theorems to direct the utilization of green energy in communication networks. <br\/><br\/>The communication protocols design and FreeNet prototyping will provide guidelines for designing resource aware communication systems. The research activities will advance the understanding of inherent relationships among the network architecture, traffic scheduling, spectrum utilization, and energy consumption. FreeNet will be deployed for offloading mobile traffic in urban areas, delivering content in rural areas, and enhancing emergency communication capacity during natural disasters. FreeNet will improve the availability and capacity of wireless networks, broaden the benefits of wireless networking, and enhance the living environment, the earth, by reducing the release of carbon footprints. Research outcomes from this project will be disseminated via publications and a website with frequent updates. Other broader impacts include integration of research and education through participation of both undergraduate and graduate students in the project, incorporation of research outcomes into course work, interactions and exchanges with invited speakers, and seeking involvement of REU students and under-represented groups.","title":"NeTS: Small: FreeNet: Cognitive Wireless Networking Powered by Green Energy","awardID":"1320468","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[550852],"PO":["557315"]},"205757":{"abstract":"Network and graph data appear in many diverse applications such as social networks, biological networks, and mobile ad-hoc networks. In many cases, there is an inherent uncertainty in the available graph data either due to the data collection process or the preprocessing of the data. Furthermore, uncertainty or imprecise information becomes a critical impediment to understanding and effectively utilizing the information contained in such graphs. In this project, we address the problem of managing and mining such uncertain graphs. To do that, we adopt the possible-world semantics to probabilistic and uncertain graphs and within this framework we study algorithms for well-established data-mining problems. Motivated by real-life applications, we focus on specific data-analysis tasks. However, we make our framework general enough so that it can be used for a wide set of tasks and applications. In particular, we develop scalable and efficient algorithms and approaches to address a number of important tasks including nearest neighbor retrieval, clustering and partitioning, finding important nodes and edges, and summarizing large uncertain graphs. <br\/><br\/>We expect the results of this project to have an impact on several application domains. For example, they can help internet-based and social-media related companies to analyze their data and improve their targeting advertisement policies and practices. This will create opportunities for making these companies viable and help the economy of internet-based business. In medicine, biology and biochemistry, networks play a very important role and many of these networks can be better modeled as uncertain networks. Our project can help analyze these networks and lead to new biological insights. The results of this project are disseminated as follows: (1) we develop publicly available prototypes; (2) we include the results of our work in our classes\/lectures; (3) we communicate our results to scientists of computer science and other fields and our industry collaborators through publications and demos. We also actively try to engage in our research graduate and undergraduate students, including women and minorities.<br\/><br\/>For further information see the web site at: http:\/\/www.cs.bu.edu\/~gkollios\/ugraphs\/","title":"III: Small: Managing and Mining Uncertain Graphs","awardID":"1320542","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550881,550882],"PO":["563727"]},"205647":{"abstract":"This collaborative research project, conducted jointly by the investigators from the Michigan State University (MSU) and the University of Michigan at Dearborn (UM-D), investigates the issues and techniques for storing and searching\/querying large scale k-mer data sets (i.e., overlapping k-length subsequences obtained from genome sequences) for sequence analysis in bioinformatics. Efficient k-mer indexing, storage and retrieval are vital to sequence analysis tasks like error correction as sequencing data set sizes increase vastly. Most existing methods for storing and searching k-mers are optimized for exact or range queries. However, this reliance limits the types of sequence analysis that can be done efficiently. Moreover, most existing methods for storing k-mers do not support efficient storage of k-mers at multiple word lengths. For many sequence analysis problems, including error correction, variant detection, and assembly, searches with multiple word lengths enable better sensitivity and specificity. In this project, various techniques for efficiently supporting so-called (discrete) box queries and other related queries (e.g., hybrid queries) on large scale k-mer data sets for sequence analysis are investigated. The approaches to optimizing box queries in solving sequence analysis problems like the error correction are examined. The storage structure and adoption of box queries for supporting searches with multiple word lengths on k-mer data sets are explored. The results from this research will advance the state of knowledge for storage, indexing and retrieval techniques for genome sequence databases. They are expected to significantly impact current practice in bioinformatics by making available new efficient on-disk solutions for sequence analysis. They will also impact a number of other popular application areas including biometrics, image processing, social network, and E-commerce, where processing non-ordered discrete multidimentional data is crucial. <br\/><br\/>This collaborative research project, conducted jointly by the investigators from the Michigan State University (MSU) and the University of Michigan at Dearborn (UM-D), investigates the issues and techniques for storing and searching\/querying large scale k-mer data sets for sequence analysis in bioinformatics. Efficient k-mer indexing, storage and retrieval are vital to sequence analysis tasks like error correction as sequencing data set sizes increase vastly. Most existing methods for storing and searching k-mers are optimized for exact or range queries. However, this reliance limits the types of sequence analysis that can be done efficiently. Moreover, most existing methods for storing k-mers do not support efficient storage of k-mers at multiple word lengths. For many sequence analysis problems, searches with multiple word lengths enable better sensitivity and specificity. In this project, various techniques for efficiently supporting so-called (discrete) box queries and other related queries (e.g., hybrid queries) on large scale k-mer data sets for sequence analysis are investigated. In particular, a new index tree, named the BoND-tree, specially designed for a non-ordered discrete data space characterized by k-mer data sets is developed. The unique properties of the space are exploited to develop new node splitting heuristics for the index tree, and theoretical analysis is performed to show the optimality of the proposed heuristics. Besides the BoND-tree, which is based on data partitioning, space-partitioning based index schemes for box quieres in such a space are also developed. To support a more flexible type of query (i.e., hybrid box and range queries), hybrid index schemes integrating strengths of both box query indexes and range query indexes are studied. To facilitate an efficient index construction for large scale k-mer data sets, bulk loading techniques are also developed for the proposed index trees. In addition, the approaches to optimizing box queries in solving sequence analysis problems like the error correction are examined. The storage structure and adoption of box queries for supporting searches with multiple word lengths on k-mer data sets are also explored. The research in the project will result in the discovery of fundamental properties of the data space for sequence data in bioinformatics, the development of a number of novel storage, indexing and retrieval techniques exploiting the properties of such a data space, and the applications of the proposed techniques for solving important problems in sequence analysis. These results will advance the state of knowledge for storage, indexing and retrieval techniques for genome sequence databases. They are expected to significantly impact current practice in bioinformatics by making available new efficient on-disk solutions for sequence analysis. They will also impact a number of other popular application areas including biometrics, image processing, social network, and","title":"III: Small: Collaborative Research: Supporting Efficient Discrete Box Queries for Sequence Analysis on Large Scale Genome Databases","awardID":"1319909","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8083","name":"Big Data Science &Engineering"}}],"PIcoPI":[550626,550627],"PO":["565136"]},"206868":{"abstract":"Natural language privacy policies have become a de facto standard to address expectations of notice and choice on the Web. Yet, there is ample evidence that users generally do not read these policies and that those who occasionally do struggle to understand what they read. Initiatives aimed at addressing this problem through the development of machine implementable standards or other solutions that require website operators to adhere to more stringent requirements have run into obstacles, with many website operators showing reluctance to commit to anything more than what they currently do. This project offers the prospect of overcoming the limitations of current natural language privacy policies without imposing new requirements on website operators.<br\/><br\/>This frontier project builds on recent advances in natural language processing, privacy preference modeling, crowdsourcing, formal methods, and privacy interfaces to overcome this situation. It combines fundamental research with the development of scalable technologies to semi-automatically extract key privacy policy features from natural language website privacy policies and present these features to users in an easy-to-digest format that enables them to make more informed privacy decisions as they interact with different websites. Work in this project also involves the systematic collection and analysis of website privacy policies, looking for trends and deficiencies both in the wording and content of these policies across different sectors and using this analysis to inform ongoing public policy debates. An important part of this project is to work closely with stake holders in industry to enable the transfer of these technologies to industry for large-scale deployment.","title":"TWC SBE: Option: Frontier: Collaborative: Towards Effective Web Privacy Notice and Choice: A Multi-Disciplinary Prospective","awardID":"1330596","effectiveDate":"2013-09-01","expirationDate":"2017-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553878,553879,553880,"563246",553882],"PO":["562974"]},"205537":{"abstract":"Geometric reconfiguration problems underlie modern mathematical investigations in robotics, mechanical design and structural engineering. In recent years, challenging questions in computational biology and computational materials science (such as protein folding, viral assembly, flexibility studies of crystalline materials and the rational design of macromolecules with desired functionality) are approached with methods originally developed for abstract, geometrically constraint structures.<br\/><br\/>This project will develop novel algorithms based on mathematically rigorous techniques, by exploiting discrete structures underlying three-dimensional articulated structures (such as linkages, panel-and-hinge chains and polyhedra). Examples include reconfigurations of robot arms within their 3D workspace, with singularity- and collision-avoidance, expansive motions and pseudo-triangulations in a 3D and in periodic settings, and origami foldability. <br\/><br\/>The research project builds upon the PIs' previous work, and extends it in new directions. It seeks to generalize concepts from Rigidity Theory (such as pointed pseudo-triangulations), which were previously applied successfully to 2-dimensional linkage reconfiguration problems. It addresses problems concerning extremal configurations of revolute jointed robotic manipulators and the intrinsic mathematical structure of their 3D workspace. It aims at bringing in novel algebraic and combinatorial rigidity-theoretic methods, and at applying them to periodic and crystalline structures. Recent results relating origami design to properties of piecewise linear surfaces will also be extended to questions concerning origami folding properties to rigidity questions of panel-and-hinge structures.<br\/><br\/>The grant provides funding for the training of graduate and undergraduate students. In particular, REU projects emerging from these topics will involve students from Smith College, an all-women college with a sustained reputation for successfully educating minority undergraduate students in the sciences.","title":"AF: Small: Collaborative:RUI: Mathematical foundations of reconfiguration algorithms for geometrically constraint structures","awardID":"1319366","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":[550373],"PO":["565157"]},"205306":{"abstract":"This project develops mathematical and computational approaches for big data exploitation. Fast and online<br\/>algorithms that learn and adapt as data arrives and changes are developed. How to automatically<br\/>understand and reduce redundancy in the data, for a given task, is also addressed in this project. Big data<br\/>comes in multiple forms, e.g., audio and video, audio and text, video and weather, video from multiple<br\/>sources, brain imaging from multiple modalities, friendship networks and individual preferences. This is<br\/>also addressed in this project. The broad impact of the research is born in the large and diverse<br\/>applicability of big data and in the techniques here developed. In the education arena, the developed<br\/>Internet classes have an audience of tens of thousands, and the project provides unique integration of<br\/>research and undergraduate education via different Duke initiatives.<br\/><br\/>The framework follows the parsimony theory of sparse modeling. Challenges are addressed with a gamechanging<br\/>paradigm: learning to optimize; on-line learning what the task-dependent optimizer is expected<br\/>to do, developing computationally efficient algorithms to approximate the ideal behavior of sometimes<br\/>unknown optimizers. The work derives novel multi-modal formulations for network inference, and realtime<br\/>on-line robust PCA and robust NMF, fundamental tools in big data modeling and exploitation; as<br\/>well as robust 3D shape, networks, and multi-modal matching. The formulation elegantly solves bilevel<br\/>optimization problems rendering it efficient for classification and signal separation tasks. Sparse<br\/>modeling is extended to new venues and algorithms, making such techniques usable for big data. The<br\/>formulations and theoretical foundations are complemented with numerous applications.","title":"AF: SMALL: Learning to Parsimoniously Model and Compute with Big Data","awardID":"1318168","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[549760],"PO":["564898"]},"205427":{"abstract":"The speed with which recent pandemics had immense global impact highlights the importance of realtime response and public health decision making, both at local and global levels. For instance, the SARS (Severe Acute Respiratory Syndrome) epidemic is estimated to have started in China in November 2002, had spread to 29 countries by August 2003, and generated a total of 916 confirmed deaths. A pandemic similar to the swine flu in 2009 is estimated to cost $360 billion in a mild scenario to the global economy and up to $4 trillion in an ultra scenario, within just the first year of the outbreak. Today, the key arsenal in the hands of decision makers who try to plan for and\/or react to these outbreaks is software that enable model-driven epidemics and as well as the impacts of pharmaceutical and computer simulations for disease spreading. These software help predict geo-temporal evolution of non-pharmaceutical control measures and interventions, relying on data and models including social contact networks, local and global mobility patterns of individuals, transmission and recovery rates, and outbreak conditions. Unfortunately, because of the volume and complexity of the data and the models, the varying spatial and temporal scales at which the key transmission processes operate and relevant observations are made, today running and interpreting simulations to generate actionable plans are extremely difficult.<br\/><br\/>If effectively leveraged, models reflecting past outbreaks, existing simulation traces obtained from simulation runs, and real-time observations incoming during an outbreak can be collectively used for obtaining a better understanding of the epidemic's characteristics and the underlying diffusion processes, forming and revising models, and performing exploratory, if-then type of hypothetical analyses of epidemic scenarios. More specifically, the proposed epidemic simulation data management system (epiDMS) will address computational challenges that arise from the need to acquire, model, analyze, index, visualize, search, and recompose, in a scalable manner, large volumes of data that arise from observations and simulations during a disease outbreak. Consequently, epiDMS fill an important hole in data-driven decision making during health-care emergencies and, thus, will enable applications and services with significant economic and health impact.<br\/><br\/>The key observation is that the modeling and execution can be significantly reduced using a data-driven approach that supports data and simulation reuse in new settings and contexts. Relying on this observation, in order to support data-driven modeling and execution of epidemic spread simulations, this team will develop<br\/><br\/>+ an epidemic data and model store (epiStore) to support acquisition and integration of relevant data and models.<br\/><br\/>+ a novel networks-of-traces (NT) data model to accommodate multi-resolution, interconnected and inter-dependent, incomplete\/imprecise, multi-layer (networks), and temporal (time series or traces) epidemic data.<br\/><br\/>+ algorithms and data structures to support indexing of networks-of-traces (NT) data sets, including extraction of salient multi-variate temporal features from inter-dependent parameters, spanning multiple simulation layers and geo-spatial frames, driven by complex dynamic processes operating at different resolutions.<br\/><br\/>+ algorithms to support the analysis of networks-of-traces (NT) datasets, including identification of unknown dependencies across the<br\/>input parameters and output variables spanning the different layers of the observation and simulation data.<br\/><br\/>The proposed NT data model and algorithms will be brought together in an epidemic simulation data management system (epiDMS). For broadest impact, the proposed epidemic simulation data management system (epiDMS) will be designed in a way that interfaces with the popular Global Epidemic and Mobility (GLEaM) simulation engine, a publicly available software suit to explore epidemic spreading scenarios at the global scale. To achieve necessary scalabilities, epiDMS will employ novel multiresolution data partitioning and resource allocation strategies and will leverage massive parallelism.","title":"III: Small: Data Management for Real-Time Data Driven Epidemic Spread Simulations","awardID":"1318788","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["558560",550083,"558561"],"PO":["565136"]},"208705":{"abstract":"This proposal seeks funding for travel support for students to participate in the ACM BCB (Bioinformatics and Computational Biology) Conference in September 2013 in Washington D.C. The ACM-BCB is expected to be a premier forum in Bioinformatics and Computational Biology. The conference will host three keynote sessions, research sessions, session-specific invited speakers, workshops, tutorials, panel discussion, and poster sessions. Special activities such as a doctoral consortium and student poster session will be specifically targeted for the students. The ACM-BCB conference has been successful in building a bioinformatics and computational biology community through the newly established ACM Special Interest Group in Bioinformatics and Computational Biology (SIGBioinformatics). The proposed grant will broaden the participation graduate students and help develop the next generation of researchers and educators in the computer science, biological science, biomedical science and other related areas.","title":"ACM BCB 2013: Conference on Bioinformatics and Computational Biology","awardID":"1341410","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[559465],"PO":["565136"]},"209926":{"abstract":"Much of the most successful software for processing and understanding natural language is based on learning from labeled examples. However, applications to diverse genres such as social media and historical documents have demonstrated the limitations of this approach since the application data differs dramatically from the training examples. Labeling training datasets for each new genre is prohibitively expensive. Methods that adapt the software between the original source domain and the target --- for example, from 20th century newspapers to Shakespearean drama --- are an attractive alternative and an active research area. However, language does not naturally fall into a few source and target domains; rather, documents exist in a multidimensional field of similarity and difference, based on metadata attributes such as the date of publication. In addition, binary source\/target adaptation ignores vast amounts of unlabeled data that may bridge the gap between, say, the 20th and 17th centuries, or between text from the Wall Street Journal and text entered on Twitter.<br\/><br\/>This EAGER award explores a new approach to adapting language technology to new application domains. Using explicit document metadata such as date of authorship (for historical documents) or product type (for online reviews), documents are situated in a network of fine-grained domains. Micro-adaptation is then performed between adjacent nodes in the network, which are expected to be more similar to each other than (distant) the source and target domains. These micro-adaptations can then be propagated across the domain graph, yielding an adaptation path from source to target. Empirical evaluations will compare this approach to the current state-of-the-art practices: adapting directly from source to target, and adapting from the source to a broader set of non-source documents. In addition, a theoretical analysis will identify conditions under which this approach is likely to succeed.<br\/><br\/>Language technology already impacts society by facilitating the retrieval, organization, and summarization of information, but its inability to transcend a small set of training domains is one of the most critical obstacles to more widespread adoption. Key application domains such as social media, patient medical records, and legal documents differ substantially from available training corpora, and the development of effective technology for these areas depends on bridging the domain gap. In addition, the sociocultural variation found in online language dramatically reduces the performance of state-of-the-art systems, creating a \"language gap\" between standard and minority dialects. This research is not tied to any specific language processing task; rather, it promises to build a more robust foundation that can apply across many tasks, bringing the benefits of language technology to new users and settings.","title":"EAGER: Exploring Adapting Language Technology Across a Network of Domains","awardID":"1349837","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[562759],"PO":[562760]},"205218":{"abstract":"The research objective of this award is to develop methods for modeling, design and control of a large number of magnetic mobile micro-robots for a given specific application. Here, micro-robots are proposed to be made of novel smart magnetic composites for remote, parallel magnetic actuation and control in two dimensions and three dimensions in small confined spaces such as inside the human body, a micro-fluidic chip, and small desktop workspaces. The research will result in methods which are general enough to apply across all magnetic micro-robotic systems and that can be exploited in diverse applications. Deliverables include a catalog of fundamental modeling, analysis, design and fabrication tools, demonstration and validation via hardware, documentation of research results, and engineering student education.<br\/><br\/>If successful, the results of this research will provide an opportunity to create new micro-robotic systems for use in health-care, microfluidic bio-devices, tissue engineering, programmable matter, and desktop micro-manufacturing applications. The results will be disseminated to allow the creation of commercial micro-robotic devices for medical, bioengineering and micro-manufacturing applications. Graduate and undergraduate engineering students will benefit through classroom instruction and involvement in the research. The research results of the project will be presented to children, K-12 students, K-12 teachers, IEEE student members, and college students through public lectures. The achievements of the project will be also exhibited in science museums for educating the public and children on micro-robotics.","title":"NRI: Small: Magnetic Mobile Micro-Robotic Swarms using Smart Magnetic Composites","awardID":"1317477","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549509],"PO":["563744"]},"209937":{"abstract":"Funds are requested to support students to attend the first IEEE Conference on Communications and Network Security (CNS), which is sponsored by IEEE ComSoc and will be held on October 14-16, 2013, in Washington DC, USA. IEEE CNS conference is a spin-off of IEEE INFOCOM, and is positioned to be a core ComSoc conference that serves as a premier forum for communications and network researchers, practitioners, policy makers, and users to exchange ideas, techniques and tools, raise awareness, and share experience related to security and privacy. It provides a unique bridge for the communications and networking community and the security community. The technical scope of this conference encompasses a large number of areas in communications and network security which facilitates gathering of prominent scientists from academia, governments and industries from around the world to discuss the emerging and multidisciplinary frontiers of networking security technology. The PI requests support from NSF (at the total amount of $14,000) to help cover the expenses of approximately 15-20 US-based graduate students. The selection process for the travel award will involve a sub-committee of the CNS 2013 Executive Committee, chaired by the PI. Priority will be given to students who will benefit from attending this conference, but are unlikely to attend due to the unavailability of travel funding. The travel grants will tend to benefit a wider demographic of graduate students, which will help to broaden participation in the research area of communications and network security.","title":"Student Travel Support for IEEE CNS 2013","awardID":"1349946","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["509531"],"PO":["557315"]},"208738":{"abstract":"In this Cyberlearning: Transforming Education-funded EAGER Project, the PIs are carrying out first steps in understanding how to use the archives of cities to draw inner-city youth into inquiry and expressive activities in the context of documenting the interesting history, geography, civics, and so on, of the places they live. The work is being carried out in Nashville, Tennessee. It is a collaborative effort of Vanderbilt University and city of Nashville, represented by The Country Music Hall of Fame and The Nashville Library System. The project uses what is known about how people learn to inform the design of technology and pedagogy in support of \"place-based education\" -- education that takes advantage of the place where learners live to promote learning and sustain their engagement. In this approach to place-based education, learners are taking on the curatorial practices of museum and library curators and using those practices to learn both the history of their city and also how to synthesize across information sources and express themselves in ways that are engaging and educational for others. New knowledge is being created about youth authoring and its potential for promoting learning and also about bringing the wealth of interesting city and cultural archives to the people who live in a city and its visitors.<br\/><br\/>Social media and \"curation\" applications have drawn the interest of many youth. The PIs in this project recognize potential in these technologies and in the interests of young people in curating collections for promoting learning of history, civics, geography, and other content and at the same time learning to synthesize across information sources and express themselves well. They envision a technical and socio-technical infrastructure for a new kind of informal after-school learning environment in which technology provides structure and aid as learners explore the archives of their city and compose compelling narratives to teach others, and social and interaction structures (the pedagogy) empower learners to engage in telling the stories of their city. The work is timely, as the tools that bring location-aware media together with mobile, personal information devices are becoming more capable and sophisticated at a rapid rate. The tool designs and pedagogy that are created through this project and follow-on projects will be applicable to supporting youth initiatives, both formal and informal, throughout our country and internationally.","title":"EAGER: Building Learning in Urban Extended Spaces","awardID":"1341882","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}}],"PIcoPI":[559553,559554],"PO":["562669"]},"199311":{"abstract":"The goals of this CAREER proposal are to help elucidate the principles that make robust object recognition possible. Object recognition is a problem that must be solved by all living organisms, from single-cell organisms to humans. Although the physical signals for recognition based on chemical events, light or sound waves are different, the computational requirements for analyzing these events appear to be similar. Specifically, there are two main properties that any system that mediates robust object recognition must have. The first property is known as \"invariance.\" It endows neurons with a similar response to the same object observed from different viewpoints. The second property is known as \"selectivity.\" Selectivity requires that neurons produce different responses to potentially quite similar objects (such as different faces) even when presented from similar viewpoints. It is straightforward to make detectors that are invariant but not selective or selective but not invariant. The difficulty lies in making detectors that are both selective and invariant. <br\/><br\/>This CAREER project will develop statistical methods for simultaneously characterizing both the invariance properties of neurons and their selectivity to specific features in the environment. The developed methods will have three distinguishing characteristics. First, it will be possible to recover new types of invariance without any prior assumptions of what the dominant type of invariance is for any given neuron or brain region. Second, they will make it possible to characterize imperfect and approximate types of invariance. Third, the methods will be geared towards stimuli typical of the natural sensory environment that are rich in objects and elicit robust responses from neurons from all stages of sensory processing. These three properties of the developed methods will make it possible to simultaneously study multiple neurons both within and across different regions, without the need to adjust stimuli to a particular neuron or brain region. Application of the developed methods to responses of neurons that mediate visual and auditory object recognition in the brain will help reveal the common principles of sensory processing in the brain and may ultimately lead to improved designs of artificial recognition systems, including sensory prostheses.<br\/><br\/>This research will be integrated into education and outreach activities involving K-12 students, undergraduate and graduate students. The educational component will help integrate knowledge acquired in computer science, physics, and neuroscience, training a new generation of scientists that are proficient in these disciplines. Outreach to local schools and museums, as well as the creation of an online course will help reach a diverse range of students both locally and worldwide.","title":"CAREER: Characterizing feature selectivity and invariance in deep neural architectures","awardID":"1254123","effectiveDate":"2013-09-15","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1045","name":"CAREER: FACULTY EARLY CAR DEV"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[534538],"PO":["564318"]},"209849":{"abstract":"Rapid advances in sensing technologies, as well as the ubiquity of online social and information networks, have led to unprecedented increase in the size and quantity of data sets that need to be stored and processed. Researchers are only beginning to grapple with the scale of data that modern systems generate. This provides a unique opportunity to design and implement algorithms at a scale that wasn't witnessed before, leading to unique challenges. The focus of this project is on designing algorithms for (arguably) the dominant computing paradigm for big data, MapReduce, and its variants. Despite its broad adoption across industries, there is very little work on developing theoretical models for these systems, or developing algorithms for natural optimization problems that arise in big-data applications. <br\/><br\/>This project seeks to develop a strong theoretical understanding of large-scale data processing and a principled approach to algorithm design in these settings. The key challenge with MapReduce-type platforms is that they incorporate and modify aspects from several computing paradigms, such as streaming and parallel computing, which leads to significant challenges in modeling such systems and in developing algorithm design principles that apply across platforms. The principal investigator (PI) first seeks to define complexity measures for MapReduce algorithms. The PI next aims to find connections to and differentiate from more classical and widely studied models such as the Parallel Random Access Machine (PRAM), streaming computation, and I\/O-efficient computation models, seeking to discover common algorithm design principles and computational limitations in this process. Via this process, the PI will develop efficient algorithms for classes of graph theoretic and statistical optimization problems that arise in big-data settings, and will also develop a suite of practical implementations for these. The problems considered in this proposal are of fundamental nature, as evinced by their long and historic roots in theoretical computer science, parallel computation, and databases. Computational science will benefit from this research. Other broader impacts include graduate and undergraduate training in research and education.","title":"CCF:AF:EAGER Algorithmic Paradigms for Computation on MapReduce","awardID":"1348696","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[562557],"PO":["565251"]},"210850":{"abstract":"NSF 10&#8208;575 SHB: Large: Collaborative Research: <br\/>Integrated Communication and Inference Systems for<br\/>Continuous Coordinated Care of Older Adults in the Home<br\/>PI Bajcsy, UC Berkeley, Co&#8208;PI Jimison, OHSU<br\/><br\/>Abstract<br\/><br\/>This research project addresses the important problem of improving and maintaining peoples? healthy lifestyles by inventing smart technology based on fundamental scientific principles. The approaches are economically feasible and socially compelling approaches, with a focus on maintaining the health and independence of older adults in a home environment. The project uses a mix of networking and monitoring technologies to connect older adults with a remote health coach (real person facilitated by a semi-automated program) and remote family members. One of the key design issues is how best to preserve privacy and enable the participants to control the distribution and sharing of their data. The intervention is designed to provide coordinated and continuous health management.<br\/><br\/>The research for this project uses the integration of data from a variety of sensors in the home, yielding information for activity monitoring, sleep monitoring, gait and movement analysis, socialization measures, as well as a variety of cognitive metrics derived from computer interactions with adaptive games. Rigorous computational engineering models of the cognitive and physical functions of the patient, as well as context and environment, are used to infer patient state and provide feedback for the patient and the remote health coach. The modeling techniques include Partially Observable Markov Process and Hybrid Control Modes. User models that incorporate behavior change principles are then used to drive algorithms to optimize automated feedback and recommendations that serve as prompts for a health coach managing a large number of patients. These approaches to remote health management are evaluated by leveraging an existing prototype platform with the capability of collecting data from the homes of elderly participants.","title":"SHB: Large: Collaborative Research: Integrated Communications and Inference Systems for Continuous Coordinated Care of Older Adults in the Home","awardID":"1407928","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[564897],"PO":[564898]},"199146":{"abstract":"To date, the application of quantitative security and privacy metrics metrics has seen its greatest successes when exploring the worst-case properties of a system. That is, given a powerful adversary, to what extent does the system preserve some relevant set of properties? While such analyses allow experts to build systems that are resistant to strong attackers, many deployed systems were not designed in this manner. In fact, there is growing evidence that users' privacy is routinely compromised as a byproduct of using social, participatory, and distributed applications. Given that people find inherent utility in using systems that are not secure against worst-case adversaries, this project investigates a complementary question: Can we help users better manage their participation in systems that are not privacy-preserving in an absolute sense?<br\/><br\/>This project is developing a principled approach that enables individuals to (i) quantitatively specify and assess their security, privacy, and utility goals; (ii) qualitatively express preferences on the relative importance of these goals; (iii) explore the implications of their system interactions by leveraging the trade-off spaces resulting from these quantitative and qualitative specifications; and (iv) enact locally-enforceable changes to their system usage to better balance competing needs. This project is designing computational tools that enable everyday users to better manage their system participation by understanding the interplay between security, privacy, and utility. Educational materials are being developed to support two undergraduate courses---one for computer science majors and one for non-majors---that explore the social, technical, and privacy implications of our increasingly digitized society.","title":"CAREER: UCPriv: User-Centric Privacy Management","awardID":"1253204","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[534206],"PO":["562974"]},"202820":{"abstract":"Excessive energy consumption is a major constraint when designing and deploying the next generation of supercomputers. Minimizing energy consumption of high performance computing requires novel energy-conscious technologies at multiple layers from architecture, system support, and applications. One obstacle that hinders the exploration of these new technologies is the lack of tools and systems that can provide accurate, fine-grained, and real-time power and energy measurement for technology evaluation and verification. <br\/><br\/>This project bridges the gap by building Marcher, a heterogeneous high performance computing infrastructure equipped with cutting-edge power-efficient accelerators including Intel Many Integrated Cores and Nvidia Graphics Processing Units, power-aware memory systems, hybrid storage with hard disk drives and solid state disks, and high performance interconnects. The Marcher system supports the development of two complementary component-level power measurement tools for major computer components: (i) pluggable Power Data Acquisition Card (PODAC) for direct and decomposed power measurement and (ii) Software Power Meter (SoftMeter) that indirectly estimates the power consumption of systems where direct measurement is not feasible or too costly. <br\/><br\/>Upon completion of this project, both PODAC and SoftMeter will be made available to a broader community and researchers to establish their own power-aware systems. Marcher will be open to external research groups and provide users with comprehensive and detailed performance and power profiles to aid the research in energy efficient software design and system development.","title":"Collaborative Research: II-NEW: Marcher - A Heterogeneous High Performance Computing Infrastructure for Research and Education in Green Computing","awardID":"1305359","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543570,543571],"PO":["565272"]},"202831":{"abstract":"Excessive energy consumption is a major constraint when designing and deploying the next generation of supercomputers. Minimizing energy consumption of high performance computing requires novel energy-conscious technologies at multiple layers from architecture, system support, and applications. One obstacle that hinders the exploration of these new technologies is the lack of tools and systems that can provide accurate, fine-grained, and real-time power and energy measurement for technology evaluation and verification. <br\/><br\/>This project bridges the gap by building Marcher, a heterogeneous high performance computing infrastructure equipped with cutting-edge power-efficient accelerators including Intel Many Integrated Cores and Nvidia Graphics Processing Units, power-aware memory systems, hybrid storage with hard disk drives and solid state disks, and high performance interconnects. The Marcher system supports the development of two complementary component-level power measurement tools for major computer components: (i) pluggable Power Data Acquisition Card (PODAC) for direct and decomposed power measurement and (ii) Software Power Meter (SoftMeter) that indirectly estimates the power consumption of systems where direct measurement is not feasible or too costly. <br\/><br\/>Upon completion of this project, both PODAC and SoftMeter will be made available to a broader community and researchers to establish their own power-aware systems. Marcher will be open to external research groups and provide users with comprehensive and detailed performance and power profiles to aid the research in energy efficient software design and system development.","title":"Collaborative Research: II-NEW: Marcher - A Heterogeneous High Performance Computing Infrastructure for Research and Education in Green Computing","awardID":"1305382","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543606],"PO":["565272"]},"200796":{"abstract":"TECHNICAL SUMMARY<br\/>The Division of Mathematical Sciences and the Division of Materials Research contribute funds to this award. It supports theoretical research and education to advance and elucidate the theory of knotted fields. Though it has been well known for more than three decades that topological defects can act on each other through commutation, it is only in recent times that we can: a) knot defects by design; b) tie and manipulate knotted disclinations; and c) study biaxial nematics, a physical system with a non-Abelian fundamental group. The stream of experimental results that has already started will be ample fodder for studying the topological dynamics of these knots. What makes this most exciting is that these are now questions that can be probed directly through experiment.<br\/>A second thrust of the research is the topological characterization and classification of defects in translationally ordered media. The PI will focus specially on smectic liquid crystals because translational order is only broken in one direction and thus there is only one associated Nambu-Goldstone mode. In addition to dislocations and disclinations, smectics enjoy a different class of geometrically precise configurations, focal conic domains. The interaction between focal conic domains and topological defects is largely underexplored. Finally, and most importantly, because smectics are so soft, dislocations, disclinations and focal conic domains are easy to observe and manipulate in experiment. Building on the work of the PI and his group over the past three years, the PI will formulate a theoretical description to combine the three types of building blocks in the case of three-dimensional smectics. <br\/><br\/>This award also supports the PI's efforts to convey the science of liquid crystals effectively to K-12 and college students, high school teachers, and the general public.<br\/><br\/>NONTECHNICAL SUMMARY<br\/>The Division of Mathematical Sciences and the Division of Materials Research contribute funds to this award. It supports theoretical research and education to advance the theory of soft matter. The PI has a track record of learning and applying modern mathematics and incorporating it into his research. In turn, these insights have been applied directly to experiments that employ traditional microscopy, polarizing filters, and hot plates. The focus of this research project is on liquid crystals, materials that pervade our modern life, from our phones and watches to our computers and cars. The optical properties that make them the backbone of a $100 billion\/year industry arise from a beautiful interplay between chemistry, theoretical physics, geometry, and topology. The PI will study the generalization of the fluid vortices that shed off the tip of a canoe paddle or an airplane wing. To do this, the PI will integrate the fields of materials physics, statistical mechanics, topology, and geometry, challenging traditional disciplinary lines with the hope of substantial technological and intellectual progress. Visually compelling, the field of liquid crystals has and will allow the PI to reach out effectively to K-12 and college students, high school teachers, and the general public.","title":"Topological and Geometrical Problems in Soft Matter","awardID":"1262047","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1765","name":"CONDENSED MATTER & MAT THEORY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0507","name":"Division of SCIENCE EDUCATION RESOURCES IM","abbr":"SER"},"pgm":{"id":"1267","name":"TOPOLOGY"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1266","name":"APPLIED MATHEMATICS"}}],"PIcoPI":["554365"],"PO":["141355"]},"210234":{"abstract":"Often referred to as microblogging, the practice of average citizens reporting on activities \"on-the-ground\" during a disaster is increasingly common. The contents of these message are potentially valuable to responder organizations and victims, but their volume makes it difficult to separate valuable messages from the stream. This project will examine microblogged messages sent during disasters to determine what aspects of the messages (individually and collectively) indicate that they are relevant, verifiable and actionable. Factors to be considered include the content of the messages, the identity of the sender and the overall pattern and spread of messages. The identified factors will then be used to instruct crowdsourced workers who will label messages to create a large corpus of labelled messages. <br\/><br\/>The project is important because microblogging data are seen as increasingly important: they are ubiquitous, rapid and accessible, and they are believed to empower average citizens to become more situationally aware during disasters and to coordinate to help themselves. The result of the project, if it is successful, will be evidence that it is possible to identify relevant, verifiable and actionable messages from a stream of microblogged messages and identification of the evidentiary factors. A further outcome will be a disaster-related, labeled dataset of messages, which will be useful to researchers, e.g., those seeking to automatically classify information within a microblogged data stream.","title":"EAGER: Collaborative Research: Establishing Trustworthy-Citizen-Created Data for Disaster Response and Humanitarian Action","awardID":"1353418","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563416],"PO":["565342"]},"203711":{"abstract":"Objective: <br\/>The objective of this project is to investigate the scientific and technological foundations on time, location and integrity of sensor recordings, by exploiting novel intrinsic fingerprints in the environment. An example is the small random-like fluctuations of the electricity frequency known as the Electric Network Frequency (ENF). These environmental fingerprints reflect the attributes and conditions of the power grid and become naturally \"embedded\" into video, audio or other types of sensor signals at the time of recording. They carry time and location information and may facilitate integrity verification of the primary sensing data. Answering questions about the time, location, and integrity of sensor recordings find important applications in crime solving, counter-terrorism, journalism, infrastructure monitoring, smart grid management, and other commercial operations.<br\/><br\/>Intellectual merit: <br\/>The intellectual merit lies on exploring the nearly unchartered research area of forensic evidences in the environment, by drawing synergy across multiple disciplines including signal processing, machine learning, sensor circuits and systems, power engineering, and information forensics\/security. The research exploration and technology development of this project are transformative and interdisciplinary in nature.<br\/><br\/>Broader impacts: <br\/>The broader impacts are two folds: through engaging students, especially female and under-represented ones, in exploratory research, the project's seamless integration of research with education and outreach will prepare the future generation of engineering workforce with interdisciplinary strength; through new technologies and tools developed to advance information forensics, the project has a high potential to benefit a number of applications of global and societal importance.","title":"Exploring Power Network Attributes for Information Forensics","awardID":"1309623","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["98543"],"PO":["565205"]},"210245":{"abstract":"Many people presently interact more with and through computers than they do with other people directly but such interaction may have negative effects that go unobserved. These balance of positive and negative effects could be considered more thoughtfully if the design of the technology created opportunities for reflection, e.g., by creating \"seams\" in interaction that make the balance between the machine's influence and the human push-back more obvious, or by allowing people to nudge one another and themselves in particular directions. The goal of this project is to create at least one proof-of-concept implementation and demonstration of such a reflective design that will provide a paradigm or model for similar development and serve as a basis for future research. The pilot project has four parts: development of a task and example reflective opportunity, technology and experimental development, conduct of an experiment and analysis. <br\/><br\/>The project is important because of the enormous role that computer systems play in the interstices of everyday existence, because the influences that computer systems have are not necessarily well-understood by users, creators or analysts and because of the potential for novel and more beneficial approaches to design that bring these influences to greater awareness. If the project succeeds in demonstrating the value of a reflective design approach, it may influence the ways people in society conceptualize computation, increasing their awareness of the need to cultivate a pro-active stance as users and designers and so provide a new paradigm for human-computer interaction research and development.","title":"EAGER: Designing Reflective Opportunities in Human-Computer Interaction","awardID":"1353723","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563439,563440],"PO":["565342"]},"210025":{"abstract":"From activities as simple as scheduling a meeting to those as complex as balancing a national budget, people take stances in negotiations and decision making. While the related areas of subjectivity and sentiment analysis have received significant attention, work has focused almost exclusively on text, whereas much stance-taking activity is carried out verbally. Early experiments suggest that people alter their speaking style when engaged in stance-taking, and listeners can much more readily detect negative attitudes by listening to the original speech than by reading transcripts. However, due to the diversity of factors that influence speech production, from individual differences to social context, isolating the signals of stance-taking in speech for automatic recognition presents substantial challenges.<br\/><br\/>This Early Grant for Exploratory Research project represents a focused exploration of spoken interactions to provide a characterization of linguistic factors associated with stance-taking and develop computational methods that exploit these features to automatically detect stance-taking behavior. Robust linguistic markers of stance-taking are identified through analysis of both controlled elicitations and archived recordings of Congressional hearings on the financial crisis. The former allow experimental comparisons to highlight sometimes subtle contrasts, while the latter enable validation and extension of those findings in real-world, high-stakes discussions. The analysis includes novel acoustic-phonetic measures of dynamic patterns in speech, such as vowel space scaling and pitch\/energy velocity, with sophisticated visualization techniques developed to support feature exploration. Findings are validated via stance recognition experiments combining acoustic and lexical cues, which lay the foundation for automatic tracking of trends and shifts in attitudes.","title":"EAGER: ATAROS: Automatic Tagging and Recognition of Stance","awardID":"1351034","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1311","name":"LINGUISTICS"}}],"PIcoPI":[562965,562966,562967],"PO":["565215"]},"205812":{"abstract":"This research project is motivated by a growing gap between trends in processor development and needs of modern data-intensive dynamic applications ranging from differential equation solvers to data mining tools to particle dynamics simulations, playing an essential role in science and humanity. These applications feature tremendous amounts of data accesses as well as complex patterns in data accesses or control flows. The properties make them a great challenge for modern processors which are evolving exactly opposite to these applications' needs: A chip's aggregate computing power is rapidly outgrowing memory bandwidth; the rise of throughput-oriented manycores makes system throughput even more sensitive to irregular computations. The new paradigm of program optimizations, non-uniformity--centric, distinctively takes the non-uniform inter-core relations in modern systems as the first-order constraint for program optimizations. <br\/><br\/>The proposed framework named PipeReg is a new way to reorganize data accesses and threads during run time to reduce the influence of irregular computations on the throughput of massively parallel processors. In addition, a novel kind of program transformations, neighborhood-aware, exploiting non-uniform interactions among threads in on-chip storage in a multi-socket multicore system. Together, the two techniques will synergistically remove some important barriers for data-intensive dynamic applications to tap into the full power of future computing systems. The outcome from this research will provide essential support for enhancing the computing efficiency of data-intensive dynamic applications in the era of heterogeneous parallel systems. Because of the critical roles of these applications, this research will help foster sustained advancement in science, commerce, health, and other areas.","title":"SHF: Small: Non-Uniformity--Centric Program Optimizations for Dynamic Computations on Chip Multiprocessors","awardID":"1320796","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":[551005],"PO":["565272"]},"205702":{"abstract":"This project focuses on the development of new analytical tools for modeling the relationships between intra-organizational communication networks and open, external sources of text data. The massive quantities of textual communications generated within organizations constitute a largely untapped source for insightful, timely organizational analytics. The tools under development for this project are designed to jointly analyze the content of communications and the socio-organizational structure comprised by communication ties, thereby allowing researchers and practitioners to identify and analyze the ways in which government officials' extra-governmental communications are related to intra-governmental communications and operations. In producing these tools, this project builds upon extant textual and network analysis methods by focusing on novel probabilistic methods for identifying topics that cut across network domains (e.g., informal email communications, official meeting minutes, and final policy records) and representing the complex flow of topics through government decision and policy-making processes. These methods, along with data collected during the course of this project, enhance organizations' ability to connect streams of external input to their internal operations. In conjunction with a new, publicly available database of local government communication records, this project showcases and builds upon the success of recent efforts, encompassing the gov2.0 movement, to improve government responsiveness through the solicitation of open outside input.","title":"III: Small: Organizational Responsiveness to Open Outside Input: A Modeling Approach based on Statistical Text and Network Analysis","awardID":"1320219","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550753,550754],"PO":["565136"]},"202798":{"abstract":"The computer architecture research community has created a rich and diverse collection of simulators, emulators, and benchmarks. Each individual research group has its own preferred tools, perhaps built on top of an existing or custom infrastructure. These artifacts are used in experiments to evaluate design trade-offs and to analyze implications to performance, energy, reliability, and other metrics. <br\/><br\/>Unfortunately, the ways in which these tools are used for evaluation often hinder the application of the scientific method. The barrier to entry for using various artifacts (i.e., simulation tools and benchmarks) is usually high, when these artifacts are made available. Experiments are rarely open, nor easily repeatable by other researchers, leading to inaccurate comparisons. Moreover, it is difficult for a single group to explore all variations due to state-space explosion.<br\/><br\/>To demonstrate that these problems can be addressed to improve computer architecture evaluation, this pilot project takes the first steps to develop an open-access repository that permits sharing artifacts and experimental results among a broad group of stakeholders in computer architecture. The project builds and engages an active community of users, establishes governance and access policies, and determines the requirements for software services of the repository. This initial prototype provides the community an opportunity to supply feedback and suggestions to evolve the prototype and refine its policies.<br\/><br\/>The project shows that significant collective effort can be saved by sharing simulators and experiments for accountable and repeatable experimentation as part of the scientific method. It avoids the burden of re-implementing and re-creating tools, experiments, data sets, benchmarks, etc. This savings in effort and more sound and complete evaluation can then be directed on innovating new architectural techniques for better computer systems.","title":"CI-ADDO-NEW: OCCAM: Open Curation for Computer Architecture Modeling","awardID":"1305220","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["554329","560516",543497],"PO":["565272"]},"205713":{"abstract":"Humans are diploid organisms with two sets of chromosomes: 22 pairs of autosomes and one pair of sex chromosomes. The two chromosomes in a pair of autosomes are homologous, i.e., they have similar DNA sequences and essentially carry the same type of information but are not identical. The most common type of variation between chromosomes in a pair is that where the base in a specific location differs between the two sequences, i.e., the corresponding alleles on the homologous chromosomes are different. The complete information about DNA variations in an individual genome is provided by haplotypes, the list of alleles at contiguous sites in a region of a single chromosome. Haplotype information is essential for medical and pharmaceutical studies, including understanding variations in gene expressions and recombination patterns.<br\/><br\/>Intellectual Merit:<br\/><br\/>This research aims to develop and analyze novel algorithms for haplotype assembly from next-generation sequencing data. It consists of three main thrusts: (1) Haplotype assembly from next-generation sequencing data is computationally challenging. The first thrust proposes branch-and-bound algorithms that exploit certain structural features of the problem to efficiently find the exact solution. (2) As the size of the haplotype assembly problem grows, the exact solution is increasingly more difficult to obtain. The second thrust is focused on the development of fast heuristic methods with guaranteed performance bounds that enable explicit complexity-accuracy trade-offs. (3) Existing haplotype assembly schemes process DNA fragments comprising nucleotides whose order is already determined by the sequencing platform. The third thrust is focused on the development of algorithms for finding joint solution to the base-calling and haplotype assembly problems, enabling significant improvements in accuracy.<br\/><br\/>Broader Impact:<br\/><br\/>The results of this research will have a major impact on a number of fields that rely on accurate haplotype assembly, including medicine and pharmacogenomics, and will enrich the educational experience of engineering students at the University of Texas at Austin.","title":"AF: Small: Algorithms for Haplotype Assembly from Next-Generation Sequencing Data","awardID":"1320273","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":[550783],"PO":["565223"]},"205834":{"abstract":"The prevention of bugs in software continues to be a major challenge. One approach is to prove the correctness of software with respect to a specification. This approach is both too challenging and unfamiliar to most programmers because of the varied knowledge needed (knowledge about specifications, programs, and theorem provers). A functional language with a dependent type system supports writing programs, specifications, and proofs that the programs adhere to their specifications in a single language, which reduces the amount of specialized knowledge required and minimizes the number of tools a programmer must learn to be able to effectively write and prove software correct. While the benefits of programming in a dependently typed language are appealing, several issues prevent the immediate adoption of this practice. First, writing dependent types (for example, to encode specifications) involves creating new and highly specific types. Hence, it becomes difficult to reuse existing code when manipulating values of these specialized types. Second, writing proofs in a dependently typed language directly can be painful owing to the lack of proof automation. Because of these issues, existing systems often use a separate language for automating the writing of proofs.<br\/><br\/>This project seeks to create a new dependently typed language that addresses both of these problems. The set of types in this language is closed, making it possible to have an eliminator for all types in a predicative hierarchy of universes. Thus, the entire language is reflected as a Martin-Lof universe, supporting generic programming over the entire language. The language addresses the first problem by making it possible to reuse generic functions over newly defined types. It addresses the second problem by interpreting the problem of writing tactics for a proof state as the problem of writing generic functions over a Pi-type. The goal of this proposal is to produce a mechanically verified dependently typed programming language with a principled reflection mechanism that lowers the cost of dependently typed programming.","title":"SHF: Small: Generic Dependently Typed Programming by Reflecting a Predicative Hierarchy of Universes","awardID":"1320934","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[551061],"PO":["564588"]},"202215":{"abstract":"Experimental exploration of self-organizing biomolecular systems, such as viruses, molecular motors and proteins in Alzheimer's disease, has been a dominating driving force in scientific discovery and innovation in the past few decades. Unfortunately, quantitative understanding of biomolecular structure, function, and dynamics severely lags behind the pace of the experimental progress. An average protein in human body has about 5500 atoms, which, together with its surrounding water molecules, involve about 100,000 degrees of freedom. The dimensionality increases dramatically for complex biological processes and biomolecular systems. The real time structure optimization, dynamic simulation, and data analysis of molecular motors and\/or viruses in human cells are intractable with full-atom models at present. A crucial question is how to reduce the number of degrees of freedom, while retaining the fundamental physics in complex biological systems. The proposed research may be transformative. As the first differential geometry based multiscale\/ multiresolution approach to biomolecular systems, it will open a new direction and foster similar approaches in multiscale modeling of other large data systems in future research. Additionally, new persistently stable manifold strategy can be applied to other fields, such as image processing, computer aided design, and fluid mechanics. Furthermore, the proposed new coupled equations will lead to new research topics in geometry, topology, PDE analysis and mathematical biology. Finally, our new theoretical framework is directly integrated into popular software packages to ensure extensive usage by the community of researchers throughout mathematics, computer science and biology. The proposed research has a solid educational component. The project will support the training of student and junior researchers in mathematical modeling, data analysis and algorithm development. The enhancement of curricula from the proposed research is planned as a continuation of PIs teaching-research practice. Special curriculum development, outreach program and annual workshops are designed to further broaden educational and societal impacts. <br\/><br\/><br\/>The proposed research addresses grand challenges in the structure, function and dynamics of self-organizing biomolecular systems due to exceptionally massive data sets. These challenges are tackled through the introduction of a new differential geometry based multiscale model, together with a multiresolution coarse grained method based on persistently stable manifolds in molecular dynamics data. This proposal offers innovative new approaches to an important area in massive data management, dimensionality reduction, computational mathematics and mathematical modeling. This project uses a number of geometric and topological approaches to address the scaling issues.. First, the multidisciplinary team will use multiscale framework which reduces the dimensionality and number of degrees of freedom by a macroscopic continuum description of the aquatic environment, and a microscopic discrete description of biomolecules. To further reduce the dimensionality of excessively large biomolecular systems, they introduce a multiresolution coarse-grained approach based on persistently stable manifolds in molecular dynamics data. A total free energy functional is introduced to bring the macroscopic surface tension and microscopic potential interactions on an equal footing. The differential geometry theory of surfaces is utilized to describe the interface between macroscopic and microscopic domains. Potential driven geometric flows are constructed to minimize the total free energy functional. Euler characteristic and total curvature are employed to analyze the topology and corresponding function of biomolecules. Frenet frames are utilized to characterize the local geometry and associated stable manifolds in dynamical data of biomolecular systems. Machine learning algorithms are proposed to extract stable manifolds. In the last step, a strategy is introduced to explore the persistence of stable manifolds, which provides the assurance for the reliability of the coarse grained model. In addition to promising and extensive preliminary results illustrating the power of this approach, extensive validation and application have been proposed to ensure that this methodology yields robust and powerful tools for biomolecular structure optimization and dynamical simulation.","title":"III: Medium: Geometric and topological approaches to biomolecular structure and dynamics","awardID":"1302285","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1106","name":"Division of HUMAN RESOURCE DEVELOPMENT","abbr":"HRD"},"pgm":{"id":"1114","name":"Cellular Processes"}}],"PIcoPI":[541932,541933,541934],"PO":["565136"]},"205614":{"abstract":"Many large scale machine learning problems are formulated as optimization problems, in which some measure of error or loss is to be minimized over a suitable training corpus. Real problems have too many data points to fit in a single computer. Hence the data and\/or computation must be distributed over a network of computers. Often the only practical methods for extremely large problems are so-called splitting methods, but their convergence properties are extremely variable: sometimes very fast, sometimes very slow, in ways that can be hard to predict. The goal of this project is to gain a better understanding of the convergence behavior and to use this understanding to construct accelerated algorithms with more consistent convergence properties. This will allow the application of machine learning techniques to a much wider class of problems.<br\/><br\/>Splitting methods (or more precisely alternating direction methods) are based on the idea that a general convex optimization problem can be split into two or more parts, each of which can be solved much more easily compared to the problem as a whole. The methods cycle through all the variables in turn, optimizing over each subset of variables leaving the rest fixed. The proposed work builds on a preliminary analysis of a simple model problem using the eigen-structure of certain matrix operators. The project is devoted to extending this analysis to more general problems, as well as developing faster solvers using well-established computational technologies for the matrix eigenvalue problem. Success will be measured in terms of the generality of the theory developed and the improvements in the observed convergence behavior on real problems.<br\/><br\/>With faster solvers, discovery of major regions of influence in a global-scale social network (e.g. Facebook or Twitter) could become practical on modest computer platforms. The same holds for tracking disease propagation and people in video sequences. With efficient solvers, tracking software could be deployed on local hardware without the need for high-powered central servers. This will lead to advances in countless areas such data mining, compressive sensing, recommender systems, signal processing, missing data imputation, analysis of large scale social, biological or computer networks, image reconstruction, denoising and classification.<br\/><br\/>The results of this research are to be disseminated in papers in the principal journals and conferences in machine learning, data mining, and optimization as well as in the form of software packages via the WWW (http:\/\/www-users.cs.umn.edu\/~boley\/ML-Optimization).<br\/><br\/>The project depends on the interaction between different disciplines and applications areas, which will attract students from a variety of backgrounds at both the graduate and undergraduate level. Some research tasks are suitable as projects in classes on linear algebra, optimization, data mining, machine learning and are to be developed for both undergraduate and graduate students. Undergraduate students, including women and members of underrepresented groups, will see the value of mathematical algorithms to solve real problems of interest to them.","title":"III: Small: Effective Convex Solvers for Machine Learning","awardID":"1319749","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550553],"PO":["565136"]},"205746":{"abstract":"A defining characteristic of modern personal computing is the trend towards extensible platforms (e.g., smartphones and tablets) that run a large number of specialized applications, many of uncertain quality or provenance. The common security mechanisms available on these platforms are application isolation and permission systems. Unfortunately, it has been repeatedly shown that these mechanisms fail to prevent a range of misbehaviors, including privilege-escalation attacks and information-flow leakage. Researchers have proposed information-flow protections for these platforms. However, such mechanisms have rarely been adopted in practice, partly due to the level of abstraction at which they allow policies to be specified.<br\/><br\/>We develop a formal, case-study-driven approach that will leverage advances in information-flow research to develop new policy-specification languages and enforcement mechanisms for today's extensible environments. First, we develop appropriate policy-specification languages that allow policies to be specified at an intermediate level of abstraction. Policy specification will be easy enough to understand and formulate to be accessible to most developers, yet sufficiently expressive to support rich policies. Second, we develop hybrid enforcement mechanisms to support policy that is specified at levels of abstraction that do not neatly overlap isolation boundaries provided by the platform. Third, the correctness of the mechanisms will be supported by formal models and proofs. To remain grounded and ensure practical relevance, we focus our work on two application domains: the Android operating system, and, secondarily, the Chromium web browser.","title":"TWC: Small: Provably Enforcing Practical Multi-Layer Policies in Today's Extensible Software Platforms","awardID":"1320470","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550854,"563508"],"PO":["564388"]},"205516":{"abstract":"The aim of this project is to explore theoretical ideas in mathematics and computer science that are related to quantum mechanics. Quantum mechanics is a physical theory that, notably, requires a new model of probability theory that can be called quantum probability, and this in turn leads to the theory of quantum computing and the possibility of building quantum computers. The project will investigate quantum algorithms (algorithms that could be run on quantum computers) for problems in topology, for example determining whether two knots are equivalent or different. The project will also investigate quantum error correction (a new kind of error correction that would be needed by quantum computers) in the setting of a new kind of geometry, \"quantum metric spaces\", in which distances are defined in the language of quantum probability. Finally the project will explore quantum algebra, which is a kind of modern algebra with non-commuting variables also inspired by quantum mechanics and which has relations to quantum computing.<br\/><br\/>This project will have at least two and potentially three broader impacts. First, all work will be disseminated on the arXiv, a widely used preprint server that includes most work in quantum computing, and the principal investigator will help support the arXiv. Second, the project will include development of lecture notes, which could be made into a future graduate textbook, on quantum probability, quantum computing, and quantum mechanics. Third, if quantum computers are ever build, they will have an extremely broad impact, and the ideas in this project will play a role.","title":"Quantum methods in mathematics and computer science","awardID":"1319245","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":[550315],"PO":["565157"]},"205637":{"abstract":"If an Integrated Circuit (IC) is designed and fabricated in a foundry that is outside the direct control of the (fab-less) design house, reverse engineering, malicious circuit modification, and Intellectual Property (IP) piracy are all possible. An attacker, anywhere in this design flow, can reverse engineer the functionality of design, and steal and claim ownership of the IP. An untrustworthy IC foundry may overbuild ICs and sell the excess parts in the gray market. Rogue elements in the foundry may insert malicious circuits (hardware Trojans) into the design without the designer's knowledge. Because of these and similar hardware-based attacks, the semiconductor industry annually loses billions of dollars. In this project, the investigators aim at leveraging techniques proposed in the context of IC testing to cope with manufacturing defects for developing defense techniques that help regain trust in electronic chips. <br\/><br\/>Benefits to society ensuing from successful completion of this project include trustworthy electronics for healthcare, defense, finance, transportation, and automotive applications. One of the investigators received a grant from the US Department of Education to recruit underrepresented minorities and women into the Electrical and Computer Engineering PhD program at NYU-Poly. This applicant pool will be used to recruit underrepresented minorities and women to work on this project. The investigators routinely advise undergraduates as part of the NYU-Poly Research Experiences for Undergraduates (REU) program, and the NSF REU program. These undergraduate students will be eligible to work on the present project as well. One of the investigators has developed a graduate course on Introduction to Trustworthy Hardware. The investigators will adapt some of these course modules for inclusion in appropriate undergraduate courses at their institution.","title":"SHF: Small: Adapting VLSI Test Principles for VLSI Trust","awardID":"1319841","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550603,550604],"PO":["562984"]},"205527":{"abstract":"In many computational and engineering problems, it is critical to solve large sparse linear systems of equations rapidly and reliably. Nevertheless, many practical but difficult large linear systems of equations remain out of reach computationally. In this project, the PI develops structured fast direct methods and pre-conditioners for solving such large sparse linear systems. These methods systematically exploit potentially rich numerical low-rank patterns within the fill-ins for large reductions in computational time and memory. The efficiency of these methods comes from three innovative design strategies: the PI develops randomized algorithms that can rapidly compute high quality low-rank approximations with low numerical compression overhead; the PI adapts these methods to preserve desirable properties of the original matrix for enhanced numerical reliability; and the PI re-organizes the computations so that no numerical compression and data communication is performed unless necessary.<br\/><br\/>The outcome of this research has the potential to create a novel class of direct solvers and pre-conditioners that in conjunction with iterative solvers can become powerful weapons for solving difficult large sparse linear systems, and the low-rank approximation schemes would become a valuable tool for the general scientific community, as effective data compression is essential in many areas of science and engineering. Mathematical software for rapidly solving large sparse linear systems of equations and for effective compression of large data sets will be made available to the scientific computing public.","title":"\"AF:Small:Efficient and reliable low-rank approximation techniques and fast solutions to large sparse linear equations\"","awardID":"1319312","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[550348],"PO":["565251"]},"205769":{"abstract":"Supervised machine learning approaches to building predictive models from data traditionally rely on labeled samples. However, in many real-world applications, samples are either unlabeled or labeled imprecisely labeled, i.e., labels are often ambiguous, conflicting, or incomplete. This presents the problem of learning predictive models under imprecise supervision.<br\/><br\/>This project aims to develop effective algorithms to address three different scenarios that lead to imprecise supervision (1) multiple labelers with varying expertise are employed to annotate samples; (2) annotated labels are associated with a set of samples instead of an individual sample; (3) annotations are derived by modeling multiple expert assessments. The project introduces a general bi-convex programming, minimax optimization, and multi-objective optimization based framework for learning predictive models from imprecisely labeled data. The resulting algorithms will be evaluated on a number of real-world applications. <br\/><br\/>Broader Impacts: The results of this research are likely to impact a range of biomedical applications, including medical image labeling, longitudinal behavioral studies, genomics, and drug safety. The project offers enhanced opportunities for curriculum development and research-based advanced training of grauduate amd undergraduate students in machine learning and its applications. Dissemination of open source software implementation of algorithms resulting from the project also contribute to the project's broader impact. Additional information about the project can be found at: http:\/\/www.labhealthinfo.uconn.edu\/home\/MachineLearning.jsp","title":"III: Small: Is Imprecise Supervision Useful? Leveraging Ambiguous, Incomplete or Conflicting Data Annotations","awardID":"1320586","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550910],"PO":["565136"]},"206869":{"abstract":"There are at least two key features of the move to cloud computing that introduce the opportunity for significant leaps forward in computer security for tenant services. First, a compute cloud provides a common software, hardware and management basis for rolling out cross-cutting services en masse that have resisted incremental deployment in a one-service-at-a-time fashion. Second, compute clouds offer providers a broad view of activity across an unprecedented diversity of tenant services. This research project leverages these features to develop new approaches to a wide array of fundamental problems in computer security. By convening Cloud Security Horizons summits with industry stakeholders, this project further seeks to both contribute to industry directions in cloud computing and to be informed by them.<br\/><br\/>Particular longstanding security challenges addressed in this project include secure transport, authorization, user and software authentication, security monitoring, and incident analysis. Moreover, since modern clouds are not sufficiently extensible to support the envisioned capabilities, this project is constructing cloud software platforms that enable the flexibility, extensibility and security needed for this research to come to fruition in practice.","title":"TWC: Frontier: Collaborative: Rethinking Security in the Era of Cloud Computing","awardID":"1330599","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553884,553885,"554764"],"PO":["565327"]},"205538":{"abstract":"Graphs and networks are of fundamental importance in discrete optimization. Several natural graph optimization problems are NP-Hard and a large body of work has been devoted to designing and analyzing approximation algorithms for these problems. Key algorithmic advances are tied to structural understanding of graphs and mathematical programming relaxations. This research has resulted in effective heuristics as well as exciting connections with different areas of mathematics. In this award, the PI will study approximation algorithms for graph optimization problems in two broad areas: multicommodity flow and cut problems in routing, and connectivity problems in network design. Some specific problems of interest are:<br\/><br\/>(i) Maximum disjoint paths, congestion minimization and relation between integer flows, fractional flows and cuts. In particular, node-capacitated undirected graphs and directed graphs with symmetric demands will be the emphasis.<br\/>(ii) Treewidth decomposition theorems and applications to fixed parameter tractability and Erdos-Posa theorems, in particular in directed graphs.<br\/>(iii) The survivable network design problem with vertex connectivity requirements.<br\/><br\/>The proposed problems on flows, cuts and network design are at the core of combinatorial optimization and approximation algorithms research. Progress on these problems will require advances in algorithms and structural understanding of graphs, in particular directed graphs, which will have several auxiliary benefits. The project will support and train two to three PhD students in the design and analysis of algorithms at the University of Illinois at Urbana-Champaign. The PI plans to write a survey outlining the recent developments on algorithms for routing, and their connection to graph theoretical results on treewidth.","title":"AF: Small: Flows, Cuts, Treewidth and Algorithms for Routing, Network Design and Related Problems","awardID":"1319376","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550375],"PO":["565157"]},"205549":{"abstract":"In this project, we are building a graph data management system and a suite of tools aimed at supporting real-time, historical, and analytics queries over very large, dynamic, heterogeneous, and noisy information networks. Examples of such information networks include social networks, communication networks, financial transaction networks, citation networks, gene regulatory networks, disease transmission networks, ecological food networks, sensor networks, social contact graphs, and many more. Network data is most naturally represented as a graph, with nodes representing the entities and edges denoting the interactions between them. There is, however, a lack of established data management systems that provide declarative frameworks for querying and analysing such graph-structured data, especially very large volumes of heterogeneous, complex-structured, and rapidly changing data. <br\/><br\/>In this project, we are developing a set of formalisms that include: (a) a declarative query language for graph data, (b) a declarative framework for specifying complex, iterative network analysis tasks like entity resolution, link prediction, etc., and (c) a general-purpose neighborhood-centric distributed programming framework. Our declarative interfaces and the programming framework are based on \"Datalog\", a well-established database query language, providing the users or the analysts a consistent abstraction of the graph data to specify their queries or tasks. <br\/><br\/>We are designing a suite of techniques, algorithms, and index data structures, to efficiently store large volumes of time-evolving graph data, and to execute queries and analysis tasks over it. We are addressing the challenges in minimizing network communication overhead during distributed computation through designing new partitioning and adaptive replication techniques. We are also developing a compression-based approach to minimize the resources needed for graph processing, and a framework for extrapolating missing historical information to enable querying over incomplete historical traces. <br\/><br\/>Managing and reasoning about graph data is increasingly becoming crucial in many real-world application domains including social media, e-science, disease epidemics, and financial markets, to name a few. The frameworks and tools that we are developing make it easier and more intuitive for domain experts and analysts to process, analyze, and extract insights from large volumes of dynamic time-evolving graph data. Our system enables temporal evolutionary analytics over very large historical traces, and continuous and real-time analytics over highly dynamic graphs, thus enabling a rich class of applications that would not have been possible before. The declarative frameworks and the query language that we are developing have the potential to transform and streamline the highly fragmented research area of graph query processing and analytics. This project provides research opportunities for graduate and undergraduate students, and is aligned with several undergraduate and graduate courses offered by the PI. For further information, see the project web site at: http:\/\/www.cs.umd.edu\/~amol\/GrDB","title":"III: Small: Enabling Declarative Querying and Analytics over Large Dynamic Information Networks","awardID":"1319432","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550401],"PO":["563727"]},"205439":{"abstract":"This project develops the foundations for automating verification of secure and trustworthy systems. It extends the range of analyses that are amenable to automated checking and addresses scalability. Symbolic techniques that represent possibly infinite sets of states by symbolic constraints have become important tools, but many systems of interest fall outside the scope of current techniques. There is a real need to extend and combine the power of symbolic analysis techniques to cover a much wider class of systems in order to develop the foundations for security and trustworthy software and systems.<br\/><br\/>Maude is a language based on rewriting logic. Maude can be used to model a system of any kind -- for example, an algorithm, a database, a hardware system, a programming language, a network protocol, a sensor network, or the molecular biology dynamics of a cell -- using a set of rewrite rules that describe the systems behavior. The current Maude implementation provides a high performance rewrite engine, as well as built-in search, unification, and model checking tools to support execution and analysis of systems specified in Maude. This project will develop general extensibility techniques for symbolic analysis that can simultaneously combine the power of Satisfiability Modulo Theories (SMT) constraint solving, rewriting- and unification-based analysis, and automata-based model checking to analyze a wide variety of systems beyond the scope of each separate technique. Specifically, the goals of the project are to: (i) develop the semantic foundations of extensible symbolic analysis combining SMT solving, rewriting and narrowing, and automata-based model checking; (ii) endow the Maude formal specification system with combined symbolic analysis capabilities based on such foundations; and (iii) demonstrate through case studies the power of such combined and extensible symbolic techniques in analyzing challenging systems in areas such as: model checking, theorem proving, programming languages, cryptographic protocols, and real-time and cyber-physical systems. <br\/><br\/>Maude has a substantial set of users who are doing research on approaches to secure and trustworthy systems. These users are poised to use the new techniques and tools as they develop. For critical systems, where even small security issues can lead to catastrophic failures, this foundational research will support rigorous techniques and tools for automated and scalable checking.","title":"TWC: Small: Collaborative: Extensible Symbolic Analysis Modulo SMT: Combining the Powers of Rewriting, Narrowing, and SMT Solving in Maude","awardID":"1318848","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550115],"PO":["564388"]},"209938":{"abstract":"Biped humanoid robots hold the potential to perform human-like locomotion and manipulation tasks in a variety of situations where either skilled humans are unavailable or where environmental conditions prevent human intervention. However, while bipedal motion maximizes maneuverability in tight workspaces, it simultaneously reduces locomotion stability. To initiate a new direction of ongoing research in legged robot motion control, this work will enable humanoid robots to transform into tripeds or quadrupeds or, more generally, \"SupraPeds\". To control the potentially numerous contact points on SupraPeds, we will also develop a software system that implements generic multi-contact control for arbitrary humanoids, which will enable autonomous balancing while satisfying contact force constraints. <br\/><br\/>This concept is intuitive to humans as everyone has experienced the need for an extra hand or two to maintain balance or navigate difficult terrain. The PIs will capitalize on this aspect to communicate to high school students, undergraduates, and graduate students, as well as educators nationwide, through live and online courses and seminars. To broaden the participation of under-represented groups, we will participate in a Research Experience for Undergraduates (REU) program especially for community colleges and minority-serving institutions. The results of the research will be published in open-access conferences and journals whenever possible, and the sensing and control software will be documented and published on public websites as open-source code. The applications of the work will impact fields such as search and rescue and scientific exploration.","title":"EAGER: SupraPed: Biped Robot Support Expansions for Rough Terrains","awardID":"1349982","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[562786],"PO":["564069"]},"199103":{"abstract":"Primates can recognize objects embedded in complex natural visual scenes at a glance. Despite the ease with which we see, visual recognition -- one of the key issues addressed in computer vision -- is quite difficult for machines. Understanding which computations are performed by the visual cortex would give scientists a powerful tool to uncover key mechanisms of human perception and cognition as well as to create a new generation of 'seeing' machines. <br\/><br\/>The PI's central research goal is to identify the perceptual principles and model the neural mechanisms underlying rapid visual categorization. By forcing processing to be fast, rapid visual categorization paradigms help isolate the very first pass of visual information before more complex visual routines take place. Hence, understanding 'vision at a glance' is arguably a necessary first step before studying natural everyday vision where eye movements and attentional shifts are known to play a key role.<br\/><br\/>Specifically, this proposal will lead to the development of a computational neuroscience model of rapid visual recognition in the primate visual system, which is both consistent with physiological properties of cells in the visual cortex and able to predict behavioral responses (both correct and incorrect responses as well as reaction times) from human participants across a range of conditions. The proposed model will integrate recent developments in computational models of vision and decision making with large-scale machine learning techniques. New stimulus sets will be generated, which are optimally tailored for testing among alternative visual representations and computations against human psychophysics data. These experiments will, in turn, enable the refinement of computational models.<br\/><br\/>The computational models developed as part of this proposal will be integrated in courses and disseminated broadly via a web graphical interface. Overall the interdisciplinary nature of the proposal will give students the opportunity to experience a research environment that crosses traditional boundaries across disciplines and departments. Increased undergraduate participation in computational neuroscience will help integrate this area into the mainstream computer science and neuroscience curricula.","title":"CAREER: Computational mechanisms of rapid visual categorization: Models and psychophysics","awardID":"1252951","effectiveDate":"2013-09-15","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1045","name":"CAREER: FACULTY EARLY CAR DEV"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[534118],"PO":["564318"]},"208409":{"abstract":"As science has become increasingly data-driven, and as data volumes and velocity are increasing, scientific advance in many areas will only be feasible if critical `big-data' problems are addressed - and even more importantly, software tools embedding these solutions are readily available to the scientists. Particularly, the major challenge being faced by current data-intensive scientific research efforts is that while the dataset sizes continue to grow rapidly, neither among network bandwidths, memory capacity of parallel machines, memory access speeds, and disk bandwidths are increasing at the same rate.<br\/><br\/>Building on top of recent research at Ohio State University, which includes work on automatic data virtualization, indexing methods for scientific data, and a novel bit-vectors based sampling method, the goal of this project is to fully develop, disseminate, deploy, and support robust software elements addressing challenges in data transfers and analysis. The prototypes that have been already developed at Ohio State are being extended into two robust software elements: <br\/><br\/>Software Element 1: GridPFTP (Grid Partial-File Transport Protocol): An Extention to GridFTP: an extention of GridFTP that allows users to specify a subset of the file to be transferred, avoiding unnecessary transfer of the entire file.<br\/>Software Element 2: Parallel Readers for NetCDF and HDF5 for Paraview and VTK: Data subsetting and sampling tools for NetCDF and HDF5 that perform data selection and sampling at the I\/O level, and in parallel.<br\/><br\/>This project impacts a number of scientific areas, i.e., any area that involves big (and growing) dataset sizes and need for data transfers and\/or visualization. This project also contributes to computer science research in `big data', including scientific (array-based) databases, and visualization. Another contribution will be towards preparation of the broad science and engineering research community for big data handling and analytics.","title":"SI2-SSE: Collaborative Research: Software Elements for Transfer and Analysis of Large-Scale Scientific Data","awardID":"1339757","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"8004","name":"Software Institutes"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":[558505],"PO":["558595"]},"202821":{"abstract":"For more than three decades, the slowest and most limiting component in computer systems has been the disk-based file system. For that reason, file systems have consistently been a critical area of computer systems research. One of the most useful techniques for studying file systems is to collect, analyze, and replay detailed activity traces. In the past, traces have been used for retrospective analysis and as a source of workloads for both simulation and live experiments. However, many file-system studies have been handicapped by a lack of publicly available trace data, often causing experiments to be performed using inadequate data sets. In many cases, those data sets are not published for use by other researchers, making comparative studies difficult or impossible.<br\/><br\/>In this work, the PIs are operating and expanding IOTTA Trace Repository (after the Storage Networking Industry Association's I\/O Tools, Traces, and Analysis Working Group) that makes standardized, high-quality data sets available to researchers worldwide. This repository is widely known in the research community as the most reliable source of file system and I\/O trace data, and has proven its worth by providing data sets to hundreds of research projects worldwide each year. The project involves defining standard formats and then creating tools to make it easy to work with traces, despite their immense size. Because the trace repository is inherently a community effort, it is supervised by an advisory panel of experienced researchers.","title":"CRI-CI-ADDO-EN: National File System Trace Repository","awardID":"1305360","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543573,543574],"PO":["565272"]},"202832":{"abstract":"The importance of mobile devices and the mobile networks that support them<br\/>can hardly be overemphasized. Despite fantastic advances in wireless<br\/>technologies and mobile devices, current mobile network architectures,<br\/>while packet based, overwhelmingly resemble their circuit-switched forebears.<br\/>To enable the fundamental research and innovation demanded to advance<br\/>mobile networking beyond the state-of-the-art, a new facility called PhantomNet,<br\/>will be developed and coupled with the Emulab testbed at the University of Utah.<br\/>PhantomNet will be a fully programmable end-to-end mobile testbed with<br\/>unique features to facilitate research efforts at the intersection<br\/>of mobile networking, cloud computing and software defined networking.<br\/><br\/>PhantomNet will enable hands-on teaching in mobile networking<br\/>technology in a manner that simply does not exist today. Further,<br\/>the facility will enable forward-thinking research that re-considers<br\/>the technical and economic factors involved with the interplay<br\/>between mobile networks, software defined networks and cloud technologies.<br\/>The availability of a physical facility will help researchers to<br\/>transition new mobile network designs from theory into practice.<br\/>Given that a large fraction of the world's population are already<br\/>users of such networks, and that fraction is growing, the research<br\/>enabled by this infrastructure will have the potential to have truly<br\/>transformative impact.","title":"CI-ADDO-NEW: PhantomNet: An End-to-End Mobile Network Testbed","awardID":"1305384","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["559299","560436"],"PO":["565303"]},"210213":{"abstract":"This EAGER project explores the use of social cameras to reconstruct and understand social activities in the wild. Social cameras are an emerging phenomenon, producing video captures of social activity from the point of view of members of the social group itself. They are proliferating at an unprecedented rate, as smartphones, camcorders, and recently wearable cameras, become broadly adopted around the world. Users naturally direct social cameras at areas of activity they consider significant, by turning their heads towards them (with wearable cameras) or by pointing their smartphone cameras at them. The core scientific contribution of this work is the joint analysis of both the 3D motion of social cameras (that encodes group attention) and the 3D motion in the scene (that encodes social activity) towards understanding the social interactions in a scene. A number of internal models (such as maximizing rigidity or minimizing effort) for event reconstruction are being investigated to address the ill-posed inverse problems involved.<br\/><br\/>This research is establishing a new area of visual analysis by providing the requisite framework for social activity understanding in 3D rather than in 2D. The ability to analyze social videos in 3D space and time provides useful tools for almost any activity that involves social groups working together, such as citizen journalism, search-and-rescue team coordination, or collaborative assembly teams. The project is integrated with education through teaching and student training, and outreaches industry through collaborations.","title":"EAGER: 3D Event Reconstruction from Social Cameras","awardID":"1353120","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[563366],"PO":["564316"]},"210114":{"abstract":"Self-assembly is a ubiquitous, natural, robust process in many living organisms and occurs through coordinated action of independent objects\/entities leading to the formation of well-defined patterns. For instance, microtubules form a framework for structures such as the spindle apparatus that appears during cell division. A goal of this project is to develop an algorithmic framework to identify the intelligence of encoded patterns in self-assembly systems. Insights gained from this project are important to computational science and engineering in order to develop novel engineered self-assembly systems. The novelty of the algorithmic framework lies in the combination of stochastic discrete event simulations and molecular\/cellular level models in cellular automata. This approach will enable the cooperation of multiple discrete events at multiple temporal levels of the self-assembly process to provide precise control of such events that exhibits specific behaviors observed in such systems. A discrete dynamical cellular automata model will be developed to represent the local behavior of interacting tubulin dimers in forming a microtubules lattice.<br\/><br\/>The research will provide a hybrid algorithmic framework for the wider algorithmic community. Other broader Impacts include multidisciplinary training for students, and integration of research into undergraduate\/graduate courses in computer science and mechanical engineering curricula. Outreach activities involve developing and organizing workshops related to self-assembly algorithms and framework. The proposed MTetris game is intended to attract middle and high school students to STEM fields. The results will be disseminated through journal and conference publications and freely available online.","title":"AF: EAGER: An Algorithmic Framework for Self-Assembly","awardID":"1351786","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[563149,"563362","563362"],"PO":["565251"]},"205813":{"abstract":"Access control refers to mechanisms for protecting access to confidential information, such as sensitive medical data. Management of access control policies, in applications that involve several collaborating parties, poses several challenges. One of these is in ensuring that each party in such a collaboration only obtains the minimal set of access permissions that they require for the collaboration. In a domain such as healthcare, it may be critical that access be minimized in this way, rather than allowing all parties equal access to the sensitive information. In practice, it is often difficult to manage access control policies to achieve this goal of minimality. This research will investigate new approaches to describing and reasoning about access control policies, to ensure that information that is shared in collaborative applications is only accessible to those parties in the application that require it. This work will develop both the theoretical foundations of, and prototype tools for, ensuring end-to-end security of data that is shared between organizations, with sharing of medical data as a particular source for use-case scenarios.<br\/><br\/>On one dimension, the project will investigate the extension of workflow languages to describe protocols for sharing information in collaborate applications. On another dimension, relationship-based access control can describe access to information based on parties' roles in a collaboration. The project will investigate the use of spatial and temporal logics for relating these two, and user interfaces for presenting and reasoning about this information. Since ultimately software programs will perform the access and transfer of patient medical data, formal specifications will be extracted from workflow specifications, and used as a basis for checking software programs for their compliance with these access control policies. The broader impact of this research will be facilitated by the P.I.'s involvement with the NIH International Epidemiologic Databases on AIDS for Central Africa project (CA-IeDEA). The fruits of the research will be disseminated through interaction with open source communities involved in healthcare IT for low and middle income countries, as well as through interactions with lead healthcare IT decision-makers in the countries involved in CA-IeDEA.","title":"TWC: Small: Workflows and Relationships for End-to-End Data Security in Collaborative Applications","awardID":"1320798","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[551007],"PO":["564388"]},"202788":{"abstract":"The ongoing revolution in next generation sequencing technologies and large-scale structural genomics projects has led to dramatic increase in genomic sequences and protein structures. This has brought biological research into a data-driven era, where computational methods and facilities are needed for handling and analyzing the huge volumes of data. In this project, the PIs develop a cloud computing infrastructure called A Biology Cloud (ABC) to support research in the areas of bioinformatics and computational biology. ABC is built based on the OpenStack, which is supported by more than 180 large companies and has quickly become the standard for cloud infrastructure. ABC will enable researchers at North Dakota State University (NDSU) to conduct pioneering research in their respective fields and promote and facilitate cross-disciplinary collaborations among them.<br\/><br\/>In this project, the PIs implement comprehensive educational plans to develop excellent academic programs at NDSU and to provide new research and educational opportunities for students. Through a variety of outreach activities, the PIs introduce modern technologies to high school students and the public to enhance their interest in STEM.","title":"II-New: ABC-A Biology Cloud","awardID":"1305175","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543470,543471,543472,543473],"PO":["565136"]},"205703":{"abstract":"Despite a projected shift to cloud computing, heightened concerns over cloud reliability remain paramount in both private and government sectors, and urge innovative solutions to meet the growing challenge of disparate reliability requirements. While existing techniques allow cloud providers to offer some fixed level of reliability to all customers, it may be either inadequate or too expensive to fit their specific requirements. This project aims to develop a novel framework for providing reliability as an elastic, transparent service that can be customized and accessed by all customers in cloud computing. <br\/><br\/>The goals of this project are: (1) holistic integration of two reliability approaches (viz., checkpointing and replication) with utility optimization and their adaptation to a distributed cloud environment with heterogeneous user demands, (2) the development of pricing schemes for cloud providers to put their ?resource white spaces? to profitable use. These two research directions collaboratively enable the realization of Reliability as a Service (RaaS). With the introduction of pay-per-use reliability services, cloud customers could choose reliability components they require on a feature-by-feature basis. Achieving a desired reliability level could be a single check box away. For cloud service providers, RaaS presents an additional source of revenue and value to their services. <br\/><br\/>By constructing realistic models and developing algorithms for resource allocation and optimization and pricing, the proposed research is expected to advance the start of the art of cloud computing. The project also includes an implementation and experimental component that will yield valuable knowledge on best practices and the main obstacles towards transitioning the results into the commercial world. This project will also carry out a number of educational activities involving K-12, undergraduate, and graduate students, and make strong outreach efforts for recruiting and mentoring under-represented students.","title":"CSR: Small: Reliability as a Service (RaaS) in Cloud Computing","awardID":"1320226","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["564881","562933","554028"],"PO":["565255"]},"202799":{"abstract":"This project develops a computing research infrastructure in The Institute for Creativity, Arts, and Technology at Virginia Tech that will support research and exploration at the boundaries of science, engineering, art, and design. The infrastructure will enable novel research in the areas of ambient environments, ubiquitous computing, and mirror worlds. The infrastructure will create a environment for researchers, educators, students, and the general public to interact both physically and virtually with other people, live performances, and interactive exhibitions within the same venue and also remote venues. The project will incorporate real-time physical interaction with ambient sensor data acquisition and processing, and provide a unique research environment for investigating the relationship between a real environment and a corresponding virtual representation. This infrastructure will enable research across many domains including the study of security, crowd modeling, human-computer interaction, gaming environments, and behavioral modeling including models of affect change.<br\/><br\/>Intellectual Merit<br\/><br\/>With the participation of researchers from Computer Science, Communications, the Arts, and Technology, the proposed infrastructure will be used to investigate five research domains: (1) Parallels between behavior in real spaces and virtual spaces. (2) The effects of fidelity in virtual environments on performance in physical environments, such as to examine navigation in a real building after training at different levels of rendering and styles of navigation through a virtual space. (3) Crowd model evaluation, especially the evaluation of socially-aware crowd behavior, social interaction, and sub-group behavior in a complex social setting such as before a musical performance or during an art exhibition. (4) The relationship between behavior and affect in a public venue, which will be studied using a combination of data pre-processing, visualization, and clustering. (5) Computational models of energy-efficient buildings, to reduce the variance and uncertainly associated with a building?s occupancy to meet the challenges in designing an effective and efficient HVAC control system. <br\/><br\/>Broader Impact<br\/><br\/>The infrastructure will enhance education by creating a novel computational research platform for investigating human behavior, emotions, and activities in a university environment. The project will advance science while promoting teaching, training, and learning by providing unique cross-disciplinary learning opportunities for undergraduate and graduate students. The project brings together computer scientists, architects, and humanities researchers to address the interaction of between real and virtual worlds. The project will increase opportunities for a diverse study body to participate in, learn about, and be recruited into science and engineering by integrating topics from many different fields including the visual and performing arts, architecture, and the humanities. The project will have lasting benefits to society through its exploration of distance learning, human experience, and social interaction.","title":"II-NEW: Living Lab for Asynchronous and Synchronous Investigation of Virtual and Real Environments","awardID":"1305231","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543499,543500,543501,543502,543503],"PO":["565362"]},"205835":{"abstract":"Moore's law continues to provide abundant devices on chip, but they are increasingly subject to failures from many sources. The hardware reliability problem is expected to be pervasive, affecting markets from embedded systems to high performance computing. There is an urgent need for research to address this problem with extremely low overheads in area, performance, and power (precluding traditional redundancy based solutions). Recently, researchers have proposed a software-driven hardware reliability solution that handles only the device faults that become visible to software and cause anomalous software behavior. This line of work has been quite successful in detecting most faults at extremely low cost. Unfortunately, some hardware faults escape detection by the proposed anomaly monitors, resulting in silent data corruption or SDC. These remaining few SDCs have been the Achilles heel of the software-driven hardware resiliency approach and a hindrance to widespread adoption. The proposed research seeks to overcome this obstacle. <br\/><br\/>The research includes methodological innovations that can determine application sites vulnerable to SDCs within a practical workflow and resiliency solution that uses this information to develop low cost detection and recovery techniques to mitigate the impact of SDCs. It builds on a recent resiliency analysis tool developed by the Principle Investigator's group called Relyzer. The key insight is that instead of trying to determine the outcome of each fault site, Relyzer can seek to determine which application sites will produce equivalent outcomes. This enables pruning a large number of sites and focusing on fault injections for just one site per equivalence class, resulting in significant reduction in resiliency evaluation time. In addition to providing a list of SDC vulnerable instructions, Relyzer also provides a wealth of information on why they are vulnerable. This motivates the use of inexpensive application-specific detectors that exploit this information. However, Relyzer has several limitations in speed, accuracy, and generality, precluding its use in a practical workflow. This research will first develop new techniques to address these limitations and to implement them in a tool. Second, this research will explore systematic techniques to develop practical resiliency solutions that exploit the wealth of fault-propagation information exposed by Relyzer. It will develop systematic low-cost detection and recovery techniques, with quantifiable tradeoffs between resiliency and performance overheads, that can be incorporated in a practical workflow for real applications. If successful, this work will address a key challenge in meeting the expectations of Moore's law performance for a wide variety of societal advances. Besides the research benefits, it will provide a concrete tool for practical full application resiliency analysis and will also train graduate students.","title":"SHF: Small: Software-Driven Hardware Resiliency","awardID":"1320941","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[551063],"PO":["366560"]},"204746":{"abstract":"This project aims at advancing the state-of-the-art in cybersecurity by developing efficient methods for generating novel biometric signatures and performing active and continuous user authentication. Current authentication procedures typically occur once at the initial log-in stage and involve user proxies such as passwords and smart cards which suffer from several vulnerabilities. This research addresses these limitations by developing probabilistic, generative models to represent multimodal biometrics of every user in the system, so that any significant deviation from the user-specific model flags the presence of an imposter. This novel technique specifically targets mitigation of masquerading attacks which are particularly challenging to detect as they are mostly carried out by insiders familiar with the activity patterns of the authorized user. The approach enables identification of intruders before they can hijack a user session of an authorized individual who may have momentarily stepped away from his\/her console.<br\/><br\/>The project also involves extensive usability tests for seamless integration of the authentication processes in real-life computer and network systems, thus ensuring that data about the behavior and performance of people using the network is fed-back and incorporated into future designs of the security protocols. The methods also readily extend to protecting both wired and wireless networks, and mobile devices.","title":"TWC: Medium: Collaborative: Long-term Active User Authentication Using Multi-modal Profiles","awardID":"1314792","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["560284",548302],"PO":["565327"]},"205604":{"abstract":"The rapid advance of program analysis greatly benefits many applications, including security vulnerability detection, software fault localization, performance optimization, to name a few. However, handling library functions and system calls (also referred to as environmental functions) presents a pervasive and critical challenge in program analysis. Even though these environmental functions are not written by developers, they are an intrinsic part of program semantics and consequently it would be ideal for a program analysis to co-analyze the program and its execution environment. Despite its importance, achieving program-environment co-analysis in practice is challenging. First, the difficulty to acquire the source code of some environmental functions precludes source code based analysis. Moreover, even if source code is available, the code base is often prohibitively large and complex, making analysis difficult.<br\/><br\/>In this project, the goal is to develop a highly automated technique that can construct models for environmental functions from their binary implementations and a set of initial inputs. The models are essentially programs that provide the same functionality of the functions being modeled, yet substantially simplified. Such programs can be included as part of the application, enabling program-environment co-analysis. The proposed technique will lead to a highly automated solution that will largely offload the onus of manually crafting models from program analysis developers' shoulders. Moreover, it will make program environment co-analysis feasible and more precise, enabling detection of security vulnerability and software defects that are otherwise undetectable. Additionally, the PIs expect the proposed research to foster learnings in both program analysis and operating systems, as well as providing many opportunities to incorporate findings to relevant courses in computer science.","title":"SHF: Small: Collaborative Research: Towards Automated Model Synthesis of Library and System Functions for Program-Environment Co-Analysis","awardID":"1319705","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550532],"PO":["565264"]},"205615":{"abstract":"Computerized systems are ubiquitous in modern society. They are present in our homes, cars, medical devices and various appliances. It is extremely important to ensure that these systems function without failing and work as they are intended to. This project explores basic research to ensure the correct functioning of such systems. The proposed research will contribute towards development of error free and high quality computerized systems, especially safety critical systems. It will also make it more difficult to break into systems, thus making them safer. It will lead to reduced down time of computer systems, thus yielding economic benefits as well. <br\/><br\/>This project addresses fundamental research on static as well as dynamic techniques for ensuring correct functioning of probabilistic computer systems. The project proposes novel research on model checking concurrent probabilistic programs that exhibit individual process level non-determinism. It investigates model checking of such systems under different classes of schedulers that more accurately represent the resolution of non-determinism in them. The project also proposes dynamic techniques, based on monitoring, for ensuring correctness of probabilistic systems at run time when their internal state is not directly observable. Techniques based on state estimation are proposed and will be analyzed to ensure timely detection of malfunctioning.","title":"SHF: Small: Static and Dynamic Techniques for Correctness of Probabilistic Systems","awardID":"1319754","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550555],"PO":["565264"]},"205747":{"abstract":"The exponential growth of wireless data traffic causes spectrum depletion and significantly stresses the capacity of existing and future wireless networks. The massive unlicensed bandwidth in the 60 GHz band provides great potential to meet the surging wireless data demand. The goal of this project is to gain a deep understanding of the 60 GHz propagation and channel characteristics, and to develop effective 60 GHz network protocols. This project falls into four interacted thrusts: (i) 60 GHz 3-D channel propagation measurement and modeling; (ii) MAC scheduling in 60 GHz networks; (iii) 60 GHz mesh network protocols; and (iv) enabling rich multimedia communications in 60 GHz networks. Various methods including field testing and measurements, mathematical modeling, network protocol design, distributed control, and cross-layer optimization are adopted to address the research challenges. <br\/><br\/>This project is part of a broad and ambitious global effort to develop novel techniques to exploit the huge license-free spectrum in the 60 GHz band in face of the exponential wireless data growth. It has the potential to accelerate the deployment of more powerful, bandwidth intensive, ubiquitous and cheaper wireless applications and services, and the support of more versatile, robust and rich-multimedia wireless networks. Complementary to the research agenda, the project also carries out a broad range of education and outreach activities, including integration of research findings into wireless engineering courses, textbook development, promoting underrepresented and undergraduate populations, collaboration with an HBCU, and participating in existing programs at the PIs' institutions for outreach to K-12 students and teachers.","title":"NeTS: Small: Collaborative Research: Exploring the 60 GHz Spectral Frontier for Multi-Gigabit Wireless Networks","awardID":"1320472","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[550857],"PO":["557315"]},"204779":{"abstract":"The project \"Complexity of Simulating Quantum Adiabatic Optimization by Quantum Monte Carlo Methods\" investigates the computational power and weaknesses of a widely used method for simulating quantum physics. The Quantum Monte Carlo method is a commonly used algorithm for analyzing and simulating large, coherent quantum systems. Although it is known that this method can not efficiently simulate all quantum mechanical systems, it is also known to provide reliable answers for a large subclass of such systems. The project focuses on the specific question whether or not a quantum computer running the Quantum Adiabatic Optimization algorithm is efficiently simulatable by the Quantum Monte Carlo method.\u00a0 <br\/><br\/>The theory of quantum computation looks at the question which problems can be solved efficiently on a quantum computer that do not have an efficient solution using classical computation. An important case of this theory concerns Quantum Adiabatic Optimization, which is a general purpose quantum algorithm that attempts to find the optimal value in an exponentially large landscape of function values. Despite more than 12 years of study, it is still not known to which extend this quantum heuristic performs better than classical heuristics. On the one hand, it is possible that a classical algorithm that uses the Quantum Monte Carlo method will be able to efficiently simulate quantum adiabatic optimization, which would prove a strong limitation on the 'quantum benefit' of the quantum adiabatic approach to solving optimization problems. On the other hand, it is also possible that one can prove that the Quantum Monte Carlo method does not succeed in efficiently mimicking the quantum adiabatic algorithm, thus providing strong evidence that quantum adiabatic optimization does indeed have computational powers that go beyond classical computation. This project aims to determine which one of these two possibilities is the case. Research in quantum computation is high interdisciplinary with impacts in a number of areas of physics and computer science. In addition, this project will support the education and training of a graduate student in this cross-disciplinary research.","title":"Complexity of Simulating Quantum Adiabatic Optimization by Quantum Monte Carlo Methods","awardID":"1314969","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"7281","name":"QUATM INFO & REVOLUTIONARY COM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":[548398],"PO":["564326"]},"198752":{"abstract":"Two major challenges are faced by computational scientists who routinely generate big data sets. The first is deciding what data are the most essential for analysis, given that only a small fraction of them can be retained. The second is transforming these data into information that conveys the most insight. As the size of simulation output continues to grow, the \"save the data first, analyze them later\" approach needs to be completely replaced with more aggressive data prioritization and reduction before any analysis can be done. In this project, core data analytics technologies are developed to facilitate effective data summarization, indexing, and triage for large-scale flow data. Fluid flow plays an important role in explaining many phenomena across a wide range of disciplines. To provide the scientists with a succinct view of the data content, and also organize the data and features based on their similarity and complexity, a graph-based model is developed to simultaneously reveal the major structure of the flow field, and to facilitate high performance and out-of-core flow line computation. We develop statistical and geometrical complexity measures for the flow lines to efficiently group and prioritize sub-regions in the vector field to allow efficient data access. To characterize the temporal complexity of flow fields, we develop time-varying analysis algorithms that allow for more detailed analysis of the data, and provide the user with flexible interface to quickly identify salient features.<br\/><br\/>The development of the proposed integrated flow analysis and visualization framework initially targeted two applications, simulations of turbo machinery in aerodynamics, and study of Madden Julian Oscillation in climate modeling. As typical flow in turbo machinery is full of evolving shocks and vortical structures, visualization allows the designers to identify loss regions and complex flow features in a relatively short amount of time if these features can be identified automatically. To understand the phenomenon of Madden Julian Oscillation, as this phenomenon is strongly related to the convection of air, the flow analysis techniques developed under this project can be used to identify and track its locations and durations. Because the size of data generated by time-varying simulations can be prohibitively large, the proposed time-varying data reduction techniques allow scientists to focus on the most salient portion of the data. The key impact of this project is to make available a working and attractive solution to assist scientists to comprehend the vast amount of data generated by large-scale simulations. Through close collaboration with application scientists, the research ideas developed in this project into will be transformed into an open source software framework.","title":"BIGDATA: Small: DA: Data Summarization, Analysis, and Triage for Very Large Scale Flow Fields","awardID":"1250752","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[533228,533229],"PO":["565272"]},"207948":{"abstract":"Proposal #: 13-37529<br\/>PI(s): Wang, Yong<br\/>Institution: Dakota State University <br\/>Title: MRI\/Acq.: Equipment to Establish a Mobile Testing Infrastructure- Bring Your Own Device for Research and Education<br\/>Project Proposed:<br\/>This project, studying fundamental issues in Bringing Your Own Device (BYOD) security, aims to identify better approaches to protect enterprise and campus networks. The following objectives are pursued:<br\/>- Creating tools to facilitate mobile penetration testing, and <br\/>- Developing a mobile early warning Intrusion Detection System (IDS) for malware utilizing network performance metrics (mobile penetration test and early warning IDS,<br\/>- Developing secret sharing schemes to protect and share custody of sensitive data on BYODs (algorithms), and<br\/>- Establishing secure zones using secret sharing schemes on BYODs (BYODLib).<br\/>These research objectives are expected to lead to corresponding opportunities for education in cyber operations and cyber security. Moreover, two educational objectives are also sought: To<br\/>- Extend laboratory assignments\/class projects to support mobile security devices (extend lab assignments) and<br\/>- Increase research productivity and enhance education on mobile security for undergraduate students selecting the mobile minor (strengthen research and education on mobile devices).<br\/>This project extends penetration to mobile devices. The emerging mobile-penetration testing area enables identification of unknown vulnerabilities in a mobile device. Notwithstanding the challenge, protecting sensitive data on BYOD is essential to protect enterprise and campus networks. A novel scheme based on secret data could be used to protect sensitive data as well as offline data, as it is likely to allow network administrators to disabled users? access even if the data is on BYOD. The BYOD library based on a secret sharing scheme should enable mobile applications to establish secure zones on mobile devices and protect sensitive data.<br\/>Broader Impacts: <br\/>The instrumentation will be made available to faculty and students from other institutions in the state. The project meets the priority of South Dakota?s Relevant and Technology Plan focusing on information technology, cyber security, and information assurance. The infrastructure should increase the research productivity on mobile devices and enhance education on mobile security for both undergraduate and graduate students. The institution is one of only four schools in the nation with NSA recognition in Information Assurance Education (CAE-IASE), Research (CAE-R), and Cyber Operations (CAE-CO). The instrumentation should contribute to conduct research in BYOD security and provide new opportunities for education on cyber security and operations in an EPSCoR state.","title":"MRI: Acquisition of Equipment to Establish Mobile Testing Infrastructure for Bring Your Own Device Research and Education","awardID":"1337529","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["548797"],"PO":["557609"]},"205407":{"abstract":"Increasing cyber security depends on our ability to guarantee that the system will provide the expected functionality under normal circumstances as well as if the system is perturbed by some random events or security threats. Providing such guarantee is often complicated due to several factors such as changes in system requirements caused by user demands, exposure to a new threat model that was not considered (or not relevant) in the original design, or identifying bugs or vulnerabilities during a system life cycle. The purpose of the project is to develop automated techniques --that provide justifiable confidence about correctness-- to transform an existing software model into a new model that satisfies both the existing functionality and the desired security requirements. <br\/><br\/>Developing algorithms that generate models that satisfy existing functionality and new security requirements poses new challenges due to the fact that existing trace-based properties do not suffice for several security properties. A characteristic of trace-based properties is that if a model satisfies a trace-based property and it is restricted by removing some undesired behaviors then the revised model still satisfies that trace-based property. Hence, adding a trace-based property can be achieved by removing behaviors that violate it. Since trace-based properties cannot express several security properties, this project will utilize a new formalism, hyperproperties, that generalizes trace-based properties and can be used for modeling security requirements. In particular, a hyperproperty consists of a set of trace-based properties and to satisfy that hyperproperty it is required that the repaired program exhibit `all? behaviors in one of these properties.<br\/><br\/>To develop algorithms that justifiably provide assurance about models developed by them, this project will first focus on formalizing commonly used security requirements using hyperproperties. It will perform complexity analysis to evaluate the complexity of adding different security properties to an existing model. To mitigate cases where the complexity is high, it will develop heuristics and algorithms that (1) identify whether adding the given hyperproperty can be achieved via adding a related stronger trace-based property, and (2) identify a subset of hyperproperties where adding the given property is more efficient. This work will also result in the development of efficient algorithms and tools that utilize the complexity bottlenecks. Thus, the results of the proposed project will enhance assurance of software systems by repairing security flaws and vulnerabilities in an automated fashion.","title":"TWC: Option: Small: Automatic Software Model Repair for Security Policies","awardID":"1318678","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["553603"],"PO":["564388"]},"206859":{"abstract":"This frontier project tackles many of the fundamental research challenges necessary to provide trustworthy information systems for health and wellness, as sensitive information and health-related tasks are increasingly pushed into mobile devices and cloud-based services. The interdisciplinary research team includes expertise from computer science, business, behavioral health, health policy, and healthcare information technology to enable the creation of health & wellness systems that can be trusted by individual citizens to protect their privacy and can be trusted by health professionals to ensure data integrity and security. Although these problems are motivated by a nationally important application domain (health and wellness), the solutions have applications far beyond that domain.<br\/><br\/>This project is developing methods to authenticate clinical staff to tablet computers in a continuous and unobtrusive way, and to provide patients a usable way to control the information that mobile sensors collect about them. One of the goals is to manage security of healthcare devices in the home and in remote clinics, without adding burden on the homeowner or clinical staff; towards this end the investigators are developing methods to verify medical directives issued to remote devices. One approach being investigated is segmenting access to medical records from mobile devices to limit information exposure, and developing methods to audit behavior of this complex ecosystem of devices and systems. The investigators will design tools to handle genomic data in the cloud while enabling patient control over information, detect malware in medical devices through power analysis, and provide contextual information to those who use health data collected in the field.","title":"TWC: Frontier: Collaborative: Enabling Trustworthy Cybersystems for Health and Wellness","awardID":"1330491","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["563534",553855,"561784"],"PO":["564388"]},"205528":{"abstract":"This project investigates trainable methods of paraphrasing natural language sentences to effectively disambiguate their meaning, using precise, bidirectional grammars induced from corpora to \"close the loop\"' between parsing and generation. The approach generalizes previous work on probabilistically avoiding ambiguity in natural language generation to a broad coverage setting, disambiguating only as necessary in order to better balance clarity and readability. Generating disambiguating paraphrases in a broad coverage setting makes it possible to explore ways of adapting parsers to new domains using crowd-sourced judgments of meaning similarity. Accordingly, the project explores methods of (1) inducing OpenCCG grammars from the dependency output of parsers such as the C&C parser, (2) generating paraphrases with OpenCCG that explicitly aim to avoid likely distractor interpretations, (3) collecting meaning similarity judgments between the original sentence and paraphrases of its most likely interpretations, and (4) retraining the parser using the collected judgments. To evaluate the approach while also conducting outreach, the project involves data collection and experimentation at Ohio State's language research pod at the COSI science museum, in addition to the use of Amazon's Mechanical Turk.<br\/><br\/>By closing the loop between interpretation and generation, the project promises to dramatically enhance the prospects for using crowd-sourcing to adapt natural language processing tools to new domains. The project will also enable international collaborations with the University of Sydney, and help to educate the public about language science and technology, providing an inspirational example of science in action to the children who attend COSI.","title":"RI: Small: Closing the Loop: Inducing High-Precision Grammars for Generating Disambiguating Paraphrases","awardID":"1319318","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["553805"],"PO":["565215"]},"205649":{"abstract":"This project develops a fragment-based intermediate-level representation for images based on shape and appearance, with shape playing the primary role. The representation can be populated in a bottom-up, category-independent fashion, which at the same time can be efficiently accessible by top-level, category dependent processes. The inherent ambiguity in generating object part hypotheses is resolved by combinatorially forming alternative image fragments by standard as well as novel perceptual operations, taking into account both shape and appearance, and both region-based and boundary-based cues. The exponential growth of the number of fragments is managed under a best-first graph representation that avoids duplication, leading to a sufficient number of diagnostic recognizable object parts among a vast pool of fragments. The project also explores an embedding of these fragments in a metric similarity space via proximity graphs and a geometric index structures for efficient nearest neighbor search. The final outcome is a representation space and an index for scalable, logarithmic object category recognition. A key aspect of this work is that categories themselves are also represented in a hierarchical similarity space, and this computationally implements ideas akin to Rosch?s basic level categorization. <br\/><br\/>The broader impacts of this activity spans a vast number of applications: any application which benefits from scalable object recognition such as indexing into databases, e.g., searching in a database of trademarks, engineering drawings and computer generated graphics, content-based web search, aerial tracking and recognition of vehicles, automated animal behavior analysis, and many others.","title":"RI: Small: A Generic Mid-Level Representation as Object Part Hypotheses for Scalable Object Category Recognition","awardID":"1319914","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550631],"PO":["564316"]},"205539":{"abstract":"Current techniques for answering questions about the influence of behaviorial and environmental factors on public health are based on surveys, which are costly and subject to response bias, or simulations, which rely on possibly incorrect or simplistic assumptions. The TwitterHealth project is developing techniques to extract reliable public health information from social media. In essence, the online population becauses a vast organic sensor network. Statistical natural language processing techniques are employed to classify tweets (or other social media postings) as self-reports of disease or particular behaviors of interest. GPS information included in postings made from cell phones allow a variety of behavioral information to be inferred about each user, such as the venues visited and the other individuals from the data set who are encountered.<br\/><br\/>Major technical challenges for using social media in this manner are the highly noisy nature of the information channel, scaling to a large number of different health conditions, and the need to discover causal influences as well as correlations between behavioral and environmental factors and health. The challenge of noise is approached by learning dynamic relational models of health states, which generalize classical epidemiological models but support individual as well as aggregate predictions. The scaling challenge is dealt with by knowledge transfer techniques, which reduce data and computational requirements by transfering information between models for different health conditions. Specific knowledge transfer techniques are cascaded training of a target classifier starting with a given classifier for a related but different disease, and the use of ensembles of general and specific classifiers. The challenge of inferring casuality is addressed by temporal-lag methods, which identify changes in behaviorial or environmental conditions that consistently precede changes in health. For example, the inference that a venue is a cause (vector) of disease spread is accomplished by tracing backward in time the GPS trails of users who post social media reports of illness. TwitterHealth employs two approaches for validating its results: first, comparing the aggregate predictions of the model against CDC statistics; second, comparing individuals' behavior in reporting or not reporting disease symptoms in status updates against the behavior predicted by the models. The project also includes planning for clinic based evaluations, in which subjects identified by their social media postings would provide swabs that would be tested for disease agents.<br\/><br\/>The TwitterHealth approach to collecting and analyzing health information has the potential to improve public health, by making detailed data about health, behavior, social structure, and geographic influences available in real time and at almost no cost. While it will not completely replace traditional methods of gathering health information, it provides an important complementary information channel, which emphases speed, reach, and scale. The project includes outreach expert medical professionals in order to plan future clinical validation. The outreach interaction provides a forum for exchange of computer science and medical expertise between researchers and students in the two fields. Information about the project is available online at http:\/\/www.cs.rochester.edu\/u\/kautz\/twitterhealth.","title":"III: Small: TwitterHealth: Learning Fine-Grained Models of Health Influences and Interactions From Social Media","awardID":"1319378","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8018","name":"Smart Health & Wellbeing"}}],"PIcoPI":[550377],"PO":["565136"]},"198774":{"abstract":"The planned research aims to radically transform the current state of the art of dimension reduction, through the development of new approaches to interpretation, resolution, and feature extraction for high-dimensional dynamical data. These extensions fit into a larger program of data analysis which seeks to find the intrinsic geometry tailored to each type of data set. The development of diffusion maps first established a new way to recover geometry from generic data sets. Recently, we showed how to recover the intrinsic geometry for observations of a dynamical system, meaning data sets that have the additional structure of a time ordering. In each case the focus on discovery of the intrinsic geometry for the particular structure of the data resulted in significant practical benefits in the form of new algorithms for dimensionality reduction and noise reduction. With this progress as a foundation, the proposal greatly expands the impact of this effort in two important directions: (1) Overcoming critical weaknesses in the current dynamical diffusion map approach, by (a) extensions to allow drift and anisotropy in the Laplace-Beltrami operator represented by the diffusion map, and (b) merging diffusion maps with discrete exterior calculus through the Hodge star operator to handle higher-dimensional dynamics; and (2) development of an automatically-constructed, data-adapted harmonic (or wavelet) basis in order to capture essential features of spatiotemporal data. Our data-adapted construction starts with an a priori spatial structure and then combines this with the data itself to form the data-adapted spatial geometry. We then propose a novel method of using the diffusion geometry, improved with the results of (1), to find symmetries in the adapted geometry that represent intrinsic features of the data. <br\/><br\/>The project involves a series of investigations in the development of computational dynamical systems theory and methods, with the goal of significantly changing the way massive spatiotemporal data sets are analyzed. The algorithms resulting from the study represent a distinctly new form of time-scale and space-scale separation that can break high-dimensional dynamical data into parts adapted to the dynamics. This new approach will handle a wide range of spatiotemporal inputs, such as high-frame-rate videos of physical experiments, spatially and temporally irregular geophysical databases such as oceanographic, weather or climate time series, econometric and logistical databases, and multivariate measurements on complex dynamic networks such as biological connectomes. The data comes from problems spanning diverse areas of sciences and engineering, with special focus on physical and biological systems. These modern high-resolution data sets are particularly vulnerable to the curse of dimensionality, making current parametric statistical techniques impractical due to exponential increases in model complexity and data requirements. Our approach implicitly eliminates redundancies and selects features of interest in an automatic data-adapted way, reducing the data requirements for statistically significant analyses to feasible levels. Educational impacts include integration of the research topics into undergraduate and graduate teaching, and the enhancement of research infrastructure through joint research with collaborators in physics, bioengineering, biology, and medicine.","title":"BIGDATA: Small: DA: Dynamical diffusion map methods for high dimensional data","awardID":"1250936","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8083","name":"Big Data Science &Engineering"}}],"PIcoPI":["540871","540873"],"PO":[533287]},"210203":{"abstract":"In an effort to cultivate literacy skills in the United States the nation's public libraries have embraced digital technologies, mostly in the form of public-access computers loaded with software, supported by library staff. But interacting with a \"keyboard-mouse-screen\" may not offer the immediacy of interacting with the printed page, and both forms of interaction are far removed from the physical, tangible and social world in which young children thrive. The PI argues that literacy can be cultivated in a space that is at once physical and digital and evocative of the book being read. To this end, in this project the PI will explore the LIT ROOM, a literacy support tool at room-scale that consists of a novel suite of user-friendly, networked, \"architectural-robotic\" artifacts embedded in the everyday physical space of the library. This physical-digital environment is transformed by words read by its young visitors, so that the everyday space of the library \"merges\" with the imaginary space of the book; the book becomes the room, the room becomes the book. And should the LIT ROOM's intelligent reconfigurations not match the imagined spaces of young readers, they can \"fine-tune\" the room through tangible interfaces. The work will proceed in two phases. First, the PI will ask children to decide what makes for a compelling LIT ROOM. He will present to children, ages 4-8, low-fidelity \"architectural-robotic\" artifacts within a library space to help capture how children define and employ this digital-physical suite to \"create the book.\" Then, he will iteratively develop and evaluate the suite as a fully-working environment that embodies what was learned from the child-centered participatory design process. The test bed implementation will be situated in the Richland County Public Library of Columbia, South Carolina, the largest public library in a state that ranks among the lowest in the State Technology and Science Index and the highest in numbers of people who are both illiterate and living below the poverty line. To tackle this challenge, the interdisciplinary team includes two investigators with complementary expertise in continuum and architectural robotics, and literacy education.<br\/><br\/>Broader Impacts: Because the prototype implementation will be located in a real-world public space it will have exposure to a large audience. Project outcomes will also advance the start-of-the-art of robotic systems for a real-world environment and application, using a mix of sensing\/actuating and deformable continuum surfaces. The findings will further advance knowledge and understanding in literacy by studying and providing experimental data on the efficacy of tangible environmental technologies in promoting literacy in children.","title":"EAGER: The LIT ROOM - A Networked Suite of Architectural-Robotic Artifacts Embedded in the Library for Advancing Literacy in Children","awardID":"1352992","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563340,563341,563342],"PO":["565227"]},"210324":{"abstract":"The goal of this project is to develop the principles to design a virtual personality assessment laboratory. This requires development of a preliminary taxonomy of mechanics, grounded on personality research, that allows researchers to use behavioral patterns of individuals in computer-generated virtual environments, to assess their real-world personality. Such virtual personality detection mechanisms can then be used by other researchers to adapt the laboratory further, which would be one ultimate goal: the design of more personalized and adaptive applications that may improve impact on large societal problems. The research activity will lead to a new methodology with the potential to transform current practices across disciplines, from social psychology to human-centered computing.<br\/><br\/>The virtual personality assessment laboratory will be developed as a set of modular challenges and situations that make use of the mechanics individuated in the taxonomy. These situations are constructed to elicit personality preferences. The design is driven by personality theory and validated by a wide range of personality measures such as the Need For Cognition, the California Q-Sort, the Reiss Motivation Profiler and the Five Factor Model. The system will be validated through two iterations to ensure that scenarios are assimilated and that they conform to the intention of the designers. A final summative evaluation will be administered utilizing data on the behavior of research subjects inside the environment, as well as various personality measures such as scores from personality questionnaires, informant interviews, and behavior coding. Correlation analysis will be used to investigate relationships between choices emerging from the context of action in the virtual environment and personality scores. <br\/><br\/>This research impacts directly a number of disciplines from psychology of personality to adaptive technologies and personalization. The research affects our understanding of personality within virtual environments, which are becoming a major part of our lives. It also has the potential of developing customizable learning environments. Understanding individual differences through the analysis of consumption and behavior of digital entertainments allows for a deeper level of adaptation and personalization of persuasive technologies aimed at fostering education, health or training, potentially increasing participation from all segments of society.","title":"EAGER: Virtual Personality Assessment Laboratory","awardID":"1355298","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563631,563632,563633],"PO":["564456"]},"210335":{"abstract":"This award enables support for the Workshop on Future Wireless Communication Networks. This workshop will bring together wireless networking experts who will discuss and debate the critical challenges in wireless networking and communication, and articulate a vision for conducting fundamental and inter-disciplinary research in the area. A tangible outcome of the workshop will be a report that will (i) identify major issues affecting future wireless research; (ii) expose the research community to new and exciting inter-disciplinary problems; and (iii) stimulate far-reaching future research initiatives, and collaborations that would help along the evolution of the wireless research community.<br\/><br\/>The workshop will help influence future research and development in wireless systems. It will fuel inter-disciplinary collaborations between members of various communities (e.g., application communities, security, information theory, etc.). It will enable diverse groups within the community will help shape the future of wireless communications. It will also educate the general public on the importance of wireless research, the challenges, and the enormous potential.","title":"NSF Workshop on Future Wireless Communication Networks","awardID":"1355367","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[563656],"PO":["565303"]},"210148":{"abstract":"Narrative generation is the process of generating textual descriptions of action in dynamic environments such as movies, sports events and educational programs. On-line narration of a dynamic environment is beneficial in a wide range of contexts, from entertainment to training and education. For example, successfully narrating a video would allow blind and visually-impaired people to follow visual cues that are important to understanding the video. Key to attaining this goal is the ability to translate natural language into a form that is understandable by computers. A particular challenge is to do this not for a specifically chosen domain, but in a general way that is suitable for adaptation to a wide range of natural dynamic environments. This project explores new directions to tackle these extremely challenging, yet crucial, issues, undertaking exploratory research towards building essential components of a domain-adaptive framework that learns to understand and generate narratives on-line for natural dynamic environments with minimal supervision by human experts. This research explores methods to generate narratives on-line by learning the natural dynamics of the environment, automatically forming templates, and deciding when and what to mention. <br\/><br\/>Many natural language applications are concerned with recognition of paraphrases and semantic understanding. The software and data resulting from this project are potentially useful for semantic analysis in natural language processing, and is being made available for research purposes. This work is designed for significant social impact through a broad range of applications including educational, entertainment, and accessibility. A narrative generation system could be beneficial to visually-impaired people to better understand videos over the internet. In addition, such a system can help broadcasting companies to report news or sports events with customized commentaries for different users. This project also provides research and collaborative work experience to undergraduate and graduate students including under-represented and minority groups.","title":"EAGER: Generating and Understanding Narratives for Dynamic Environments","awardID":"1352249","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[563219],"PO":["565035"]},"205803":{"abstract":"Data is being generated at a tremendous rate in diverse applications, such as health care, genomics, energy management and social network analysis. Indeed, the recent moniker of Big Data emphasizes that massive volumes of data are ubiquitous. Thus, there is a great need for developing scalable and sophisticated methods for analyzing these data sets. This project is aimed towards one aspect of this challenge, namely, developing scalable and state-of-the-art numerical methods for modern problems that arise in machine learning. <br\/><br\/>This project will aim to develop divide-and-conquer methods for representative, concrete problems that arise in contemporary applications. These include (a) classification: kernel support vector machines, (b) regression: kernel regression and high-dimensional sparse approximation, (c) structure learning: graphical model estimation, (d) spectral approximation: multi-scale SVD computation, and (e) missing value estimation: matrix factorization. The project will develop specialized algorithms for each of these problems, in particular, developing tailored ways of dividing the problem into subproblems, solving the subproblems, and finally conquering the subproblems. Thus, general principles for applying the divide-and-conquer approach to other problems in large-scale machine learning will be uncovered. The project will lead to software for large-scale data analysis that will be efficient on modern multi-core computers. Impact of the new algorithms on various application areas, such as bioinformatics and network analysis, will be studied. Within computer science and applied mathematics, the project will have a broad impact on research in a variety of disciplines, including numerical analysis, numerical optimization, statistics, machine learning, data mining and parallel computing.","title":"AF:Small: Divide-and-Conquer Numerical Methods for Analysis of Massive Data Sets","awardID":"1320746","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550986],"PO":["565251"]},"204714":{"abstract":"How does web-based malware spread? We use the term web-based malware to describe malware that is distributed through websites, and malicious posts in social networks. We are in an arms race against web-based malware distributors; and as in any war, knowledge is power. The more we know about them, the better we can defend ourselves. Our goal is to understand the dissemination of web-based malware by creating \"MalScope\", a suite of methods and tools that uses cutting-edge approaches to build spatiotemporal models, generators and sampling techniques for malware dissemination. From a scientific point of view, this project brings together two disciplines: Data Mining and Network Security. The outcome is a suite of novel, sophisticated, and scalable techniques and models that will enhance our understanding of malware dissemination at a large scale. We use two types of web-based malware dissemination data: (1) user machines accessing dangerous sites and downloading web-based malware; and (2) Facebook users being exposed to malicious posts. We already have and will continue to obtain more data from our industry partners (e.g. Symantec's WINE project), open-access projects, or collect on our own (e.g MyPageKeeper).<br\/><br\/>The broader impact of our work is that it will enable the development of security solutions for end-users and industry. A 15-minute network outage costs a 200-employee company about $40K, while identity theft costs about $1,500 per person on average. By knowing the enemy better, security researchers and industry can more effectively stop the interconnected manifestations of Internet threats: identity theft, the creation of botnets, and DoS attacks. The PIs have a track record of technology transfer, with collaborators at industrial labs (Yahoo, MSR, Symantec, AT&T, IBM), national labs (LLNL, Sandia), open-source software (``Pegasus''), and spin-off startups (StopTheHacker). Educational impacts include developing a new course, providing publicly available educational material, and open-source software.","title":"TWC: Medium: Collaborative: Know Thy Enemy: Data Mining Meets Networks for Understanding Web-Based Malware Dissemination","awardID":"1314632","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":[548220],"PO":["565327"]},"205704":{"abstract":"This project addresses fundamental challenges in incorporating and conveying uncertainty in the process of data analysis and visualization. In order to compute uncertainty, the project aims to develop a general model for uncertainty analysis that is independent of the visualization method and application domain. An important and novel aspect of the technical approach is the division of uncertainty into high and low conceptual levels, with separate but complementary methods of analysis for each level. The ultimate goal is to allow assessment of uncertainty at the level of tasks and queries, aided by effective visualization. The project focuses on three principal research problems: (1) development of a hybrid probability-possibility uncertainty analysis framework for representing low-level computational uncertainty, along with methods for displaying this uncertainty in various visual modalities; (2) formulation of a fuzzy analysis for representing high-level, task-related uncertainty that handles human input and visual\/perceptual uncertainty, while bridging the gap between low-level uncertainty and high-level uncertainty; and (3) investigation into ways to visually display the evolution of uncertainty in computation, enabling uncertainty navigation, in which exploration and modification of uncertainty can occur in the same context. The resulting framework is expected to effectively enable verifiable visualization of uncertainty in data analysis.<br\/><br\/>This project draws from many fields of research outside of visualization, including management of uncertainty, fuzzy logic, information theory, data analysis, simulation, computer vision, human-computer interaction, computer graphics and high performance computing and its potential impact extends to these areas and beyond. The ultimate goal of verifiable visualization is beneficial to visualization and visual analytics, but also facilitates the adoption of visualization in other fields, such as medical imaging, computational biology, and visual analytics to name a few. The project results will be disseminated to the visualization community and beyond through annual conferences, workshops, and tutorials, and also through the project website (http:\/\/vis.cs.ucdavis.edu\/NSF\/IIS1320229), which will include project status updates and deliverables such as images, videos, and prototype software. Complementing the proposed research is an educational agenda, consisting of integration of research results into teaching, arrangement of summer internships for participating students at the collaborating scientists' laboratories, and involvement of graduate and undergraduate students in research.","title":"CGV: Small: A General Framework for Expressing, Navigating, and Querying Uncertainty in Data Analysis and Visualization Tasks","awardID":"1320229","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["552243"],"PO":["563751"]},"205715":{"abstract":"With the increasing quantity and diversity of wireless devices, the study of network algorithms that communicate over radio links has received renewed interest. Most of the models used to analyze these algorithms assume static links (i.e., link quality is fixed over time). In real wireless networks, by contrast, it is common to encounter links that exhibit dynamic behavior (e.g., rapid, unpredictable changes in quality) due to changing environmental conditions and\/or interference from unrelated protocols in shared spectrum. This project aims to reduce this gap between theory and practice by studying wireless models that include varying degrees of dynamic behavior -- seeking new algorithm strategies for solving fundamental problems efficiently and proving new lower bounds that establish the limits of such efforts.<br\/><br\/>In more detail, this project focuses on dynamic variants of both graph-based and Signal-to-Noise-and-Interference-Ratio models of wireless communication. In both settings, it seeks new upper and lower bounds for fundamental communication problems under varying degrees of dynamic behavior. There are three goals for the lower bounds: (a) to determine the threshold of dynamism at which existing solutions fail; (b) to determine the (presumably greater) threshold at which no efficient solutions are possible; and (c) to develop new general methods for proving fundamental limits in this setting. The project also seeks new upper bounds that are more robust than existing solutions in dynamic settings, including an exploration of the power of the recently introduced link detector formalism -- an abstraction that captures the low-level link probing services common in real wireless networks.<br\/><br\/>This project will impact both the theory and practice of wireless networks. On the theory side, it introduces new models that include precisely-bounded amounts of dynamic behavior, and develops new upper and lower bound techniques for these settings. On the practice side, it will lead to new, provably correct and efficient communication algorithms that are robust to significant amounts of unpredictable link behavior. Such algorithms are crucial for the migration of mission-critical tasks (e.g., as required in healthcare, first responder, military, and coordination\/control applications) to wireless platforms.","title":"AF: Small: Algorithms for Wireless Networks with Dynamic Links","awardID":"1320279","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550787],"PO":["565157"]},"204747":{"abstract":"This project aims at advancing the state-of-the-art in cybersecurity by developing efficient methods for generating novel biometric signatures and performing active and continuous user authentication. Current authentication procedures typically occur once at the initial log-in stage and involve user proxies such as passwords and smart cards which suffer from several vulnerabilities. This research addresses these limitations by developing probabilistic, generative models to represent multimodal biometrics of every user in the system, so that any significant deviation from the user-specific model flags the presence of an imposter. This novel technique specifically targets mitigation of masquerading attacks which are particularly challenging to detect as they are mostly carried out by insiders familiar with the activity patterns of the authorized user. The approach enables identification of intruders before they can hijack a user session of an authorized individual who may have momentarily stepped away from his\/her console.<br\/><br\/>The project also involves extensive usability tests for seamless integration of the authentication processes in real-life computer and network systems, thus ensuring that data about the behavior and performance of people using the network is fed-back and incorporated into future designs of the security protocols. The methods also readily extend to protecting both wired and wireless networks, and mobile devices.","title":"TWC: Medium: Collaborative: Long-term Active User Authentication Using Multi-modal Profiles","awardID":"1314803","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":[548304,548305,548306],"PO":["565327"]},"205605":{"abstract":"There is no doubt that we live in an environment that is massively recorded by multiple people at any point in time. Although we have the technology to combine such information in the visual space (e.g., with PhotoSynth), there is currently no good way to combine audio streams from multiple recordings of the same event. This projects fills that gap by developing new techniques in spectral decompositions and landmark-based localization methods to support taking large amounts of low-level audio recordings of the same event and resynthesize them as one high-quality version, eliminating the artifacts and noise of each individual recording while taking advantage of their strong points.<br\/><br\/>This project aims to introduce new computational tools to combine uncurated recordings at a large scale, and produce information that no single recording can provide. Combining all available information and producing objective representations will enable effective sifting through data from massively recorded events (e.g., social unrest, natural disasters, historical moments) and focus on the needed information. The resulting tools will support creation of high-quality recordings from historical events that might not otherwise be well documented, by using the power of the crowds. The project web site (http:\/\/www.cs.illinois.edu\/~paris\/crowdmic) will provide access to the research results, including a service that allows the consolidation from user-submitted recordings, publication and source code in order to to stimulate activity in this field. Research results will also be incorporated in the development of classes on social and crowdsourcing aspects of audio and signal processing.","title":"III: Small: MicSynth: Enhancing and Reconstructing Sound Scenes from Crowdsourced Recordings","awardID":"1319708","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[550534],"PO":["563751"]},"205726":{"abstract":"Computational techniques have greatly extended the range of problems that can be solved by numerical analysis. However, few tools support the combined practice of collaborative learning and discovery in the computational sciences. This project draws on recent work in embodied cognition, which suggests that the process of building models--particularly models with dynamic visualizations--can aid conceptual understanding in complex problem domains. The researchers will develop a system to support and enhance collaborative discovery by re-representing abstract scientific problems in an embodied way. The proposed system will allow researchers to couple visuo-spatial skills with computational techniques to develop understanding and collaborate in the modeling of complex biological systems. Simultaneously, the project will develop techniques to support collaborative modeling in science and engineering. The project will thus test how theories from embodied cognition can inform interaction design and create explicit knowledge about how collaborative discovery happens in the sciences. <br\/><br\/>By changing the way abstract scientific problems and techniques are represented, the proposed system will make these scientific problem easier for researchers to understand and manipulate through embodied experience. This re-representation of complex problems may have several benefits: 1) help researchers find solutions to problems that would otherwise take too long or seem too difficult to solve; 2) enhance problem-driven learning approaches in science and engineering disciplines; 3) enable interdisciplinary groups of learners or researchers to come together and work on (and develop a shared representation of) complex problems in a hands-on manner; and 4) make complex science and engineering problems more accessible to everyday citizens. The project may also have significant impacts on emerging interactive technologies by establishing embodied cognition as a framework for interaction design.","title":"HCC: Small: Getting a Grip on the Numerical World: Kinestheic Interaction with Simulations to Support Collaborative Discovery in Systems Biology","awardID":"1320350","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[550811],"PO":["565342"]},"205737":{"abstract":"Why are humans so much better than machines at recognizing speech? This research aims to measure differences between humans and machines in how they compute similarities between sounds. A computational model of speech perception will be trained on speech production data, using several types of features that are typically used in speech recognitions systems. It will then be tested on its ability to predict human listeners' responses in speech sound discrimination tasks. Results are expected to provide information about how the speech that listeners hear shapes their perception of sounds, as well as how well the information used by automatic speech recognition systems matches the information used by human listeners.<br\/><br\/>By allowing us to compare how humans and speech recognition systems use information when perceiving speech, this research will provide a tool that can help make speech recognition systems more human-like. Reverse engineering human perception can improve the way these systems generalize to new dialects, talkers, and noise conditions. This has the potential to facilitate the construction of systems for low-resource languages, broadening the impact of speech recognition technologies.<br\/><br\/>[Supported by SBE\/BCS\/PAC and CISE\/IIS\/RI]","title":"Integrating low-level speech features into a model of speech perception","awardID":"1320410","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7252","name":"PERCEPTION, ACTION & COGNITION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550836],"PO":["563458"]},"205979":{"abstract":"Title: Collaborative Research: SAVI- Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field.<br\/><br\/>This award is designated as a Science Across Virtual Institutes (SAVI) award and is being co-funded by NSF?s Office of International Science and Engineering. Four US environmental observatories (National Ecological Observatory Network (NEON), Ocean Observatories Initiative (OOI), Advanced Modular Incoherent Scatter Radar (AMISR) and collectively EarthScope, comprising USArray and the Plate Boundary Observatory (PBO), operated respectively by the Incorporated Research Institutions for Seismology (IRIS) and UNAVCO) are working together with their respective counterparts from the European Union on developing common data policies and standards relevant to global research infrastructures in the environment field. <br\/><br\/>The project goals include developing new understanding through broad harmonization of data and constructing multi- discipline, synergistic data products that have wider societal importance. In addition, these activities will help the scientific community to better develop coupled models that better capture critical feedbacks and interactions of the earth system. These models are needed to help make these types of data more accessible to decision-makers at many levels. <br\/><br\/>Linking these existing programs provides a unique opportunity for early career researchers and students to learn in a multi-disciplinary environment and to benefit not only from the US participants but also their European counterparts. The observatories will be developing graduate and post-graduate courses and the workshops will seek to include early career scientists and students. <br\/><br\/>The proposed activities will focus on addressing societally relevant challenges. Through case studies, ways of harmonizing data will be investigated with the goal of making the data from these observatories easier to include in effective decision-making. Disasters, carbon and water will likely be the first set of issues addressed.<br\/><br\/>This program is creating opportunities for enhancing the career trajectories of a new generation of researchers in Europe and the U.S. The virtual institute is providing mechanisms for exposing early-career scientists to interdisciplinary, multi-institutional activities focused on environmental data and cyberinfrastructure; arming them with new scientific tools to address challenging questions in harmonizing environmental data to help in effective decision making; and showing them how international partnerships can help to solve global problems. It is recruiting a diverse set of US and European students to create collaborative networks of environmental data and cyberinfrastructure experts across these countries.","title":"SAVI: Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field","awardID":"1321640","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1679","name":"INTERNATIONAL COORDINATION ACT"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7959","name":"MACROSYSTEM BIOLOGY"}}],"PIcoPI":[551465],"PO":["565235"]},"205627":{"abstract":"This research takes an information theoretic approach to understanding principles that govern a coordinated rate-efficient sampling of multiple signals and centralized compression of the sampled subset. The goal is to reconstruct the entirety of the signals within acceptable distortion levels. The research has three main components: an integrated analysis of sampling and rate distortion behavior, and their tradeoffs; sampling rate distortion theory for Markov random fields; and sampling rate distortion performance for signals with memory. Expected outcomes are a characterization of fundamental performance limits of optimum sampling rate and lossy compression rate and their interplay, together with the best choice of sampling mechanisms and attendant processing for reconstruction.<br\/><br\/>The investigators' technical approach involves the development of a principle of \"sampling rate distortion\" which lies at the intersection of specialities in information theory and signal processing, and has the larger objective of elucidating material connections between sampling and rate distortion performance. The performance of specific sampling and rate distortion processing schemes will be investigated. Specific groups of open problems chosen for investigation address a general class of multisignal models for sampling and lossy data compression. These are motivated by potential applications including dynamic thermal management for on-chip temperature control during runtime; network function computation; and image restoration, surface reconstruction and visual integration in computer vision.","title":"CIF: Small: Sampling Rate Distortion","awardID":"1319799","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["561385"],"PO":["564924"]},"205748":{"abstract":"Cloud computing is emerging as an attractive computing paradigm, where<br\/>both hardware and software resources can be leased from a cloud<br\/>provider to achieve better economy of scale, elasticity, availability,<br\/>and flexibility for a plethora of services. A major challenge in<br\/>designing such cloud platforms is in providing high and predictable<br\/>performance along with Quality of Service (QoS) guarantees. The<br\/>performance unpredictability is attributed to a myriad of factors<br\/>including system scale, workload dynamism, virtualization overheads,<br\/>resource sharing and contention.<br\/>The objective of this proposal is to investigate performance<br\/>enhancement and performance quantification techniques in a cloud<br\/>environment through three intertwined tasks.<br\/>First, a comprehensive scheduling framework consisting of a high-level<br\/>job scheduler and a fine-grained resource manager will be developed<br\/>for efficient management of cloud resources for performance enhancement.<br\/>Second, performance prediction models will be developed for estimating<br\/>job completion time (JCT) in a cloud platform.<br\/>Third, for providing QoS guarantees, a control-theoretic model<br\/>incorporating the proposed scheduler and analytical models will be<br\/>developed. Measurements on real platforms with MapReduce <br\/>and other representative cloud workloads will be used to validate the<br\/>proposed ideas.<br\/><br\/>This research addresses one of the main concerns of cloud computing -<br\/>What are the performance implications if my application is moved to a<br\/>cloud? If successful in answering this question, it would have a tremendous<br\/>impact on the cloud-enabled application domains. The cross-cutting<br\/>nature of this work can foster new research avenues.<br\/>In addition to undergraduate and graduate student training in areas<br\/>such as architecture, distributed systems, and performance modeling,<br\/>special attention will be given to involving women and minority<br\/>students in this project. The modeling tools and techniques developed<br\/>in this research will be made publicly available.","title":"CSR: Small: PROM in Clouds: Exploiting Scheduling for PeRformance OptiMization in Clouds","awardID":"1320478","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[550859],"PO":["565319"]},"205869":{"abstract":"The goal of this project is to determine how turn-taking in human-human dialogue works, and use this as a basis for developing a flexible model of turn-taking for use in spoken dialogue systems. The first two aims focus on determining whether turn-taking is solely determined by the current speaker or is negotiated by both conversants. The first aim measures how well human subjects, listening to excerpts of speech, can predict whether the current speaker will continue the turn or release it. The second aim analyzes timing lags at turn transitions. Many turn transitions have short lags, which favors a speaker-control model; however this might be an artifact of common speech act sequences, back channels, early onsets, and provisional turns. The third aim, using the findings of the first two aims, is to determine the cues and mechanisms that are used by conversants in taking the turn.<br\/><br\/>Better understanding of turn-taking in human-human dialogues is important as it will help in building more efficient and natural spoken dialogue systems, allowing the user and system to better collaborate to solve complex tasks. It should also allow spoken dialogue systems to deal with a broad range of users, from experts to novices: for experts, letting them take the initiative (and the turn), in order to efficiently complete the task, while guiding novices with more directions and examples. Furthermore, understanding human-human turn-taking might have biomedical applications. For example, Autism, which is a disorder that affects social communication, might impact how people engage in turn-taking, and so turn-taking biomarkers might help in diagnosing it.","title":"RI: Small: Flexible Turn-Taking for Mixed-Initiative Spoken Dialogue System","awardID":"1321146","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[551154],"PO":["565215"]},"205639":{"abstract":"In this Cyberlearning: Transforming Education Exploration project, researchers focus on the challenge of helping students in grades 4 through 6 develop data science skills -- understanding the significance of data and where it comes from, and developing capabilities involved in manipulating data and using it to draw conclusions and make informed decisions. The researchers are developing and refining software, called Local Ground, designed to allow learners to collect data relevant to local scientific or socio-scientific challenges and to help learners manipulate those data and use them to achieve the challenge, in the process developing charts, graphs, and narratives to be shared with others. The system is designed to provide scaffolding that helps students iteratively develop and refine representations and understandings of what their data represent. It is designed to display and translate between a variety of distinct representations. The intention is that Local Ground will act as an \"auxiliary stimulus\" -- a cultural form wedged between students' na\u00efve ways of thinking and spontaneous inclinations to represent that thinking. The research team is using use of Local Ground as a context for investigating fundamental questions associated with developing data science skills, including how children convert na\u00efve representations into more usable and communicable forms, how and if those forms are appropriate by others, how those processes support understanding of core mathematic, statistical, and computational constructs, and the impact of locally collected and relevant data on learning these competencies.<br\/><br\/>Understanding, manipulating, and using data are essential skills for 21st century citizens. The researchers in this project are exploring a new approach to helping pre-teens begin to develop data skills and designing a software platform called Local Ground that supports 4th through 6th graders as they work on community projects they care about that require significant data collection, manipulation, analysis, and application. The pedagogical approach has students using data in sophisticated ways to address issues of importance to their communities; the tool helps them successfully gather, analyze, and use the data. In this context, the researchers are engaging in research that will add to what is known about how to help young learners understand core mathematical, statistical, and computational constructs important to data use and analysis.","title":"EXP: Local Ground: A Contextually Grounded Approach for Learning Data Science Skills","awardID":"1319849","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[550609],"PO":["562669"]},"198786":{"abstract":"The fruits of data mining pervade every aspect of our lives. We have books and movies recommended; we are given differential pricing for insurance; screened for potential terror threats; diagnosed with various diseases; and targeted for political advertising. The ability to sift through massive data sets with sophisticated algorithms has resulted in applications with impressive predictive power. And yet there is still a gap between what such tools can deliver, and what the users of data mining really need. It is often hard to interpret the answers produced by a learning algorithm, due to its sophistication and the use of large data sets to build models. The results of mining are often \"one-size-fits-all\", and convincing a user that results are actually relevant to them is difficult. Finally, there is the important problem of validation. As the results of data mining affect more and more of our lives, the more crucial it is that the user be able to validate decisions made on their behalf and that affect them. The common theme tying these issues together is a user-centric perspective on the problems of data mining. Rather than asking \"What patterns can be found in this mountain of data?\" this work instead asks \"What structures in this data affect me?\" These issues arise precisely because of the vast amounts of data we now have the ability to mine, and the sophisticated methods at our disposal to analyze this data. In this research, the PIs develop a computational framework and key tools for user-centric data mining. A central theme in this research is the idea of interaction. In both machine learning and in the foundations of complexity theory, interaction has been used to allow a (weaker) entity to probe a much more powerful system and determine answers that it lacks the resources to compute directly itself. The PIs use formal interaction mechanisms both from the perspective of a user interacting with a powerful algorithm, as well as a client interacting with a computing source with access to large data, in order to enable the user to interpret and validate the results of data mining. <br\/><br\/>The goal of this project is to develop a computational framework for user-centric data mining that enables existing users to tailor data analysis to their needs and facilitates the use of data mining in new areas where existing The team proposes interactive mechanisms that start with the results of a learning process and, via interaction with the user, produce an explanation expressed in terms of meaningful features, drawing on ideas from active learning, feature selection, and domain adaptation. 2. Locality: Answers that are relevant. Here, the focus is on providing information that depends more on a user?s local neighborhood, achieved via a new local notion of stability. 3. Verifiability: Answers you can check. The team proposes a framework for the validation of computationally-intensive data mining by the computationally-weak user, with ideas from interactive proof theory and stream algorithms. Tools for analyzing patient medical data have become more sophisticated and individual medical profiles play a far more significant role in diagnosis and treatment.The research examines user-centric data mining via three core primitives (classification, regression and clustering), and studies the three problems of interpreting results, providing local explanations, and validating the results of data mining. Firstly, the research draws on ideas from active learning, feature selection and domain adaptation to build interpretable results via interaction with users. Secondly, it introduces local notions of stability as a way of validating predictions for a specific user. Finally, it develops a general framework for validation of an analysis by a computationally-weak user, by drawing on ideas from the theory of interactive proofs and streaming algorithms.","title":"BIGDATA: Small: DA: Collaborative Research: From Data to Users: Providing Interpretable and Verifiable Explanations in Data Mining","awardID":"1251049","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["542076",533314],"PO":["565136"]},"209819":{"abstract":"Although many virtual organizations (VO) are quite effective, not all VO practitioners are effective in each area, and there is no organized body of knowledge or set of ?best practices? among VOs to draw upon for key issues. Therefore centers are likely not as effective as they could be. This proposal involves the creation of an online knowledge exchange. This Virtual Organization Resources and Toolkits Exchange (VORTEX) would provide leaders of virtual organizations with resources about running virtual organizations and access to relevant organizational scientists. VORTEX is intended to aid in building a community among virtual organization leaders so that they can collaborate, share, and learn with and from each other. <br\/><br\/>Specific Objectives of the work include development, evaluation, and improvement of an online Virtual Organization Resources and Toolkits Exchange (VORTEX) environment to aid scientists and engineers to more effectively lead virtual organizations. This type of environment is necessary in order to: <br\/>(1) Connect leaders of virtual organizations with appropriate organization scientists; <br\/>(2) Provide online educational and reference materials for issues associated with managing virtual organizations; and<br\/>(3) Establish a center for leaders of virtual organization to share and collaborate with each other.","title":"EAGER: Virtual Organization Resources and Toolkits Exchange (VORTEX)","awardID":"1348461","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7396","name":"NEES RESEARCH"}}],"PIcoPI":[562479],"PO":["565272"]},"199138":{"abstract":"It will not be long until vanishingly-small, ubiquitous sensors find their way into the 1000?s of objects that people unconsciously interact with on a daily basis. With half of the global population projected to have over 1000 sensors in their lives by 2020, this translates into over 3 trillion sensors deployed ? a number that easily dwarfs the entire semiconductor market today. <br\/><br\/>Energy-autonomous sensors containing rechargeable local energy storage, energy harvesting, and ultra-low power electronics are therefore essential for enabling devices to be deployed at this scale. This CAREER project addresses critical needs in the area of wireless communication for this growing field of ubiquitous, energy-autonomous sensing devices. <br\/><br\/>There is a growing energy gap between the power consumed by electronics in wireless computing devices, and the amount of energy that can be stored and harvested considering the small form-factor of these devices. This project is exploring new technologies to address the power consumption of wireless communication, which typically dominates the total power consumption of modern sensor nodes. The research addresses the needs for improving the modeling of wireless channels for a wide range of applications and improving the means by which these channels are measured. Additionally, new wireless communication circuits and architectures are being developed, including wakeup and clock harvesting receivers for the purpose of synchronizing sensor networks. Ultra-low power radios are also being developed to address the tradeoffs between power consumption of the radios, and their performance in dense wireless environments.<br\/><br\/>This project is important because it addresses an obstacle in scaling wireless sensor nodes -- drastic reductions in the power consumption of wireless communication and innovations in RF energy harvesting may enable new sensor network applications.","title":"CAREER: Ultra-Low Power Radios for Energy-Autonomous Systems","awardID":"1253172","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[534190],"PO":["551712"]},"202823":{"abstract":"The Speech Recognition Virtual Kitchen <br\/><br\/>Performing successful research on end-to-end speech processing problems requires the integration of many individual tools (e.g. for data cleaning, acoustic model training, language modeling, data analysis, real-time audio, decoding, parsing, synthesis, etc.). It is difficult for new researchers to get started in the field, simply because a typical lab environment consists of a hodgepodge of tools suited to a particular computing set-ups. This environment is hard to recreate, because few people are experts in the theory and practice of all these fields, and can debug and replicate experiments from scratch. <br\/><br\/>This research infrastructure project creates a \"kitchen\" environment based on Virtual Machines (VMs) to promote community sharing of research techniques, and provides solid reference systems as a tool for education, research, and evaluation. We liken VMs to a \"kitchen\" because they provide an environment into which one can install \"appliances\" (e.g., toolkits), \"recipes\" (scripts for creating state-of-the art systems using these tools), and \"ingredients\" (spoken language data). The kitchen even holds \"reference dishes\" in the form of complete experiments with baseline runs, log-files, etc., together with all that is needed to recreate and modify them. <br\/><br\/>The project is developing a community and repository by (a) building pilot VMs, (b) engaging the community in using and continuing to develop them on its own, and (c) evaluating the impact of providing VMs for education and research. We envision researchers as well as students downloading a VM, reproducing the baseline experiment, implementing changes, posting their results in the community, discussing with other users who have worked on the same VM, merging improvements back into the VM, which get re-distributed, and finally publishing easily reproducible results. Work with curriculum and project development will support the creation of engaging activities to specifically encourage students at undergraduate and graduate levels.","title":"CI-ADDO-NEW: Collaborative Research: The Speech Recognition Virtual Kitchen","awardID":"1305365","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543578],"PO":["565215"]},"205705":{"abstract":"This project addresses the growing need for improved synchronization methods for large-scale, multi-party distributed systems.\u00a0 As users migrate information to cloud storage, cloud providers may use multiple loosely consistent replicas because of the overhead of keeping replicas synchronized at all times.\u00a0 Further, users themselves may create loosely synchronized replicas on laptops, tablets, or other devices when they are disconnected from the cloud storage.\u00a0 Periodic synchronization, or reconciliation, becomes a requirement in such settings.<br\/><br\/>Reconciliation has been most studied in the specific setting of two connected users, each containing a set of keys, and both desiring the union of the sets.\u00a0 The costs of these protocols is typically in terms of the size of the symmetric set difference between the two sets.\u00a0 The Principal Investigator (PI) will focus on generalizing reconciliation methods to settings where many parties communicate over a network represented by a graph.\u00a0 The new framework will aim to encompass standard problems such as rumor spreading and network coding, as well as generalize to other objects such as sequences with other measures such as edit distance. The primary theoretical and practical challenge the PI will pursue is to develop schemes where the amount of communication necessary for object synchronization depends only on the size of the difference among objects that need to be synchronized, and not the sizes of the objects themselves.\u00a0 For example, for large databases, the synchronization cost should depend on the delta between the databases, which will generally be much smaller than the databases themselves.\u00a0 A further goal is that the framework will have practical consequences for modern cloud-based deployments, especially large-scale big data systems. Because the goals of reconciliation include both algorithmic efficiency as well as communication efficiency, the PI will work to bring ideas from both the theoretical computer science and information theory communities together in this work.\u00a0 The new algorithms and data structures developed for these problems are expected to have significant additional uses for other problems as well.","title":"AF: Small: Data Synchronization : Theory, Algorithms, and Practice","awardID":"1320231","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["560092"],"PO":["565251"]},"205958":{"abstract":"Title: Collaborative Research: SAVI- Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field.<br\/><br\/>This award is designated as a Science Across Virtual Institutes (SAVI) award and is being co-funded by NSF?s Office of International Science and Engineering. Four US environmental observatories (National Ecological Observatory Network (NEON), Ocean Observatories Initiative (OOI), Advanced Modular Incoherent Scatter Radar (AMISR) and collectively EarthScope, comprising USArray and the Plate Boundary Observatory (PBO), operated respectively by the Incorporated Research Institutions for Seismology (IRIS) and UNAVCO) are working together with their respective counterparts from the European Union on developing common data policies and standards relevant to global research infrastructures in the environment field. <br\/><br\/>The project goals include developing new understanding through broad harmonization of data and constructing multi- discipline, synergistic data products that have wider societal importance. In addition, these activities will help the scientific community to better develop coupled models that better capture critical feedbacks and interactions of the earth system. These models are needed to help make these types of data more accessible to decision-makers at many levels. <br\/><br\/>Linking these existing programs provides a unique opportunity for early career researchers and students to learn in a multi-disciplinary environment and to benefit not only from the US participants but also their European counterparts. The observatories will be developing graduate and post-graduate courses and the workshops will seek to include early career scientists and students. <br\/><br\/>The proposed activities will focus on addressing societally relevant challenges. Through case studies, ways of harmonizing data will be investigated with the goal of making the data from these observatories easier to include in effective decision-making. Disasters, carbon and water will likely be the first set of issues addressed.<br\/><br\/>This program is creating opportunities for enhancing the career trajectories of a new generation of researchers in Europe and the U.S. The virtual institute is providing mechanisms for exposing early-career scientists to interdisciplinary, multi-institutional activities focused on environmental data and cyberinfrastructure; arming them with new scientific tools to address challenging questions in harmonizing environmental data to help in effective decision making; and showing them how international partnerships can help to solve global problems. It is recruiting a diverse set of US and European students to create collaborative networks of environmental data and cyberinfrastructure experts across these countries.","title":"SAVI: Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field","awardID":"1321565","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1679","name":"INTERNATIONAL COORDINATION ACT"}}],"PIcoPI":[551409],"PO":["565235"]},"205848":{"abstract":"This project explores techniques for learning acoustic features for speech recognition, based on multi-view learning using acoustic and articulatory recordings. Recent work has shown recognition improvements using this strategy via linear and nonlinear canonical correlation analysis, in which transformations of acoustic features are learned so as to maximize correlation with (transformations of) articulatory measurements. Prior work has been limited to a single database and a single language.<br\/><br\/>The main goals of this project are to learn better universal features for arbitrary speakers and languages and to develop improved multi-view techniques. Project activities include: learning time-varying projections; multi-view techniques based on neural networks; \"many-view\" learning using articulation, video, labels, etc.; efficient implementations; new input features such as spectro-temporal filters; and visualization tools for related research and education.<br\/><br\/>A critical component of automatic speech recognition is a representation of the audio signal that encapsulates useful information while discarding acoustic noise, speaker identity, and so on. This project aims to automatically learn improved representations using statistical analysis of audio recordings paired with positions of the speech articulators (lips, tongue, etc.) and other measurements. The project starts with basic statistical techniques, and develops new techniques that address challenges and opportunities specific to speech and related signals.<br\/><br\/>The project's impact extends beyond speech processing. Applications of multi-view representation learning include neurology, meteorology, chemometrics, computer vision, and text processing; all of these can benefit from the improved techniques. The work impacts education by generating materials for a Speech Technologies course and visualization tools for speech and other signals.","title":"RI: Small: Multi-View Learning of Acoustic Features for Speech Recognition Using Articulatory Measurements","awardID":"1321015","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[551099],"PO":["565215"]},"205969":{"abstract":"Title: Collaborative Research: SAVI- Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field.<br\/><br\/>This award is designated as a Science Across Virtual Institutes (SAVI) award and is being co-funded by NSF?s Office of International Science and Engineering. Four US environmental observatories (National Ecological Observatory Network (NEON), Ocean Observatories Initiative (OOI), Advanced Modular Incoherent Scatter Radar (AMISR) and collectively EarthScope, comprising USArray and the Plate Boundary Observatory (PBO), operated respectively by the Incorporated Research Institutions for Seismology (IRIS) and UNAVCO) are working together with their respective counterparts from the European Union on developing common data policies and standards relevant to global research infrastructures in the environment field. <br\/><br\/>The project goals include developing new understanding through broad harmonization of data and constructing multi- discipline, synergistic data products that have wider societal importance. In addition, these activities will help the scientific community to better develop coupled models that better capture critical feedbacks and interactions of the earth system. These models are needed to help make these types of data more accessible to decision-makers at many levels. <br\/><br\/>Linking these existing programs provides a unique opportunity for early career researchers and students to learn in a multi-disciplinary environment and to benefit not only from the US participants but also their European counterparts. The observatories will be developing graduate and post-graduate courses and the workshops will seek to include early career scientists and students. <br\/><br\/>The proposed activities will focus on addressing societally relevant challenges. Through case studies, ways of harmonizing data will be investigated with the goal of making the data from these observatories easier to include in effective decision-making. Disasters, carbon and water will likely be the first set of issues addressed.<br\/><br\/>This program is creating opportunities for enhancing the career trajectories of a new generation of researchers in Europe and the U.S. The virtual institute is providing mechanisms for exposing early-career scientists to interdisciplinary, multi-institutional activities focused on environmental data and cyberinfrastructure; arming them with new scientific tools to address challenging questions in harmonizing environmental data to help in effective decision making; and showing them how international partnerships can help to solve global problems. It is recruiting a diverse set of US and European students to create collaborative networks of environmental data and cyberinfrastructure experts across these countries.","title":"SAVI: Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field","awardID":"1321600","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1679","name":"INTERNATIONAL COORDINATION ACT"}}],"PIcoPI":["560434"],"PO":["565235"]},"205617":{"abstract":"The typical tools of complexity theory and algorithms tend to be biased toward low-level models of computation that refer directly to bit-level representations of data. But when programming in the large, there are layers upon layers of abstraction over any low-level details, and a key feature of modern programming is the ability to compose different abstractions without regard to such details. Compositionality has yielded very reliable software development methodologies, in large part because the tools developed for reasoning about program correctness have developed alongside these notions of abstraction. However, tools for reasoning about complexity have not kept pace. The big picture of the principle investigators' research program is to develop techniques for compositional reasoning about complexity, thereby allowing for reasoning about run-time cost in all its facets of large-scale programming with methods similar to those so successfully deployed for reasoning about correctness. The research project funded by this grant concentrates on characterizing sensible notions of feasibility for programs that use coninductively-defined data such as streams (a program that produces or processes streaming media is just the right model to have in mind) and more generally quantifying, in a machine-checkable manner, resource usage for such programs.<br\/><br\/>Although feasibility has been well-studied in the context of finite structures, extensions to potentially infinite data structures such as streams have been somewhat piecemeal. One facet of this research project is to develop principled notions of feasibility in this setting. To do so, the PIs will extend tools such as logic and programming language formalisms that have previously been used to give resource-free characterizations of complexity classes for finite structures. Such tools are already more closely tied to a compositional view of programming, and they give a jumping-off point for analyzing notions of cost in domains where the very definition of resource usage may not be so obvious. One such tool that they developed in previous work is a framework for compositional cost analysis of higher-order programs. This time-complexity semantics is essentially a translation of target-language programs into a domain of complexities, which encode information about evaluation and usage cost. The translation can be automated, and so this framework not only provides a tool for reasoning compositionally about cost, but provides machine-checkable assertions about the cost of target programs.","title":"SHF: Small: Collaborative research:Complexity and feasibility for programs over coinductively-defined data","awardID":"1319769","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550560],"PO":["565264"]},"202229":{"abstract":"The technical goal of this project is to develop and refine the micro-fluidic 3D IC cooling technology. While 3D integration offers significant potential for improving the performance, energy efficiency and functionality of electronic systems, the problem of heat removal is significantly exacerbated. Conventional air cooling alone would be incapable of addressing the future 3D IC heat removal requirements. In this project, the PIs are investigating use of aggressive micro-fluidic cooling technology for cooling 3D ICs. The team comprises researchers from University of Maryland and Georgia Institute of Technology. The Georgia Tech team would bring forth significant expertise in fabrication and modeling of 3D ICs with interlayer micro-fluidic cooling. The Maryland team will bring forth expertise in VLSI design methodologies. The primary focus of this proposal is: development of techniques and tools for co-design of micro-fluidic embedded cooling and electrical aspects of 3D ICs.<br\/><br\/>This proposal would directly support several PhD students in different disciplines. Because of the cross disciplinary nature of this proposal, these students would need to learn diverse set of topics pertaining to fluidics, chip design and thermal management. Undergraduates will also be involved through various programs at Georgia Tech and Maryland. The outcomes of this research will be published in respectable venues in both electrical\/computer engineering and mechanical engineering. The tools, models and experimental data will also be made available on the web. The PIs plan to organize tutorials at various conferences and educational forums. Special emphasis will be givenon minority involvement via collaboration with local HBCUs.","title":"SHF:Medium:Collaborative Reseach: Electrical-thermal Co-Design of Microfluidically-Cooled 3D IC's","awardID":"1302375","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[541967],"PO":["565272"]},"205507":{"abstract":"The objective of this project is to understand the roles of general morphogenetic principles such as heterogeneity of components, differentiation\/re-differentiation of components, and local information exchange among components in the self-organization of biological collectives, and to utilize them to improve the performance of collective artificial systems. The research is conducted in the following four steps: (1) Study the effects of morphogenetic principles on self-organizing patterns of heterogeneous collectives using mathematical\/computational models. (2) Apply the results of theoretical investigation to construct models of actual self-organizing patterns found in real-world heterogeneous biological collectives. (3) Introduce the morphogenetic principles to existing collective artificial systems and develop mechanisms for programming their structures and behaviors. (4) Measure the overall performance of the proposed morphogenetic collective systems in decentralized optimization and exploration tasks. This research contributes to biology and ecology by providing new theoretical insight into how heterogeneous collectives may self-organize without centralized control, and also to computational science and engineering by bringing an unexplored combination of morphogenetic principles into bio-inspired computational systems. It is expected to produce societal impacts by providing a novel framework for the design of growing, self-organizing, self-repairing, and evolving artifacts. This project involves the following educational activities: (1) Interdisciplinary graduate certificate program in complex systems science. (2) New course on computational approaches to the origin and evolution of life. (3) Regional high school research program. (4) Public exhibition of morphogenetic collective systems at New York Hall of Science. Efforts will be made to attract female and ethnic minority students who are currently underrepresented in STEM.","title":"RI: Small: BCSP: Robustness and Adaptation in Morphogenetic Collective Systems","awardID":"1319152","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7659","name":"ANIMAL BEHAVIOR"}}],"PIcoPI":[550286],"PO":["564069"]},"205628":{"abstract":"Noise, which corresponds to random variations extrinsic to the signals of interest, is an ubiquitous aspect that affects the performance of many tasks in signal processing. Even with the improving quality and sophistication of the modern acquisition devices, digital signals still carry noise due to many incontrollable factors. This research focuses on the fundamental problem of estimating parameters of the random noise model directly from a noise corrupted signal. As an immediate consequence, the results of this investigation will be applicable in a wide range of fields, including the forensic analysis of digital images, automatic processing of medical images, spectrum sensing in wireless communications and data processing in sensory neuroscience. <br\/><br\/>The technical approach taken in this research exploits the regular statistical properties of the original signals in multiple signal representations and their relationship with the noise parameters. Specifically, we will investigate the use of domains constructed from random band-pass filters that are more effective in revealing ?typical? statistical properties of the signals, especially when compared with deterministic representations such as Fourier, DCT, and wavelet. Concurrently, we will investigate the mathematical relationship between the observed statistics of noisy signal and the noise parameters. Drawing on these theoretical findings, this research is expected to lead to more effective and efficient algorithms for blind noise estimation. More generally, the proposed work will also explore efficient algorithms for blind local noise estimation in the presence of non-stationary noise statistics.","title":"Blind Noise Estimation Using Signal Statistics in Random Band-Pass Domains","awardID":"1319800","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[550584],"PO":["564898"]},"205749":{"abstract":"The goal of this research is to develop a navigation methodology for robots using relational geometric models that are constructed and evaluated using minimal sensing. The use of qualitative states is both intuitive to human perception, and amenable to long-term operations in robotics because of the inherent scalability and simplicity in sensing. The technical approach consists of four tasks. The first task is to create a mathematical representation that supports relational sensing\/maps\/navigation. The second task is to extend the representation and methods to include uncertainties, which allows the work to be of practical significance. The third task is to develop a hypothesis based navigation methodology that naturally enables planning over minimal, uncertain relational maps and sensor data. Finally, the theory will be validated using an increasingly complex set of experimental validation tests. This research enables robust navigation of robots in applications such as autonomous driving and personal robotics, as well as applications that have unstructured environments with no GPS access, such as exploration (under water, planetary, caves), disaster relief and search and rescue. The research is particularly well suited to robots that have sensing and computational constraints. In the long term, it is envisioned that the algorithms and software enable life-long learning in robotics because of the ability to scale to long-term operations.","title":"RI: Small: Qualitative Relational Navigation using Minimal Sensing","awardID":"1320490","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["553705"],"PO":["564069"]},"208918":{"abstract":"The IEEE International Symposium on Workload Characterization (IISWC) is a premier forum for presenting, discussing and debating new and innovative ideas and techniques for the characterization of computing system workloads. This symposium brings together researchers in fields related to processor architecture, compilers, operating systems, and languages. This award will provide travel support for students in order to defray the costs of attending and participating in the IISWC conference in September, 2013 in Portland, Oregon, USA. Supporting student travel to attend professional conferences and workshops is a very important mission of the NSF. Broader impacts include training the next generation of researchers in this important research area.","title":"Student Travel Support for the 2013 IEEE International Symposium on Workload Characterization (IISWC-2013)","awardID":"1342915","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":[559984],"PO":["565264"]},"210832":{"abstract":"This project will design algorithms for the advanced scheduling of communications and computation resources. A growing number of scientific computing applications require reliable services involving large amounts of data. These services may range from the simple transfer of a large data set to the complex execution of a collection of interdependent tasks that have varying requirements with respect to processing, storage, and communication. The support of such services requires a robust computing and network infrastructure that is capable of providing dynamic on-demand provisioning of bandwidth and scientific computing resources. The basis of this infrastructure is a collection of scientific computing resources that are interconnected by optical networks. Many on-demand provisioning techniques for optical networks focus on immediate reservations, in which resources are reserved and used immediately following a request; however, immediate reservations may be unable to guarantee the availability of resources with high probability, particularly if a significant amount of resources are required for an application. Advance reservation mechanisms enable dedicated resources to be reserved by an application in advance with the task not starting execution until it is possible to guarantee the availability of the needed resources. Advance reservation also allows more efficient use of resources and may be applied to both optical network resources and scientific computing resources in a coordinated environment. <br\/><br\/>The project will create a holistic multi-layer coordinated framework to support the coordinated advance provisioning of Grid and optical network resources. Specific goals of the project include 1) the development of theoretical bounds and algorithms to support generic advance reservation of optical network bandwidth resources by considering the time, space, bandwidth, and destination domains, 2) the design of a novel delayed allocation algorithm for supporting advance reservation requests, 3) the design and evaluation of coordinated Grid and network resource scheduling algorithms that incorporates different domains of flexibility, such as lightpath switching and supports enhanced services, such as anycasting, and 4) the design and implementation of the proposed advance reservation and co-scheduling algorithms on the nation-wide Energy Sciences network using the OSCARS framework.<br\/><br\/>Broader Impact:<br\/>The integrated outreach and group mentoring program will promote female students' recruitment and retention in computer science and engineering disciplines. The implementation of the CARGONET algorithms on the OSCARS framework will assist in conducting efficient resource allocation of optical and Grid resources.","title":"NeTS: Small: Coordinated Advance Reservation for Grid over Optical Networks (CARGONET)","awardID":"1406370","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":[564860],"PO":["564993"]},"210964":{"abstract":"This project is funded as part of the United States-Israel Collaboration in Computer Science (USICCS) program. Through this program, NSF and the United States - Israel Binational Science Foundation (BSF) jointly support collaborations among US-based researchers and Israel-based researchers.<br\/><br\/>Many important problems in the modern world can be solved by finding shortest paths in some network. Some examples include computing driving directions from one city to another in a road network, or routing internet traffic from one computer to another in a communication network such as the internet. The networks in such applications are not only complex, but can also be extremely large. The development of fast, reliable and scalable algorithms for shortest paths is thus of crucial importance. The major goal of the proposed research is to provide such algorithms in a variety of settings, providing mathematical guarantees on their performance and scalability.<br\/><br\/>The research will focus on two notions of data structures maintaining shortest paths, both focusing on storing distance information using small space. The first are Distance oracles (DOs), data structures that compactly represent the path structure of a network with the ability to quickly retrieve approximate distances and shortest paths between any two given nodes. The research aims at deepening our understanding of DOs in several different ways: by obtaining DOs with improved guarantees in new (e.g. distributed) settings, by developing faster algorithms for constructing DOs, and by proving conditional, or preferably unconditional cell-probe lower bounds showing that the obtained guarantees are essentially optimal. The second type of shortest paths data structures that will be considered are Distance sensitivity oracles (DSOs). These are data structures that provide a compact representation of the distances in a graph in which edges can become unavailable. A query to a DSO consists of a failed edge and two nodes, a source and a target, and the DSO must return a shortest path from the source to the target that does not use the failed edge. The goal here is to develop faster algorithms for constructing DSOs with fast query times, and to prove relationships between DSOs and other closely related problems such as all-pairs shortest paths. In addition to their intrinsic value, DSOs may also help develop efficient dynamic shortest paths algorithms which is another objective of this project.<br\/><br\/>Besides the clear practical motivation behind the project, the problems to be studied have intriguing relations to many concepts in mathematics (metric embeddings, graph and geometric spanners, etc). Thus the impact of this research goes beyond the strict boundaries of computer science. The PI is whole-heartedly committed to diversity. The PI has experience in recruiting and mentoring minority students, and will continue to take an active role in seeking and recruiting students from diverse cultures and backgrounds.","title":"BSF:2012338:Shortest Paths: Upper and lower bounds","awardID":"1417238","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":[565156],"PO":[565157]},"210524":{"abstract":"The emergence of \"Big Data\" provides us with some of the biggest challenges and opportunities. The challenges include the capture, storage, sharing, search, visualization, analysis, understanding, and processing of massive amounts of data that are complex and heterogeneous and are being gathered at rates that exceed our current capacity for handling such data. The opportunities stem from the potential for extracting useful knowledge from Big Data for more informed decisions, helping accelerate discovery and innovation, and supporting their transition into practice to benefit society. To address the challenges and opportunities of \"Big Data\", the U.S government has launched, in March 2012, the National Big Data Research and Development Initiative. But making the most of Big Data requires a joint effort from all the stakeholders: the government, industry, universities, and non-profits.<br\/><br\/>The workshop \"Making the Most of Big Data: Current and Future High-impact Collaborations\" aims to examine high-impact collaborations and identify additional areas for possible collaboration between the public and private sectors, with a special emphasis on projects and initiatives that: advance technologies that support Big Data and data analytics; educate and expand the Big Data workforce; develop, demonstrate and evaluate applications of Big Data that improve key outcomes in economic growth, job creation, education, health, energy, sustainability, public safety, advanced manufacturing, science and engineering, and global development; demonstrate the role that prizes and challenges can play in deriving new insights from Big Data; and foster regional innovation.<br\/><br\/>The workshop engages the key Big Data stakeholders from academia, government, and industry: representatives of the many federal<br\/>agencies that support significant Big Data projects, leading academics engaged in Big Data research, leading Big Data innovators from industry, as well as participants from the state and local governments, non-profits, foundations and other organizations engaged in Big Data research, applications, workforce development, and technology transfer activities. Discussion of existing high-impact collaborations, keynote addresses on Big Data topics, privacy and ethical issues, a plenary panel and in breakout sessions covering critical Big Data issues (such as Workforce Development, Big Data Research and Development, Issues of Public Concern, and Big Data Innovation) is expected to lead to a better understanding of the current status of the Big Data efforts, and the identification of the best ways to address the challenges that remain and leverage the combined resources of private and public sectors to more fully realize the potential of Big Data. <br\/><br\/>Anticipated broader impacts of the project include recognition and appreciation of current high-impact collaborations; increased collaborations among the multiple Big Data stakeholders leading to transformative advances in the core Big Data technologies of capturing, storing, sharing, searching, visualizing, analyzing, understanding, and processing of huge, diverse, complex, and distributed data sets; and fostering of innovation in science, engineering, and education necessary for advancing national goals and priorities in economic growth, education, health, clean energy, and security.","title":"Making the Most of Big Data: Current and Future High-Impact Collaborations","awardID":"1358747","effectiveDate":"2013-09-01","expirationDate":"2014-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[564109],"PO":["565136"]},"202835":{"abstract":"Immersive virtual environments offer tremendous potential for fundamental and transformative advances in education, training, rehabilitation, architectural design, as well as a wide range of other application areas. The technology has special potential to enhance the process of architectural design by enabling designers to work with their ideas at full scale from the earliest stages of the design process, and to experience the interior spaces of their designed structures from a firsthand egocentric perspective before they are built. The research and educational opportunities made possible by the requested equipment will enable designers and their clients to experience a virtual environment as if the designers and their clients were truly standing in the physical environment that is represented by the virtual environment, and to make decisions based on their experience in the virtual world that are equivalent to the decisions that they would have made as a result of a similar experience in the physical world. The project will teach students of architectural design the value of developing their design ideas from an egocentric as well as allocentric perspective. Through this work, architects, and their clients, will be able to make reliable design decisions that can enhance the livability of a planned space based on their first-person experience of the 3D spatial layout. The project will enable archaeologists, historians, city planners, tourists, and others to gain an intrinsic, egocentric understanding of the spatial layout of large, remote sites through virtual exploration.<br\/><br\/>Intellectual Merit<br\/><br\/>An interdisciplinary team consisting of an associate professor of computer science in the college of science and engineering and an associate professor of architecture in the college of design requested funding for three pieces of major equipment and related supplies that will enable them to (a) pursue a multi-faceted research agenda in the development of groundbreaking methods to enhance the cognitive and perceptual realism of immersive virtual experiences, leveraging fundamental insights from visual perception and cognition, and (b) maximize the potential of virtual reality technology to fundamentally enhance the process of design education, emphasizing the importance of integrating an experiential understanding of planned spaces into the earliest stages of the design process. The proposal requests funding for the purchase of (a) a wide field of view head mounted display with an embedded stereo eye tracking system that the researchers will augment with lightweight, close-range depth sensors and a custom-built dual camera and mirror system to achieve a convergence-adaptable stereoscopic video-see-through augmented reality capability, (b) a set of cameras that will enable real time tracking throughout the full extent of the space available in a virtual reality design lab, and (c) an untethered, moderate field of view head-mounted display with optional optical see through capabilities that will, in conjunction with a backpack-worn laptop computer, allow unencumbered free physical movement through large virtual spaces and, in conjunction with the other head-mounted display, enable the project to pursue research in self-embodied multi-person interaction in immersive virtual environments.<br\/><br\/>Broader Impact<br\/><br\/>The requested equipment will benefit society by permitting research that will advance an understanding of how people can have experiences in immersive virtual environments that are equivalent to experiences in real world environments. This equipment will support significant advances in design education by enabling the broader effective use of virtual environments technology in teaching fundamental concepts of visual imagination and will support closer interdisciplinary collaboration between faculty and students in computer science, architecture and design. The project will promote science and engineering to the general public through community outreach such as through frequent lab tours for local K-12 students, and will promote participation in science through mentoring and summer programs for middle and high school students.","title":"II-NEW: Virtual Reality Infrastructure and Technology Development to Support Architectural Education and Basic Research in Immersive Design, Embodied Interaction, Spatial Cognition","awardID":"1305401","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543617,"565146"],"PO":["565362"]},"210227":{"abstract":"People with visual impairments face significant barriers to participation in public life and productive contributions in the workplace. Barriers due to inaccessible information and communication technology can make it difficult for a person with visual impairments to excel in school or find employment commensurate with his or her abilities and potential. Fortunately, accessible computing technology enables people with visual impairments to access information that was previously inaccessible. For example, people with visual impairments can use screen reader software to translate electronic documents into speech or Braille. However, barriers still exist in the context of using computers to participate in the contemporary classroom or workplace because these are now places where active, real-time, face-to-face collaboration with other people has become the norm, and because there is a gulf between the look and feel of assistive technology such as screen readers and the look and feel of contemporary graphical user interfaces such as those used routinely by people without disabilities. Screen readers are difficult to learn and use, and operate quite differently than contemporary graphical user interfaces, and most sighted people do not know how to use screen readers, and so the opportunity for shared artifacts in communication and collaboration is reduced.<br\/><br\/>The focus of this proposal is to explore and identify barriers to collaboration between people with and without visual impairments. This research will support the development of tools that will bridge the gap between the non-graphical user interfaces used by people with visual impairments and the graphical interfaces used by sighted people, enabling individuals with any level of visual ability to collaborate at school or work. Building on prior research on developing accessible user interfaces for people with visual impairments, this project will explore the challenges of collocated synchronous collaboration between people with and without visual impairments. The researchers will conduct formative interviews and observational studies of professional adults with and without visual impairments to understand the barriers to collaboration within and across these populations. Very little prior research has explored these issues. This project will conduct a deep exploration to reveal accessibility barriers that were previously hidden and unknown.<br\/><br\/>Broader Impacts: This research will identify existing barriers to employment that can be addressed through changes in policy or practice, or through the development of new technology. Enabling people with visual impairments to work alongside their sighted peers will significantly improve educational and employment outcomes for millions of people. The research project itself will be developed in the context of a collaboration between people with and without visual impairments, and will include blind and visually impaired people from the local community.","title":"EAGER: Understanding Barriers to Workplace Collaboration for People with Visual Impairments","awardID":"1353312","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563399],"PO":["565362"]},"210007":{"abstract":"This project explores a new direction in computer vision, which is to model the context dependent visual semantics associated with images in social multimedia. The context dependent visual semantics, e.g., the intended and perceived sentiment of an image in social multimedia, are dynamically formed based on the various contextual information associated with it. This is different from the static visual semantics that conventional computer vision research focused on studying, such as the object category presented in the image.<br\/><br\/>The project develops a set of new networked and context aware probabilistic latent semantic models, which integrate situated contextual information into visual content analysis for modeling context dependent visual semantics. The research team is verifying two hypotheses: 1) the context dependent semantics needs to be holistically modeled and jointly inferred from a collection of related images; and 2) related context dependent visual semantics, such as intended and perceived meaning of an image, also needs to be jointly modeled for more robust recognition.<br\/><br\/>The project is integrated with education through training graduate and undergraduate students. The outcome of the research can be applied to many domains, such as targeted online advertisements; open source information analysis and social event prediction; and social multimedia security.","title":"EAGER: Towards Human Centered Visual Understanding: Exploring the Intended and Interpreted Meaning of Images in Social Multimedia","awardID":"1350763","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[562929],"PO":["564316"]},"202758":{"abstract":"Prior to the start of this project, the investigators developed a novel RF-powered sensing and computing platform called the WISP that harvests the entirety of its operating power from RF signals emitted by RFID readers. They also developed a companion SDR (Software-Defined Radio) reader platform that sends RF signals to power and communicate with the WISPs using a power efficient communication technique known as backscatter. The hardware and software for both the WISPs and readers was open-sourced and made available to the research community. This proposal enhances that previously developed infrastructure, and makes it more widely available to the research community, by donating next generation, enhanced versions of the WISPs, readers, and software to qualified researchers. Compared to the first generation infrastructure, the enhanced infrastructure has improved read range, program memory, computing power, and programming abstractions.<br\/><br\/>This project facilitates a long-term transformation in which computing becomes more and more deeply connected to the physical world. This transformation can enable smarter living and working environments, better medical care, improved manufacturing and logistics, reduced energy usage and pollution, and other benefits. A fundamental difficulty has been how to power these physically-embedded microelectronic systems. Batteries limit devices lifetimes, size, weight, and form factor. However, improvements in the energy efficiency of microelectronics have recently made it possible to run embedded computers using only power harvested from RF signals. This proposal provides infrastructure to the research community to enable RF-powered computing research, and thus deeper embedding of computation in the physical world.","title":"CI-ADDO-EN: Infrastructure for the RF-Powered Computing Community","awardID":"1305072","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543393,543394],"PO":["564778"]},"204716":{"abstract":"The Internet has become one of the most effective and common means of conveying expression that is likely to be controversial or suppressed. This freedom of expression is threatened by the now widespread practice of Internet censorship by both private and state interests.<br\/><br\/>The Tor network is one of several tools designed to allow users to safely and efficiently access censored content through one or more relay computers. Using these relays makes it difficult for a censor to determine what content is being accessed by whom. Unfortunately, this also makes it hard to measure the use and performance of these tools, leading to challenges in assessing the impact of the tools, and the security and performance of proposed alternatives.<br\/><br\/>This project is developing protocols and software to address these challenges, addressing three specific aims:<br\/><br\/>1. Developing and deploying cryptographic protocols that safely and privately measure and release important characteristics of the dynamics of the Tor network, including: number of users over time, user uptime and downtime distributions, user churn, network load, and exit statistics. <br\/><br\/>2. Enhancing the security and performance of Tor, using the data collected to improve evaluations.<br\/><br\/>3. Improving generic methods to circumvent relay blocking, including proxy distribution schemes, protocol steganography, and decoy routing.<br\/><br\/>The project is expected to result in widely-used datasets for evaluating censorship circumvention schemes, training of undergraduate and graduate students, and broad dissemination of free software that promotes freedom of expression on the Internet, against regimes that strongly oppose such expression.","title":"TWC: Option: Medium: Measurement-Based Design and Analysis of Censorship Circumvention Schemes","awardID":"1314637","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[548224,548225],"PO":["562974"]},"205816":{"abstract":"This research is concerned with discrete computational tasks, mainly mathematical decision and optimization problems. Often such problems can be attacked directly by discrete methods. The focus of this research is to study situations where algebraic approaches produce better solutions. Even though the problem formulation can be entirely discrete, significant insight and efficient algorithms might be obtained by applying sophisticated algebraic methods. It is not uncommon that combinatorial problems have simple and elegant formulations, yet the obvious algorithms are too slow for their solutions except for very small instances. In such situation, algebraic methods might provide the decisive insights.<br\/><br\/>The graph isomorphism problem exemplifies a combinatorial problem where algebraic methods have successfully produced efficient algorithms. This research also deals with other foundational mathematical problems having these characteristics, like efficient multiplication of long integers and the monomer dimer problem, and the counting of matchings in grid graphs. Typical algebraic tools used are group theory, the discrete Fourier transform, as well as the zeta transform and its inverse, the Mobius transform. Usually, there is no obvious way of how to apply these tools most effectively. For example, the fastest integer multiplication algorithms are all based on Fourier transforms, but the efficiency heavily depends on the type of Fourier transform applied. <br\/><br\/>This project deals with fundamental combinatorial and algebraic tasks for which efficient algorithms are desirable. This research is significant, because it contributes to a better understanding of the mathematical structures behind these problems and leads to the discovery of more efficient algorithms.<br\/><br\/>An important goal of this project is to contribute to the development of a new generation of graduate students, who appreciate the development of mathematical insight into difficult combinatorial and algebraic problems with the goal of producing efficient algorithms. In particular, integer multiplication is such a fundamental arithmetic task that understanding and improving it is an obvious basic intellectual challenge. Such theoretical goals are foremost in this project. But there is a potential for an impact on the search for Mersenne primes and on general purpose computations with high degree polynomials. Other aspects of this research involve topics with applications in Physics and Chemistry.","title":"AF: Small: Algorithms Based on Discrete and Algebraic Methods","awardID":"1320814","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[551015],"PO":["565157"]},"204727":{"abstract":"Risk communication is an important part of many cyber security mechanisms. Android's current risk communication mechanism is based on security warnings and has been demonstrated to be ineffective because users become habituated to ignore such warnings and tend to consent to all prompts. This multi-disciplinary research project aims at developing holistic solutions to usable risk communication and control for the Android platform. <br\/><br\/>This project investigates an approach that presents risk information at multiple granularities, including a high-level numerical risk summary, an intermediate-level summary of risk for different dimensions, and detailed risk information. The high-level risk summary is computed by information integration techniques, using information discovered from multiple sources, e.g., user reviews and app source code. This summary enables proactive risk communication (e.g., when the user searches for apps) so that users can take this information into the decision process. <br\/><br\/>This project also introduces a multi-mode approach that, in addition to communicating risks, also controls risks in the sense of discouraging risky applications and ensuring that users truly understand the risks. The project develops mechanisms that aggregate, communicate, and control risks incurred by apps at runtime, and ways to personalize risk integration, communicate, and control techniques to accommodate differences among users.<br\/><br\/>This project is expected to advance the state of the art in principles and techniques to risk communication and control, and has the potential to impact the Android app ecosystem by collaboration with Google researchers.","title":"TWC SBE: Medium: Collaborative: User-Centric Risk Communication and Control on Mobile Devices","awardID":"1314688","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[548253,548254,548255],"PO":["565136"]},"205827":{"abstract":"Crowdsourcing-based cooperative spectrum sensing (CCSS) refers to a spectrum-sensing service provider (SSP) outsourcing the spectrum-sensing tasks over a large geographic region to distributed mobile users referred to as mobile detectors. The promise and feasibility of CCSS are deeply rooted in the ubiquitous penetration of increasingly powerful mobile devices into everyday life and in the anticipated prevalence of dynamic spectrum access (DSA) in future mobile communication systems. CCSS is expected to be much more cost-effective than deploying a large-scale dedicated network of distributed spectrum sensors. This research is to investigate a secure and privacy-preserving CCSS architecture. The research tasks include: (1) incentive-aware and reputation-aware selection of mobile detectors whereby the SSP can select an optimal set of mobile detectors for a sensing task; (2) secure combination which enables the SSP to minimize the impact of false sensing reports on the final detection result; (3) a reputation system which records the past sensing performance of mobile detectors and provides crucial input into the selection of mobile detectors and the secure combination of sensing reports; and (4) spectrum-misuse detection to enable the realtime detection of unauthorized spectrum use. <br\/><br\/>This research will expand the fundamental understanding of security, privacy, and incentive issues in CCSS. Materials of this project will be made publicly available online as tutorials, talks, publications, and software toolkits. The education plan of this project is to develop new cross-disciplinary teaching materials on DSA and involve undergraduates, underrepresented students, and graduates in networking and security research.","title":"NeTS: Small: Secure Crowdsourcing-Based Cooperative Spectrum Sensing","awardID":"1320906","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[551041],"PO":["557315"]},"206806":{"abstract":"Natural language privacy policies have become a de facto standard to address expectations of notice and choice on the Web. Yet, there is ample evidence that users generally do not read these policies and that those who occasionally do struggle to understand what they read. Initiatives aimed at addressing this problem through the development of machine implementable standards or other solutions that require website operators to adhere to more stringent requirements have run into obstacles, with many website operators showing reluctance to commit to anything more than what they currently do. This project offers the prospect of overcoming the limitations of current natural language privacy policies without imposing new requirements on website operators.<br\/><br\/>This frontier project builds on recent advances in natural language processing, privacy preference modeling, crowdsourcing, formal methods, and privacy interfaces to overcome this situation. It combines fundamental research with the development of scalable technologies to semi-automatically extract key privacy policy features from natural language website privacy policies and present these features to users in an easy-to-digest format that enables them to make more informed privacy decisions as they interact with different websites. Work in this project also involves the systematic collection and analysis of website privacy policies, looking for trends and deficiencies both in the wording and content of these policies across different sectors and using this analysis to inform ongoing public policy debates. An important part of this project is to work closely with stake holders in industry to enable the transfer of these technologies to industry for large-scale deployment.","title":"TWC SBE: Option: Frontier: Collaborative: Towards Effective Web Privacy Notice and Choice: A Multi-Disciplinary Prospective","awardID":"1330141","effectiveDate":"2013-09-01","expirationDate":"2017-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553722],"PO":["562974"]},"205717":{"abstract":"A significant amount of sensitive data communicated over wireless media makes wireless communication security an issue of paramount importance. The standard implemented solution to security over wireless networks is based on a modular approach in which transmission and encryption are carried out separately. State-of-the-art encryption algorithms are thus agnostic to the characteristics of the underlying network, but this separation approach is increasingly hard to justify as fast and reliable communications over wireless networks require more effective security architectures.<br\/><br\/>Several theoretical insights have been obtained from information theory in the past few years that suggest that fundamentally secure transmission in wireless interference networks is possible by jointly designing for reliability and security at the outset. This project takes a distinctive approach to accelerate the deployment of information-theoretically secure design of wireless networks by analyzing models with realistic assumptions, developing practical coding schemes and validating with a wireless test-bed. The project investigates two intertwined research tasks: (i) overcoming modeling assumptions, by providing security guarantees irrespective of adversaries' channels, modeling and countering adversaries that can manipulate channel conditions to their advantage, and removing idealized assumptions in implementation; (ii) providing explicit channel code designs that ensure strong secrecy and developing secure codes for multi-terminal wireless networks.<br\/><br\/>Additional activities related to the project include : (i) outreach to the computational security community towards integration of information-theoretic and computational security principles; (ii) dissemination of research results in various forms; (iii) a research exchange program between Georgia Tech and Penn State; (iv) a jointly taught graduate level course by the PIs that incorporates the research results of this project; and (v) recruitment of and mentorship for women in engineering and science. The research conducted in this project will facilitate the implementation of information-theoretic security principles on wireless systems and lead to novel cost-effective security techniques at the physical-layer.","title":"CIF: Small: Collaborative Research: Realizing the Vision of Information-Theoretic Security for Wireless Communications","awardID":"1320298","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["550793"],"PO":["564924"]},"205838":{"abstract":"When humans \"recognize\" things one of answers can always be \"unknown\" or \"that's new.\" Existing vision and machine learning research has made great progress but have done so in a closed set paradigm - which explicitly minimizes risk\/errors over what is known. As a computer vision system is moved toward real problems, it must face up to an open world. This project develops technologies for a new fundamental theory of \"open vision\" and corresponding set of tools that are explicitly designed to address open set recognition. At the heart of this research are three key concepts: 1) extending classical learning theory to include the risks of labeling open\/unknown spaces, and then building classifiers that balance empirical risk, smoothness and open space risk; 2) meta-recognition - bringing a statistically-grounded probabilistic interpretation to classifiers, improving their ability to produce \"confidence\" in their answers; 3) operational adaptation - developing new approaches to address, at operation\/run time, missing data or new data incorporating both open set and meta-recognition technologies. The work is also developing new approaches for open set evaluation, addressing problems in face recognition and visual object recognition as well as adapting classical machine learning datasets.<br\/><br\/>The open vision paradigm is embodied in open source tools that provide performance at or significantly advancing the state of the art while providing greater protection form unknown unknowns. Since most science is exploring the unknown, providing easy to use open source learning\/recognition tools design for both known and unknown data, the project have broad impact to many different applications.","title":"RI: Small: Open Vision - Tools for Open Set Computer Vision and Learning","awardID":"1320956","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[551070],"PO":["564316"]},"202208":{"abstract":"A framework will be developed to help scientists and engineers create brain-inspired, brain-sized networks that can carry out practical applications. Large-scale spiking neural networks, which follow the brain's architecture and activity, have been used to successfully model phenomena such as learning and memory, vision, auditory processing, neural oscillations, and many other important aspects of neural function. Additionally, spiking neural networks are particularly well suited to run on neuromorphic hardware, state of the art computers that emulate the brain?s structure and dynamics. These neuromorphic systems depend on the binary nature of spikes to lower communication bandwidth and energy consumption. Although significant progress has been made towards the specification and simulation of large-scale spiking neural networks on a variety of hardware platforms, many challenges remain before these neurobiologically inspired algorithms can be used in practical applications. While biology does provide increasingly abundant empirical data that can constrain these systems, many parameter values must be chosen manually by the designer to achieve appropriate neuronal dynamics, a task that is extremely tedious and often error-prone. To meet this challenge, an automated parametertuning framework will be developed that is capable of quickly and efficiently tuning large-scale spiking neural networks. The framework will leverage recent progress in evolutionary algorithms and optimization techniques for off-the-shelf graphics processing units (GPUs). The parameter search will be guided by the idea in neuroscience that biological networks adapt their responses to increase the amount of transmitted information, reduce redundancies, and span the stimulus space. This notion of efficient coding will guide the tuning process of the artificial spiking neural networks. Computer scientists and engineers will be able to use the resulting automated parameter-tuning framework to create brain inspired applications, such as vision and memory systems, on neuromorphic hardware. Moreover, the resulting framework will allow neuroscientists to more readily create models that better describe their empirical data and generate new quantitative hypotheses that can be tested in the laboratory.","title":"RI: Medium: Collaborative Research: BCSP: Automated Parameter Tuning of Large-Scale Spiking Neural Networks","awardID":"1302256","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[541913],"PO":["564318"]},"205739":{"abstract":"To achieve high data rates over wireless communication channels, feedback has customarily been used either to learn the channel state information, or to request the re-transmission of a failed reception. Most studies of feedback have focused on a one-way perspective, meaning data travels in one direction, and feedback -- typically assumed to be perfect -- in the other. This project extends beyond perfect one-way feedback to study noisy feedback in two-way time-varying networks. In modern two-way networks, feedback travels over the same channel as the data in the opposite direction, resulting in a tradeoff between improving rates in one direction and feeding back information to improve rates in the other. This tradeoff is affected by the rate of channel time-variation: no variation enhances the value of the channel state information feedback, whereas rapid variation renders such feedback to the transmitter outdated. If re-transmissions are needed, the re-transmitted messages may be judiciously combined using methods similar to network coding, or they may be collaboratively re-transmitted by several users, leading to additional and interrelated throughput tradeoffs.<br\/><br\/>This research will develop a foundation for the study of feedback in the context of two-way time-varying wireless Gaussian networks. The intellectual merit lies in proposing a unified framework that captures the key tradeoffs particular to two-way networks and the presence of different types of feedback, including quantized channel state information (Q-CSI), Automatic Repeat reQuest (ARQ), or extensions and combinations thereof. This research will rely on, and contribute to, communication and information theory. Given the ubiquity of two-way wireless communications, this research seeks to significantly increase the information rate of future wireless networks. The project comprises outreach components through mentoring students, integrating theory with practice, and extending the impact of the investigator's research and teaching to broader audiences.","title":"CIF: Small: Optimizing Two-way Communications with Feedback","awardID":"1320419","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[550840],"PO":["564924"]},"202803":{"abstract":"Vast quantities of digital information are being generated today on a daily basis through social networks, blogs, online communities, news sources, and mobile applications as well as our increasingly sensed surroundings. Tremendous insight can be gained by storing and making such ?Big Data? available for exploration in a wide variety of domains. Likely beneficiaries include business, social sciences, public health, national security, political science, public safety, medicine, and government policy. Researchers exploring these benefits need software to manage and analyze Big Data, and researchers investigating algorithms and programming models for Big Data can benefit tremendously by being provided with shared building blocks to use as a foundation for their efforts.<br\/><br\/>Over the past 3.5 years we have developed an initial version of AsterixDB, a powerful new Big Data Management System (BDMS) for scalably storing, managing, searching, and analyzing collections of Big Data using clusters of commodity computers. AsterixDB has a layered code base that consists of a scalable runtime platform (Hyracks), a model-neutral framework for parallel query compilation for Big Data (Algebricks), and the end-user-targeted AsterixDB BDMS itself. This NSF project is turning AsterixDB and its internal software stack into robust, supported, open-source resources for use by the Big Data applications and technology research communities. AsterixDB and its components will be helpful in training students in Computer Science and other data-related sciences, at universities everywhere, about Big Data technologies. This is critical for addressing the information explosion being brought to us courtesy of social media and the mobile Web.","title":"CI-ADDO-NEW: ASTERIX: A Community Software Platform for Big Data Research, Analysis, and Management","awardID":"1305253","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543515],"PO":["565136"]},"205707":{"abstract":"Distributed and cloud storage are emerging technologies that transform the way personal and enterprise data are managed today. This project proposes network coding as a modeling tool to study efficient design and operation of distributed and cloud storage systems. The principal aim of this project is to develop mathematical tools that build upon the submodularity of Shannon entropy to capture the structure of such engineering network coding problems, yielding strong yet easy-to-compute network coding bounds. Two specific focuses of this project are: 1) combinatorial characterization of subset entropy inequalities; and 2) generalized cut-set bounds for broadcast networks.<br\/><br\/>Our proposed study of network coding represents a significant deviation from the traditional view, where network coding is seen as a collection of pure mathematical problems. This deviation allows us to focus on the appropriate tools from combinatorics and information theory and draw strength from engineering intuitions. Both the theoretical development and the engineering results from this project are expected to impact how the field of network coding evolves over the next few years. The proposed research programs are hand-in-hand with our continuing efforts in training undergraduate and graduate students, broadening the participation of women and African Americans in engineering, and encouraging future scientists through outreach to high-school students.","title":"CIF: Small: Structured-Network Coding: Fundamental Limits via Submodular Function Optimization","awardID":"1320237","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["297790"],"PO":["564924"]},"205828":{"abstract":"Augmented reality (AR) systems, which are computer systems that enhance the viewing of physical objects in the world with computer data, are currently held back from widespread use for many real-world applications because of the unsolved human-computer interaction problem of how to accurately convey to a person how far away from that person a computer-generated object is intended to appear. People using AR systems routinely misjudge the depth of AR-presented objects. This is especially true for AR objects that should appear to be located behind opaque occluding surfaces; in this case AR should produce an \"x-ray vision\" perceptual experience that makes the occluding surface appear to become transparent. The perceptual phenomena that underlie this problem relate to (a) conflicting depth cues that naturally arise with AR technology, especially incorrect occlusion cues in optical \"x-ray vision\" AR, (b) conflicting findings from techniques that have been developed to measure depth perception within reaching distance, and (c) the role of practice and feedback in training to correct these depth misjudgments.<br\/><br\/>This project will evaluate AR depth representation methods and explain the underlying phenomena, with an emphasis on medical AR tasks and applications. The project will develop and evaluate a head-worn haploscope to allow researchers to study the depth cues of accommodation and vergence AR. The project will create and evaluate vergence-based methods for rendering AR information in depth; that is, techniques in which people can control the appearance of computer data inside of a physical object by rotating their eyes as is needed to look at near and far objects. The researchers on this project will collaborate with experts on the use of AR for medical applications to develop new vergence-based techniques for AR \"x-ray vision\" in the medical domain. <br\/><br\/>Broader Impacts: Vergence-based AR applications have the potential to improve health outcomes for a broad array of medical procedures, and also to improve human capabilities in task domains such as manufacturing and equipment maintenance. This project will hasten the timeframe for successfully developing and deploying such applications. Students working on this project will be trained in an interdisciplinary context that rigorously studies the intimate interplay between computer graphics and human perception. The interdisciplinary and human-centered aspects of the project will help to recruit students who might otherwise be less likely to gravitate to computer science.","title":"HCC: Small: Effective Augmented Reality Depth Representation Methods and Accuracy Evaluations Inspired by Medical Applications","awardID":"1320909","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[551043],"PO":["565362"]},"206807":{"abstract":"This frontier project tackles many of the fundamental research challenges necessary to provide trustworthy information systems for health and wellness, as sensitive information and health-related tasks are increasingly pushed into mobile devices and cloud-based services. The interdisciplinary research team includes expertise from computer science, business, behavioral health, health policy, and healthcare information technology to enable the creation of health & wellness systems that can be trusted by individual citizens to protect their privacy and can be trusted by health professionals to ensure data integrity and security. Although these problems are motivated by a nationally important application domain (health and wellness), the solutions have applications far beyond that domain.<br\/><br\/>This project is developing methods to authenticate clinical staff to tablet computers in a continuous and unobtrusive way, and to provide patients a usable way to control the information that mobile sensors collect about them. One of the goals is to manage security of healthcare devices in the home and in remote clinics, without adding burden on the homeowner or clinical staff; towards this end the investigators are developing methods to verify medical directives issued to remote devices. One approach being investigated is segmenting access to medical records from mobile devices to limit information exposure, and developing methods to audit behavior of this complex ecosystem of devices and systems. The investigators will design tools to handle genomic data in the cloud while enabling patient control over information, detect malware in medical devices through power analysis, and provide contextual information to those who use health data collected in the field.","title":"TWC: Frontier: Collaborative: Enabling Trustworthy Cybersystems for Health and Wellness","awardID":"1330142","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["554404",553725],"PO":["564388"]},"205718":{"abstract":"Networked wireless systems have significantly affected society, as cellular and WiFi networks are now ubiquitous in our daily life. An important class of networks is that of decentralized networks, which are characterized by a lack of fixed infrastructure, a multi-hop nature, and node mobility; such features provide significant flexibility, but also introduce a variety of design issues. A fundamental problem in decentralized networked systems is the coordination of activities of different nodes, where the availability of sufficient communication resources is critical in order for agents to cooperate and coordinate. This project takes a distinctive approach to develop a fundamental understanding of the interplay between communication and cooperation to achieve coordination and addresses the communication and coordination of probabilistic actions in networked multi-agent systems. This can be seen as an instance of network-based stochastic open loop control where a single communication message is sent to the agents to control their behavior in the absence of feedback. The project investigates two interrelated research tasks. First, the PIs wish to understand both the fundamental information-theoretic limits of coordination in small multi-terminal networks and the dependency of these limits on the communication network topology. The second research goal addresses the design of explicit coordination codes from error correction codes and to assess their performance under stringent delay and complexity constraints.<br\/><br\/>The proposed research has the potential to enable advances in applications where actions must be communicated to ensure a specific behavior. Examples are multi-agent systems for the exploration of an unknown topology, distributed surveillance applications, automatic traffic control applications, and load balancing in large computer networks and in power grids. Additional activities related to the project include the integration of research outcomes in courses at the undergraduate and graduate level and the mentorship of minorities and women in science and technology.","title":"CIF: Small: Collaborative Research: Coordination and Cooperation in Networked Multi-Agent Systems","awardID":"1320304","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[550793],"PO":["564924"]},"202209":{"abstract":"This proposal attacks waste in virtual memory. Virtual memory was invented in a time of scarcity (e.g., sub-MB memory) but is now deployed to a changed world with vast memory (e.g., GB to TB) where programs often don?t exploit virtual memories generality (e.g., web servers sized to not swap). At the same time, virtual memory can waste substantial execution time (on translation look aside buffer (TLB) misses) and power (accessing TLBs on each memory reference). Moreover, system extensions to support general-purpose graphics processing units <br\/>(GP-GPUs) and virtual machine (VMM) challenge virtual memory more. <br\/><br\/>The central hypothesis of this proposal is that memory is too virtual: most workloads do not need all of the benefits of virtual memory most of the time. To this end, we seek to de-virtualize virtual memory with multiple translation mechanisms that offer better performance with less functionality so that applications pay (performance, power\/energy, space, etc.) for the features they require instead of for page-based virtual memory?s full generality always. <br\/><br\/>This work reduces waste in computer systems to enable continued improvements in thier cost and performance. This improvement facilitates application innovations like higher performance for big-data workloads, including graph analysis, and environmental benefits from reduced power consumption. Beyond technical advices, this work develops and disseminates tools that leverage other NSF investments, such as the open-source ?gem5? computer-system simulation system.","title":"CSR: Medium: WasteNot: Streamlining Virtual Memory for Modern Systems","awardID":"1302260","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[541915,541916,"559530"],"PO":["565255"]},"206818":{"abstract":"INTELLECTUAL MERIT<br\/>Perhaps the most important structure for cellular life as we know it is the lipid bilayer. Lipid molecules, consisting of a water-soluble \"head\" and water-insoluble \"tails\", spontaneously assemble into sandwich-like bilayer membranes, which surround all living cells and further compartmentalize the cellular interiors of all eukaryotic organisms the domain of life to which plants, fungi, animals, and humans belong. The network of membranes in a typical eukaryotic cell is very complex and highly dynamic: small compartments bud off from certain membranes like bubbles, carrying cargo from one part of the cell to another, where they can fuse with yet other membranes, including the outer membrane of the cell. Bilayer fusion is therefore a ubiquitous biological process, tightly linked to the transport of material and information, and therefore it is exquisitely controlled by several classes of membrane-associated proteins. These proteins clearly perform work on the fusing membranes, but the intricate sequence of geometric and topological shape transformations they induce on the molecular scale are impossible to observe directly in experiment. In contrast, molecular simulation offers a window onto these details, but until now the relevant length- and time-scales have proven too big to observe even a single fusion event for a realistic system size. This project establishes a collaboration between two investigators with the aim to meet this challenge by combining recent advances in multiscale coarse-grained modeling with enhanced-sampling molecular simulation. Since this strategy allows incorporating important chemical detail while simultaneously representing large-scale membrane deformations, the investigators will be able to elucidate how molecular-level mechanisms drive fusion events across the relevant physiological length- and time-scales. The project proceeds through three phases, namely: (i) modeling the fusion of pristine bilayers with enhanced sampling, (ii) development of coarse-grained models of model fusogenic proteins, the SNARE system, and (iii) combining these two steps into a single methodology. The project will pursue many topics of energetic, morphological, and mechanistic relevance, in particular questions revolving around the so-called hemifusion intermediate state, for which the two outer bilayer leaflets have already fused but a membrane formed by the two inner leaflets still separates the two compartments.<br\/><br\/>BROADER IMPACTS<br\/>This project will impact many topics in the biological sciences due to the central importance of bilayer fusion in a variety of biological processes, including intracellular trafficking, viral entry, neurotransmitter release, fertilization, and more. Beyond the specific questions under study, the computational approach envisioned here takes early steps towards efficient simulation of more complicated multiple-protein\/multiple-membrane phenomena and will therefore benefit future studies of a wider class of molecular biological topics. To broaden applicability of the research outcomes, the simulation framework developed in this project will be made freely available with tutorials that will support efficient learning and facilitate the transformation of existing techniques and modules towards novel applications. This project establishes cross-disciplinary exchange between engineering and (bio)physics, fostering a stimulating interdisciplinary environment for the academic growth of students mentored in this project. It will further the transfer of theoretical and computational methodologies from engineering and physics into the life sciences and their increasingly quantitative set of problems. The ubiquity of bilayer fusion and its connection to a wide class of fascinating themes in biological physics, which is in itself an intriguing cross-disciplinary subject, also present excellent opportunities for the expertise developed in this project to feed outreach specifically tailored towards groups underrepresented in STEM fields for instance through classroom material, lecture demonstrations, and public talks and both investigators will implement such activities, building on both their experience and existing successful programs at their respective institutions.","title":"Collaborative Research: Multiscale molecular simulations of protein-mediated bilayer fusion","awardID":"1330205","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}}],"PIcoPI":[553748],"PO":["565043"]},"205729":{"abstract":"\"Big data\" have been growing in volume and diversity at an explosive rate, bringing enormous potential for transforming science and society. Driven by the desire to convert data into insights, analysis has become increasingly statistical, and there are more people than ever interested in analyzing big data. The rise of cloud computing in recent years offers a promising possibility for supporting big data analytics. However, it remains frustratingly difficult for many scientists and statisticians to use the cloud for any non-trivial statistical analysis of big data. <br\/><br\/>The first challenge is development---users need to code and think in low-level, platform-specific ways, and, in many cases, resort to extensive manual tuning to achieve acceptable performance. The second challenge is deployment---users are faced with a maddening array of choices, ranging from hardware provisioning (e.g., type and number of machines to request), software configuration (e.g., number of parallel execution slots per machine), to execution parameters and implementation alternatives.<br\/><br\/>This project aims to build Cumulon, an end-to-end solution for making statistical computing over big data easier and more efficient in the cloud. For development, users can think and code in a declarative fashion, without worrying about how to map data and computation onto specific hardware and software. For deployment, Cumulon presents users with best \"plans\" meeting their requirements, along with completion time and monetary cost to help them make decisions. A plan encodes choices of not only implementation alternatives and execution parameters, but also cluster resource and configuration parameters. This project develops effective cost modeling and efficient optimization techniques for the vast search space of possible plans. Cumulon addresses the challenges of uncertainty and extensibility (in terms of not only functionality but also optimizability). Cumulon also features a performance trace repository, which collects data from past deployments and uses them to improve cost modeling and optimization.<br\/><br\/>Cumulon aims to make statistical computing over big data easier and more cost-effective for a wide range of users including scientists and statisticians. Besides leveraging the cloud to provide on-demand, pay-as-you-go access to computing resources, Cumulon further simplifies development and deployment, reduces reliance on programming and tuning support, and accelerates data-driven discoveries. More than a one-shot solution, Cumulon is designed as a basis for an evolvable, open-source ecosystem that keeps up with advances in big-data analytics. Its repository of performance traces benefits the community in independent ways. <br\/><br\/>With the growing importance of quantitative, data-driven methods, Cumulon can impact many domains. The interdisciplinary team of PIs---from computer science, statistics, etc. ---is applying Cumulon to concrete applications in biomedical research and computational journalism. Through collaboration, the PIs seek to attract diverse talents, motivate them to work on problems with potential societal impacts, and help prepare them for the new challenges of big data.<br\/><br\/>For further information see the web site at: http:\/\/db.cs.duke.edu\/projects\/cumulon","title":"III: Small: Cumulon: Easy and Efficient Statistical Big-Data Analysis in the Cloud","awardID":"1320357","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550817,"508314",550819,550820],"PO":["563727"]},"198789":{"abstract":"The goal of this research project is to transform the way that scientists, engineers, and medical professionals make use of big data in order to further science, solve challenging problems in medical device engineering, and, ultimately, improve the nation's health. The approach makes use of big data from medical imaging, supercomputer simulation, and other sources. These exciting datasets are just now becoming available and have great potential to revolutionize science, engineering, and medicine. Unfortunately, scientists today cannot take full advantage of these data because they lack the appropriate tools to work with such large and complex datasets. This research addresses that problem by creating new computer graphics-based data visualization tools and coupling these with novel human-computer interfaces that specifically support data-driven engineering design. The new style of interactive visual computing that these tools provide can be especially powerful for supporting the human process of design. Thus, an anticipated major scientific result of the work is a new approach for supporting creative human-in-the-loop tasks when working with big data. The impact of this new approach is demonstrated through a series of applications to medical device design problems, such as designing improved breast biopsy devices and cardiac leads.<br\/><br\/>The long-term objectives of this project are to enable a paradigm-shifting future for simulation-based engineering with big data and to demonstrate this future through specific applications to challenging problems in medical device design. The approach is to couple the intense amounts of medical imaging, physical simulation, and other life sciences data that are generated today with new computational tools that not only support automated data analysis but also powerfully leverage our own human capabilities to see, touch, explore, and analyze. By adopting a human-centric approach to big data science, including significant new research in the areas of data visualization and human-computer interfaces, the work is expected to not only accelerate basic research and discovery but also make the results of big data science accessible to doctors, medical device engineers, and countless other creative thinkers who do not necessarily have a core background in computational methods. There are three main thrusts to the research: (1) Advancing medical device engineering through applications of new data-intensive design tools; (2) Developing a creative new as-direct-as-possible inverse method for simulation-based engineering; and (3) Coupling data-intensive virtual design with new tangible tools for working with big data.","title":"BIGDATA: Small: DA: Coupling Data-Intensive Modeling, Simulation, and Visualization with Human Facilities for Design: Applications to Next-Generation Medical Device Prototyping","awardID":"1251069","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8083","name":"Big Data Science &Engineering"}}],"PIcoPI":[533320,"538054","565145"],"PO":["565136"]},"202815":{"abstract":"The Speech Recognition Virtual Kitchen <br\/><br\/>Performing successful research on end-to-end speech processing problems requires the integration of many individual tools (e.g. for data cleaning, acoustic model training, language modeling, data analysis, real-time audio, decoding, parsing, synthesis, etc.). It is difficult for new researchers to get started in the field, simply because a typical lab environment consists of a hodgepodge of tools suited to a particular computing set-ups. This environment is hard to recreate, because few people are experts in the theory and practice of all these fields, and can debug and replicate experiments from scratch. <br\/><br\/>This research infrastructure project creates a \"kitchen\" environment based on Virtual Machines (VMs) to promote community sharing of research techniques, and provides solid reference systems as a tool for education, research, and evaluation. We liken VMs to a \"kitchen\" because they provide an environment into which one can install \"appliances\" (e.g., toolkits), \"recipes\" (scripts for creating state-of-the art systems using these tools), and \"ingredients\" (spoken language data). The kitchen even holds \"reference dishes\" in the form of complete experiments with baseline runs, log-files, etc., together with all that is needed to recreate and modify them. <br\/><br\/>The project is developing a community and repository by (a) building pilot VMs, (b) engaging the community in using and continuing to develop them on its own, and (c) evaluating the impact of providing VMs for education and research. We envision researchers as well as students downloading a VM, reproducing the baseline experiment, implementing changes, posting their results in the community, discussing with other users who have worked on the same VM, merging improvements back into the VM, which get re-distributed, and finally publishing easily reproducible results. Work with curriculum and project development will support the creation of engaging activities to specifically encourage students at undergraduate and graduate levels.","title":"CI-ADDO-NEW: Collaborative Research: The Speech Recognition Virtual Kitchen","awardID":"1305319","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543555],"PO":["565215"]},"205807":{"abstract":"Smartphones are becoming the predominant computing device throughout the world. Because of their lower cost relative to laptops and desktops, they have become an important device for bringing computing to disadvantaged people in both the developed and undeveloped worlds. Smartphones differ from traditional computing devices in that their primary power source is a battery, and they are expected to operate for a day or more on a single charge. This has forced the abandonment of traditional, automatic power management techniques and adoption of manual programmer management of power and power-consuming hardware, i.e., \"power-encumbered programming.\" This in turn has led to \"energy bugs,\" which cause apps to fail and to reduce battery life. The proposed work will develop tools to automatically detect energy bugs resulting from power-encumbered programming, increasing reliability and battery life, and bringing a better user experience to hundreds of millions of users.<br\/><br\/>To increase battery life, a smartphone OS constantly attempts to turn off a phone. Apps contain time-critical sections, i.e., program regions where the phone must stay on to function correctly. Power-encumbered programming requires the programmer use wakelocks to keep the phone hardware on during a time-critical section (to ensure an app functions correctly), but no longer (to minimize battery drain.) A correct app has a perfect correspondence between regions where wakelocks keep the phone awake and time-critical sections. The proposed research focuses on runtime and compile-time techniques to automatically identify time-critical sections and regions protected by wakelocks, and to use that information to automatically identify and prevent energy bugs in apps. Because smartphone apps are event-driven programs, techniques to analyze event-driven code will be developed, and these techniques should be broadly applicable to the compile-time analysis of general event-driven programs. Successful completion of this work will lead to automatic techniques for detecting an ameliorating energy bugs, which in turn will maximize the available battery life, increase the reliability of smartphones, and lead to significant advances in the compiler analysis of event-driven programs.","title":"SHF: Small: Detecting and Mitigating Smartphone Energy Bugs using Compiler and Runtime Analysis","awardID":"1320764","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550994,550995],"PO":["564388"]},"205818":{"abstract":"Multi-core computer architectures offer unprecedented performance benefits and present new challenges for the efficient synchronization of concurrent computations. Transactional memory is a prominent programming model that simplifies the synchronization of shared memory accesses, and avoids the complications of fine-grained locking mechanisms. A memory transaction represents a sequence of (read\/write) shared memory operations that need to be performed atomically by a computation thread. A transaction either commits, or aborts in case of conflicts with other transactions that concurrently access the same shared resources. This project aims to design, develop, and analyze contention managers that schedule efficiently memory transactions in a variety of systems. The goal is to provide schedulers that have provable formal performance guarantees and at the same time are practically efficient; thus, bridging the gap between theory and practice that currently appears in the literature.<br\/><br\/>The project considers a wide range of distributed systems, including tightly-coupled systems such as multi-core processors, and larger scale systems such as distributed networked processors. One of the main objectives is to provide scheduling algorithms which scale gracefully with the various system sizes and complexities. In order to fulfill this objective, this work proposes new analytical techniques to obtain good formal bounds with appropriate performance metrics, and also conducts experimental evaluations in real world workloads to obtain good performance in practical scenarios. The project establishes foundations for investigating the performance of transactional memory systems, and also provides analytical tools to the research community for exploring transactional memory to its full potential. The proposed research impacts the larger computing community because it affects the efficiency of distributed and parallel programs running on widely used distributed and multi-core systems.","title":"AF:Small: Foundations of Transactional Memory Scheduling","awardID":"1320835","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[551019],"PO":["565251"]},"210989":{"abstract":"The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing and computational linguistics. The ACL's annual conference is the major international conference in this field. This project is to subsidize travel, conference and housing expenses of students selected to participate in the ACL Student Research Workshop, which will take place during the main ACL conference on August 4-9, 2013 in Sofia, Bulgaria. The Student Research Workshop accepts papers in two categories: thesis\/research proposal and general research papers. The thesis\/research proposal can have only one author who must be a student. The research papers can have multiple authors, with the first author being a student. The workshop is organized and run by students.<br\/><br\/>The Student Research Workshop provides a valuable opportunity for the next generation of natural language processing researchers to enter the research community. It allows the students in the field to take an important step towards becoming professional computational linguists by receiving critical feedback on their work from experts outside of their dissertation committee, and by making contacts with other students and senior researchers in their field. The students who are involved in running and reviewing for the student workshop also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The ACL Student Research Workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing community.","title":"ACL 2013 Student Research Workshop","awardID":"1418815","effectiveDate":"2013-09-01","expirationDate":"2014-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[565214],"PO":[565215]},"210428":{"abstract":"The relational join is central to relational database processing, which is the dominant way data is processed today. The join also models problems in biological and social networks, coding theory, compressed sensing, machine learning, and constraint satisfaction. Recently,\u00a0\u00a0the investigators described the first ever worst-case optimal algorithm (the NPRR algorithm) for join queries.\u00a0\u00a0These new results open a line of new tools to attack a diverse set of fundamental problems related to the join. This project aims to further exploit the new algorithmic techniques developed for NPRR to address the following three classes of problems:<br\/><br\/>(1) Optimal Join algorithms. Developing algorithms that are instance optimal when the data are stored in either traditional database indexes or new indexing structures is a goal of this project. (2) Coping with and Leveraging Noise. This project will extend the latest work to handle and leverage both worst-case and statistical noise models, bridging to coding theory and compressed sensing.\u00a0\u00a0(3) Expressive Query Languages. The project will explore a series of extensions to join queries that will pave the way to overcome challenges in motif finding, search, databases with functional dependencies, and more powerful classes of queries and join operations.<br\/><br\/>If successful, the results of this grant will apply to a variety of pattern extraction problems in modern massive, dynamic, and noisy data sets, which have a wide range of applications in complex network analysis, coding theory, and compressive sensing.","title":"AF:III:Small:Collaborative Research: New Frontiers in Join Algorithms: Optimality, Noise, and Richer Languages","awardID":"1356918","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[563875],"PO":["565251"]},"205808":{"abstract":"The objective of this project is to undertake a comprehensive study of synergistic gains for multi-state communication networks and develop strategies that can exploit them. The project is organized into three complementary thrusts, focusing on 1) dynamic channels, 2) dynamic channel knowledge, and 3) mixed channel knowledge. The first thrust seeks ways to exploit channel variations starting with simple and optimal joint coding schemes over infinite horizons and leading, from a practical perspective, to efficient coding schemes over finite horizons, and, from a theoretical perspective, to alternative canonical models for information-theoretic insights into networks where the static setting has been intractable. The second thrust seeks ways to exploit alternating patterns of channel knowledge, either arising naturally out of channel dynamics or deliberately enforced through alternating feedback from different groups of users, to identify the most efficient forms of channel knowledge spread over time. The third thrust explores ways to exploit the simultaneous availability of distinct forms of channel knowledge, that have previously been studied only in isolation, and are therefore associated with coding schemes that are mutually incompatible.<br\/><br\/>With existing wireless networks struggling to keep up with the exploding data rate demands of an increasingly mobile and information-dependent society, it is imperative to uncover new opportunities for expanding the throughputs of next-generation systems. Any such effort must grapple with the enormous complexity of wireless networks, such as time-varying fading, dynamic network states, and the availability of many forms of network state information across users. The classical approach is to study the essential elements of these networks in isolation: decomposing a fundamentally multi-state network into its constituent states and studying each state individually. The motivation for this reduction typically comes from the conventional wisdom that 1) studying each element in isolation is easier than studying them together, and 2) understanding each element individually will lead to an understanding of their collective behavior. Surprisingly, recent work has shown that this wisdom can be misleading on both counts: multi-state networks are often not only more tractable, practical, and insightful, but also demonstrate significant synergistic gains that are not accessible through isolated studies of the constituent states. By shifting the focus from static to dynamic models, the proposed research will advance our understanding of information-theoretic limits of wireless interference networks where the intractability of the static setting has long stunted progress. Because joint coding across multiple states is often simpler, more insightful, and practical, it will bring the information-theoretic results closer to practice. A joint consideration of multiple states will provide the foundation for proper accounting of the overheads and benefits of channel knowledge. These research efforts are complemented by outreach efforts that include a mixture of traditional activities, such as tutorials at international conferences and summer schools, and novel ventures, such as the publication of an e-book on interference management.","title":"CIF: Small: Collaborative Research: Exploring Synergies of Multi-State Networks","awardID":"1320773","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[550997],"PO":["564924"]},"206919":{"abstract":"INTELLECTUAL MERIT: <br\/>Environmental factors change with reproducible characteristics. While changes in oxygen can happen within seconds to minutes, temperature changes occur at a much slower pace over hours to days. Regulatory networks in an organism are tuned through evolution to elicit a response that matches the characteristics of a given environmental factor (e.g. a rapid response to fast changing factors). The overarching hypothesis of this project is that the relative use of regulation at different stages of information processing (transcriptional, post-transcriptional, translational and post-translational) is dictated by the necessary characteristics of an environmental response. In order to test this hypothesis under varying environmental conditions, changes in synthesis and degradation rates will be measured for all transcripts and proteins in strains of Halobacterium salinarum. Building in this proof-of-concept, the research project will systematically investigate interplay across different levels of regulation in the context of specific environmental responses.<br\/><br\/>BROADER IMPACTS:<br\/>Resources: This research will generate fundamental principles of integration across different levels of regulation, and also generate innovative computational and experimental methodology that will be published as well-documented, open-source data and software.<br\/><br\/>Education: Rapid advances in interdisciplinary science are widening the gap in the way biology is taught and how it is practiced. An award winning high school (HS) education program in the lab is bridging this divide through inquiry-based teaching practices at the grassroots. This ten-year-old program is centered on summer internships for students of disadvantaged backgrounds who are encouraged to apply through numerous partnerships including programs for inner city youth and a UW-based program for advancement of women in science. Educational kits developed through this program have been aligned with state standards, iteratively improved through classroom pilots, and disseminated through professional teacher development to 14 states across the US. In addition to training postdoctoral fellows in systems biology, this project will also develop inquiry-driven, standards-based, HS educational materials. Developed together with local science educators, this novel curriculum module has been requested by teachers and will help students internalize the concepts and methods for using systems biology to understand cellular dynamics in the context of environmental change. These curricula will be aligned with the new Next Generation Science Standards and regularly assess its impact on student learning and career choices through a rigorous evaluation program.","title":"Interplay of Transcriptional, Translational Regulatory Mechanisms and Kinetics of an Environmental Response","awardID":"1330912","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0807","name":"Division of MOLECULAR AND CELLULAR BIOSCIE","abbr":"MCB"},"pgm":{"id":"8011","name":"Networks and Regulation"}}],"PIcoPI":[554018,554019,554020],"PO":["560660"]},"210605":{"abstract":"Future integrated circuits will contain tens, hundreds, or even thousand cores per chip. However, technology downscaling that can make this possible may also make the underlying hardware less reliable due to an increasing number of defects and wear out mechanisms. Therefore, one of the major problems facing the design of multiprocessor systems-on-chip is reliability. Because either the cores or the network-on-chip (used for communication between the cores) can become a reliability bottleneck for these systems, it is imperative that the reliability be addressed in a unified manner. To address the reliability challenge, this research develops a novel unified theoretical lifetime reliability modeling framework. This framework is based on efficient Monte Carlo methods to treat multiprocessor systems-on-chip as a combination of computation and communication units. The goal of this research is to develop new dynamic reliability management techniques based on dynamic voltage and frequency scaling and application remapping. Based on control theory concepts, these techniques proactively improve the lifetime reliability of multicore systems.<br\/><br\/>The proposed dynamic reliability management techniques enable the development of more reliable multiprocessor systems-on-chip, which have a dramatic impact on society via applications ranging from entertainment and gaming to bio-engineering, military and space. More broadly, the results of this project impact significantly the design of future integrated systems by advancing the understanding of the tradeoffs between reliability as a new design concern and power consumption, performance and area as traditional objectives.","title":"Lifetime Reliability of Systems-on-Chip: Unified Modeling and Dynamic Reliability Management","awardID":"1360439","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[564305],"PO":["366560"]},"202806":{"abstract":"This infrastructure proposal, acquiring two Kinova JACO Research arms to upgrade aging mobile manipulators, will enable fundamental research in interactive perception with non-rigid objects. Traditionally, robotics has focused upon rigid objects. The PI team proposes to enable study of the problem of sifting through a cluttered pile of non-rigid objects, many of which are occluding one another, and properly classifying and manipulating each object. The proposed work is not merely concerned with computing grasp points of rigid objects for which training examples are available for off-line learning; rather the focus is upon segmenting, grasping, and interacting with non-rigid objects for which models are difficult to construct. The aim is to merge ideas from interactive perception with those of the textile manipulationn to learn object properties over time and interact with objects in unstructured<br\/>environments.<br\/><br\/>Non-rigid objects are important in a number of highly-visible industries, such as textiles and automotive manufacturing, so broader impacts are numerous. Moreover, automated laundry folding will have a significant impact on service robots, where products such as vacuum cleaning robots and lawn mowing robots have already reached the commercial sector. The findings of the research will be made available to the research community via conference proceedings and journal publications, and the research will be integrated into existing courses to reach undergraduate and graduate students. Presentations to nearby schools to introduce disadvantaged and underrepresented high school students to STEM careers is included in the outreach plan.","title":"II-NEW: Robot Arms for Interactive Perception of Highly Non-Rigid Objects","awardID":"1305267","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["563342","563342",543525],"PO":["543539"]},"200639":{"abstract":"The Fukushima nuclear disaster was an awakening moment for robotics. Despite years of research and a wide range of robots, performing tasks like turning valves and attaching hoses, could not be done. If such tasks were executed in the earliest stages of the accident, much of the aftermath could have been averted. The fact that radiation levels were too high, forced rescue workers to wait and the disaster grew. Power plants and industrial sites are human-centered environments that are often characterized by doors, hatches, stairs and ladders that need to be entered or climbed. Additionally such environments feature valves, hoses and switches that need to be manipulated and connected. Robots however are not mature enough to efficiently function and effectively operate in such environments. <br\/><br\/>This 3-year IRES project partners American and Korean roboticists who respectively have expertise in the software and hardware design of humanoids. The objective is to collaboratively design humanoids to operate in human-centered environments. This is both important and urgent, addressing both national and international needs. Such service and mitigation by robots demands the coupled development of software and hardware where the robot?s form must follow its function. <br\/><br\/>The project?s intellectual merit stems from the formulation of a unified algorithmic framework. Current humanoids have the form to mimic a person?s motions but lack the software to function and operate in human-centered environments. Through the IRES, at least 12 American students will each spend 6-months in Korea working at the KAIST Hubo Lab. This partnership enables the American software developers to drive Korean hardware efforts and together design future versions of KAIST?s world-class humanoid. The net effect will be a robot that can efficiently function and effectively operate in human-centered environments like power plants and industry sites for collaborative tasks, humanitarian assistance and disaster response.<br\/>The project broadly impacts research, education, business development and K-12 curricula: humanoid design will advance research in legged locomotion, whole-body motion planning, perception and dexterous manipulation; the 6-month experience at KAIST educates US students in international research collaboration; the involvement of Korean robotics companies helps US students develop products; and the after-school programs with a Philadelphia High School will yield robotics-based teaching modules. The net effect is a set of broader impacts which are enabled by the IRES and sustainable well after the project completes.","title":"IRES: US-Korea Collaborative Software- and Hardware-Design of Humanoids for Real-World tasks in Human-Centered Environments","awardID":"1261170","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"7727","name":"IRES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["403839"],"PO":["399099"]},"210209":{"abstract":"The goal of this project is to lay the foundations for algorithmic research on self-organizing particle systems. Particle systems are physical systems of simple computational particles that can bond to other particles and that can use these bonds in order to communicate with neighboring particles and to move from one spot to another (non-occupied) spot. These particle systems are supposed to be able to self-organize in order to adapt to a desired shape without any central control. Self-organizing particle systems have many interesting applications like coating objects for monitoring and repair purposes and the formation of nano-scale devices for surgery and molecular-scale electronic structures. While there has been quite a lot of systems work in this area, especially in the context of modular self-reconfigurable robotic systems, only very little theoretical work has been done in this area so far. This project will prepare the ground for rigorous algorithmic research on self-organizing particle systems by proposing some basic models and solving some basic algorithmic problems in this area.<br\/><br\/>The main objectives of this one-year project are (i) to develop appropriate models for particle systems; (ii) to develop self-organizing algorithms for smart paint problems; and (iii) to better educate the Computer Science (CS) Theory\/Algorithms community on self-organizing particle systems. The proposed NSF sponsored workshop will foster collaboration of research in this interdisciplinary area.","title":"EAGER: Self-organizing particle systems: Models and algorithms","awardID":"1353089","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[563358],"PO":["565251"]},"202839":{"abstract":"Vast quantities of digital information are being generated today on a daily basis through social networks, blogs, online communities, news sources, and mobile applications as well as our increasingly sensed surroundings. Tremendous insight can be gained by storing and making such ?Big Data? available for exploration in a wide variety of domains. Likely beneficiaries include business, social sciences, public health, national security, political science, public safety, medicine, and government policy. Researchers exploring these benefits need software to manage and analyze Big Data, and researchers investigating algorithms and programming models for Big Data can benefit tremendously by being provided with shared building blocks to use as a foundation for their efforts.<br\/><br\/>Over the past 3.5 years we have developed an initial version of AsterixDB, a powerful new Big Data Management System (BDMS) for scalably storing, managing, searching, and analyzing collections of Big Data using clusters of commodity computers. AsterixDB has a layered code base that consists of a scalable runtime platform (Hyracks), a model-neutral framework for parallel query compilation for Big Data (Algebricks), and the end-user-targeted AsterixDB BDMS itself. This NSF project is turning AsterixDB and its internal software stack into robust, supported, open-source resources for use by the Big Data applications and technology research communities. AsterixDB and its components will be helpful in training students in Computer Science and other data-related sciences, at universities everywhere, about Big Data technologies. This is critical for addressing the information explosion being brought to us courtesy of social media and the mobile Web.","title":"CI-ADDO-NEW: ASTERIX: A Community Software Platform for Big Data Research, Analysis, and Management","awardID":"1305430","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["385755",543631],"PO":["565136"]},"197922":{"abstract":"The PI plans to apply the grant money towards a one-year visit to UC-Berkeley, where he intends to closely collaborate with Umesh Vazirani and his colleagues and students on two fundamental issues about ground states of local Hamiltonians - whether they satisfy the quantum analog of the PCP theorem, and whether they satisfy an area law. To gain the needed insight into the ground states of local Hamiltonians, the PI will employ his knowledge of classical PCP theory, a deep theory in computer science that he helped develop, and couple it with the expertise of the colleagues in Berkeley on quantum information theory.<br\/><br\/>A quantum analog of the PCP theorem would say something profound about quantum systems. Besides its implications in quantum information theory, it would also have more direct implication in physics, such as the realization, that quantum systems retain their exponential complexity at high temperature. In addition, an area law would explain the often localized behavior of wave functions for many-body systems.<br\/><br\/>If the PI and colleagues are successful in their efforts, this would be a development akin to the invention of classical PCP theory, that should attract a number of computational complexity theorists, and connect them with quantum computing. A deeper understanding of the behavior of ground states of local Hamiltonians should effect disciplines ranging from quantum chemistry to black hole physics.","title":"Ground States of Local Hamiltonians and Quantum PCPs","awardID":"1246641","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"8082","name":"CISE-MPS QIS Faculty Program"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":[531145],"PO":["565157"]},"204709":{"abstract":"How does web-based malware spread? We use the term web-based malware to describe malware that is distributed through websites, and malicious posts in social networks. We are in an arms race against web-based malware distributors; and as in any war, knowledge is power. The more we know about them, the better we can defend ourselves. Our goal is to understand the dissemination of web-based malware by creating \"MalScope\", a suite of methods and tools that uses cutting-edge approaches to build spatiotemporal models, generators and sampling techniques for malware dissemination. From a scientific point of view, this project brings together two disciplines: Data Mining and Network Security. The outcome is a suite of novel, sophisticated, and scalable techniques and models that will enhance our understanding of malware dissemination at a large scale. We use two types of web-based malware dissemination data: (1) user machines accessing dangerous sites and downloading web-based malware; and (2) Facebook users being exposed to malicious posts. We already have and will continue to obtain more data from our industry partners (e.g. Symantec's WINE project), open-access projects, or collect on our own (e.g MyPageKeeper).<br\/><br\/>The broader impact of our work is that it will enable the development of security solutions for end-users and industry. A 15-minute network outage costs a 200-employee company about $40K, while identity theft costs about $1,500 per person on average. By knowing the enemy better, security researchers and industry can more effectively stop the interconnected manifestations of Internet threats: identity theft, the creation of botnets, and DoS attacks. The PIs have a track record of technology transfer, with collaborators at industrial labs (Yahoo, MSR, Symantec, AT&T, IBM), national labs (LLNL, Sandia), open-source software (``Pegasus''), and spin-off startups (StopTheHacker). Educational impacts include developing a new course, providing publicly available educational material, and open-source software.","title":"TWC: Medium: Collaborative: Know Thy Enemy: Data Mining Meets Networks for Understanding Web-Based Malware Dissemination","awardID":"1314603","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":[548207],"PO":["565327"]},"205809":{"abstract":"A recent survey conducted among developers of the Apache, Eclipse, and<br\/>Mozilla projects showed that the ability to recreate field<br\/>failures--failures of the software that occur after deployment, on<br\/>user machines--is considered of fundamental importance when<br\/>investigating bug reports. Unfortunately, the information typically<br\/>contained in a bug report, such as memory dumps or call stacks, is<br\/>usually insufficient for recreating the problem. Even more advanced<br\/>approaches for gathering field data and help in-house debugging tend<br\/>to provide too little information to developers and to be therefore<br\/>ineffective.<br\/><br\/>The overall goal of this project is to improve the state of the art by<br\/>allowing, supporting, and partially automating, actual in-house<br\/>debugging of field failures. Specifically, this research will develop<br\/>novel techniques and tools that let developers reproduce, analyze, and<br\/>understand, in-house, failures observed in the field. Given a field<br\/>failure, the developed techniques will (1) collect a suitable set of<br\/>data about the failure on the user machine, (2) generate one or more<br\/>inputs that can be executed against the failing application and result<br\/>in a failure analogous to the one observed, and (3) provide hints on<br\/>the root causes of the failure and possible fixes for these causes. To<br\/>achieve this goal, the research will combine static and dynamic<br\/>program analysis techniques and leverage and extend techniques for<br\/>testing deployed software, input generation and anonymization, and<br\/>software debugging. If successful, this research will provide<br\/>unprecedented advantages to developers by allowing them to debug field<br\/>failures in the same way in which they debug in-house ones, which will<br\/>improve software quality and benefit all segments of society that<br\/>depend on software. Furthermore, the project will develop and make<br\/>available to the broader scientific community educational materials<br\/>that incorporate research findings, tools that implement the<br\/>techniques developed within the project, and samples of the software<br\/>benchmarks used in empirical evaluations. The availability of<br\/>curriculum materials, tools, infrastructure, and benchmarks will<br\/>advance knowledge, enable additional research in the area, and<br\/>ultimately further benefit society.","title":"SHF: Small: BugX: In-house Debugging of Field Failures to Improve Software Quality","awardID":"1320783","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550999],"PO":["564388"]},"202829":{"abstract":"This project will deploy an optically networked systems research infrastructure named BOLD that integrates high-performance, low-power, optical networking devices and programmable packet switches to enable transformative and inter-disciplinary research. It enables experimental exploration of different architectural design choices and it will potentially suggest new approaches to integrated hardware-software designs. At the hardware level, it enables research on novel optical network devices that can provide powerful communication capabilities, possibly tailored to the specific needs of big data applications, as well as experimentation with the device prototypes under real application traffic. BOLD enables a broad range of transformative big data-driven research. At the system software level, it enables research on storage, network, and application control software that are designed from the ground up to coordinate for optimal performance. At the application level, BOLD motivates research on how fundamental algorithms for big data applications should be designed to leverage the new network capabilities. BOLD has the unique potential to bridge the gap between nano-photonics researchers, networked systems researchers, and big data application researchers, creating inter-disciplinary research opportunities. Furthermore, because BOLD is designed to operate alongside Rice?s existing NSF-funded computing infrastructures BOLD can support big data experiments at a substantial scale.<br\/><br\/>Results from the inter-disciplinary research enabled by BOLD will lead to future big data processing system architectures that dramatically speed up a wide range of computational scientific discoveries. Because optical networking devices are unique in that they consume very little power, yet can support enormous data rates, BOLD will inspire a new class of high performance, high energy efficiency system architectures. Research enabled by BOLD could inform the design of future nation-wide networking infrastructures by showing how optical networks can be harnessed as shared ?cloud? resources. BOLD will serve as a platform for the training and education of numerous undergraduate and graduate students, including under-represented groups, in cutting edge big data-driven research.","title":"II-NEW: BOLD: Big Data and Optical Lightpaths-Driven Networked Systems Research Infrastructure","awardID":"1305379","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543597,"551076",543599,543600,"544864"],"PO":["564993"]},"198748":{"abstract":"The need for accurate and unforgeable identity recognition techniques has become an issue of increasing urgency. Biometric approaches such as iris recognition hold huge promise but still have significant limitations, including susceptibility to 'spoofing'. This project seeks to advance our knowledge of security and accuracy of multibiometric systems by inventing, evaluating, and applying innovative methods and tools to combine highly accurate static traits, such as iris patterns, with novel traits based on the dynamics of eye movements. The strategy is to use existing iris recognition hardware to combine three different biometrics approaches related to the eye: measurement of iris patterns, unique characteristics of the eye globe and its muscles, and the brain's strategies for guiding visual attention. This multimodal ocular biometrics approach has the potential to improve liveness detection and resistance to sophisticated counterfeiting techniques and coercion attacks, while improving identification accuracy. This research tackles important questions related to the individuality, variability, scalability, and longevity of these ocular traits, building a foundation for security and accuracy improvement when those traits are combined with iris recognition. This project aims to benefit efforts such as the Unique Identification project in India, which seeks to use biometric information of 1.2 billion individuals to fight fraud.<br\/><br\/>Educational activities include three initiatives: 1) creation of a strong outreach activity to K-12 students, 2) expansion of an interdisciplinary research-oriented educational program previously created by the PI for undergraduate and graduate students, and 3) mentoring and guidance to interest undergraduate students in scientific careers and encourage more students from diverse backgrounds to pursue graduate study.","title":"CAREER: Secure and Trustworthy Ocular Biometrics","awardID":"1250718","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[533217],"PO":["565327"]},"202809":{"abstract":"This infrastructure proposal supports the acquisition of a personal robot and a high-end multi-<br\/>GPU workstation to develop a new robot testbed for designing and evaluating the next generation of parallel<br\/>robot algorithms and open-source software systems on modern commodity computing platforms. The robot<br\/>platform will be based on a Meka M1 robot, a state-of-the-art robot with compliant, dextrous arms plus<br\/>camera and range sensors. This robot will be used to develop a new set of motion strategy and planning<br\/>algorithms and evaluate the capabilities of the robots for two driving applications: (1) assisting older adults<br\/>and people with disabilities with activities of daily living and (2) active tele-presence to give people in a<br\/>remote environment the ability to physically interact with people in the robot?s environment.<br\/><br\/>This robot testbed could lead to novel technologies for the development of personal<br\/>robots for assistance with tasks of daily living and active telepresence. These new capabilities could significantly<br\/>improve the quality of life for the elderly, people with disabilities, and other individuals. The development<br\/>of real-time motion strategies could also benefit other areas, such as surgical simulation, CAD\/CAM,<br\/>virtual prototyping, and virtual reality environments. The software libraries developed under this project<br\/>would be made widely available through the public-domain release for all research and educational activities<br\/>across science, engineering, and medical domains. Teh research team will also integrate research with education, reach out<br\/>to under-represented groups via programs such as the IBM-sponsored Girls? summer camps, actively involving<br\/>undergraduates in the proposed research, and organizing workshops on many-core computing for real-time<br\/>motion strategies. Finally, the team expects the new robotics curriculum enabled by the proposed equipment acquisition<br\/>to help increase enrollment in Computer Science.","title":"II-NEW: A Robot Testbed for Real-Time Motion Strategies and Autonomous Personal Assistants","awardID":"1305286","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["559716","550455","562687","562635",543535],"PO":["543539"]},"209190":{"abstract":"This INSPIRE award is partially funded by the Linguistics Program and the Perception, Action & Cognition Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral & Economic Sciences; by the Robust Intelligence Program in the Division of Information & Intelligent Systems in the Directorate for Computer & Information Science & Engineering; and by the Algorithmic Foundations Program in the Division of Computer and Network Systems in the Directorate for Computer & Information Science & Engineering.<br\/><br\/>Discrete, combinatorial systems of structured symbols permeate human cognition in domains such as language, motor control, complex action planning, learning, and higher-level vision. Nonetheless, the computational apparatus that the brain exploits is based on continuous, activation-based propagation of information through complex networks of neurons. A fundamental problem of the cognitive sciences is how to integrate gradient, continuous neural computation with the discrete combinatorial dimension of cognition. The solution to this puzzle will provide a deeper understanding of the mind and may also serve as the basis of a new generation of computing systems capable of authentically brain-like behavior.<br\/><br\/>Under the direction of Dr. Smolensky, the research team will develop an approach to this puzzle by exploring and testing the predictions of their theory of Gradient Symbolic Computation (GSC) in the domain of language. Their efforts will include the development of the formal, mathematical foundations of GSC. In parallel, the PIs will develop a framework for modeling Gradient Symbolic Processing. To that end, the PIs will use computational modeling and experimental psycholinguistic studies of phenomena that typify the morpho-phonological, syntactic, and semantic characteristics of language and language processing. <br\/><br\/>The broader impacts of the work include the potential to transform general computing for future approaches to computer design, to provide innovations in computer language processing, and to empower major advances in our understanding of human language, its impairment in disease, and its learning and remediation. The project also strongly engages STEM education. Undergraduate, graduate, and post-doctoral researchers will all play key roles in highly interdisciplinary STEM research integrating experimental, theoretical, and computational methods. The new type of computation created will provide an integrative framework for developing courses bridging computation theory, psychology, and linguistics. Pedagogical materials developed in these courses will be made publicly available to facilitate undergraduate and graduate program development at other institutions.","title":"INSPIRE Track 1: Gradient Symbolic Computation","awardID":"1344269","effectiveDate":"2013-09-15","expirationDate":"2018-02-28","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"1311","name":"LINGUISTICS"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7252","name":"PERCEPTION, ACTION & COGNITION"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":[560763,560764,560765,560766,560767],"PO":["564500"]},"196539":{"abstract":"This project's objective is to enable assertion-driven development and debugging of cyber-physical systems (CPS), in which required conditions are formalized as part of the design. In contrast with traditional uses of assertions in software engineering, CPS demand a tight coupling of the cyber with the physical, including in system validation. This project uses mathematical models of key physical attributes to guide creation of assertions, to identify inconsistent or infeasible assertions, and to localize potential causes for CPS failures. The goal is to produce methods and tools that use physical models to guide assertion-based verification of cyber-physical systems.<br\/><br\/>An assertion language is being developed that is founded in mathematical logic while providing the familiarity of commonly used programming languages. This foundation enables new automated debugging techniques for CPS. By leveraging models that encode laws of physics and an automated decision procedure, the techniques being developed help identify causes of CPS failures by distinguishing inconsistent or infeasible physical states from valid ones. This model-based approach incorporates means to assess these physical states using both probabilistic and non-probabilistic measures.<br\/><br\/>Two safety-critical applications guide the research and demonstrate the impact on the development of CPS: coordinated control of autonomous vehicles and monitoring and control of left-ventricular assist devices (LVADs). The focus on these safety-critical applications are motivational for recruiting and educating engineering students who have high expectations for how their lives should be enabled by computing advances. Further, this research advances methods needed to validate safe and effective CPS, promoting the public's confidence in their application to safety-critical systems.","title":"CPS: Synergy: Physically-Informed Assertions for CPS Development and Debugging","awardID":"1239498","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[527050,"550525",527052,527053],"PO":["561889"]},"198838":{"abstract":"Pronouns like 'he' and 'she' are among the most frequently used words in English. Yet neither a dictionary nor a grammar book will satisfy a learner of English as a second language who is trying to understand their full meaning and use. Pronouns often occur in contexts that contain more than one possible referent. Native speakers accumulate knowledge of patterns in the language that shift the interpretive preference one way or another. One such pattern involves the nature of the event being described: for example, a pronoun is more likely to refer to the doer of the last-mentioned action when that action is incomplete. Another pattern involves prosody, that is, what parts of a sentence receive emphasis. Native speakers deftly integrate such information to unconsciously anticipate how a conversation is going to continue, building expectations about who will be referred to next even before a name or pronoun is heard.<br\/><br\/>This project investigates how Japanese- and Korean-speaking learners of English interpret pronouns, asking specifically whether their interpretation is affected by patterns of event structure and prosody. Early evidence indicates that even when learners have advanced knowledge of English grammar, they may have a reduced ability to generate expectations during language comprehension, resulting in interpretation of pronouns different from that of native speakers. Five experiments will examine how native speakers versus learners continue written and spoken stories with pronouns, and how their eye fixations reveal, in a fraction of a second, which person in a story they think is most likely to be referred to next. <br\/><br\/>There are more second language users of English worldwide than there are native speakers as a first language. Understanding the nature of their language comprehension has the potential to improve strategies for successful communication, language teaching techniques, and our general understanding of how the human mind functions.","title":"Discourse and prosody in non-native speakers' reference resolution","awardID":"1251450","effectiveDate":"2013-09-01","expirationDate":"2017-02-28","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"1311","name":"LINGUISTICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[533444,533445],"PO":["564343"]},"209091":{"abstract":"This INSPIRE award is partially funded by Energy,Power and Adaptive Systems Program in the Division of Electrical, Communications and Cyber Systems in the Directorate For Engineering, Control Systems Program in the Division of Civil, Mechanical, and Manufacturing Innovations, in the Directorate for Engineering, Robust Intelligence Program in the Division of Information and Intelligent Systems in the Directorate for Computer and Information Science and Engineering, Applied Mathematics Program in the Division of Mathematical Sciences in the Directorate for Mathematical and Physical Sciences. <br\/><br\/>While we are witnessing a veritable revolution in bipedal robot construction technology, the science of balance, as embodied in the feedback control algorithms that allow these robots to stand, walk, and step over obstacles is still in its infancy. Current feedback algorithms demand laborious, time consuming trial-and-error tuning specific to each machine and each gait. If bipedal robots are ever to fulfill their potential roles from assisting the elderly or infirm in their homes to disaster and rescue response, their control software development process must be founded on better science.<br\/> Intellectual Merit: The project draws upon mechanics, MEMS devices, mathematics, and feedback systems to make a sea change in the practice of modeling and feedback control design in bipedal robots. The work seeks to advance scientific understanding of legged locomotion so that performance objectives can be tested mathematically for feasibility on reliable models and principled methods exist for model-based feedback controller design. The theoretical results of the project are being evaluated experimentally on a 3D bipedal robot at the University of Michigan.<br\/>Broader Impacts: Important medical applications of bipedal locomotion research include lower-limb prostheses and devices for the rehabilitation of walking and balance after injury. The PIs use their interdisciplinary work in mathematics, sensing, feedback systems, and robotics to promote STEM subjects. The researchers are actively involved in rapid dissemination including videos posted on YouTube channels, television programs, and weekly tour of their robotics laboratory.","title":"INSPIRE Track 1: The Mathematics of Balance in Mechanical Systems with Impacts, Unilateral Constraints, Underactuation and Hyper-sensing: Application to Agile bipedal Locomotion","awardID":"1343720","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1385","name":"SPECIAL STUDIES AND ANALYSES"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7607","name":"ENERGY,POWER,ADAPTIVE SYS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1266","name":"APPLIED MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1632","name":"CONTROL SYSTEMS"}}],"PIcoPI":[560444,560445],"PO":["564728"]},"207080":{"abstract":"This project will investigate techniques to power cellular network base stations from renewable sources (wind turbines and photovoltaic modules) in order to address cellular networks' carbon footprint and resiliency concerns. To better manage operation and increase resiliency, base stations are clustered in small groups in a microgrid configuration. In this configuration, cellular traffic management is integrated with the operation of local renewable electric power generators and energy storage, forming a single power and communication sustainable smart infrastructure system. Consequently, this project will develop an integrated management system by (a) researching algorithms that use next-day weather forecasts to predict the short-term available renewable energy; (b) studying algorithms to predict the cellular traffic-dependent power consumption; and (c) investigating algorithms that will integrate the information from (a) and (b) to shape the traffic at base stations to maximize operation powered from renewable sources while also maintaining communication quality within limits.<br\/><br\/>The outcomes from this project will help overcome barriers challenging the use of renewable energy and will increase the resiliency of cellular networks to allow continued operation under extreme events. Also, the outcomes from this project will be of value in other applications, such as demand-response and sustainable operation of data centers and other networks. This project will also implement a course, to be co-taught by the investigators through virtual meetings, that integrates technology in computing, communications and power systems to address the scarcity of graduating professionals with an understanding of the smart grid as a system integrating power, computational and communications notions.","title":"Cybersees: Type 1: Integrated Communications And Power Management Architecture For Supporting Cellular Base Stations Operation Using Renewable Energy In A Microgrid Configuration","awardID":"1331788","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[554449,554450],"PO":["564924"]},"208180":{"abstract":"CONTEXT <br\/>Energy investment and management decisions are increasingly facilitated and driven by information technology; these decisions have important impacts on the environmental, economic, and social characteristics of global energy systems--a collection of social-physical systems that weave together electric grids, fossil fuel production and utilization, regulation, development, markets, and diverse users and managers. Researchers from multiple disciplines are finding new ways of understanding and transforming energy systems as information technology and clean energy converge at an information-energy nexus. <br\/><br\/>INTELLECTUAL MERIT<br\/>This project will facilitate a new research community and build research infrastructure focused on understanding and shaping information-energy nexus research with a focus on electric power systems' critical role in human development and environmental performance. <br\/><br\/>The research community will bring together researchers who work in the developing and developed world. It will support a multidisciplinary and growing community of experts in energy and power systems analysis, engineering, social science approaches to energy development and public policy. The core community will be formed by colleagues at the University of California, Berkeley and Lawrence Berkeley National Laboratory with partners that include energy practitioners working in low-income communities in the US and in rural Kenya.<br\/><br\/>The research infrastructure will focus on data collection and sensor design efforts that enable the exchange of perspectives. It will result in three sets of innovations that lead to research findings at the information-energy nexus: 1) new data sources and tools for household and firm energy analysis that leverage ubiquitous information technology, 2) improved data structures for linking interdisciplinary energy development datasets and models in time, space, and across other factors, and 3) targeted data analysis tools for addressing the critical issues of privacy, ownership, and the value of emerging energy datasets. <br\/><br\/>BROADER IMPACTS <br\/>Environmental and Social Impacts: Investments in infrastructure and modern loads are enabling factors for increasing the efficiency of the energy system, a key benefit for society and the most important broader impact of this work. Affordable and sustainable provision of energy services is the goal in both the US and Africa. Beyond environmental benefits, superefficient appliances and modern electricity infrastructure hold the promise to deliver more reliable and widely accessible energy services to global end-users, including the global poor. <br\/><br\/>Technological Impacts: This research will contribute broadly to understanding new architectures for organizing social-technical systems and specifically to innovation in clean energy technology and policy. Both data collection and data sharing challenges currently hinder these efforts. <br\/><br\/>Educational Impacts: The concepts in this research will be incorporated into undergraduate education in engineering and public policy, and in environmental studies, and will be shared with a diverse international network of educators and scholars working on issues of energy access.","title":"BCC Information-Energy Nexus Research Network","awardID":"1338539","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}}],"PIcoPI":[557866],"PO":[557867]},"209061":{"abstract":"This INSPIRE award is partially funded by the Perception, Action, and Cognition Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral, and Economic Sciences and the Robust Intelligence Program in the Division of Information and Intelligent Systems in the Directorate of Computer and Information Science and Engineering.<br\/><br\/>This research will address and bridge two grand challenges: (1) To understand how action, perception, and social interaction were supported by the brain of the last common ancestor of macaque and human, complementing modeling elsewhere on great apes, and (2) To build on evolutionary insights to better understand how different parts of the human brain work together when we use language. Key entry points will be signed and spoken languages and the use of hand gestures (e.g., novel hand gestures by apes) to convey meaning. Going further, a particular focus will be on systems that link the brain's capacities to generate as well as recognize actions, and their interactions with other brain systems. <br\/><br\/>An international group of scientists in linguistics, primatology, neuroanatomy, neurophysiology, and neurocomputational modeling of motor, cognitive and language processes will pool data on the anatomy, physiology, behavior and communication of the various primate species. To support this extended collaboration, the researchers will build a novel online collaborative environment (\"Collaboratory Workspaces\") to test, make predictions, and challenge both the modeling and experimentation. This infrastructure may catalyze a new style of collaboration between modelers, experimentalists, and clinicians. <br\/><br\/>The research also has the potential to support modeling of the damage that results in the clinical disorders of apraxia and aphasia. Integration of models of vision, action and language is also important for creating robots that can flexibly and usefully interact with individual people and for \"neuromorphic architecture,\" in which a building's sensors and action systems adaptively adjust to the human inhabitants.","title":"INSPIRE Track 1: Action, Vision and Language, and their Brain Mechanisms in Evolutionary Relationship","awardID":"1343544","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7252","name":"PERCEPTION, ACTION & COGNITION"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":[560350],"PO":["563458"]},"209260":{"abstract":"GameSec is a research conference focused on analytical models based on information, communication, optimization, decision, control and game theories that are applied to diverse security topics. At the same time, the connection between theoretical models and real world security problems are emphasized to establish the important feedback loop between theory and practice. The conference helps to promote a \"science of security\" with a theoretical foundation that helps us understand security risk in a principled way.<br\/><br\/>This grant provides travel support to allow more students to attend GameSec 2013. Preferences for travel support are based on the credentials of the students, with a focus on attracting women and underrepresented minority students to support their participation in this event.","title":"GameSec 2013 Student Travel Support Request","awardID":"1344783","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561002,561003],"PO":["565327"]},"209150":{"abstract":"The use of multidisciplinary scientific evidence based practice (EBP) guidelines during hospitalization can assist low income and minority populations to regain and maintain health, thus reducing rehospitalization. However, EBP guidelines may not be equally effective across all populations. The national mandate for all health professionals to implement interoperable electronic health records (EHRs) by 2015 provides an opportunity for reuse of EHR data to address new research questions that explore patterns of patient characteristics and resources, EBP interventions (actions of health professionals in treatment of the patient), and improvement in health, e.g., the effectiveness of multi-disciplinary EBP during hospitalization and follow-up. This exploratory project is interested in groups of patients that (1) share a particular condition, e.g., severe sepsis and septic shock or patients with diabetes or diabetic complications, (2) were hospitalized for this condition or related complications, (3) were treated as an outpatient in a clinic during a succeeding period of time, and (4) have an identifiable outcome, e.g., rehospitalization, emergency room (ER) visits, death related to the condition, or condition under control without rehospitalization. These patients will be analyzed to understand the differences between patients with the same condition but different outcomes, with the goals of (1) evaluating whether EBP guidelines made a difference and (2) discovering interventions which lead to improvement in outcomes that may need to be added to EBP guidelines. Achievement of these goals requires the development of new analysis techniques for deriving insights into health outcomes from EHR data. <br\/><br\/>The algorithms and approaches developed in this project will advance health informatics by enabling researchers to extract, from the relatively raw and unorganized mass of data in an EHR, a higher level view of the evolution of the patient's health and treatment over time and use that information to analyze the differences between patients with favorable and unfavorable health outcomes. More specifically, new techniques and tools will be developed to (1) create patient and intervention profiles that summarize important characteristics of the patient, their environment, and their treatment, (2) find groups (clusters) and patterns in these profiles, and (3) use the profiles, clusters, and patterns to analyze the differences in outcomes between patients with a common health condition. <br\/><br\/>Achievement of these goals poses significant challenges. For instance, EHR data in its original form is, for research and analysis purposes, mostly in a relatively unorganized and low-level format, e.g., flowsheets, which contain primarily nursing documentation, have numerous rows of data representing patient assessments and results as well as laboratory and other diagnostic tests. This necessitates the extraction and summarization of information relevant for the task. Because time plays such an important role in this data, extracting useful features from the data across time is critical. However, the time series involved are often irregular. More generally, not all patients have the same set of information and information is not available at regular intervals. Furthermore, data may need to be viewed at multiple temporal resolutions, e.g., sudden increase in blood pressure versus gradual, but noisy increase over several years. Additional complexities arise from population substructure, differences in the types of features, incorporating knowledge of prior dependencies among features, and incomplete and missing data. This project will address these challenges. Success in these efforts will advance data mining in the areas of classification, clustering and pattern mining, as well as various types of temporal data analysis, including trend, change point, and anomaly detection. <br\/><br\/>The novel pattern mining approaches proposed for this project will help generate the insights biomedical researchers need to make progress in understanding a number of serious health problems and avoiding poor outcomes. Such progress is likely to advance personalized health care and thus has the potential to improve human health and reduce health care costs. Beyond health applications, this work has broad and immediate applications to any complex system for which creating a comprehensive predictive model for complex entities is often unrealistic, at least in the near future, and the best that can be hoped for is to identify specific patterns that provide insight into the current or future state of an entity or system with respect to certain specific conditions of interest. Examples include transportation and energy systems, business and government organizations, ecosystems, sophisticated machinery, and computer \/ network systems. The creation of the proposed frameworks and algorithms will also directly train a number of graduate and undergradu","title":"SCH: EXP: Discovering Patterns to Improve Health to Overcome Health Disparities","awardID":"1344135","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8018","name":"Smart Health & Wellbeing"}}],"PIcoPI":["563608","563607",560629,560630,560631],"PO":["565136"]},"208193":{"abstract":"A collaborative team of investigators from Michigan State University and the University of Idaho is working with instructors and students in interdisciplinary environmental science programs (IESPs) to conduct systematic, critical analysis of ethical concepts in the context of interdisciplinary environmental science. IESPs have multiplied in post-secondary and graduate curricula in response to pressing environmental problems. Ethical considerations are of immense importance in addressing environmental topics, because they often involve value judgments and the ethical issues involved are complex, multi-faceted, and high-stakes. Yet, ethics education materials for graduate IESP courses have not kept pace. This education and research project is developing an approach to graduate ethics education in IESPs that can better prepare the next generation of environmental scientists to solve complex problems. Materials development is being guided by two pedagogical commitments: (a) a constructivist paradigm that recognizes the social and collaborative nature of learning; and (b) a commitment to problem-driven models of education. A structured, yet flexible, framework is being created to enable development of course modules that can subsequently be adopted for use in classroom dialogue, building on results of work previously funded by NSF (the Toolbox Project - NSF award #0823058). The framework consists of materials for instructors and students that help them identify contexts in their IESP where ethical dilemmas generated by value judgments are likely to occur. The framework has two dimensions, corresponding to \"content\" and \"method\". The \"content\" concerns value judgments that relate to (a) the conduct of interdisciplinary scientific research, and (b) the impact of interdisciplinary environmental science on societal needs and policies. The \"method\" engages students, as teams or groups, in critical, conceptual analysis of value and policy aspects of their own practice as scientists and engineers. Students in courses using these modules will participate actively in identifying and assessing values-related challenges that are at the intersection of multiple environmental perspectives and at the intersection of science with policy. In addition to developing this framework, the project team is conducting a rigorous evaluation of its effectiveness for facilitating better consideration of ethical issues in IESP courses. Dissemination of the framework broadly through presentations and publications will strengthen interdisciplinary undergraduate and graduate education and will also provide professional development opportunities, ethics training, and mentoring for graduate research assistants.","title":"Collaborative Research: Values and Policy in Interdisciplinary Environmental Science: A Dialogue-based Framework for Ethics Education","awardID":"1338614","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7787","name":"EESE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7657","name":"ORGANISM-ENVIRO INTERACTIONS"}}],"PIcoPI":[557905,557906,557907,557908],"PO":["562265"]},"198809":{"abstract":"In the context of social networks, \"big data\" generally involves information on very large social systems whose elements of interest display complex dependence. State-of-the-art statistical models for such systems require the use of computationally expensive stochastic simulation techniques to capture this dependence; these techniques do not generally scale well to the large-population case. One potential solution to this problem is to focus detailed modeling efforts on smaller subpopulations (e.g., groups, communities, etc.) extracted from the larger system. While scalability of the subsystem models is less challenging in this case, one must have appropriate methods for sampling from large networks in such a manner as to permit principled inference, and modeling techniques that recognize the coupling between local subpopulations and the broader network in which they are embedded.<br\/><br\/>The PI will bridge the gap between expensive, highly detailed models and the limits of computability imposed by Big Data by combining expertise from machine learning and social network modeling within a unifying exponential family framework. The research will develop novel methods for the scalable measurement and analysis of large social networks, validating these techniques by deploying them in the context of dynamic data collection from online social networks. Specifically, the researchers will combine probabilistic graphical models and exponential family random graph models (ERGMs) to: (i) identify models with low computational requirements by exploiting limited-range dependence; (ii) develop machine learning techniques for identifying weakly coupled regimes in large networks to facilitate sampling and subgraph modeling; and (iii) develop integrated sampling and modeling strategies for inference from subgraphs of large networks that capture coupling to the structures in which they are embedded. This proposal investigates these questions in both the cross-sectional and dynamic contexts, for networks with and without vertex attributes. The sampling techniques created via this project will be deployed as an extension of a broader infrastructure for data collection in online social networks developed and maintained by one of the PIs, allowing for evaluation in a practical setting.<br\/><br\/>The methods developed via this research will allow for analysis of data relating to many problems of public interest, including epidemiological, security, and emergency management applications; data collection and analysis activities within the project will include applications in the natural hazard context, with the potential to inform policies that can save lives and property during disasters. The project will be integrated with graduate and undergraduate education, as well as postdoctoral mentoring. Tools developed via this project will be released as part of a widely used open-source toolkit for statistical network analysis (statnet), allowing widespread dissemination to researchers and practitioners in a range of fields.","title":"BIGDATA: Small: DA: DCM: Measurement and Learning in Large-Scale Social Networks","awardID":"1251267","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8083","name":"Big Data Science &Engineering"}}],"PIcoPI":["537799","534528"],"PO":["565136"]},"209140":{"abstract":"Machine learning has many important applications in science and industry. Modern machine learning uses a mix of insights from different disciplines, most notably artificial intelligence, statistics and optimization - areas that traditionally have not had much overlap. The participants will take part in tutorials given by experts from several different areas of machine learning - an opportunity that many students do not have at their home institutions. <br\/><br\/>The project supports student participation in a Machine Learning Summer School to be held at Carnegie Mellon University in Pittsburgh during June 16-27, 2014. The summer school emphasizes big data and scalable machine learning algorithms. It features speakers from academia and industry with established experience in large scale data analysis. Approximately 50 graduate students from around the U.S. are expected to participate in person. The content will be streamed live as well as archived online making it possible for a much larger number of students from academia and industry to benefit from the summer school. In addition to in-depth tutorial lectures given by leading researchers, the summer school will include exercise sessions that provide the participants hands-on experience with large scale data (using the Kaggle platform and Amazon cloud services). <br\/><br\/>Broader Impact: The Summer School provides state-of-the art knowledge of machine learning and big data analytics to graduate students - an opportunity that many students do not have at their home institutions. Thus, it would not only help train an new generation of machine learning and big data analytics researchers, but also reduce the barrier to entry of researchers who want to apply state-of-the-art machine learning techniques to applications in areas such as social network analytics, bioinformatics etc.","title":"Machine Learning Summer School Pittsburgh 2014","awardID":"1344017","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[560596,560597],"PO":["565136"]},"208172":{"abstract":"SMA-1338507<br\/>Susan F. Martin<br\/>Sidney Berkowitz<br\/>Jeffrey R. Collman<br\/>Lisa Singh<br\/>Georgetown University<br\/><br\/>This project will assemble a multidisciplinary community of scholars and practitioners to create community and capacity for a large-scale, data intensive early warning system for detecting forced migration or population displacement. The system will be based on Raptor, a vast unstructured archive at Georgetown University of over 600 million publicly available open-source media articles. Mobilizing vast amounts of open source data will enable discovery of patterns of acute events (triggers) and\/or slow-onset processes (trends) in the context of pre-existing stressors. Developing an effective early warning system of population displacement requires collaboration and shared learning between subject matter experts who understand the factors that contribute to forced migration at the macro, meso and micro levels and technical experts who understand how to collect, store, mine and analyze masses of data derived from international, national and local sources. Bringing together social scientists and computer scientists will expose social scientists to new modeling approaches for analyzing their subject matter. At the same time, computer scientists will exploit domain expertise in the social sciences. This expertise will provide insight for the development of beyond state of the art data mining of very large open source data bases for event detection, sequential mining and change detection. Participants will include scholars from many universities and practitioners from relief and migration-oriented organizations. The results of this endeavor will be: 1) methods and algorithms that can serve as a blueprint for integrating computational models into new avenues of social science research, and 2) a community and a plan for improving early warning of forced population displacement through human-computer analysis that address two key societal concerns, population changes and social disparities.<br\/><br\/>Broader Impact<br\/>Effective early warning of forced population displacement will help in state- and organization-level planning and preparation for such movements, as well as directly aid potential refugees and displaced persons before, during and after their exodus. Planning can lead to action to try to avert mass displacement, preferably by tackling the triggering events and stressors and providing options to those who would otherwise be forced to relocate (e.g., getting food to villages at risk of famine). Earlier warning may also help divert forced migrants from risky modes of movement (e.g., via non-seaworthy boats or across landmine infested borders). Early warning of displacement would enable the pre-positioning of shelter, food, medicines and other supplies in areas that are likely to receive large numbers of refugees and displaced persons.","title":"BCC Forecasting the Break: Building Community and Capacity for Large-scale, Data-Intensive Research in Forced Migration Studies","awardID":"1338507","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}}],"PIcoPI":[557842,557843,557844,557845],"PO":["561290"]},"209162":{"abstract":"This INSPIRE award is partially funded by the Information Integration and Informatics Program in the Division of Information and Intelligent Systems in the Directorate for Computer and Information Science and Engineering and the Solid State and Materials Chemistry Program in the Division of Materials Research and the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.<br\/><br\/>The past two decades have seen a rapid development in experimental high-throughput experimentation (HTE) methodologies that would be extremely valuable for (i) the discovery of new applied materials with high complexity and (ii) the generation of deep understanding of structure\/function, structure\/activity and structure\/performance relationships. Especially high photon flux X-ray techniques have enormous transformative potential in materials discovery. The research team leverages the data being collected by the Cornell High Energy Synchrotron Source (CHESS) and at Caltechs Joint Center for Artificial Photosynthesis (JCAP). While high-throughput inorganic library synthesis is relatively well-established, high-throughput structure determination, which is at the heart of the proposed research, is in its infancy. X-ray diffraction is well-suited for rapidly collecting information on the atomic arrangements in an inorganic sample, but the data do not immediately reveal a crystal structure. The development of data analysis, data mining and interpretation methodologies has not kept pace with the development of experimental capability. Consequently, data acquired in a week can take many months of traditional analysis by researchers. Automation and machine-intelligent processing of the data are absolutely necessary to maximise the impact of complex multidimensional datasets. <br\/><br\/>This project addresses this state of affairs head-on; It investigates computational techniques that allow dealing with the multiparameter space associated with HTE structure determination of materials libraries, through constraint guided search adn optimization, statistical machine learning, and inference techniques in combination with direct human input into the process. Anticipated advances include new probabilistic methods and computational discovery tools that integrate soft and hard constraints that capture the complex background knowledge from the underlying physics and chemistry of materials with insights gained from high throughput data analytics and machine learning. If the project succeeds in achieving the anticipated enormous efficiency gains in complex structure determination, it could have have a transformative impact on materials discovery and complex solid state chemistry and physics. <br\/><br\/>The ability to reduce complex materials dicovery and optimization from timeframes of months or years to hours or days could lead to a paradigm shift in the development of products benefiting society, with technological advances as well as commercial impact on energy, sustainability, health and quality of life. The planned free dissemination of data sets and computational tools to the larger scientific community is likely to enhance the broader impacts of the project. The project facilitates increased interdisciplinary interactions between computer scientists and material scientists at Cornell University and offer enhanced opportunities for training of a new generation of researchers at the interface between the two disciplines.","title":"INSPIRE Track 1: UDiscoverIt: Integrating Expert Knowledge, Constraint-Based Reasoning and Learning to Accelerate Materials Discovery","awardID":"1344201","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[560668,560669,560670,560671],"PO":["565136"]},"208073":{"abstract":"Proposal #: 13-38102<br\/>PI(s): Yu, Shucheng<br\/> Chiang, Chia-Chu; Xie, Mengjun; Yoshigoe, Kenji<br\/>Institution: University of Arkansas-Little Rock<br\/>Title: MRI\/Acq.: Cloud Computing Infrastructure for Research and Education <br\/>Project Proposed:<br\/>This project, acquiring private cloud instrumentation, aims to conduct six research projects spanning areas of social networks, secure cloud data system, cryptography, biomedical computing, and data stream analysis. The instrumentation will enable the following projects:<br\/>- Analysis of Sybil Attacks in Large-Scale Social Networks,<br\/>- Secure Data Access Control for Cloud Storage Systems,<br\/>- Study of Cache Pollution Attacks in Named-Data Networking,<br\/>- Development of a Cloud-Based Graph Theoretical Analysis Framework for Human Brain Connectomes,<br\/>- Parallel Processing of Fully Homomorphic Encryption, and<br\/>- Performance Real-Time Stream Analysis on the Cloud.<br\/>This private cloud, serving three universities (U AR-Little Rock (UALR), U AR-Pine Bluff (UAPB), and <br\/>U Central AR (UAC)), will be the first in the state dedicated to research and teaching.<br\/>Broader Impacts: <br\/>The instrumentation enriches the research and educational activities within the universities and overall state. Hence, new courses can be developed and implemented. Moreover, the cloud infrastructure provides unique opportunities for minority students that will then have access to cutting-edge high-performance ubiquitous computing resources. The computing infrastructure creates many education and training opportunities for the students through the process of its installation, operation, and maintenance. A Summer Undergraduate Program of Entrepreneurship and Research (SUPER) program and the ACM club at the university will offer training within and beyond the institution. Also, an undergraduate summer research program on cloud computing will be established to expand the opportunities throughout this EPSCoR jurisdiction.","title":"MRI: Acquisition of a Cloud Computing Infrastructure for Research and Education","awardID":"1338102","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["564179","562075",557523,"564180"],"PO":["557609"]},"205280":{"abstract":"Imagine that you would like to make use of the powerful information processing capability of quantum devices but you are not yet equipped to implement the necessary quantum operations. You may purchase quantum devices that come with a classical interface. However, you may not necessarily trust the manufacturer's claim on the inner-working of the devices. Even if you do, the devices may be corrupted for many reasons. An important question thus arises: what can we do through classically interacting with quantum devices that may deviate from the ideal specification? <br\/><br\/>To address this question, the researchers are developing a theory of untrusted quantum devices through this project. Such a theory would be supported by two pillars:<br\/>(a) Robust self-testing of quantum states and quantum operations. That is, methods for pinning down the quantum device (its state and measurements) through classical interactions.<br\/>(b) General principles for proving cryptographic properties of untrusted quantum devices.<br\/><br\/>A successful theory will identify the power and limitations of classical interactions with quantum devices. In particular, it may provide new tools for designing and analyzing the security of cryptographic protocols that rely on untrusted quantum devices. This will bring the tremendous potential of quantum information processing closer to reality. The project also serves as a training program for both undergraduate and graduate students for cutting-edge research on quantum information processing.","title":"AF: Small: Theory and Applications of Untrusted Quantum Devices","awardID":"1318070","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":[549690,549691],"PO":["565157"]},"204070":{"abstract":"The goal of this project is to combine computational modeling with behavioral and neuroimaging studies to characterize the mechanisms of navigational abilities in humans and understand how they decline with age. The PIs will focus on an important navigational circuit in mammals, which consists of the hippocampus and associated areas, and includes grid cells of the entorhinal cortex as well as place cells. Place cells have highly location-specific responses, turning on at one location in an environment and firing little elsewhere; grid cells by contrast fire at multiple locations within an environment, with periodically separated activity blobs in a striking triangular lattice pattern. Studies in rodents have detailed the properties of grid and place cells, and led to neural network models whose additional predictions have often been borne out by single-unit neuron recordings. However, much less is known about grid cells and place cells in humans, and the nature of interactions between different parts of the navigation circuit remains unclear, in rodents and humans. <br\/><br\/>In this project, the PIs bring to bear virtual-reality-based behavioral experiments, ultra-high-resolution fMRI recordings during virtual navigation, and neural network modeling, to better understand the circuit for spatial navigation in humans. The PIs plan a three-pronged approach to these questions. The first is to characterize phenomenologically the characteristic errors made by humans, through navigation environments with and without accurate external landmark cues, and under other externally varying conditions, in aged and non-aged subjects. The second is to employ neural network models of grid cells, to model the network parameters that could give rise to the observed deficits, and in turn test the predictions of these models with the neuroimaging experiments. The experimental setup will permit systematic variation in the fidelity of external sensory cues, to probe the relative contributions of the complementary computations of dead-reckoning (path integration) versus landmark-based navigation, and uncover their potential neural substrates in humans. The results will help to develop models of how parallel streams of spatial information are combined and processed across brain areas to aid in navigation. The third component is to develop accurate algorithms for extracting spatial information from high-resolution fMRI data from regions and sub-regions of the entorhinal-hippocampal complex. The aim is to map the distribution of location information across areas and learn where it is most compromised in old age.<br\/><br\/>This award is being co-funded by NSF's Office of the Director, International Science and Engineering. A companion project is being funded by the German Ministry of Education and Research (BMBF).","title":"US-German Collaboration: Toward a quantitative understanding of navigational deficits in aging humans","awardID":"1311213","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"5345","name":"BIOMEDICAL ENGINEERING"}}],"PIcoPI":[546555],"PO":["564318"]},"209691":{"abstract":"Bootstrapping security among devices is a crucial step for deploying wireless ad hoc networks in sensitive and mission-critical applications. Most existing solutions to security bootstrapping assume the existence of common resources such as additional hardware, auxiliary secure out-of-band (OOB) channels, or pre-existed trust among devices. In pervasive environments, however, these assumptions are not always practical. Given billions of available wireless devices nowadays, devices can be heterogeneous and highly diversified in hardware capabilities or user interfaces, and have limited common resources. This project conducts an exploratory study on bootstrapping security in wireless ad hoc networks without assuming any additional hardware, OOB channel or pre-existed trust. To this end, only physical-layer channel characteristics, which are automatically available to any wireless device, are used for security bootstrapping. Contextual factors such as device mobility and location are studied regarding their impacts on channel characteristics; algorithms for authenticated secret key establishment based on the contextual channel characteristics are designed; the effectiveness and efficiency of the security bootstrapping algorithms and techniques are evaluated through both theoretical analysis and experiments on a real testbed.<br\/><br\/>The success of this project provides a guideline on feasibility of utilizing channel characteristics for security bootstrapping in different ad hoc network contexts; the algorithms generated by this project provide extremely lightweight security bootstrapping solution that can be deployed on any commercial-off-the-shelf (COTS) devices in pervasive environments. The research results of this project will be integrated into education, and disseminated in the forms of tutorials, talks, publications, and online software toolkit.","title":"EAGER: Utilizing Contextual Channel Characteristics for Secure Communications in Wireless Ad Hoc Networks","awardID":"1347552","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[562075],"PO":["557315"]},"208294":{"abstract":"This award is to support student participation in the annual ACM Symposium on Computer and Communication Security conference (CCS), to be held November 4-8, 2013.<br\/><br\/>The ACM Conference on Computer and Communications Security (CCS) is the flagship annual conference of the Special Interest Group on Security, Audit and Control (SIGSAC) of the Association for Computing Machinery (ACM). The conference brings together information security researchers, practitioners, developers, and users from all over the world to explore cutting-edge ideas and results. It provides an environment to conduct intellectual discussions. From its inception, CCS has established itself as a high standard research conference in its area.","title":"Supporting Student Travel to 2013 ACM Conference on Computer and Communications Security (CCS 2013)","awardID":"1339013","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["446168"],"PO":["564223"]},"209031":{"abstract":"This project designs and deploys a multi-disciplinary framework to model spatial, temporal and social dynamics of cyber criminals. The framework fuses theories in both computer science and criminology. Specifically, project objectives are a) Apply and validate existing theories in the realm of general criminology (in particular Akers? social learning theory and Gottfredson and Hirschi?s general theory of crime) to study cyber crimes; b) Derive novel Internet usage features as fingerprints for cyber crimes; c) Design classification algorithms (based on multi-fractal analysis and petri-net designs) to subsequently model multiple dynamics of cyber criminals by integrating theoretical and practical outcomes from the above two objectives; and d) Extensively test and validate project outcomes. The core novelty of this project is in using real Internet data from subjects (initially a cyber savvy college sample) that is collected continuously, unobtrusively, while still preserving a high degree of privacy.<br\/><br\/><br\/>Outcomes of this project will have far reaching impacts. It lays a foundation for fusing expertise in social sciences (specifically criminology) and cyber security, as a result of which existing theories in general criminology can be empirically tested for practical validity in studying cyber crimes. The identification of unique Internet fingerprints associating with cyber crimes will provide new insights into human centered aspects of cyber crimes, which is lacking today. The classification algorithms designed will provide cyber defenders with new tools to combat cyber crimes from multiple perspectives including prevention, detection, forensic investigations and prosecution.","title":"EAGER: Collaborative: A Multi-Disciplinary Framework for Modeling Spatial, Temporal and Social Dynamics of Cyber Criminals","awardID":"1343453","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[560279],"PO":["565327"]},"209284":{"abstract":"Understanding and responding to climate change is a key scientific and societal challenge of the 21st century. Recent advances in satellites and environmental sensors have made it possible to gather vast quantities of data concerning temperature, sea ice, sea level, rainfall, vegetation, etc. There is an urgent need for scientific and technical expertise for developing and effectively applying computational tools that can make use of such data to build increasingly accurate predictive models that offer insights to inform our understanding of, and response to, climate change. <br\/><br\/>This award supports a series of three workshops on Climate Informatics, an emerging discipline at the intersection of climate sciences and data sciences. The workshops, through a combination of tutorials that introduce climate scientists and data scientists to each other's disciplines, invited talks by established researchers in climate informatics, breakout sessions and for identifying research challenges and opportunities for interdisciplinary collaborations, serve a critically important role in building a vibrant Climate Informatics research community. The award provides support for the participation of approximately 60 to graduate students in each of the three workshops. The workshops contribute to the education and interdisciplinary training of a diverse workforce in STEM areas that span Climate Sciences and Data Sciences (including Machine Learning, Data Mining, Inference, Decision Making) as well as efforts to broaden the participation of women and other underrepresented groups in STEM research and education. Additional information about the Climate Informatics Workshop Series can be found at: https:\/\/sites.google.com\/site\/1stclimateinformatics\/","title":"Climate Informatics Workshop","awardID":"1345052","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"8074","name":"EarthCube"}}],"PIcoPI":[561068],"PO":["565136"]},"208195":{"abstract":"A collaborative team of investigators from Michigan State University and the University of Idaho is working with instructors and students in interdisciplinary environmental science programs (IESPs) to conduct systematic, critical analysis of ethical concepts in the context of interdisciplinary environmental science. IESPs have multiplied in post-secondary and graduate curricula in response to pressing environmental problems. Ethical considerations are of immense importance in addressing environmental topics, because they often involve value judgments and the ethical issues involved are complex, multi-faceted, and high-stakes. Yet, ethics education materials for graduate IESP courses have not kept pace. This education and research project is developing an approach to graduate ethics education in IESPs that can better prepare the next generation of environmental scientists to solve complex problems. Materials development is being guided by two pedagogical commitments: (a) a constructivist paradigm that recognizes the social and collaborative nature of learning; and (b) a commitment to problem-driven models of education. A structured, yet flexible, framework is being created to enable development of course modules that can subsequently be adopted for use in classroom dialogue, building on results of work previously funded by NSF (the Toolbox Project - NSF award #0823058). The framework consists of materials for instructors and students that help them identify contexts in their IESP where ethical dilemmas generated by value judgments are likely to occur. The framework has two dimensions, corresponding to \"content\" and \"method\". The \"content\" concerns value judgments that relate to (a) the conduct of interdisciplinary scientific research, and (b) the impact of interdisciplinary environmental science on societal needs and policies. The \"method\" engages students, as teams or groups, in critical, conceptual analysis of value and policy aspects of their own practice as scientists and engineers. Students in courses using these modules will participate actively in identifying and assessing values-related challenges that are at the intersection of multiple environmental perspectives and at the intersection of science with policy. In addition to developing this framework, the project team is conducting a rigorous evaluation of its effectiveness for facilitating better consideration of ethical issues in IESP courses. Dissemination of the framework broadly through presentations and publications will strengthen interdisciplinary undergraduate and graduate education and will also provide professional development opportunities, ethics training, and mentoring for graduate research assistants.","title":"Collaborative Research: Values and Policy in Interdisciplinary Environmental Science: A Dialogue-based Framework for Ethics Education","awardID":"1338626","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7787","name":"EESE"}}],"PIcoPI":[557912,557913,557914],"PO":["562265"]},"209053":{"abstract":"Armed conflict and natural disasters are increasingly driving people from their homes. This displacement--being torn from physical possessions, familiar surroundings and community--generates acute demands for connectivity, communication and connectedness. The displaced must navigate their connectivity and information needs while experiencing two types of mobility. First, they face macro-mobility as they leave their homes and communities in search of safety and shelter. Next, they must contend with resituated micro-mobility. As they work to establish and maintain ties within their new location, whether a refugee camp or temporary shelter.<br\/><br\/>These mobilities, as well as the physical environment of the displaced, create challenges for successful implementation of wireless networks, applications and devices. Moreover, displacement also creates extreme resource scarcity, dependency, geographic change and conflicts between individuals and organizations, further challenging information technology implementation and use. This situation demand robust and rapidly deployable network technologies, together with the applications to support victims in their time of need. It also calls for novel social science theories, providing a deeper understanding of the information requirements of the displaced along with informing technical development.<br\/><br\/>This award supports a workshop to generate relevant novel technologies and theories, and to define a research agenda derived from insights from a variety of scientific disciplines, namely computer, information, organization and geographic information sciences, as well as communication and refugee studies. The workshop will also include practitioners, particularly those working with refugees and displaced populations. Outcomes of the workshop will include a research agenda, with implications for academic research in wireless network architectures, information service design, geospatial visualization, community self-organization and international organization governance. An additional outcome will include publicly available multimedia use-case studies, designed to focus discussions during the workshop but also for subsequent educational use.<br\/><br\/>To assemble this diverse group, the workshop will be held in advance of the ACM DEV and ICTD2013 conferences at the University of Cape Town, South Africa in December 2013. This location allows the organizers to draw upon the diverse scientific expertise present at these two conferences. It also enables them to tap into local expertise developed as South Africa has and continues to host refugees fleeing conflicts in many parts of the continent. In particular, the organizers will invite officials from the field office of the United Nations High Commissioner for Refugees (UNHCR) and the Cape Town Refugee Center.<br\/><br\/>Intellectual Merit:<br\/><br\/>The workshop will generate a research agenda for technologies and social science theories to support the diverse forms of mobility and their effects as experienced by displaced persons. Together, the technologies and theories must overcome challenges including limited access to power and network infrastructure, dense settlements, rapidly changing geography, and new and changing social relations among individuals and between individuals and organizations. The research agenda will prioritize technical developments and their associated social scientific implications to meet the most pressing information needs, generating innovations for both organizations and individuals.<br\/><br\/>Broader Impacts:<br\/><br\/>The technologies and theories resulting from these efforts will potentially enhance the lives of tens of millions of displaced persons. In addition to making their lives easier, the application of technologies and social theories may also enhance their resiliency, thereby making recovery faster and more complete, limiting the overall negative impacts of armed conflict and disasters. The interdisciplinary nature of the resulting project teams will foster cross-fertilization between computer and information sciences and social sciences. Also, the international nature of the workshop and subsequent teams will enable U.S. scientists to draw upon the resources and expertise of international counterparts. Finally, the multimedia use-case studies can be used in a variety of educational contexts, enhancing the international dimension of STEM education.","title":"Workshop on Robust Socio-technical Architectures in Support of Displaced Persons","awardID":"1343520","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8031","name":"Science of Organizations"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":[560332,560333],"PO":["565090"]},"205490":{"abstract":"We seek to investigate the systematic design and dynamic-control of cooperative payload transport and manipulation by teams of semi-autonomous mobile robotic-crane modules. The Intellectual Merit of the proposed research lies in its contribution to the development of design and control theory for general mechanical systems under various restrictions. First, the cooperative systems under consideration require unidirectional control inputs. Second, the proposed research will also contribute to the fundamental design and control theory for multiple nonholonomic systems under physical constraints. Ultimately, the proposed research also provides fundamental understanding of networked robotic systems that must operate in complex, dynamic, and unstructured environments. There is still relatively little work that treats the physical-interactions and physical power-flows within the broad multi-agent-systems literature, which we seek to remedy.<br\/><br\/>The Broader Impacts of this research are in application areas such as cooperative payload transport, technologies to assist first responders at accident scenes, search and rescue applications, and remote scientific studies of many kinds, such as experiments conducted in undersea environments, rain forest environments, or arctic environments. Remote construction, remote cleanup of hazardous environments, land mine search and remote deactivation, are just a few of the numerous applications that may be impacted by semi-autonomous human\/robot interaction. In adition, this project will develop courses, tools and the human resource base to conceive, investigate, implement and validate such systems-of-systems. In addition to graduate and undergraduate student training, planned educational initiatives include: (i) developing and integrating a new graduate course in robotics into the curriculum; (ii) creating educational technology curricular modules for encapsulating knowledge and for dissemination, and (iii) outreach within the university, with local high schools and industry and the educational community at large.","title":"RI: Small: Dynamic Payload Transport and Manipulation by Teams of Cooperating Mobile Robotic-Cranes","awardID":"1319084","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550248],"PO":["564069"]},"206480":{"abstract":"In this Cyberlearning: Transforming Education DIP project, the PIs are building and evaluating a technological and curricular infrastructure to empower scalable, low-cost experimentations for undergraduates and K-12 students in the life sciences. The infrastructure exploits two technologies that have been developed by the PIs: biotic systems, which enable low-cost remote experimentation in biology, and bifocal modeling, which motivates students to engage in deep scientific inquiry by comparing physical and virtual models in real time. Using a low-cost local or remote Biological Printing Unit (BPU), learners will 'print' organisms into petri dishes and run experiments with live materials. Each learner will have control over the empirical apparatus for as long as needed, due to the low cost and scalability of the system. The living material itself has a small footprint, and ever-advancing high-throughput technologies, such as automated microscopy and DNA sequencing, enable cost-effective, automated multiplexing of massive numbers of simple, learner-guided experiments, allowing learners to each personally carry out successions of experiments. The second innovation is juxtaposition, in real time, of the physical experimentation with computer modeling, known as Bifocal Modeling, allowing students to observe and explore the real-world growth, interactions, and characteristics of their organisms in parallel with observing outcomes of models representing their understanding of the biological phenomena underlying organism activity and subsequently refine their explanations and understanding. Research focuses on cognitive affordances and constraints of this new bifocal epistemic form, the supports needed by learners to handle the complexities of this potentially powerful approach, and the progressions in student proficiency in design and execution of experiments when they have both real and virtual models available.<br\/><br\/>The PIs are addressing the significant challenge of how to bring modern bioscience into classrooms and informal learning environments, at low cost and with substantial variety. The platform they are creating, which integrates Biological Printing Devices with Bifocal Modeling, provides for more open exploration and realism in what learners can experience than is possible simply with simulations and virtual experiments. Schools and universities will be able to integrate more advanced biology experiments into their lab curricula as a result of this investigation, broadening exposure of all students, especially the less privileged, to the life sciences.","title":"DIP: Collaborative Research: Taking Hands-on Experimentation to the Cloud: Comparing Physical and Virtual Models in Biology on a Massive Scale","awardID":"1324753","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7511","name":"TUES-Type 2 Project"}}],"PIcoPI":[552808,"562668"],"PO":["562669"]},"205270":{"abstract":"The research goal of this award is to explore different methods for upper limb amputees to control dexterous artificial hands with brain waves. Biomedical signal processing techniques will be developed to enable this with a single, small recording electrode placed noninvasively on the amputees' heads. The recorded brain waves will be wirelessly transmitted to the hand in real time. A top-level controller will be developed to interpret the intent of the amputees while a low-level controller will be used to synchronize the dexterous grasp motions of the artificial hand. Algorithms will also be developed using tactile feedback from the fingertips to automatically prevent grasped objects from being accidentally dropped when they are transported or disturbed. Amputees will participate in a study to compare the newly developed artificial hand control techniques with brain waves to conventional control techniques with muscle signals during common tasks of daily life.<br\/><br\/><br\/>If successful, this research will result in a noninvasive and economic method for amputees to control a dexterous artificial hand with brain waves. This could substantially improve the functionality of prosthetic hands for many amputees. The use of minimal hardware will facilitate the clinical adoption of this technique and the autonomous low-level control algorithms will produce a brain machine interface that places a low cognitive burden on the operator. This research can also be readily applied to benefit many disabled people including stroke victims and quadriplegics and can positively impact other areas of robotics such as improvised explosive disarmament, underwater and space exploration, and rescue robotics. Underrepresented engineering students will benefit from being included in this research plan. Additionally, undergraduate and graduate engineering students will benefit from newly developed classes and laboratory exercises resulting from this research.","title":"NRI: Small: EEG and EMG Human Model-Based Adaptive Control of a Dexterous Artificial Hand","awardID":"1317952","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549666],"PO":["563744"]},"206491":{"abstract":"A major factor influencing learning is students' emotions and their general affective state. Given the pivotal role that affect plays in learning activities it is not surprising that there has been a good deal of interest in developing affect-aware technologies. The overwhelming majority of this work, however, has focused on modeling affect, i.e., designing computational models capable of inferring how students are feeling while interacting with an Intelligent Tutoring System (ITS). While modeling of affect is a critical first step in providing adaptive support tailored to students' affective needs, very little work exists on systematically exploring the impact of affective interventions on students' performance, learning, affect and attitudes, i.e., how to respond to students' emotions such as frustration, anxiety, boredom, and hopelessness as they arise. The research fills this gap by analyzing the value of tailoring different types of interventions to negative affective states for individual students and groups of students. <br\/><br\/>The project has two main goals. First, it addresses how to respond to negative student emotion (e.g., frustration, anxiety, boredom) in computer-based learning environments through a variety of interventions. Some correspond to specially-designed digital characters, integrated into the learning environment, which are intended to act like students' learning companions. These agents support students through (a) non-verbal behaviors (e.g., having the characters express empathy in response to student frustration), (b) messages targeting students' cognitive and meta-cognitive skills, as well as motivation and affect. Other interventions involve supporting collaboration between students to mitigate negative emotional states when detected. The impact of these interventions are investigated through a series of eight experiments with a total of 800 students. These experiments help to unveil general prescriptive principles to address student affect. <br\/><br\/>The second goal accomplished by this research is that the experiments provide valuable data to continue to extend and validate existing models of emotion. Specifically, the project triangulates and integrates a complex space of partially overlapping models and constructs of affect in learning (i.e. emotions, attitudes, incoming moods, motivation, engaged use or misuse of software). The project refines several well-established models, in particular the control-value theory of emotions to provide a more stable theoretical framework for the field of emotions in educational software. <br\/><br\/>This research is unique and ground breaking, as few researchers have targeted students' emotion in classrooms, gathered fine grained data on emotions during learning, or assessed the impact of specific affective treatments on a moment-to-moment basis. Students using the tutoring systems have already shown statistically significant gains and learning outcomes, as well as increased positive affect and attitudes. The new affective interventions will greatly increase the broad impact of these systems.<br\/>This research is developing: (1) prescriptive principles about how to respond to student affect; (2) new understanding about the impact of cognitive, affective, and meta-cognitive interventions on emotions and learning; (3) new understanding about individual differences in learning, unveiling the extent to which emotion, cognitive abilities, and gender impact learning; (4) instruction that is sensitive to individual differences; and (5) refined theories of student emotion.<br\/><br\/>This research is also: (1) increasing participation in mathematics of underrepresented populations (women and minorities) who often avoid STEM careers; (2) creating broad access to web-based technologies that help to engage more students by addressing their affective and social needs; and (3) addressing the one-size-fits-all approach to education, by responding to individual student needs with alternative representations of content and pathways through which material is presented.","title":"DIP: Collaborative Research: Impact of Adaptive Interventions on Student Affect, Performance and Learning","awardID":"1324825","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[552839,552840,552841],"PO":["565121"]},"207250":{"abstract":"Stanford University will bring together faculty and researchers for a series of workshops to assess the potential for (1) expanding the number of researchers and specifically PhD students in Computer Science education research; and (2) establishing the legitimacy of computer science education research as a research discipline within CS departments at research-oriented schools. The first workshop will identify key research directions in computing education drawing from other STEM education disciplines. The second workshop will address the many concerns of how Computer Science education research can effectively be housed with CS departments.","title":"Future Directions for Computer Science Education: A Workshop Proposal","awardID":"1332686","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7382","name":"Computing Ed for 21st Century"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7513","name":"TUES-Type 1 Project"}}],"PIcoPI":["351237"],"PO":["560704"]},"209582":{"abstract":"Many important electronics applications, including medical imaging, cyber security and digital entertainment, require processing massive amount of data under limited time, power and price budgets. Modern many-core processor chips offer a promising platform for computing such data-intensive workloads, however, their utilization is hindered due to the inherent difficulty and unproductivity of parallel software development at scale. <br\/><br\/>The project aims to address this critical problem by developing methodologies and tools for productive and scalable development of software that would efficiently run on thousands of processor cores. Central to the approach is development of \"functionally-consistent structurally-malleable streaming specification,\" which enables design-time exploration of a rich software implementation space, under the constraints of hardware platform, while maintaining the application's end-to-end functional behavior. The project results are expected to improve performance, energy consumption, and application development cost in a large number of data-driven disciplines, ranging from medical imaging and cyber security to multimedia and beyond.","title":"EAGER: Productive and Scalable Utilization of Manycores in Big Data Applications","awardID":"1346812","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":[561804],"PO":["565272"]},"208163":{"abstract":"In recent years, scholars have made great strides using computer-assisted methods to collect and analyze large volumes of machine-readable information from open sources such as online news feeds and the new social media to aid in these efforts. The result has been the production and analysis of massive amounts of data at unprecedented scales and speeds. Individual scholars and research teams have experimented with multiple approaches to collecting and analyzing such data. This experimentation has promoted innovation and exploration, but it has also produced an array of data sets and methodologies that are not well integrated or coordinated. The result is that scholars and policy makers alike are learning less from these data than they could. This project seeks to overcome that obstacle by developing the community and capacity needed to establish common best practices and foster greater integration and coordination regarding the collection, dissemination, and analysis of data. As part of this development process researchers will focus on extracting information from public documents such as news reports. The scope includes events which involve disagreement ranging from nonviolent political protest to terrorism and interstate war - issues which have long been a subject of inquiry in the social sciences.<br\/><br\/>The project will accomplish this through a series of workshops that will bring together national leaders in the areas of inter-state and intra-state conflict, data management, quantitative methods, and computer science to develop a coordinated strategy for moving forward. In conjunction with these workshops, the project will develop a prototype system of software tools designed to collect and archive such data, as well as conducting extensive educational outreach and training programs in the use of the data and tool sets. The entire project will be supervised an advisory board of specialists.<br\/><br\/>The project will advance the capacity for learning about the early warning signs of contentious political events, what leads to their escalation or de-escalation, and the resulting consequences of such events. The project will also advance understanding of massive near-real-time data collection for the analysis of conflicts as well as the tools needed to manage, share, and analyze such data sets more generally. More broadly, this project will provide access to a vast new open data resource for researchers from academia, government, industry, and the non-profit sector. It will provide training opportunities for using the data, and also provide the foundation for data-oriented education programs designed for students ranging from middle school through graduate school that can help students learn about the causes and consequences of inter-state and intra-state conflict. Finally, the project will help policy makers make better decisions, which could potentially reduce the frequency and\/or scale of political conflicts, and ultimately help to save lives.","title":"A Method for Leveraging Public Information Sources for Social Science Research","awardID":"1338470","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}}],"PIcoPI":[557802],"PO":["565192"]},"209142":{"abstract":"This INSPIRE award is partially funded by the Special Projects program of the Division of Astronomical Sciences and the Office of Multidisciplinary Activities, both in the Directorate for Mathematical and Physical Sciences, and by the Information Technology Research and Information Integration and Informatics programs in the Division of Information and Intelligent Systems in the Directorate for Computer and Information Science and Engineering.<br\/><br\/>This project will construct a software infrastructure for filtering and annotating alerts generated by astronomical time-domain surveys, where an alert is a change in an astronomical source when compared with a reference image. Current and future surveys are capable of producing enormous numbers of such alerts: in particular, the planned Large Synoptic Survey Telescope project could produce a million or more every single night for a decade. Within this veritable flood will be a small number of rare and unusual sources with short lifetimes that must be recognized in real time, or else the opportunity for thorough study will be lost.<br\/><br\/>This research will meet that need: the system will include a core flow that quickly annotates alerts with already known details, creating a useful database in its very first pass, followed by a method to derive broad characteristic features that can be used to filter out and divert those events not in need of rapid follow-up. Advanced, more computationally intensive processing can be applied to those that pass the filters, including multiple paths that allow users to select for their particular interests. The final product for this prototype is those alerts that are the most unusual. The system is designed to be flexible, so no alerts are lost and the stream can be tapped by external users at any point for their own processing. Use of the Arizona Machine Experimentation Lab allows for model construction and testing, and for creation of novel, perhaps more efficient, filtering processes. Throughout, decisions are completely driven by astronomical expertise.<br\/><br\/>The study of known but rare objects will transform fields ranging from stellar evolution to growth of active galactic nuclei to the fundamental structure of the Universe, while the discovery and characterization of previously unknown objects could create entirely new fields. Although the prototype will focus on the rarest of objects, ultimately the built-in flexibility and community-generated filtering processes will enable finding items of specific interest to any user. In addition, a system that can take alerts, aggregate ancillary information, and filter to identify specific cases, will be of general use in many fields, from epidemiology to network protection to homeland security and beyond, wherever rapid analysis of new events properly associated with existing information is critical.","title":"INSPIRE Track 1: Arizona-NOAO Temporal Analysis and Response to Events System (ANTARES)","awardID":"1344024","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1798","name":"SPECIAL PROJECTS (AST)"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[560601,560602,560603,560604],"PO":[560605]},"209043":{"abstract":"This project designs and deploys a multi-disciplinary framework to model spatial, temporal and social dynamics of cyber criminals. The framework fuses theories in both computer science and criminology. Specifically, project objectives are a) Apply and validate existing theories in the realm of general criminology (in particular Akers? social learning theory and Gottfredson and Hirschi?s general theory of crime) to study cyber crimes; b) Derive novel Internet usage features as fingerprints for cyber crimes; c) Design classification algorithms (based on multi-fractal analysis and petri-net designs) to subsequently model multiple dynamics of cyber criminals by integrating theoretical and practical outcomes from the above two objectives; and d) Extensively test and validate project outcomes. The core novelty of this project is in using real Internet data from subjects (initially a cyber savvy college sample) that is collected continuously, unobtrusively, while still preserving a high degree of privacy.<br\/><br\/><br\/>Outcomes of this project will have far reaching impacts. It lays a foundation for fusing expertise in social sciences (specifically criminology) and cyber security, as a result of which existing theories in general criminology can be empirically tested for practical validity in studying cyber crimes. The identification of unique Internet fingerprints associating with cyber crimes will provide new insights into human centered aspects of cyber crimes, which is lacking today. The classification algorithms designed will provide cyber defenders with new tools to combat cyber crimes from multiple perspectives including prevention, detection, forensic investigations and prosecution.","title":"EAGER: Collaborative: A Multi-Disciplinary Framework for Modeling Spatial, Temporal and Social Dynamics of Cyber Criminals","awardID":"1343482","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[560309],"PO":["565327"]},"209076":{"abstract":"Big Data analytics requires bridging the gap between data-intensive computing and data-driven computing to obtain actionable insights. The former has primarily focused on optimizing data movement, reuse, organization and storage, while the latter has focused on hypothesis-driven, bottom-up data-to-discovery and the two fields have evolved somewhat independently. This exploratory project aims to investigate a holistic Ecosystem that optimizes data generation from simulations, sensors, or business processes (Transaction Step); organizes this data (possibly combining with other data) to enable reduction, pre-processing for downstream data analysis (Organization Step); performs knowledge discovery, learning and mining models from this data (Prediction Step); and leads to actions (e.g., refining models, new experiments, recommendation) (Feedback Step). <br\/><br\/>Intellectual Merit: As opposed to the current practice of considering optimizations in each step in isolation, the project considers scalability and optimizations of the entire Ecosystem for big data analytics as part of the design strategy. The project aims to consider big data challenges in designing algorithms, software, analytics, and data management. This strategy contrasts with traditional approaches that first design algorithms for small data sizes and then scale them up. The project aims to treat data complexity, computational requirement, and data access patterns as a whole when designing and implementing algorithms, software and applications. <br\/><br\/>Broader Impacts: The project could advance the state of the art in big data analytics across a number of key applications such as Climate Informatics and Social Media Analytics. The software resulting from the project is being made available to the broder scientific community under open source license. The project offers enhanced opportunities for education and training of graduate students and postdoctoral researchers at Northwestern University.","title":"EAGER: Scalable Big Data Analytics","awardID":"1343639","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[560392,560393,560394],"PO":["565136"]},"202180":{"abstract":"This project will develop a new computational framework to advance the understanding of epigenetic gene regulation in the human malaria parasite. Epigenetics is the study of heritable changes in gene expression or cellular phenotype caused by mechanisms other than changes in the underlying DNA sequence.At the core of the computational framework is the ability to solve a set of hard computational questions, which are the focus of the research plan. The computational challenges require the study of novel combinatorial optimization problems, the development of new time- and space-efficient algorithms, and ultimately the implementation and deployment of user-friendly web-based software tools. The ability to analyze the epigenome of the human malaria parasite will improve our comprehension of its biology and possibly enable molecular biologists to identify new antimalarial strategies. The proposed computational framework will also enable life scientists to make novel epigenetic discoveries and ultimately improve the understanding of the complex mechanisms that drive gene expression inother eukaryotic organisms. Software tools will be placed into the public domain, which will benefit researchers and the public worldwide, and potentially lead to new international and industrial collaborations. This project will support two graduate students and one post-doc in a highly interdisciplinary environment.<br\/><br\/>Most eukaryotic genomes have a second layer of information which is embedded on chemical marks added to DNA and to the protruding tail of special proteins that package DNA into a complex called the nucleosome. One of the most astonishing discoveries in molecular biology of the past decades is that this \"covert\" layer, called the epigenome, affects a variety of cellular and metabolic processes. Epigenetic marks not only controls what genes are accessible in each type of cell, but also determine when the accessible genes may be activated. Molecular biologists have also confirmed that the epigenome is affected by the interactions of the organism with the environment and that changes to the epigenetic marks induced by these interactions are inherited across cell division, despite not being encoded directly in DNA. <br\/><br\/>This project will study a set of computational challenges that will be brought about by the increasing number of epigenome projects. Specifically, the goal is to develop methods and software tools for (1) the analysis nucleosome and methylation maps(using a modified Gaussian mixture model and expectation maximization); (2) the study of dynamics of nucleosome positioning, histone tail modifications and DNA methylation patterns (using graph theoretical approaches, e.g., k-partite matching); (3) the analysis of DNA motifs for stable nucleosomes and specific histone modifications (using combinatorial optimization approaches); (4) the discovery of new genes using nucleosome or methylation landscapes (using machine learning classifiers); (5) the identification of statistically significant genome-wide correlations between nucleosome positioning, histone modifications, DNA methylation patterns and gene expression (using dynamic Bayesian networks). These five computational tasks will require the study of novel combinatorial optimization and machine learning problems, the development of new time- and space-efficient algorithms, and ultimately the implementation and deployment of user-friendly web-based software tools.<br\/><br\/>The \"platform\" on which the algorithms will be developed is P. falciparum, the parasite responsible each year for 350-500 million cases of malaria, and between one and three million of human deaths world-wide. There is no vaccine against malaria (one is currently on clinical trials) and the parasite is developing resistances to almost all drugs currently available. The methods and tools developed will not be malaria-specific, and will scale to a variety of other eukaryota with much larger\/complex genomes.<br\/>Updates and additional information about this project will be made available at http:\/\/www.cs.ucr.edu\/~stelo\/iis13.htm","title":"III: Medium: Algorithms and Software Tools for Epigenetics Research","awardID":"1302134","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}}],"PIcoPI":[541839,541840],"PO":["565136"]},"205590":{"abstract":"The need to efficiently store and access data is central to modern computing. Speeding up the actual performance of fundamental problems is not limited to better experience for users; it is also about using computer resources more efficiently. Faster algorithms and data structures mean less energy consumption, and less physical computing resources needed. This project will investigate the discovery of provably best data structures for several fundamental problems. The dictionary, priority queue and packed memory array are three abstract data structures considered in this project. The dictionary supports the efficient insertion, deletion, and search of ordered data. The priority queue supports insertion and the removal of the minimum element. The packed-memory array keeps ordered data in sorted order in a limited amount of space.<br\/><br\/>For binary search trees (BSTs), this research will make substantial progress on many fundamental problems that include finding a dynamically optimal search tree, proving better explicit upper and lower bounds on BSTs, inventing methods to combine BSTs, and transforming amortized BSTs into ones with good worst-case runtimes. In the case of heaps, the project will analyze performance bounds for decrease-key operations in natural models, search for a geometric view of heaps, investigate dynamic optimality, and improve the analysis of pairing heaps. Cache-oblivious heaps with close to optimal decrease-key operations, and efficient packed memory array (PMA) structure are two fundamental building blocks of many cache-oblivious algorithms. This project will develop PMAs that perform well on sequences with certain types of order, and construct a randomized structure whose runtime beats the lower bound for deterministic structures. The project will also examine the role of in-degree in pointer-model persistence and develop general transformations for cache-oblivious persistence. <br\/><br\/>Data structures are foundational for algorithms. Improving fundamental data structures will have profound broader impact on computing practice. Advances in data structures are accessible to undergraduates and the PI will engage under-represented students in this research.","title":"AF: SMALL: Fundamental Data Structures","awardID":"1319648","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550499],"PO":["565251"]},"205381":{"abstract":"Despite rampant criticism of passwords and an abundance of alternative proposals for user authentication (e.g., biometrics), passwords are not likely to be replaced in the near future due to their ease of deployment and familiarity to users. Indeed, while a number of policies for improving password systems have emerged, the most widely adopted of these is to simply increase the size of the space from which passwords are drawn. Even so, for user-chosen secrets, these policies generally make passwords harder to remember and type, leading to user frustration. Worse, users generally fulfill these policies in predictable ways, impairing the security benefits they are intended to provide.<br\/><br\/>This project leverages linguistic expertise to develop techniques that provide memorable passwords through both user input and automated processes. Specifically, we explore the formation of pronounceable authentication strings that lead to improved system security and a decreased burden on users (by providing memorable, hint-able, easily type-able passwords that are resistant to attack). Our user-influenced, but system-generated, pronounceable strings combine source words to make lexical blends (aka portmanteaus, e.g., flamingo + mongoose -> flamongoose), or elicit user-generated blends from system-suggested semantic domains. Additionally, we examine techniques for rating pronounceability of word-like strings, allowing us to quantify pronunciation difficulty and proactively apply rigorous security analysis techniques to the space of pronounceable word-like strings. The broader significance of the work is to increase our collective understanding of the driving forces behind string pronounceability and the complex relationships involved in human and automated formation of lexical blends.","title":"TWC: Small: Toward Pronounceable Authentication Strings","awardID":"1318520","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["565326",549960,549961],"PO":["565327"]},"205271":{"abstract":"The objective of this research is to develop a new 3D printing\/additive manufacturing process that will allow the direct fabrication from digital designs of integrated, multi-material devices such as soft robots with embedded actuators, sensors, and circuitry. The focus of the research will be (1) to design, prototype, characterize, optimize, and demonstrate the new process, and (2) to design, model, fabricate, and test an electromagnetic actuator and capacitive sensor made using the process. A testbed \"robot printer\" will be developed, along with control software and process design files. The research will require a number of technical and scientific advancements to provide the desired capabilities, including co-deposition of multiple materials in multi-layered patterns and formation of low-resistance electrical junctions. <br\/><br\/><br\/>If successful, the results of this research will lead to a new manufacturing process for soft robots, an emerging class of devices promising greater safety, better manipulation of delicate and irregular objects, the ability to squeeze through small openings in search and rescue operations, etc. The challenge of manufacturing soft robotic components that include a large number of distributed actuators, sensors, and associated circuitry can be economically approached by concurrent 3-D printing with two types of materials: one that is conductive for interconnects, and the other insulating and flexible. Today's 3-D printing works with one type or the other, but not both. Overcoming this limitation would be transformative and enable a wide range of sophisticated, active structures to be readily fabricated. Beyond robotics, applications of the research include active prosthetics, minimally-invasive surgical instruments, and smart implants, as well as stretchable and wearable electronics. Undergraduate and graduate students will be heavily involved, and a partnership with the Perot Museum of Nature and Science (Dallas, TX) to develop a semi-permanent display and live demonstrations will expose a wide public audience to 3-D printing and robotics.","title":"NRI: Small: Additive Manufacturing of Soft Robot Components with Embedded Actuation and Sensing","awardID":"1317961","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549668,549669,549670],"PO":["563744"]},"209693":{"abstract":"Bayesian probability is an important theory of robust decision making. Domains as diverse as physical science, engineering, medicine, and law have applied Bayesian inference successfully. Nevertheless, Bayesian inference is fraught with problems during practical development and deployment. The standard techniques used to construct the implementations are semantically far from the \"whiteboard presentation\" (mathematical description), are untrustworthy, and expensive to apply. This research addresses this problem by providing an axiomatic foundation with a built-in approximation system that can verify implementations.<br\/><br\/>This research develops an automatic, trustworthy compiler from the whiteboard math used in the development of a theory to an efficient inference model implementation ready for evaluation. This environment provides compilation to a measure-theoretic model of the theory and to an efficient implementation that is provably connected to the measure-theoretic model. This compilation technique delays approximation as long as possible to achieve correctness and allow varied options for approximation, including the use of a novel algorithmic sampling technique, and performs high-powered optimization to compile them to parallelized implementations.","title":"EAGER: A Measure Theory Semantics of Probability Theory","awardID":"1347556","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[562082],"PO":["565264"]},"209099":{"abstract":"End-users' online behavior can significantly affect the reliability and security of next-generation software systems. For instance, skipping repeated requests to update software or ignoring security warnings while visiting unknown websites, while extremely dangerous, are not uncommon. Although end-users' actions (or inactions) often open up the opportunity for cyber-attacks, the lack of emotional appeals and poor design of the current software update\/warning messages are to blame to a large extent for such risky behavior, which is addressed as follows. First, this project identifies the limitations of the current software update\/warning messages from an affective-cognitive perspective, and addresses the shortcomings by incorporating emotional appeals that command attention and evoke emotions more effectively. Also, message designs are changed adaptively based on the context and user's past behavior to counter the habituation-effect. Finally, to increase the compliance rate, this project develops emotional education and inoculation strategies that enable users to recognize and avoid the temptation associated with risky online behavior. Successful completion of this project will greatly advance the understanding of the impact of emotion on the decision making process in regards to safe online behavior, and will revolutionize the way software vendors communicate the risks of running vulnerable software and warn users. The research findings will be published in premier academic venues, and will be disseminated through project website. Research agendas will be integrated with the curriculum development activities. Existing programs at UConn will be leveraged to promote diversity and engage students from underrepresented communities.","title":"EAGER: The Role of Emotion in Risk Communication and Warning: Application to Risks of Failures to Update Software","awardID":"1343766","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[560477,560478],"PO":["565327"]},"205580":{"abstract":"This research addresses the bottleneck caused by the difficulty and expense of designing mixed-signal interface circuits for modern integrated circuits. These interface circuits translate between real-world analog signals and the corresponding digital signals that are processed by digital electronics. A huge challenge is that the cost of designing these mixed-signal interface circuits is skyrocketing and now dominates the overall cost of integrated circuit design. Difficulties in modeling and the worsening manufacturing variability, related to shrinking transistor size, mean that mixed-signal design requires a huge effort. Furthermore, to ensure good manufacturing yield, even the best designs are not optimal in terms of energy efficiency or performance. A fresh new approach is needed. This research investigates a completely new way of creating mixed-signal circuits. In the new approach, the integrated circuit, itself, assembles a mixed-signal function by combining and configuring simple building blocks. An integrated circuit creates a function from an uncommitted set of simple primitives and cells, such as charge-processing cells, comparators and simple amplifiers. This approach pushes design decisions and tradeoffs to each individual integrated circuit. It delivers the best possible solution for a given set of design objectives in the presence of manufacturing variations. It also avoids the need for good models and accurate simulation during design.<br\/><br\/><br\/>The broader impact of this research is that it addresses the crisis of cost and complexity in mixed-signal integrated-circuit design with a radically new self-design approach. It moves design decisions to the individual integrated circuit, greatly simplifying the design process and removing mixed-signal design from the straightjacket of worst-case design. The broader impact of this research is enhanced with substantial efforts in outreach and education. These include outreach to K-12 students and research experience for undergrads from under-represented minorities. An advanced graduate-level course in analog-digital interfaces will involve graduate students in the research of mixed-signal self-design. To help change the mindset of the next generation of circuit designers, the undergrad major design experience course in analog circuits will introduce students to analog circuit optimization.","title":"CIF: Small: Self-Synthesizing Mixed-signal Circuits","awardID":"1319592","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[550475],"PO":["562984"]},"205591":{"abstract":"This research involves the development and validation of a novel framework for signals and systems defined in dimensions greater than two. Such multilinear signals can be viewed as high dimensional data tensors, and the associated systems as multidimensional linear operators acting on this class of signals. These mathematical objects are ubiquitous in science and engineering and of growing practical importance. They arise in time varying tomographic problems such as medical imaging, hyperspectral remote sensing, and seismic data acquisition and processing for rapid exploration for oil and gas, to name but a few applications. More recently multilinear signals have become prominent in the context of data analytics for social networks, online big-data management, data visualization, predictive modeling for disease propagation and forecasting of complex events.<br\/><br\/>Effective multilinear computation and processing requires generalization of the traditional 2-D factorization methods to higher dimensions. To date, tensor factorization techniques are either optimal in terms of compactness of data or operator representation but NP-hard to evaluate, or on the other extreme computationally feasible but can be severely non-compact thereby loosing structural information. This adversely affects the efficiency in sampling (tensor data compression) and computational costs in recovery of multilinear data using regularized inversion algorithms. In this context, the investigators develop fundamentally new approaches for tensor decompositions, which, while being computationally feasible, are also provably compact. Specifically, the investigators will develop: Algorithms for handling large scale processing for multilinear signals and systems; Optimal sampling and recovery of multilinear data as non-trivial extension of the theory of compressive sensing; and Tensor representation based, regularized inversion models and algorithms for multilinear inverse problems.","title":"Optimal sampling and recovery for multilinear signals and systems","awardID":"1319653","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[550501,"561951",550503],"PO":["564898"]},"205360":{"abstract":"Current petascale platforms can perform large-scale simulations and generate massive amounts of data at unprecedented rates. These rates are expected to increase as exascale platforms are introduced. The generation of more and more data presents new challenges for scientists who struggle with the analysis, sorting, and selection of scientifically meaningful results. When very large amounts of data records are located across a large number of nodes in a distributed memory system, even a small number of comparisons can be costly or even impossible. Therefore, new methodologies are necessary to analyze large scientific datasets at scale.<br\/><br\/>The goal of this project is to develop a transformative analysis method to model the properties of large scientific datasets in a distributed manner on petascale systems today and exascale systems in the future. The research activity includes (1) the design of new algorithms for encoding properties embedded in distributed data in a parallel manner by using space reduction techniques; (2) the design of new algorithms for clustering and classifying these properties by using distributed paradigms such as MapReduce; (3) the deployment of the algorithms for diverse datasets in structural biology and astronomy; and (4) the tuning of the algorithms for both result performance and accuracy on emerging storage technologies. <br\/><br\/>The analysis method will provide the scientific community with infrastructures and instrumentations to identify features that can be used to predict class memberships; find recurrent patterns in datasets; and identify class memberships from a specific feature or property. By effectively and accurately capturing scientific information in a scalable manner, these infrastructures and instrumentations will break the traditional constraint of data centralization and allow scientists to overcome the difficulties associated with the fully distributed nature of the data considered.<br\/><br\/>The project's educational component promotes training and learning in computational modeling and analysis techniques as well as data-intensive algorithms and platforms by involving undergraduate and graduate students in research activities and integrating big data analytics into the undergraduate curriculum at the University of Delaware. The research-based educational materials developed in this project will be made available to the scientific community through the project portal and through tutorials at XSEDE and Supercomputing (SC) conferences.","title":"SHF: Small: Collaborative Research: Modeling and Analyzing Big Data on Peta- and Exascale Distributed Systems Supported by MapReduce Methodologies","awardID":"1318417","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":[549902],"PO":["565272"]},"205492":{"abstract":"The modernized Smart Grid (SG) is expected to enable several new applications such as dynamic pricing, demand response and fraud detection; however, collection of such fine-grained data raises privacy issues. This project aims to design and implement several novel mechanisms for securing data collection and communication in SG Advanced Metering Infrastructure applications while preserving user privacy when the data are to be accessed. The underlying communication infrastructure, namely Neighborhood Area Networks, is to be built with wireless mesh networks using the IEEE 802.11s, an IEEE 802.11 amendment for mesh networking. The project investigates user privacy preservation mechanisms using partially and fully homomorphic encryption during data collection in the Neighborhood Area Networks. For the collected data at the data repository, attribute-based access control mechanisms are studied. As part of these access control mechanisms, novel scalable key establishment and group key management schemes are investigated. A testbed consisting of IEEE 802.11 Linux routers is part of the project to assess the overhead of privacy mechanisms under quality of service constraints. <br\/><br\/>The testbed will be a community resource for researchers, educators and students as well as utility companies interested in SG communications and privacy. Ensuring practical user privacy solutions will increase the participation of customers in SG applications saving cost and energy. The testbed will be a great help to utilities, researchers and educators who work on SG communications. The software and technical manuals will be made available as open-source documents. New graduate courses on the Smart Grid will be introduced into the academic curriculum.","title":"TWC TTP: Small: Collaborative: Privacy-Preserving Data Collection and Access for IEEE 802.11s-Based Smart Grid Applications","awardID":"1319087","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":[550252],"PO":["565239"]},"205261":{"abstract":"This proposal develops a new motion planning paradigm for enabling robots to work in the presence of humans or cooperatively with humans as co-workers. The paradigm is intended to be fast, reactive, and responsive to the requirements that arise from human-robot interaction. It involves the definition and subsequent implementation of constraints that encode properties of human-aware paths and can be translated to cost functions characterizing path quality. New motion planners are proposed. The operation of these planners is guided by constraints and their corresponding cost functions. The planners produce paths compatible with a given set of constraints. A mechanism to incorporate user feedback on the relative importance of constraints is provided and semi-autonomous operation of the robots is considered. Importantly, the planners are embedded in a novel hybrid-systems framework that allows a robot to automatically switch among planners, and hence behaviors, in order to take into account different constraints and user preferences that arise in the context of semi-autonomous operation. Besides the actual implementation of the planners on specific platforms, this project also disseminates all developed libraries and planners. Compatibility will the Robot Operating System (ROS) is provided for wide adoption, while tutorials at major conferences are planned. The training of graduate students and female undergraduate students are a central focus of this proposal.","title":"NRI: Small: Collaborative Research: Rethinking Motion Generation for Robots Operating in Human Workspaces","awardID":"1317849","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["557516",549639],"PO":["564069"]},"205272":{"abstract":"The proposed work will contribute to the fundamental understanding of dexterous manipulation, especially in how low-dimensional hands that have traditionally been used for pure grasping tasks can be leveraged for excellent manipulation capabilities. The PI team will investigate how the passive mechanics of compliant and underactuated hands can be utilized for manipulation strategies include vibrational stick-slip, equilibrium point manipulation, and finger gaiting, as well as develop low-level planning and control schemes to implement those for a wide range of manipulation movements. This work will lead to the development of general-purpose robot hands having low-dimensional actuation and sensing. Furthermore, the focus on design for open-source dissemination through rapid prototyping techniques will result in novel fabrication strategies in furtherance of fast innovation in a range of fields.<br\/><br\/>A key thread of the proposed work is that simple hand designs and control schemes will be made freely available through an open-source repository. The hands will be able to be easily and inexpensively fabricated with rapid prototyping techniques, lower the entry barrier from expense hand hardware and encouraging continual design improvements. Additionally, the team will kick-start a user community around those hands through an on-site tutorial at Yale for approximately 20 students and postdocs from top manipulation research groups. For the education and outreach component of this proposal, a major thrust involves enhancing RoboticsCourseWare.org, the PI's open-access repository for robotics teaching materials, in order to highlight open-source and other inexpensive hardware platforms for educational purposes. A strong focus on involving a diverse group of undergraduates and high school students in the research plan is also highlighted.","title":"NRI: Small: Dexterous Manipulation with Underactuated Hands: Strategies, Control Primitives, and Design for Open-Source Hardware","awardID":"1317976","effectiveDate":"2013-09-15","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549672],"PO":["564069"]},"205162":{"abstract":"Tremendous progress has been made in the field of computational geometry in the last few decades. Efficient algorithms have been developed to solve geometric problems. However, there are many problems in computational geometry that do not yet have good algorithms or even do not have any algorithmic solutions. The main goal of this project is the development of new fundamental techniques in algorithms for solving those geometric problems.<br\/><br\/>This project aims to investigate a particular group of problems in computational geometry: problems in polygonal domains (or polygons with holes) in the plane. These problems are particularly interesting and fundamental because they model many geometric problems in the plane. The specific problems include computing shortest paths measured by Euclidean or other metrics, minimum link paths, other geometric paths, geodesic Voronoi diagrams, visibility polygons, geodesic diameter and center, polygon decomposition, facility locations, and geometric data structures, etc. The goal is to develop new algorithmic techniques for solving these problems efficiently. This research will bring diverse methodologies from other areas such as discrete mathematics, graph theory, operations research, combinatorial optimization, computational complexity, data structures, etc. The project is expected to greatly improve our understanding of the geometric structures of the problems in polygonal domains and to produce new algorithmic tools for solving these problems. <br\/><br\/>This research will potentially enrich the area of computational geometry by introducing new techniques. The algorithms and observations obtained from the project will also benefit computer science and other related disciplines where geometric algorithms are widely used. Another important part of this project is to train graduate students to practice in doing research, especially in developing algorithms for solving problems in computational geometry.","title":"AF: Small: Algorithms for Computational Geometry Problems in Polygonal Domains","awardID":"1317143","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":[549350],"PO":["565157"]},"209760":{"abstract":"This award will help to support student attendance at the 54th IEEE Annual Symposium on the Foundations of Computer Science (FOCS) in Berkeley, CA on October 27 through October 29, 2013, as well as to support attendance by some qualified postdoctoral fellows who do not have other sources of travel funding. FOCS and its sister conference, the ACM STOC meeting, are the premier broad-based conferences on the Theory of Computing. FOCS has a record of strongly encouraging participation by students, for whom the conference serves as a valuable educational experience, both for the technical content of the talks and for the opportunities for networking that it provides. In recent years, the annual FOCS conference has been attended by eighty to one hundred students who have comprised more than a third of all attendees. This award will provide partial support to twenty or more students, covering shared hotel rooms and travel. (Student registration will be supported by other means.)","title":"IEEE Symposium on Foundations of Computer Science (FOCS) 2013, Berkeley, CA Oct 27-29, 2013","awardID":"1348020","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[562312],"PO":["565251"]},"209881":{"abstract":"A key challenge for interactive learning environments is how to automatically co-regulate ? balancing learners? autonomy with the pedagogical processes intended by educators. In this Cyberlearning: Transforming Education EAGER project, the PIs are exploring the use of experience management (EM) to address this issue. They are collecting preliminary data about (1) the relationship between a learner's goal orientations and play style and (2) the impact of dynamically adjusting the learning environment using a variety of EM strategies and their impacts on learners' autonomy and learning outcomes. These issues are being addressed in the context of an interactive learning environment called Solving the Incognitum. Using this environment, learners learn about geological time and the fossil record. The setting is the historical Charles Willson Peale Museum of Art and Science, the largest US natural history museum of its day (1801-1827). Included in this virtual museum are all the dinosaur and ancient animal bones that Peale and his group brought back from his expeditions. Learners are challenged to find the bones that are missing from a skeleton, and clues are scattered around the museum. Learners with a goal-achievement orientation may not explore enough and may need to be encouraged to do that, while those who are more natural explorers may need to be guided to move towards the planning needed to achieve their goal. Reflection in action and reflection on action are supported.<br\/><br\/>As ailing governments cut funding for schools, there is a push towards using technology for providing the kinds of help that aides and specialized teachers might have provided. For such an effort to be successful, we need to learn more about how to design engaging learning environments that can help struggling learners. Learning environments where learners get to explore, design, build, and solve problems are engaging, but using them to promote learning requires understanding how to give learners the autonomy they need to remain engaged and enthusiastic along with the guidance they need to successfully learn. This project represents an early attempt at addressing that need.","title":"EAGER: TAEMILE: Towards Automating Experience Management in Interactive Learning Environments","awardID":"1349082","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}}],"PIcoPI":[562644,"563558",562646],"PO":["562669"]},"209155":{"abstract":"This INSPIRE award is co-funded by the Polymers Program of the Division of Materials Research and the Software and Hardware Foundations Program of the Division of Computing and Communications Foundations, with additional contributions from the Office of Multidisciplinary Activities of the Directorate for Mathematical and Physical Sciences.<br\/><br\/><br\/>TECHNICAL SUMMARY <br\/><br\/> The aim of this project is to lay the foundations for new research directions in \"materials that compute\". To achieve this goal the work will focus on developing integrated sensing, computing and responsive systems by using non-linear oscillating chemical reactions to perform spatio-temporal processing and recognition tasks. The specific materials that will be considered for these tasks are polymer gels undergoing the oscillatory Belousov-Zhabotinsky (BZ) reaction; these materials are flexible and stimuli-responsive and encompass distinct oscillating domains. The temporal coherence of the BZ reactions in these gels will be harnessed to perform associative processing between sets of stored or learned patterns and input stimuli in the form of patterns of light, pressure, or chemistry. The effective resonance between the input patterns and the oscillators will be used as the basic computation paradigm. In effect, the aim of the studies will be to \"let the physics do the computing\". Hence, the research will enable the design of systems where the behavior of dynamic attractors will make possible rapid convergence to the solution. Consequently, these materials systems will respond to their environment in programmable ways. The development of these new materials systems -- which are capable of autonomously sensing, communicating, and responding to environmental changes -- will provide an opportunity for new interactions between the materials and computer science communities. <br\/><br\/><br\/>NON-TECHNICAL SUMMARY <br\/><br\/> The overarching goal of this work is to transform computing platforms away from desktops or even hand-held mobile appliances fabricated from a large collection of heterogeneous parts to a computational \"fabric\" built with new material systems and implementing new computational paradigms. The ideal computing material would be lightweight and mechanically compliant, and would sense and respond to human touch and motion in order to perform a level of computing that will enrich the lives of its users. Furthermore, the material would perform these \"sense, compute and respond\" functions in a relatively autonomous manner, enabling it to operate without connections to an external power supply. This work seeks to achieve these objectives by investigating the co-evolution of energy-transducing, soft materials, such as oscillating chemical gels, and modes of computation, such as non-Boolean associative processing, which can exploit these materials characteristics. The impact of this work will be to nucleate a new field of \"materials that compute\"; that is, systems where the computer and the material are one and the same entity. The ability to create smart materials that can sense the environment, process information and react to complex stimuli should enable new systems that will interface with humans to provide tactile, temperature, and photonic inputs to smart clothing, robotic manipulators and possibly prosthetic limbs.","title":"INSPIRE Track 1: Sensing and Computing with Oscillating Chemical Reactions","awardID":"1344178","effectiveDate":"2013-09-15","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1773","name":"POLYMERS"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[560649,560650],"PO":["565354"]},"209089":{"abstract":"Healthcare is being transformed by information technology. However, with the promises of the technology have come concerns about the privacy, confidentiality and operational integrity of healthcare information and systems. Physicians and other healthcare professionals increasingly rely on networked applications for tasks as diverse as accessing patient records, remote diagnoses and consultations, in-home patient monitoring, healthcare related analytics and even remote surgical procedures. In addition to these domain specific applications, healthcare professionals use all the other more typical vocational applications, often from the same device. This diversity of applications and in particular the fact that the same individual, possibly using the same device, might be concurrently using these different applications, presents a particular challenge to healthcare information technology (IT) operations. Ideally, the role an individual is playing by using a specific networked application, should determine both the security and the performance context associated with that role. This project develops, integrates, and tests a prototype system for end-to-end isolation and containment of healthcare data and applications based on Software Defined Networking (SDN) technology. Health care applications are generally trusted, but need to operate securely and protect the data they access, in a host and network environment that might contain untrusted applications and entities. The project addresses these concerns by developing and prototyping an SDN End-to-end Application Containment ArchitecTure (SeaCat), and demonstrating its utility with electronic health record (EHR) and medical imaging applications. The project will combine software defined networking (SDN) primitives with application containment mechanisms to realize end-to-end application containment in a health care IT environment. After a EHR application user has authenticated with SeaCat, an EHR specific context is dynamically created from the EHR Server, through the network and extending into the endpoint. The EHR application and any temporary local data it is using are contained within this context and protected from data leakage and malicious actors in both the network and the endpoint. Once the user application ends, the complete end-to-end context, including any data temporarily stored within the endpoint or network devices, is removed and the environment reverts to a clean state.<br\/><br\/>The importance of safeguarding healthcare data is well known. The HIPAA privacy and security rules reflect legislated mandates for such safeguards. Yet at the same time healthcare IT applications are being extended over a larger geographic region beyond healthcare campuses, and applications are making use of high-bandwidth, low-latency networks linking to the new locations, often with smartphones and tablets. The application containment architecture in this project will support isolation and privacy concerns for healthcare and facilitate the deployment of future gigabit healthcare applications.","title":"EAGER: SeaCat: An SDN End-to-End Application Containment ArchitecTure to Enable Secure Role Based Network Access in Healthcare","awardID":"1343713","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[560436],"PO":["564993"]},"205790":{"abstract":"The exponential growth of wireless data traffic causes spectrum depletion and significantly stresses the capacity of existing and future wireless networks. The massive unlicensed bandwidth in the 60 GHz band provides great potential to meet the surging wireless data demand. The goal of this project is to gain a deep understanding of the 60 GHz propagation and channel characteristics, and to develop effective 60 GHz network protocols. This project falls into four interacted thrusts: (i) 60 GHz 3-D channel propagation measurement and modeling; (ii) MAC scheduling in 60 GHz networks; (iii) 60 GHz mesh network protocols; and (iv) enabling rich multimedia communications in 60 GHz networks. Various methods including field testing and measurements, mathematical modeling, network protocol design, distributed control, and cross-layer optimization are adopted to address the research challenges. <br\/><br\/>This project is part of a broad and ambitious global effort to develop novel techniques to exploit the huge license-free spectrum in the 60 GHz band in face of the exponential wireless data growth. It has the potential to accelerate the deployment of more powerful, bandwidth intensive, ubiquitous and cheaper wireless applications and services, and the support of more versatile, robust and rich-multimedia wireless networks. Complementary to the research agenda, the project also carries out a broad range of education and outreach activities, including integration of research findings into wireless engineering courses, textbook development, promoting underrepresented and undergraduate populations, collaboration with an HBCU, and participating in existing programs at the PIs' institutions for outreach to K-12 students and teachers.","title":"NeTS: Small: Collaborative Research: Exploring the 60 GHz Spectral Frontier for Multi-Gigabit Wireless Networks","awardID":"1320664","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[550958],"PO":["565303"]},"205680":{"abstract":"As society becomes increasingly reliant on software systems to help<br\/>plan medical treatments, predict the future climate, guide investments<br\/>in financial markets, and mine noisy data to infer scientific facts,<br\/>the risk that uncertainties in the input data and defects in the<br\/>software can create false alarms, or a false sense of security, is<br\/>high. This project will help address the gap between highly complex<br\/>probabilistic programs and tools for reasoning about them.<br\/>Probabilistic programs arise in important everyday applications that<br\/>include medical, engineering and financial risk analysis\/decision<br\/>making systems, large-scale simulations, data mining, sensor noise<br\/>filtering algorithms for cyber-physical systems, and randomized<br\/>algorithms. The presence of input uncertainties and randomness built<br\/>into the behavior of the programs can cause undesirable behaviors and<br\/>variable performance. Therefore, it is important to accurately<br\/>predict the probabilities of such undesirable behaviors, and expected<br\/>values for important performance measures.<br\/><br\/>This project investigates automatic program analysis tools for<br\/>probabilistic programs that will model the sources of uncertainty<br\/>appropriately, and infer bounds on the probabilities of assertions and<br\/>expectations of performance measures. Two flavors of inference<br\/>procedures are being investigated: symbolic procedures<br\/>combining decision procedures with the theory of martingales and<br\/>statistical procedures using statistical hypothesis testing<br\/>methods to infer probabilistic annotations with statistical<br\/>guarantees.","title":"SHF: Small: Reasoning Rigorously About Probabilistic Programs","awardID":"1320069","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550702],"PO":["565264"]},"205691":{"abstract":"Datacenter-based cloud services exhibit unpredictable performance variations due to multi-tenant interferences and the heterogeneity in datacenter hardware. The investigators attribute the causes of such performance unpredictability to missing two important service guarantees from existing cloud providers: resource capacity and application agility. <br\/><br\/>To provide guaranteed resource capacity and enhanced application agility, this project develops independent but complementary approaches at system and middleware levels to reduce performance variations of in-cloud applications without compromising other objectives such as high datacenter utilization and good average performance. Anticipated deliverables include new system support in cloud resource management to account for interferences and hardware heterogeneity in shared infrastructures and middleware approaches to perform agile, non-invasive and application-centric resource provisioning. <br\/><br\/>The research methodology combines architectural knowledge on the complex interplay between simultaneous multi-threading, multicore, and non-uniform memory access architectures with statistical learning algorithms to quantify interference and heterogeneity, and integrates the strength of self-optimizing learning and control techniques to automate resource provisioning under dynamic workloads. <br\/><br\/>This project broadens impact by exploring inter-disciplinary techniques in computer system design and enhancing cloud services with predictability guarantees. The success will guide resource management and metering in future cloud systems. This project also involves industry collaboration, curriculum development, and provides more avenues to bring women, minority, and underrepresented students into research and graduate programs.","title":"CSR: Small: System and Middleware Approaches to Predictable Services in Multi-Tenant Clouds","awardID":"1320122","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[550727,550728],"PO":["565255"]},"205581":{"abstract":"Mobile computing devices offer increasingly rich human-computer interaction opportunities through the use of new sensor technology such as multi-touch surfaces, microphones, and cameras. These advances provide users with richer sets of interaction modalities and motivate new and novel human-centric interfaces. Among the new input modalities, touch-based interaction is the most advanced and widely deployed, while speech is gaining increased attention. However, even with recent breakthroughs in computer vision and depth and motion sensing technology, the imaging modality remains the least developed for mobile devices. And yet image-based sensing (a) would permit richer interaction opportunities for a general user population and (b) holds special promise for individuals with disabilities who find touch-based interfaces very challenging to use.<br\/><br\/>This project focuses on the development of Head-Activated Technology for off-the- shelf Mobile Devices (HAT-MD). HAT-MD systems will employ the next-generation imaging technology that will be included in a wide array of mobile devices. Advanced algorithms will detect and track user head and facial features, and map specific movements to specific interface controls and application actions. HAT-MD is initially targeted towards individuals with physical disabilities, and will both (a) extend to mobile platforms the head control interaction techniques that are often employed on desktop computers and (b) broaden the set of head and face movements that can be employed.<br\/><br\/>Project deliverables include: (a) HAT-MD Algorithms: Next-generation imaging sensors will be used for detection, tracking, and 3D reconstruction. Head position and facial features will be detected, tracked, and mapped to interface controls. Computation will be distributed between the mobile devices and cloud computing services. (b) HAT-MD Applications: Head activated interfaces will be developed to address the challenges that individuals with physical disabilities have when using contemporary mobile platforms. (c) HAT-MD Evaluations: Rigorous human subject studies will evaluate the effectiveness and ease-of-use of the general HAT-MD methodology as well as specific interface and applications that are developed. Algorithms will be benchmarked to current state-of-the-art detection, tracking, and reconstruction methods.<br\/><br\/>Broader Impacts: The increased accessibility provided by the HAT-MD project will create new opportunities for people to interact with mobile devices, especially for individuals with physical disabilities who currently have limited independent control of such devices. The project will also feature direct involvement by individuals with disabilities at the research, development, and evaluation phases in order to focus the intellectual development in a truly useful direction that will complement the broader impact.","title":"HCC: Small: Head Activated Technology for Off-the-Shelf Mobile Devices","awardID":"1319598","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[550477,550478],"PO":["565362"]},"202193":{"abstract":"This project develops an alternative architecture for large-scale text search in which the document corpus is decomposed into index shards that are expected to have skewed utility distributions, thus enabling most index partitions to be ignored for most queries. This selective search architecture is as effective as conventional search engine architectures, but has far lower computational costs and reveals new challenges and opportunities in large-scale search. The decomposition process creates text collections, thus inviting research on what characteristics are desired or to be avoided in a text collection to enable accurate search. New resource selection algorithms are developed to address efficiency problems in existing algorithms and dynamically adjust search costs based on query difficulty. The project includes collaboration with three research groups at other universities, to help their research, leverage their expertise in designing new approaches to problems, and investigate the effectiveness of our research in more varied situations. The result is an \"off-the-shelf\" method that provides an order of magnitude reduction in search costs over the current state-of-the-art, especially on corpora of more than a billion documents, and that can be easily customized or extended to support varied needs.<br\/><br\/>Selective search is significant in part because it provides a new perspective on how to organize a very large collection of documents so that it can be searched accurately and efficiently. This new understanding reveals new research problems and undiscovered weaknesses in existing algorithms that will have impact within the scientific community. Text search is one of the most widely used computer science technologies; hence selective search is of practical significance. The state-of-the-art in many areas of industry and science is increasingly associated with large-scale datasets, which makes it difficult for organizations with modest computational resources to compete. This project reduces the computational costs of searching large-scale text collections by an order of magnitude or more. It has the potential to reduce the energy and other costs associated with the data centers of large search providers, which has important economic and societal benefits. Research results from this project are disseminated via project web site (http:\/\/www.cs.cmu.edu\/~callan\/Projects\/IIS-1302206\/); in research publications; in the Lemur Project's open-source search engines, which are used by a broad international scientific community; and in the Lemur Project's ClueWeb public search services, which integrate research and education by enabling scientists and classroom students to do experiments on large, state-of-the-art text corpora.","title":"III: Medium: Selective Search of Large-Scale Text Collections","awardID":"1302206","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[541869],"PO":["563751"]},"206450":{"abstract":"A major factor influencing learning is students' emotions and their general affective state. Given the pivotal role that affect plays in learning activities it is not surprising that there has been a good deal of interest in developing affect-aware technologies. The overwhelming majority of this work, however, has focused on modeling affect, that is designing computational models capable of inferring how students are feeling while interacting with an Intelligent Tutoring System (ITS). While modeling of affect is a critical first step in providing adaptive support tailored to students' affective needs, very little work exists on systematically exploring the impact of affective interventions on students' performance, learning, affect and attitudes, i.e., how to respond to students' emotions such as frustration, anxiety, boredom, and hopelessness as they arise. The research fills this gap by analyzing the value of tailoring different types of interventions to negative affective states for individual students and groups of students. <br\/><br\/>The project has two main goals. First, it addresses how to respond to negative student emotion such as frustration, anxiety, boredom in computer-based learning environments through a variety of interventions. Some correspond to specially-designed digital characters, integrated into the learning environment, which are intended to act like students' learning companions. These agents support students through non-verbal behaviors such as having the characters express empathy in response to student frustration, or by messages targeting students' cognitive and meta-cognitive skills, as well as motivation and affect. Other interventions involve supporting collaboration between students to mitigate negative emotional states when detected. The impact of these interventions are investigated through a series of eight experiments with a total of 800 students. These experiments help to unveil general prescriptive principles to address student affect. <br\/><br\/>The second goal accomplished by this research is that the experiments provide valuable data to continue to extend and validate existing models of emotion. Specifically, the project triangulates and integrates a complex space of partially overlapping models and constructs of affect in learning (i.e. emotions, attitudes, incoming moods, motivation, engaged use or misuse of software). The project refines several well-established models, in particular the control-value theory of emotions to provide a more stable theoretical framework for the field of emotions in educational software. <br\/><br\/>This research is unique and ground breaking, as few researchers have targeted students' emotion in classrooms, gathered fine grained data on emotions during learning, or assessed the impact of specific affective treatments on a moment-to-moment basis. Students using the tutoring systems have already shown statistically significant gains and learning outcomes, as well as increased positive affect and attitudes. The new affective interventions will greatly increase the broad impact of these systems.<br\/><br\/>This project is developing: (1) prescriptive principles about how to respond to student affect; (2) new understanding about the impact of cognitive, affective, and meta-cognitive interventions on emotions and learning; (3) new understanding about individual differences in learning, unveiling the extent to which emotion, cognitive abilities, and gender impact learning; (4) instruction that is sensitive to individual differences; and (5) refined theories of student emotion.<br\/><br\/>This research is also: (1) increasing participation in mathematics of underrepresented populations (women and minorities) who often avoid STEM careers; (2) creating broad access to web-based technologies that help to engage more students by addressing their affective and social needs; and (3) addressing the one-size-fits-all approach to education, by responding to individual student needs with alternative representations of content and pathways through which material is presented.","title":"DIP: Collaborative Research: Impact of Adaptive Interventions on Student Affect, Performance, and Learning","awardID":"1324385","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[552730],"PO":["565121"]},"205493":{"abstract":"The rapid advancement of techniques for secure computation on protected data offers a major incentive for development of tools for general-purpose secure computation that protects data privacy, as opposed to computation of specialized tasks. The recent emergence of cloud computing and the need to protect privacy of sensitive data used in outsourced computation serves as another major motivation for this work. With this in mind, this project targets at developing a compiler suitable for privacy-preserving execution of any functionality specified by a user program. Our compiler transforms a program written in an extension of the C programming language, where data to be protected are marked as private, into its secure distributed implementation in C, which can be consecutively compiled into a binary executable by a native compiler. The goal of the project is to preserve all features of the programming language and enable efficient implementation of user programs using a number of optimization techniques. The secure distributed computation is implemented using secure multi-party techniques on secret-shared data and is suitable for secure collaborative computation and secure computation outsourcing.<br\/><br\/>These techniques will allow for more secure cloud computing, even in the presence of attackers. Other broader impacts include outreach to female middle school girls to increase interest in computer science, a summer school program for graduate students on secure computation in the cloud, and involving students from a nearby predominantly African-American university in the research program.","title":"TWC: Small: Techniques and Tools for General-Purpose Secure Computing and Outsourcing","awardID":"1319090","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550254],"PO":["565327"]},"207572":{"abstract":"The research objective of this award is to develop systematic understanding of the performance and limitations of local algorithms for combinatorial optimization problems on random networks. The scale of modern technological, communication and social networks renders many computational tools and concepts too impractical to meet the contemporary computational speeds. In light of this challenge, the research on network computational models has recently focused on local algorithms paradigm. One of the most common generative models for real life networks is the random graph model. Thus the research focuses on the performance and limitations of local algorithms for random graphs. A particular focus is on studying the so-called solution space geometry of combinatorial problems on random graphs and its implications for the design and analysis of local algorithms. It is now known that certain optimization problems undergo a phase transition property, dubbed shattering, described as splitting of the space of feasible solutions into many disconnected components. Many attempts to construct algorithms which perform beyond this phase transition point did not succeed. Thus the shattering property is conjectured to be the main \"culprit\" for the non-existence of local algorithms beyond the phase transition point. A recent work of the PI establishes the non-existence of local algorithms for some optimization problems beyond this phase transition point, thus establishing for the first time the direct link between the shattering phase transition property and the performance of algorithms. The research focuses on the systematic study of this fascinating link between the phase transition property and the design of algorithms. <br\/><br\/>If successful, the results of this research will provide a deep connection between the performance and algorithmic complexity of local algorithms and structural properties of random networks. The research agenda is truly interdisciplinary, lying at the crossroads of several fields. The activities draw on tools from a variety of disciplines, including the theory of algorithms, combinatorics and graph theory, applied probability and statistical physics, thus bringing together ideas from a diverse set of communities. The results of this research will be disseminated in top journals and conferences in the respective fields. Research seminars will be conducted to introduce students to the topic of algorithms and computations on networks.","title":"Local Algorithms for Random Networks: Power, Limitations and Applications","awardID":"1335155","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"5514","name":"OPERATIONS RESEARCH"}}],"PIcoPI":[555822],"PO":["565139"]},"205273":{"abstract":"The research objective of this award is to develop a set of advanced engineering principles for design of the exoskeleton systems that reduce the internal musculoskeletal forces and oxygen consumption in a person during a variety of laborious tasks in various workplaces. The hypothesis is that these systems will decrease the severity and number of work-related back injuries while enhancing worker safety in automobile assembly plants and distribution centers. The research approach progresses from the exploration of the human-machine interaction when both the flow of power and information between the device and human dictate the overall functionality and performance of the device. Employing the dynamic models of various elements in this human-machine interaction, passive impedances and actuators will be developed for the exoskeletons. A set of experiments will be conducted to quantify the effect of the exoskeleton system on human trunk in sagittal plane, in frontal plane, and rotation around the spine. <br\/><br\/>If successful, the technologies proposed here will manifest in the development of broad classes of exoskeleton devices for workers who repeatedly move objects in factories, warehouses and distribution centers. This project will yield a set of engineering principles that decrease the risk of injuries due to repetitive maneuvers in distribution centers and in auto assembly plants. This project further will increase the availability of affordable assist systems for workers and improve the quality of life for workers. The educational impact of this research is derived from the proposed concerted effort to integrate education with research. Graduate and undergraduate engineering students will benefit through involvement in the research. The prototypes developed in this research project will enrich educational platforms for the study of design, modeling, and control of assist devices interacting with humans.","title":"NRI: Small: Reducing Trunk Musculoskeletal Forces During Manual Work","awardID":"1317978","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549674,549675],"PO":["563744"]},"209640":{"abstract":"Past research investigating the human side of cybersecurity has treated people as isolated individuals rather than as social actors acting within a web of relationships and social influence. We are investigating how social influence can and does affect cybersecurity. More specifically, we are studying how people learn about new security behaviors from their social network, using both qualitative and quantitative methods.<br\/><br\/>We are also developing and evaluating novel techniques for leveraging social influences to improve people?s awareness and knowledge of cybersecurity, as well as motivation to act securely. These techniques make use of established principles from social psychology for influencing people, specifically similarity and social norms. We are applying these same ideas towards influencing people?s attitudes and behaviors towards cybersecurity.<br\/><br\/>Success in this work will open up a new dimension to cybersecurity and add to the toolbox for researchers and practitioners, in terms of applying known ideas from an established field in a new context to improve cybersecurity. Our work will also improve the community?s understanding of how people learn their cybersecurity behaviors (of which currently there is little literature), and will lead to a rigorous evaluation of how effective two different social mechanisms are in influencing people?s awareness, knowledge, and motivations towards acting more securely online.","title":"EAGER: Social Cybersecurity: Applying Social Psychology to Improve Cybersecurity","awardID":"1347186","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561942,561943],"PO":["565327"]},"209893":{"abstract":"Innovative mobile technologies offer interesting opportunities in many domains, such as health care, transportation, and commerce. They enable distant monitoring and permit consideration of parameters such as patient's and physician's mobility. This makes it possible to develop novel applications, such as mobile health services for telemedicine and assisted ambient living (particularly in rural areas) and mobile traffic services. Nevertheless, the amount of data to be generated and queried is very large and diverse and is collected from multiple sources. The combination of big data and mobility leads to a major challenge: how to efficiently process queries from a myriad of mobile devices on a large amount of data, especially when the data are to be stored in a novel data management system supplied by several cloud providers with possibly different pricing models? To solve this challenge, this project develops novel mobile cloud data management architectures and novel query processing algorithms that leverage mobile users' storage and computation power and take mobile users' mobility, disconnection, energy limitation, and cloud service providers' pricing models into consideration in order to improve query response time, while reducing the amount of money that must be paid to the cloud service providers. The research is evaluated using both real and synthetic datasets by means of prototyping.<br\/><br\/>The project is an international collaboration effort between the University of Oklahoma (OU) and Blaise Pascal University in France. For research, both universities participate in the design, prototype and evaluation of the architectures and algorithms. For education, via Skype, OU provides lectures on mobile and big data management for the Big Data Management course at Blaise Pascal University, while Blaise Pascal University provides lectures on cloud data management for the Advanced Database Management course at OU. The students in both courses participate in testing the constructed prototype as a part of their class assignments. The project makes important impacts not only on research but also on education as it provides training for graduate and undergraduate students in the areas of critical national needs: cloud and mobile database management systems, big data and high-end computing. The developed architectures, algorithms, prototype, datasets and performance evaluation results are made available to the public at the Website: http:\/\/cs.ou.edu\/~database.","title":"EAGER: Cost- and Energy-Aware Query Processing in Mobile Clouds","awardID":"1349285","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[562677],"PO":["565272"]},"209200":{"abstract":"This INSPIRE award is partially funded by the Networking Technology and Systems Program in the Division of Computer and Network Systems in the Directorate for Computer Information Science and Engineering; the Mathematical Physics Program in the Division of Physics in the Directorate for Mathematical and Physical Sciences; and the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.<br\/><br\/>Two grand challenges in two disjoint fields of science are network dynamics prediction and quantum gravity theory. The lack of understanding of fundamental laws driving the dynamics of many complex networks, and consequently our inability to predict and control their behavior, are among the reasons why many practical problems of great significance remain unsolved for decades. The lack of a complete theory of quantum gravity, unifying all the fundamental interactions, is perhaps the most fundamental problem in physics after Einstein. Motivated by the recent results suggesting that the two problems might in fact be intimately related via a geometric duality between hyperbolic and de Sitter spaces---the former reflecting the latent geometry of complex networks, the latter representing the asymptotic geometry of spacetime in the universe---this project addresses both grand challenges and advances science in both fields by deriving fundamental laws of network dynamics. <br\/><br\/>The main hypothesis that the project investigates is that one can extend and apply the canonical approach in physics used to study all the fundamental interactions in nature to a wide class of physical systems---complex networks. One part of the project focuses on finding Hamiltonians defining Hamilton's equations of network dynamics, and validating the derived equations against the dynamics of real networks. These equations are expected to be simpler than the corresponding equations in gravitational theories. Other parts of the project focus on building tools to predict this dynamics based on the derived equations, and investigating connections between this dynamics, its conformal invariance in the de Sitter\/conformal field theory correspondence (dS\/CFT) context, and network navigability that may lead to a different interpretation of the dark energy problem in cosmology. This interdisciplinary project combines concepts and methods from mathematical physics and network research by extending the canonical approach in physics to complex networks to advance our understanding of their dynamics, and to explore whether applying theoretical concepts in network science will advance our understanding of dark energy. This project thus opens exciting new research directions by hypothesizing a fundamental connection between Hamiltonian dynamics and network dynamics that until now have been considered completely unrelated. The transformative project thus challenges the conventional wisdom that neither the canonical approach in physics can be useful in studying complex networks, nor network science has anything to offer theoretical physics. <br\/><br\/>Broader Impact: Many problems of broader impacts in science and society are blocked on network dynamics prediction. Examples include disease treatment, drug design, and a variety of link-prediction problems, which are sub-problems of network dynamics prediction. In general, connections between systems as different as the brain, the Internet, and the universe, appeal to general public, foster creative thinking, and attract wider and more diverse circles of students to science and engineering.","title":"INSPIRE Track 1: Geometry and Physics of Network Dynamics","awardID":"1344289","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1287","name":"MATHEMATICAL PHYSICS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[560811],"PO":["565090"]},"208034":{"abstract":"Proposal #: 13-37899<br\/>PI(s): Huan, Jun<br\/> Blumenstiel, Justin P.; Kuczera, Krzysztof; Peterson, A. Townsend; Vakser, Ilya A.<br\/>Institution: University of Kansas<br\/>Title: MRI\/Acq.: Computing Equipment for Supporting Data-intensive Bioinformatics Research<br\/>Project Proposed:<br\/>This project, acquiring a multicore hybrid cyberinfrastructure instrument, aims to enable and empower several lines of research requiring processing and\/or storage of big and complex data. The equipment services critical areas in data and computing intensive bioinformatics research, mainly high-throughput sequencing data analysis, molecular dynamics simulation, cell signaling and molecular recognition, computational chemical biology environmental and evolutionary biology. Providing an open, extensible, and scalable platform to support data-intensive science programs, the instrumentation serves as a testbed for computing and simulation methodology development utilizing advanced multi-core architecture as well as supporting large-scale data analysis needs. The project responds to the clear trend towards data driven science, in which large amounts of information coming from instruments for automated sequencing, high throughput screening systems, and other high volume analysis techniques, is mined and analyzed to look for patterns from which hypotheses can be developed. Along with data-driven science as a source of insights, detailed modeling and simulation of biological structures and processes are becoming indispensable tools to test models, to evaluate experimental methodologies, and to interpret the results. Evaluation and interpretation drive the need across a broad spectrum of research. This acquisition supports the current need and near-term research goals and aims to maintain a forward position in computationally intensive research.<br\/>Broader Impacts: <br\/>The instrumentation impacts the research it enables. It is also well aligned to the institution?s strategic initiative it enables and contributes in seeking other research opportunity initiatives such as BigData and XSEDE (eXtreme Science and Engineering Discovery Environment). The equipment enhances research opportunities for graduate students, increasing opportunities for student to gain experience in parallel computing as part of course work, and establishes a broader high-performance parallel computing base within the institution and the surrounding region. The School of Engineering?s Diversity Programs will help identify qualified students from underrepresented groups and assist with the recruitment and retention for women and minorities possibly with scholarships. The computing resource should have significant impact in the region. Working with the Kansas IDeA Network for Biomedical Research Excellence Bioinformatics core, bioinformatics expertise will be delivered to minority and undergraduate-serving schools.","title":"MRI: Acquisition of Computing Equipment for Supporting Data-Intensive Bioinformatics Research at the University of Kansas","awardID":"1337899","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":[557333,557334,557335,557336,557337],"PO":["557609"]},"208166":{"abstract":"Social networks--that is, collections of individuals linked by inter-relationships--are by now well-known important factors in understanding social and behavioral phenomena. Missing from prior considerations is the fact that these relationships represent sequences of dynamic short-term interactions, where each interaction reflects the concerns of a particular environment. Present survey-based research methods are not able to capture accurate, detailed data on interactions within social networks over long timescales, and yet, such data would likely lead to significant new insights into the contextualized behavior of individuals, the way in which communities emerge, and the flow of information\/transformation within human societies. <br\/><br\/>This project brings together a multi-disciplinary team from sociology, anthropology, criminology, psychology, public health, and education, to identify critical research problems whose resolution would be advanced by the availability of dynamic interaction data. The research requirements of these diverse disciplines will be synthesized by computer scientists, leading to the design of a new cellphone-based system which will be capable of revealing the form and evolution of dynamic interaction networks in a privacy-preserving manner. To evaluate the design, a small-scale prototype will be developed, and applied in a case study exploring the impact of dynamic student interactions on individual academic performance. Potential future iterations of this tool will ensure the confirmed willing participation of study participants by the inclusion of a component requirement to 'opt in' in order to take part in the automated data-collection process.<br\/><br\/>By leveraging recent advances in proximity-based network technology and data science to yield the design of a general-purpose cellphone-based system that can provide much-needed access to dynamic interaction network data, this project furthers a deeper understanding of human societies by enhancing long-term research capabilities across a range of social, behavioral and economic sciences. The case study will validate the system design, while also providing new insights on patterns of interaction in social networks, and their potential impact on student achievement.","title":"Towards A Cellphone-based Infrastructure For Harvesting Dynamic Interaction Network Data","awardID":"1338485","effectiveDate":"2013-09-15","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0401","name":"Division of SBE Off of Multidisciplinary A","abbr":"SMA"},"pgm":{"id":"8068","name":"Data Infrastructure"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}}],"PIcoPI":[557814,557815,557816,557817,557818],"PO":["564343"]},"209167":{"abstract":"Overview:<br\/>This INSPIRE award is partially funded by the Electronics, Photonics, and Magnetics Devices Program in the Division of Electrical, Communications and Cyber Systems in the Directorate for Engineering; the Software and Hardware Foundations program in the Division of Computing and Communication Foundation in the Directorate for Computer & Information Science and Engineering; and the Electronic and Photonic Materials program in the Division of Materials Research in the Directorate for Mathematical and Physical Sciences.<br\/><br\/>SPINTOP will explore, in detail, a new paradigm of automata computing that is based on a non-Boolean, non-Von Neumann architecture and is enabled by the electrical coupling of Spin Torque Nano-oscillators (STNOs). The project's goal is to build a simple, yet clear demonstration, of how this can achieve significant speedups in pattern recognition to match detailed simulations. This project addresses the 'Big Data' challenge with a non-conventional solution that can compute on the fly. The interdisciplinary nature of the INSPIRE program will make it possible to combine magnetic materials research with device design and test and computer architecture development involving three NSF Programs and faculty from Physics, Materials Science and Electronic and Computer Engineering at University of Virginia (UVa).<br\/><br\/>Intellectual Merit:<br\/>SPINTOP will be pushing the frontiers of spintronic devices, circuits and computing architectures. Key to the success of this project will be the demonstration of a novel hybrid STNO that was invented at UVa. Magnetic materials with low magnetization, low damping, high spin polarization, controlled anisotropy are key to the success of the proposed hybrid device. The proposed novel STNO combines the best features of existing devices to provide both pure sine wave output and a large voltage signal. Electrical coupling of arrays of these STNOs will be demonstrated in this project. According to detailed simulations, phase lock will occur when their individual frequencies are close - this phase locking will take place across the array rather than only nearest neighbor locking as has been previously demonstrated using spin wave coupling. This coherent phase locking is the key to the design for an associative memory for pattern recognition. SPINTOP will also develop the modeling tool used to simulate the behavior of the innovative design for STNO arrays. Further development of this tool is key for designing and implementing circuits as it will be integrated with existing codes for performing the modeling, simulation and design of hybrid CMOS-spintronic circuits.<br\/><br\/>Broader Impacts:<br\/>SPINTOP, if successful, will have a big impact on the scientific community by creating a new paradigm for computing, especially pattern recognition. This will help solve many of the Big Data applications that are a grand challenge today. However its impact will be much broader; this project will address many of the key issues facing society today. This project is committed to fostering diversity in the workforce and will support at least one student from an underrepresented group for the duration of the project. As the INSPIRE project will fall under the umbrella of the UVA nanoSTAR Institute and the PI is also the director of nanoSTAR, this project will be closely integrated into the outreach programs and benefit from the resources of nanoSTAR. The SPINTOP faculty and students will participate in outreach visits to Virginia high schools to highlight this new technology. SPINTOP faculty and students will also participate in Nanodays, a highly interactive event held each Spring that brings over 650 local students from K-12 and their parents to UVA to experience some of the wonders of nanoscience and technology. These visitors will see a poster highlighting SPINTOP and will be invited to tour the SPINTOP laboratories. SPINTOP along with NanoSTAR will co-sponsor a club for undergraduates called NeXT (for Nano and Emerging Technologies) that hosts frequent seminars with guest speakers and will be a way for students in many of the nano-related disciplines to connect with one another. SPINTOP will leverage programs that support undergraduate research to recruit undergraduates to participate in summer research in the SPINTOP labs. In collaboration with UVA's Center for Diversity in Engineering, opportunities will be provided to several minority senior high school students to experience research in the SPINTOP facilities. SPINTOP will also have an impact on courses at UVA; there will be at least one new class developed under the auspices of this program. It will be a class on automata based computing which is the computing paradigm that the SPINTOP project will use for mapping applications at the system level. Finally, an existing class on Spintronics taught by the PI will be updated to include several lectures on spin torque oscillators and their application to computi","title":"INSPIRE Track 1: SPINTOP: Spin Torque Nano Oscillator Arrays for \"Big Data\" Applications","awardID":"1344218","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1775","name":"ELECTRONIC\/PHOTONIC MATERIALS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1385","name":"SPECIAL STUDIES AND ANALYSES"}}],"PIcoPI":[560683,560684,"563652"],"PO":["564900"]},"209178":{"abstract":"This INSPIRE award is partially funded by the Oceanographic Centers and Facilities Program in the Division of Ocean Sciences in the Directorate for Geosciences, the Transforming Undergraduate Education in STEM Program in the Division of Undergraduate Education in the Directorate for Education and Human Resources, and the Robust Intelligence Program in the Division of Information and Intelligent Systems in the Directorate for Computer and Information Science and Engineering.<br\/><br\/>This project is aimed at expanding the ways scientists conduct field work from oceanographic research platforms. To do so, the team will assemble a cohort of 12 undergraduate students who will be involved in a telepresence-based research cruise, whose mission is to investigate greenhouse gas exhalation from the seafloor. The students will be active in all phases of the planning, execution and conclusion of the project. The students will be mentored by the PIs as well as a group of early-career scientists. <br\/><br\/>In addition to the ocean science objectives, the PIs will conduct an evaluation of the effectiveness of the activity for recruiting and entraining undergraduate students in STEM. They will also assess the impact the experience has on the learning experience of the students. They will also improve the capability of researchers to utilize increasingly advanced robotic systems for this activity, and in the future. <br\/><br\/>Another important aspect of the project is integrating an ethnography component, defined as the study of the practices through which a community makes meaning. The PIs will compare ship-based versus telepresence interactions, paying attention to where conflicts arise and how they are resolved. They will evaluate the cultural processes involved in human-machine interactions for this activity. They will assist in developing the best practices for this approach to science and help communicate the outcomes broadly.","title":"INSPIRE Track 1: Collaborative Research: Transforming Remotely-conducted Research through Ethnography, Education and Rapidly-Evolving Technologies","awardID":"1344250","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"6899","name":"OCE"}}],"PIcoPI":[560721],"PO":["564339"]},"202260":{"abstract":"To maximize energy efficiency, future mobile devices will include a diverse range of hardware, such as large and small general-purpose processor cores, vector units, graphics processing units (GPUs), digital signal processors (DSPs), and semi-custom and custom accelerator cores. This \"heterogeneity\" could power a new wave of innovation in mobile computing but is blocked by several fundamental challenges. Some of the biggest challenges are that such heterogeneous systems are highly challenging to program; that it is very difficult for software applications that use the diverse hardware to be portable across different mobile devices; that the memory systems in these devices are inflexible and inefficient; and that the semi-custom and custom accelerators are poorly integrated with the rest of the memory system and the programming environments.<br\/><br\/>A key insight behind this project is that a carefully designed hardware abstraction layer --- a \"Virtual Instruction Set\" --- that abstracts away the differences in parallelism and memory subsystems across the different compute units can provide a framework in which all of the above interrelated problems can be solved extremely effectively. The project is developing a framework called Virtual Instruction Set Computing that uses this approach to address the above challenges. The framework uses just two or three models of parallelism and a uniform, rich model of communication to capture the full spectrum of heterogeneous hardware. The hardware memory architecture supports specialized memory sub-systems and novel memory optimizations customized for those sub-systems, while compilers partition the memory used by applications to make use of these partitions; together, these specialization techniques will provide an order of magnitude improvement in memory efficiency. Semi-custom accelerators for the key domain of Machine Learning are driving new programming and memory system design techniques to integrate and use semi-custom accelerators in such systems. The overall research builds on the widely used LLVM virtual instruction set and compiler infrastructure (previously developed by members of this research team), which are already widely used in industry, enhancing the potential for technology transfer from this work. If this project is successful, it can enable far more powerful mobile phones, tablets, and other such devices, and far more advanced software applications that can make full use of the rich capabilities of these devices.","title":"SHF: Medium: Programmability, Portability, Performance and Energy Efficiency for Heterogeneous Systems","awardID":"1302641","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["551063",542045,542046],"PO":["564588"]},"202271":{"abstract":"Machine learning (ML) algorithms have become ubiquitous across applications as diverse as science, engineering, business, finance, education and healthcare. However, development of ML software that can scale to massive datasets and that are also easy-to-use remains a challenge in part due to the fact that developing an ML tool currently requires the implementation of a deep software stack, from the actual runtime (i.e., how an ML algorithm is executed) to the API exposed to the users.<br\/><br\/>This project aims to develop DeML, a system to support the authoring and execution of ML tools. Specifically, DeML would allow ML algorithms to be formulated in the form of a declarative query over the training dataset. DeML optimizes the execution of the query over a computing platform (e.g., Amazon EC2 or SQL Azure), taking into account the characteristics of the algorithm, the data, and the available computational resources. Adoption of DeML would greatly reduce the effort required to develop scalable implementations of ML algorithms. The project is organized around three thrusts: (i) Development of a declarative query language, based on extensions of Datalog; (ii) Analysis of runtime of DeML queries; (iii) Optimization of dataflow of DeML queries based on the characteristics of data sources and the capabilities of the underlying execution platform. The resulting open source DeML prototype implementation will be made freely available to the community through the project web page at: http:\/\/deml.cs.ucla.edu.<br\/><br\/>The availability of the DeML could greatly lower the effort needed to author scalable implementations of ML algorithms for analysis of massive datasets, which in turn would increase the availability of such tools to the broader community. Experience gained by implementing and deploying ML algorithms at scale over modern cloud-computing platforms, could help inform critical design choices in the development of future cloud computing platforms for big data analytics, and hence impact a broad range of scientific, engineering, national security, healthcare and business applications of big data analytics. The project offers enhanced opportunities for research-based advanced training of graduate and undergraduate students, including members of groups that are currently under-represented in computer science, in databases, machine learning, and cloud computing.","title":"III: Medium: Collaborative Research: Scaling Machine Learning to Massive Datasets---A Logic Based Approach","awardID":"1302690","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[542080],"PO":["565136"]},"205791":{"abstract":"A dominant feature of computing today is that parallelism is present in virtually all computing environments, from laptops and desktops to supercomputers. The proposed research will address foundational issues that explore the power of parallelism as well as methods to harness its potential with maximum efficiency. This research has two main components:<br\/><br\/>(1) It will revisit classical complexity theory developed for parallel computing to address new issues of speed-up efficiency, communication costs, and the diversity of parallel architectures, especially the challenges present in moving between shared-memory and distributed-memory environments.<br\/><br\/>(2) It will develop efficient and portable multicore algorithms for many fundamental graph-theoretic problems as well as efficient run-time schedulers, caching and cache replacement strategies, and strategies for dealing with false sharing, an inevitable consequence of the shared-memory environment in a parallel setting that is found in multicores. <br\/><br\/>It will also investigate models and algorithms for supercomputing environments configured as networks of multicores, and for GPU (graphics processing unit) computing.<br\/><br\/>Over the past several decades theoretical computer science has made many fundamental advances in our understanding of parallelism. The project aims to expand the scope of traditional complexity theory to bring communication costs and other key parameters in parallel computation into the fold of complexity theory. It proposes to develop new efficient and portable algorithms for multicores, which are of major importance in the current times, as parallelism enters mainstream computation.","title":"AF: Small: Theoretical Frameworks for Modern Parallel Computing Environments","awardID":"1320675","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550960],"PO":["565251"]},"205571":{"abstract":"Cellular networks are challenged by the limited spectrum available at microwave frequencies. In the past several years, various technologies have been proposed to achieve ultra-high levels of spectral efficiency involving the use of multiple antennas, serving multiple users, and shrinking cell sizes. Many of these technologies have significant potential but do not provide the high data rates due to the limited spectrum available. An alternative is to use the millimeter wave (mmWave) band between 3-300 GHz to provide high bandwidth communication channels. Realizing mmWave cellular communication, however, requires addressing the fundamental difference between mmWave and microwave communication. mmWave networks will require high gain directional antennas at the transmitter and receiver to overcome path loss at higher frequencies and to ensure sufficient signal-to-noise-ratio at the receiver. These directional antennas must be implemented using a combination of analog and digital signal processing techniques, due to power hungry mixed signal hardware. The cellular systems must be designed to support highly directional transmission and reception from their inception, and must also deal with the increased impact of signal blockages. <br\/><br\/>This research project will establish the potential of millimeter wave cellular networks by incorporating the key features and constraints of mmWave communication. To understand the potential of such networks, this project will develop mathematical tools to analyze the performance of large-scale millimeter wave cellular networks, explicitly incorporating directional antennas and blockages. To enable robust communication, this project will create communication strategies that are simultaneously suitable for the harsh millimeter wave propagation environment and the limited capabilities of millimeter wave hardware. To leverage potential coexistence with lower frequency signals, this project will investigate the design and analysis of a hybrid-access system that exploits the coexistence of microwave and mmWave cellular systems. A main theme in the research thrusts is to incorporate key features of mmWave communication into the algorithms and analysis. <br\/><br\/>Broader impacts of the project's theory, algorithms, and architectures are expected in diverse areas. The mathematical tools developed in this research will impact the design and realization of cellular networks with directional communication. The signal processing algorithms based on array processing and stochastic geometry will pave the way for a new understanding of millimeter wave wireless communication. Industry impact will occur through the Wireless Networking and Communications Group (WNCG) industrial affiliates incorporating the results of the research into their wireless networking technologies. The project will foster the training of graduate students in course projects and will reach out to the community through public demonstrations.","title":"CIF: Small: Realizing Millimeter Wave Communication Systems","awardID":"1319556","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[550451],"PO":["564924"]},"205582":{"abstract":"Sequence and order are prevalent in data. In particular, time, as often perceived as the \"fourth dimension\", plays a pivotal role in data streams as well as in stored time series data. The ability to define events, detect and query complex events over noisy sequence data is a much needed addition to the standard data management systems and SQL. This is because complex event detection is about discovering the correlation and co-occurrence of many tuples in the same relation - different from the cursor model in SQL, and because it is sensitive to sequence errors. The goal of this research is to tightly integrate SQL queries and complex event detection over noisy streams and stored tables in a high-performing system. Specific techniques include: (1) Using a query suite to encapsulate the interactions between SQL queries and complex event detection, and proposing a comprehensive semantics for complex events; (2) Incorporating correlated error models into a novel complex event matching algorithm; (3) Designing more efficient algorithms that only get the top-k highest probability matching paths, as well as algorithms that handle other variants of semantics; and (4) Devising several indexing mechanisms for stored sequence tables, and performing overall system optimization for both streams and tables.<br\/><br\/>The QUEST project has applications in dietary and exercise monitoring\/analysis, ECG monitoring, shopping behavior study, security monitoring, and travel pattern discovery, among many others. These applications play a key role in the nation's economy (business intelligence), health, and security. The project greatly enriches the course contents of the undergraduate directed research course in addition to a new graduate course. Further information on the project can be found on the project web page: <br\/>http:\/\/www.cs.uml.edu\/~ge\/projects\/quest\/ .","title":"III: Small: QUEST: An Integrated Query and Event System on Noisy Streams and Tables","awardID":"1319600","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550480],"PO":["563727"]},"208992":{"abstract":"As the digital divide closes and more groups have access to the Internet, cybersafety is an increasingly relevant issue. With the connectivity provided by the Internet comes an inherent interdependency between devices connected to the Internet. Compromised devices are used in various forms of cybercrime (e.g., spam campaigns) that can affect the Internet as a whole. Thus, to increase cybersecurity, it is important to understand the ways and extent to which certain groups may be more susceptible to Internet crime. Previous studies have focused on how factors such as age, gender, occupation, and level of STEM background affect one?s susceptibility to Internet crime (i.e., phishing attacks), however little work has focused on how social class factors into Internet crime susceptibility. <br\/><br\/>This project focuses on understanding new sociological factors that may render particular groups more or less likely to be affected by cybercrime (in particular, phishing attacks that highlight opportunities for economic advancement). <br\/><br\/>The expected results from this project impact both the fields of cybersecurity and sociology. From a cybersecurity perspective, understanding whether particular demographics are disproportionately affected by online attacks can lead to: 1) efforts for prevention (e.g., Internet Service Providers that offer government subsidized broadband access can also provide cybersafety training); and 2) the identification of the spawning point of a cybercrime ecosystem, which if caught and addressed early, can make cyberspace at large more secure. Sociologically speaking, attention to whether social class influences susceptibility to cybercrime helps to highlight some of the previously understudied consequences of economic inequality.","title":"EAGER: Collaborative: Winning the Internet Lottery: Growing Income Inequality, Social Class, and Susceptibility to Cybercrime","awardID":"1343258","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[560166],"PO":["565327"]},"205241":{"abstract":"The objective of this research is to develop algorithms that can make robots and simulated characters move like humans. A range of dynamic locomotion tasks including walking, running, getting up and climbing, as well as task variations such as walking backwards, and concurrent tasks such as holding a cup of water while walking, will be studied. The approach is based on optimal control theory. Human movements will be analyzed, and the performance criteria with respect to which they are optimal will be identified. Algorithms that optimize the same performance criteria will then be developed.<br\/><br\/>Intellectual merit: Movement analysis will be based on a new mathematical framework where inference of performance criteria from observed movements becomes a convex optimization problem. Control synthesis will exploit new algorithms for real-time optimization which are able to plan long movement sequences involving multiple contact events. These algorithms rely on novel formulations of the physics of contact which are more amenable to numerical optimization, as well as a new physics simulator which exploits advances in parallel processing.<br\/><br\/>Broader impact: This research will change how robots and simulated characters move. Currently many robotic control systems with the appearance of dynamic movements are controlled in open loop, or are designed to execute one specific task. This work will enable robots to express more natural and versatile movements, as well as make robot programming more automated. The resulting controllers will also serve as models for human motor control.","title":"NRI: Small: Dynamic Locomotion: From Humans to Robots via Optimal Control","awardID":"1317702","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549589,549590],"PO":["563109"]},"209861":{"abstract":"This award provides funding for the travel and accommodation expenses for graduate students in computational linguistics whose papers have been selected for presentation at the Sixth International Conference on Generative Approaches to the Lexicon (GL2013) conference in Pisa, Italy, in September 2013. The overall goal of the GL conferences is to bring together researchers in theoretical and computational linguistics, computer science, cognitive science, and lexicography to explore the problem of semantic compositionality -- how the meaning of expressions in natural languages derives from the structure of the lexicon, or dictionary, of semantic formatives (words or idiomatic multi-word expressions) in a natural language. GL2013 specifically aims at exploring the relation and potential synergies between generative approaches, which assume that semantic formatives are structured objects, and distributional semantics, whose proponents typically assume that they are internally unstructured and analyze their semantic contribution by means of their distribution in linguistic contexts. <br\/><br\/>This award enables talented students working on computational semantics to interact with professionals in a variety of fields and perspectives related to that domain, and to present their work in a major international venue. This in turn will help to nurture a lasting interest in the upcoming generation of computational researchers in what is likely to remain a major area of interdisciplinary study for a very long time.","title":"Outstanding Student Research at GL2013","awardID":"1348830","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1311","name":"LINGUISTICS"}}],"PIcoPI":[562587],"PO":["565215"]},"205274":{"abstract":"The broad research goal of this project is to better understand the mechanics, balance, and energetics of bipedal walking. The first specific goal is to design, build and test a two-legged walking 'platform', a waist-down robot, and its computer controller. One can think of this platform as a legged Segway, for use by others putting on robotic upper bodies with arms and heads. The goal is to match the robustness of existent bipedal robots but with no need for electrical cords or hydraulic tubes and with approximately a 90 percent reduction in energy use. This will be achieved by the development and use of a reflex-based controller that minimizes the needed sensor bandwidth and on-board computation. The second goal is to advance the theories of walking balance and energetics including, especially, basic models for understanding the tradeoffs between robustness, versatility, energy efficiency, sensor accuracy and motor size. Deliverable outcomes include a demonstrable robot with a new 'reflex-based' controller, public access to complete design and control information, basic design formulas for the fundamental design trade-offs, student design and research experience, and inspiring demonstrations for public viewing.<br\/><br\/>If successful, robots based on this platform design could replace wheel and tread based systems used for, and proposed for use for, service robotics. The advantages of legs over wheels include a more anthropomorphic look, for human interaction, and, ultimately with further development, the ability to function on rough terrain. The basic theories of balance and energetics developed in this work will add insight concerning human walking, ultimately helping with the diagnosis and remediation of human disabilities, including aiding the design of assistive and prosthetic devices. The hands-on development work will educate and inspire engineering students working on the project. The demonstrations and demonstration videos will serve as an inspiration to those considering engineering careers.","title":"NRI: Small: Reflex approximation of optimal control for an energy-efficient bipedal walking platform","awardID":"1317981","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549677],"PO":["563744"]},"205395":{"abstract":"The computing community is increasingly facing the challenge of processing and interpreting huge amounts of data that is being generated all around us by personal devices, digital sensors, data centers, scientific studies, and social networks. The nature of this data is usually high dimensional, that is, when interpreted as points, they reside in an high dimensional Euclidean space. The curse of `dimensionality' together with `size' puts up a formidable challenge to decipher knowledge from them in a principled way. Among the various approaches proposed to `mine' this data, topological approaches are emerging as robust and global methods that could complement the other approaches, be it statistical or geometrical. How can one deal with the difficulty of `big' data by developing new algorithms in computational topology and geometry is the focus of this proposal. The proposed research aims to investigate three techniques, namely, subsampling, localization, and simplicial collapse to handle the menace of `size' and `dimension'. This requires developing innovative computational tools grounded in algorithmic theory and mathematics from algebraic topology, analysis, and discrete geometry.<br\/><br\/>The proposed topological methods would enhance the understanding of `big' data in general which could originate from images in the medical fields, videos of some events, trends in finance, or connectivities in social networks. The goal is to produce practical algorithms that can be turned into usable software which would be useful for both academia and industry. Other than standard academic forums, there are plans to disseminate the results from this project through course notes, tutorials, and web-pages to reach wider audience. The PI also plans to develop course materials on topological data analysis that will include results obtained in the project. The support from the project will train graduate students. Efforts will be made to recruit students from under-represented groups.","title":"AF: Small: Topological Data Analysis for Big and High Dimensional Data","awardID":"1318595","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":[549998],"PO":["565157"]},"205175":{"abstract":"The overarching scientific goal of this project is two-fold: (1) to develop a robotic architecture endowed with moral emotional control mechanisms, abstract moral reasoning, and a theory of mind that allow corobots to be sensitive to human affective and ethical demands, and (2) to develop a specific instance of the architecture for a co-robot mediator between people with \"facial masking\" due to Parkinson's disease (PD) that reduces their ability to signal emotion, pain, personality and intentions to their family caregivers, and health care providers who often misinterpret the lack of emotional expressions as disinterest and an inability to adhere to treatment regimen, resulting in stigmatization. To tackle these problems, the project brings together two roboticists with extensive prior experience in robot ethics and modeling emotions as well as implementing them in integrated autonomous robotic systems. The robotics expertise is combined with that of an expert in early PD rehabilitation and daily social life. The project will build on extensive software, hardware and data set resources, including complex robotic control architectures with ethical control mechanisms, personality and emotion models, and affect and natural language capabilities.<br\/><br\/>The general expected outcome of the project is an architecture for co-robots that can be adapted to a great variety of health care scenarios in an effort to enrich and dignify already stressed and stigmatized relationships between humans. The project also includes novel educational efforts such as a course in occupational therapy robotics as well as significant K?12 outreach through the Tufts Centers for STEM Diversity and for Engineering Education and Outreach, as well as various important community and public activities such as presentations on health care robotics to focus and patient groups.","title":"NRI: Small: Collaborative Research: Don't Read my Face: Tackling the Challenges of Facial Masking in Parkinson's Disease Rehabilitation through Co-Robot Mediators","awardID":"1317214","effectiveDate":"2013-09-15","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549387],"PO":["564069"]},"209531":{"abstract":"The IEEE Security & Privacy Symposium is the premier venue for cybersecurity research, and has been for much of its 30-plus-year history. Student attendance at S&P is an important part of graduate education in cybersecurity. The 2014 Symposium will consist of approximately 40 technical papers, a keynote speaker and a series of work in progress presentations. In addition to the core research papers, the symposium will also feature Systematization of Knowledge papers, which provide valuable background to students who are learning the literature.<br\/><br\/>This grant provides travel support to encourage participation in this IEEE Symposium by students who would normally find it difficult to attend. Criteria for selection include evidence of a serious interest in the field, as demonstrated by coursework and\/or project experience. Organizers will make particular efforts to encourage participation of women and under-represented minorities.","title":"Student Travel Support for the IEEE Security and Privacy Symposium 2014","awardID":"1346555","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561672],"PO":["565327"]},"209300":{"abstract":"The 9th International Conference on emerging Networking EXperiments and Technologies<br\/>(CoNEXT) will take place from December 9th to 12th in Santa Barbara, CA. CoNEXT is a highly selective (<20%), high-quality conference sponsored by the ACM Special Interest Group on Data Communication (SIGCOMM). The conference is a major forum for presentation and discussion of novel networking protocols and technologies. <br\/><br\/>The proposal requests funds to partially support the travel of 20 US-based graduate students. The support amount will be $1,000 or the actual documented amount of expenses, whichever is less. Each student will need to cover expenses in excess of the fixed amount from other funds. If students spend less than the $1,000 limit the surplus funds will be used to support students from under-represented institutions. Preference in the award of the travel grants will be given to students who have their work accepted to the CoNEXT student workshop which will take place the first day of the conference. Other criteria will include evidence of a serious interest in networking, as demonstrated by coursework and\/or project experience. <br\/><br\/>Broader Impact: <br\/>This project integrates research and education of students through exposure to a premier technical meeting in computer networks and communications. Students will have the opportunity to observe high-quality presentations and interact with senior researchers in the field both in the main conference and the associated workshops. The PI will especially encourage women and under-represented groups to participate.","title":"Proposal to Support Student Travel for the ACM CoNEXT 2013 Conference","awardID":"1345158","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[561112],"PO":["564993"]},"209542":{"abstract":"The proposed grant will promote the design, development and analysis of embedded and networked sensor technologies, systems, and applications by encouraging student participation in the ACM Sensys 2013 conference. Besides technical paper presentations, Sensys, through this grant, will be able to stimulate several intellectual activities by students. Furthermore, the conference offers several opportunities for intellectually stimulating discussions between the students and researchers from around the world. <br\/><br\/>The participation of students in Sensys can have an important impact on their development as a junior researcher. The grant will also promote diversity by encouraging and enabling women and other underrepresented minorities to participate.","title":"Student Travel Award for SenSys (The 11th ACM Conference on Embedded Networked Sensor Systems) 2013","awardID":"1346597","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[561708],"PO":["565303"]},"208222":{"abstract":"There is widespread agreement that scientists and engineers should receive training in ethics. But there is no consensus on how best to incorporate ethics into science and engineering education. Little is known about how to achieve and how to assess the desired outcome of helping scientists and engineers to make good ethical decisions. This research and education project addresses a major gap in ethics education: connecting ethics education to ethical decision making. The PI will develop an alternative, gaming approach that seeks to recast the ethics education of scientists and engineers as less a matter of memorizing rules through content delivery systems and more a matter of practicing virtuous behavior in life-like gaming environments. The PI's approach builds on the growing literature on pedagogical uses of games, as well as his own prototype research ethics games developed with prior NSF support. Rather than instructing students in \"the right thing to do\" then testing whether they know \"the right thing to do\", games immerse students in situations in which they must decide what to do. The gaming environment serves as a training ground for developing practical ethical skills and forming habits of character. Games are more true to the ambiguous, dynamic, and complex situations faced by scientists and engineers than any pre-packaged, static, and unidirectional content. In this project, the PI will test the hypothesis that an ethics game can improve ethical decision making. This will entail finalizing three prototype research ethics games, facilitating game play with diverse graduate student populations from multiple institutions, improving the games through an iterative process, assessing game playability and impacts on ethical decision making using a well-validated evaluation tool, and disseminating the games and final research results.<br\/><br\/>Broader Impacts: This project advances knowledge of the least understood area of ethics education: how to assess the connection between ethics education and ethical decision making. By using virtue ethics theory and taking an interactive gaming approach, this project has the potential to transform ethics education for a new generation of scientists and engineers. Assessments will address not only user participation in and satisfaction with the games developed, but also their improvement in developing the skills needed for ethical decision making in real-world contexts. Project results will be disseminated broadly; the games will be made available to the partner institutions and adapted for inclusion in UNT's RCR training program for graduate students.","title":"EESE: Graduate Virtue Ethics Education in Science and Engineering","awardID":"1338739","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7787","name":"EESE"}}],"PIcoPI":[557983,557984,557985,557986,557987],"PO":["565227"]},"209223":{"abstract":"US Ignite was launched in 2012 to foster the development of next-generation apps for public benefit. In the next 5 years, US Ignite efforts are targeting the development of 60 next-generation applications, 200 community testbeds, and a forum for collaboration between industry, academia, and gigabit communities. Because US Ignite services a broad range of stakeholders in next-generation app development, the activities of the partnership are not always clearly visible. US Ignite, and the overall goals of the partnership, are better served by transparent, shared communications infrastructure. <br\/><br\/>This project addresses part of this need by convening a platform design workshop to solicit feedback, ideas, and expertise from key stakeholders and drive the development of a new web-based platform for the development of next generation applications. The platform will serve as a hub for research and best practices in software-defined networking, symmetric gigabit networks, local cloud computing and other advanced networking applications. It will include an index of a variety of advanced networking application projects and demonstrate how these networks can be used. Additionally, by providing opportunities for engagement and collaboration across diverse groups that do not naturally interact, the platform provides possibilities for the discovery of new ideas and potentially transformative applications. During this workshop the participants will: identify and define key priorities of leadership and stakeholders; agree on core user demographics and needs; define the site design (look and feel) and requirements (functionality); identify existing content that can be leveraged to bootstrap the site; and agree to development roadmap and schedule. The goal of the workshop is to prepare fully the development team to operationalize this platform and build a sustainable next-generation apps development community. <br\/><br\/>The workshop will result in a roadmap and core group to produce a platform that will increase participation in the next-generation apps community. It will provide a consolidated gateway to people, software, and documentation. It will enable the small team at US Ignite to scale up this ecosystem and reach a broader audience. It will also decrease the development time from idea to usable application and propel applications out of the lab and into [the] community.","title":"US Ignite Next Generation Applications Hub Design Workshop","awardID":"1344383","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[560884],"PO":["564993"]},"210180":{"abstract":"Vector field data analysis is indispensable for many applications in science and engineering, ranging from climate study, physics, chemistry, automobile design, to medical practice. Most existing analysis techniques for vector field data are not scalable to the real-world data with ever-increasing sizes and complexity. More importantly, the inherent limited visual perception channel largely constrains the ability to understand the complex geometric and physical behaviors of vector fields as a whole or in detail. To address these challenges, this exploratory project investigates a graph-based vector field data reduction for the subsequent extraction of a multi-scale vector field data summary. The summary serves as a condensed, yet informative, representation of the original vector field, supporting data interpretation and interaction and shielding the user from the underlying complexity of the flow dynamics. The key to computing such a summary representation is the construction of a novel, enhanced graph representation that encodes both the global structural information and local characteristics of the vector field, as well as other derived information. The approach focuses on development and validation of critical issues in graph-based vector field data reduction , including; (1) identification of the key information of a vector field for the construction of the enhanced graph: (2) efficient storage of the graph; and (3) new graph algorithms for extracting features of interest from the obtained graph. To address these issues, theories and algorithms from dynamical system, algebraic topology, tensor calculus, information theory, and graph theory are extended and integrated in a novel framework. To validate the approach, the PI is working closely with domain scientists from mechanical engineering and aerodynamics to receive advice on the representation of the summary and its utility in specific applications. <br\/><br\/>The expected results in vector field summary represents will yield an important addition to the existing summarization techniques for various data forms. The analysis and abstraction are based on the enhanced graph and can enrich the conventional graph theory and graph algorithms. The ability to handle both steady and unsteady vector fields improves the theory and practice of dynamical systems in describing fluid dynamic phenomena, benefiting a wide variety of disciplines. Knowledge learned from the vector field summarization can be adapted to the study of summarized representation of more complex geometric data, such as tensor field data. In addition, the research on vector field summary represents one step towards a unified framework of knowledge discovery and integrity from heterogeneous data forms. The developed techniques are expected to be implemented as a software tool that will be applicable in a wider range of scientific and engineering domains. Furthermore, the new theory stemming from this work is expected to enrich the existing education on data analysis and visualization, enabling the development of new courses at both undergraduate and graduate levels in many academic disciplines. The project web site (http:\/\/www2.cs.uh.edu\/~chengu\/vf_summary\/vf_summary.html) will provide access to project results, including developed software tools.","title":"EAGER: Define and Construct an Enhanced Graph Representation for Multiscale Vector Field Data Summarization","awardID":"1352722","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[563289],"PO":["563751"]},"210191":{"abstract":"This project advances a scientific understanding of how \"socio-technical capital\" - ties that are created, maintained, or exploited through the use of information and communication technologies - is developed and used across different socioeconomic groups and populations. It is hypothesized that, like other valuable resources, the benefits of computer-mediated opportunities for building socio-technical capital are unequally distributed in society. For example, it appears as if far more effort has gone into building social networking tools and online markets for highly paid professionals (such as LinkedIn) than for handymen or day laborers. And online labor markets that exist for low-skill, low-commitment jobs (such as Amazon Mechanical Turk) do not appear to offer a path toward building socio-technical capital that might lead to more-stable, higher-wage jobs.<br\/><br\/>This project seeks to understand the prospects of tailoring the technologies of social networking tools and online labor markets to meet the needs of specific socio-economic populations, such as populations in Detroit, Michigan, and other cities in economic decline. The project will follow a human-centered approach of contextual inquiry, conducting interviews and focus groups employing a range of \"design probes\". These probes will examine technologies that currently support the employment process (such as LinkedIn, CareerBuilder, ODesk, TaskRabbit, and Angie's List) as a springboard to identify fundamental barriers to usage, and also to generate ideas for features that might be especially useful. In later sessions, design probes will include low-fidelity prototypes embodying features generated in earlier sessions. The end result will be an articulation of the special needs, barriers, and opportunities for using technology to help people in economically vulnerable communities to build, maintain, and use social capital to start moving up the economic ladder. The project will investigate how information and communication technology can help to create and maintain of social and economic bridges between individuals within specific economic communities and people who can provide access to employment opportunities outside of those communities.<br\/><br\/>Broader Impacts: The project will have broad social impact by informing approaches to cultivating pathways to upward mobility in communities hit hardest by economic decline. If the research finds promising opportunities and surmountable barriers to the use of social networking tools and online labor markets, it will inform the design of future technologies and computer-mediated approaches to help these populations better prosper. If the research finds limited opportunities or insurmountable barriers, practitioners will know to look elsewhere to help vulnerable populations find essential socio-technical capital.","title":"EAGER: Identifying Barriers and Opportunities for Building SocioTechnical Capital","awardID":"1352915","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563312],"PO":["565362"]},"200060":{"abstract":"Materials change their properties in response to the environment in ways that are often detrimental to performance. However, a new generation of \"smart materials\" capitalizes on environmental responsiveness to improve performance in the laboratory. This project investigates how such \"smart materials\" evolve and function in nature, focusing on how adhesives adapt to fluctuating humidity in the environment. Spider capture threads are sophisticated composite structures that generate adhesion through multiple mechanisms that could respond synergistically or independently to the environment. Adhesion starts with the surface contact of sticky glycoproteins that are encased in liquid glue droplets and is then enhanced when those droplets and the underlying axial thread to which they are attached stretch and resist thread \"pull-off\" by forming a broad, suspension bridge-like interface. This whole process is controlled in part by cocktails of salts in the glue droplets that absorb atmospheric water, which then lubricates the glue and controls the extension of the droplets and axial threads. This project combines biology and materials science to compare the molecular compositions of glues, the adhesiveness of capture threads, and how webs capture insects across a community of spiders to understand how biological \"smart\" materials respond to their environments. It will inspire the development of new synthetic adhesive systems and \"smart\", environmentally responsive materials. The project will provide in depth training in interdisciplinary research to PhD, undergraduate and high school students. It will also develop new educational resources for K5-K12 summer camps and teacher workshops.","title":"Collaborative Research: Performance and Evolution of Environmentally Responsive Biomaterials in a Unique Biological Adhesive System : Spider Orb Web Capture Threads","awardID":"1257809","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7657","name":"ORGANISM-ENVIRO INTERACTIONS"}}],"PIcoPI":[536306,536307],"PO":["564983"]},"206991":{"abstract":"This project is funded as part of the United States-Israel Collaboration in Computer Science (USICCS) program. Through this program, NSF and the United States - Israel Binational Science Foundation (BSF) jointly support collaborations among US-based researchers and Israel-based researchers.<br\/><br\/>In recent years, many genetic studies have been performed, revealing many new associations between human genetic variation and complex diseases. These studies, referred to as genome-wide association studies, are limited to common genetic variants because the technology which collected the genetic variation was limited to only collecting common variants. There is evidence suggesting that rare variants have an important role in disease architectures. Recently, sequencing technologies have been introduced which are capable of collecting both genetic common and rare genetic variation. Sequencing technologies generate enormous amounts of data, raising new computational challenges. In this project, the PIs will develop methods for addressing these computational challenges including the design of efficient algorithms and the modeling of the sequencing process. In addition, the researchers will develop methods for incorporating rare variants into the analysis of genetic studies. The immediate broader impact of our project is the availability of these tools for general use by geneticists, leading to an improved understanding of the disease genetics. Particularly, the PIs will apply their methods to studies of non-Hodgkin's lymphoma, bipolar, dyslipidemia, neurodegenerative dementia, and Tourette syndrome, which will result in a direct impact on our understanding of these particular conditions.<br\/><br\/>Current computational methods for the analysis of sequencing data exist, however they are limited to the analysis of a single sample. In this project the PIs will design efficient computational methods for the analysis of sequence data across a population. For population samples, the tremendous size of the data requires the design of highly efficient algorithms in terms of memory and runtime. Specifically, the PIs propose to design algorithms for the compression of sequencing data, for the search of regions identical by descent across multiple samples, and for high-resolution haplotype inference from sequence data. The PIs will explicitly model rare variants and the sequencing process, and use machine learning techniques and convex optimization to estimate the model parameters efficiently. These methods will allow for a fine-scale analysis of population data, resulting in improved understanding of complex diseases and human history. The collaborative nature of the project will expose the students involved in the project to the medical and genetics worlds, both in Israel and in the US, and it will improve their abilities to design and implement solutions to complex algorithmic problems. The methods developed in this project will be part of the teaching material of courses in UCLA and Tel-Aviv, and these materials will be made publicly available.","title":"BSF:2012304:Methods for Preprocessing Population Sequence Data","awardID":"1331176","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":[554197],"PO":["565157"]},"205781":{"abstract":"Processing geometry efficiently is crucial for many domains including computer-aided design, scientific computing, urban planning, and cultural heritage preservation, to name just a few. As a consequence, the amount of geometric data generated and stored is rapidly increasing. Raw geometric data comes in a variety of forms (e.g., range images, LIDAR data, and volumetric data), and most commonly is converted to unstructured meshes which enjoy many attractive features but inherently limit efficiency and accuracy of algorithms for geometric processing, processing data on surfaces and physical simulation. Conversion of geometry to an image-like, regularly sampled form has been demonstrated to have significant advantages in these contexts. Mesh parameterization is the fundamental technique used both for resampling surfaces on structured grids and for mapping data to surfaces. Global parameterization algorithms can generate seamless tilings of arbitrary surfaces with quadrilateral domains enabling regular resampling everywhere excluding isolated points. The continuing progress in this area has led to increasingly reliable and high-quality algorithms, yet no fully robust automatic method is available yet. The PI's goal in this project is to tackle this problem and to develop fundamental algorithms for global parameterization supported by rigorous analysis, as well as robust and scalable implementations of these algorithms. Specifically, he aims to address the following closely interconnected problems: Quality (optimization of suitably chosen distortion measures, and explicit guarantees on local distortion); Robustness (direct parameterization and resampling of possibly noisy geometric data avoiding intermediate mesh representations, with guarantees on the resulting parameterization structure); and Scalability and Efficiency (to enable meshes with hundreds of millions of sample points to be processed on a desktop computer). Although the PI will build on the substantial recent advances in the area, achieving the stated goals will require rethinking some of the core aspects of the problem. <br\/><br\/>Broader Impacts: This research will lead not only to the development of practical and efficient algorithms, but also to advances in our fundamental understanding of related geometric problems (for example, intrinsic limitations on distortion and geometric resampling). The software the PI plans to develop has the potential to enable new approaches to a variety of tasks in geometric processing, which can be performed on structured grids and in the parametric domain if a robust tool for parameterization can be assumed to be available. The interest in robust and scalable parameterization techniques extends far beyond the domain of geometric processing, to many domains of science and engineering where complex geometric models are used (e.g., large-scale fluid flow simulation in complex geometry, which requires high-order approximation of surfaces extracted from different sources).","title":"CGV: Small: Scalable high-quality surface parameterization and resampling","awardID":"1320635","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[550936],"PO":["565227"]},"206771":{"abstract":"Cyber-Physical Systems (CPS) encompass a large variety of systems including for example future energy systems (e.g. smart grid), homeland security and emergency response, smart medical technologies, smart cars and air transportation. One of the most important challenges in the design and deployment of Cyber-Physical Systems is how to formally guarantee that they are amenable to effective human control. This is a challenging problem not only because of the operational changes and increasing complexity of future CPS but also because of the nonlinear nature of the human-CPS system under realistic assumptions. Current state of the art has in general produced simplified models and has not fully considered realistic assumptions about system and environmental constraints or human cognitive abilities and limitations. To overcome current state of the art limitations, our overall research goal is to develop a theoretical framework for complex human-CPS that enables formal analysis and verification to ensure stability of the overall system operation as well as avoidance of unsafe operating states. To analyze a human-CPS involving a human operator(s) with bounded rationality three key questions are identified: (a) Are the inputs available to the operator sufficient to generate desirable behaviors for the CPS? (b) If so, how easy is it for the operator with her cognitive limitations to drive the system towards a desired behavior? (c) How can areas of poor system performance and determine appropriate mitigations be formally identified? The overall technical approach will be to (a) develop and appropriately leverage general cognitive models that incorporate human limitations and capabilities, (b) develop methods to abstract cognitive models to yield tractable analytical human models (c) develop innovative techniques to design the abstract interface between the human and underlying system to reflect mutual constraints, and (d) extend current state-of-the-art reachability and verification algorithms for analysis of abstract interfaces, iin which one of the systems in the feedback loop (i.e., the user) is mostly unknown, uncertain, highly variable or poorly modeled.<br\/><br\/>The research will provide contributions with broad significance in the following areas: (1) fundamental principles and algorithms that would serve as a foundation for provably safe robust hybrid control systems for mixed human-CPS (2) methods for the development of analytical human models that incorporate cognitive abilities and limitations and their consequences in human control of CPS, (3) validated techniques for interface design that enables effective human situation awareness through an interface that ensures minimum information necessary for the human to safely control the CPS, (4) new reachability analysis techniques that are scalable and allow rapid determination of different levels of system safety. The research will help to identify problems (such as automation surprises, inadequate or excessive information contained in the user interface) in safety critical, high-risk, or expensive CPS before they are built, tested and deployed. The research will provide the formal foundations for understanding and developing human-CPS and will have a broad range of applications in the domains of healthcare, energy, air traffic control, transportation systems, homeland security and large-scale emergency response. The research will contribute to the advancement of under-represented students in STEM fields through educational innovation and outreach. The code, benchmarks and data will be released via the project website.<br\/><br\/>Formal descriptions of models of human cognition are in general incompatible with formal models of the Cyber Physical System (CPS) the human operator(s) control. Therefore, it is difficult to determine in a rigorous way whether a CPS controlled by a human operator will be safe or stable and under which circumstances. The objective of this research is to develop an analytic framework of human-CPS systems that encompasses engineering compatible formal models of the human operator that preserve the basic architectural features of human cognition. In this project the team will develop methodologies for building such models as well as techniques for formal verification of the human-CPS system so that performance guarantees can be provided. They will validate models in a variety of domains ranging from air traffic control to large scale emergency response to the administration of anesthesia.","title":"CPS: Synergy: Collaborative Research: Formal Models of Human Control and Interaction with Cyber-Physical Systems","awardID":"1329878","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["555791"],"PO":["565136"]},"205561":{"abstract":"Virtualized infrastructure is becoming increasingly routine, and so the ability to manage it well is important. Cloud computing is built on virtualized computing infrastructure. Software Defined Networks are virtualized data networks. The combination of virtualized computing and communications constitutes the next wave of innovation in provisioning bespoke infrastructure. This project addresses ways of making such virtualized infrastructure more reliable, by planning and managing the migration of the virtual network structure and associated computation from one set of supporting physical infrastruture to another. Triggers for VN migration include attacks, massive failures, performance collapse or degradation, or changes in the service demands that require an increase or decrease in resource allocation in order to both satisfy users and reduce power consumption. This ability to adapt to network dynamics is referred to as VI migration (VIM). <br\/><br\/>There are many challenges in deploying VIM for the applications above. First, different network agility objectives might require different VIM models and techniques. For example, the trigger to migrate for proactive defense to defeat reconnaissance would need to be time based , whereas, for power management the trigger would be signi&#64257;cant change of network traf&#64257;c load. Second, VN migrations may need to be planned in advance (for example, in the case of proactive defense) but in some cases they cannot be planned in advance and need to be deployed in real time (in response to faults, for example). Third, although most existing theories permit reasoning about correctness of network operation only once as the system stabilizes to a static state, investigating formal frameworks that support migration strategies that are correct-by-construction under continuous movement is a prime research task. To address these challenges this project will develop a VIM architecture that is designed to work in the context of the GENI and PlanetLab virtualization technologies, but which could be used as the basis for other industrial systems. The architecture consists of two main components: a Strategy Synthesizer block and a Migration Mechanism block. The inputs to the Strategy Synthesizer block uses models of the VI and the Migration Mechanism block interacts directly with the substrate to implement the desired migration. <br\/><br\/>VIM is a powerful technique for energy control in Cloud infrastructure and effective tools for managing VIM are expected to facilitate significant savings in IT energy consumption. In addition to the potential commercial and network security impacts of the work, the PIs have identified local and outreach activities that further the participation of women and minorities. The experimental and evaluation aspects of the work will be incorporated as project elements in graduate courses.","title":"NeTS: Small: Collaborative Research: Enabling Network Agility Through Virtualized Infrastructure Migration","awardID":"1319490","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[550428,550429],"PO":["564993"]},"205352":{"abstract":"Genomes evolve and diversify through different mechanisms, including small point mutations, and large structural variations (SV). As entire populations of individuals get sequenced, we observe a complex mosaic of patterns. Some of these are characteristic of a selective constraint such as tolerance to lack of oxygen (for highlander populations), or lactose tolerance. In one aim of the proposal, the investigators develop computational techniques for identifying characteristic genetic patterns to identify genes that are adapting to these selective constraints. The other aims to reconstruct regions with complex variation patterns such as the Killer cell Immunoglobulin-like Receptor (KIR) region. KIR diversity plays a significant role in mediating immune response, helping with an understanding of diseases including rheumatoid arthritis, control of HIV disease progression as well as the success rate of cell replacement therapy for certain leukemias (blood cancer). The investigators will use a mix of techniques from combinatorial algorithms, machine learning, and population genetics to decode the genetic patterns. The proposal has broader impact in the field as part of a larger effort to develop efficient computational tools for genetic analysis; a critical problem in the modern era of inexpensive sequencing. The tools and technologies described here will have a direct impact on understanding the genetic diversity of populations, and towards a personalized approach to healthcare.<br\/><br\/>The proposal seeks to decipher the observed genetic variation across populations using two thrusts. In one thrust, it looks to haplotype genomic structural variation, and discover the genomic architecture of complex immunological regions like KIR and HLA. In a second thrust, the investigators analyze patterns of variation that are indicative of selective constraints. For selection signatures, the investigators will provide a better understanding of currently available tests using the scaled site frequency spectrum, and use an algorithmic approach to identify a better discriminator. For the rearranged genomic regions, the investigators will use optimization algorithms to adjust read coverage in highly repetitive regions. The proposal has broader impact in the field as part of a larger effort to develop effcient computational tools for genetic analysis; a critical problem in the modern era of inexpensive sequencing. The tools and technologies described here as well will have a direct impact on understanding the genetic diversity of under-represented populations, and towards a personalized approach to healthcare. The proposed research is tightly connected to undergraduate and graduate education, as all research here will be directly incorporated in interdisciplinary classes. The PI has a strong track record mentoring womena and other under-represented students in Computer Science.","title":"III: Small: Algorithms for decoding complex patterns of genomic variation","awardID":"1318386","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[549885],"PO":["565136"]},"205473":{"abstract":"This award addresses geometric optimization problems from three areas: shape fitting, image segmentation, and geometric versions of combinatorial optimization problems like set cover. The PIs will examine new problems that have been unearthed by recent work, and will also attack some old problems that have thus far eluded satisfactory solutions.<br\/><br\/>The shape fitting problem is one of finding the best shape that fits a given set of points from an implicitly given family of shapes. A well known example is: given a set of points in the plane, find the best line that fits them. The PIs will examine questions connected with the development of approximation algorithms for such problems with near-linear running time. In a typical instance of the geometric set cover problem, we are given a set of points and a set of disks in the plane, and we wish to find the smallest subset of the disks that covers all the points. In this example, we want to cover with disks, but in other instances we want to cover with triangles, rectangles, or some other shape. Such algorithmic problems are typically NP-hard, and the PIs aim to develop improved approximation algorithms. One approach to the image segmentation problem views the image as a weighted graph and seeks to find the subset of nodes in the graph with maximum weight, subject to a shape constraint such as one that requires the subset to be ``star-shaped''. Here the PIs are interested in exact algorithms with polynomial running time.<br\/><br\/>Consider the problems of (a) clustering or identifying the linear trend in data; (b) cheapest placement of sensors to monitor a given region; and (c) automatically identifying the lung portion of a medical image of a lung. The research that this award supports views computer programs for these problems as algorithms for geometric optimization, where we want to maximize a certain quantity subject to several constraints. The research investigates the existence of fast algorithms for such optimization problems. Progress on the problems studied will enhance core knowledge within Computational Geometry, and expand the reach of a graph theoretic approach to medical image analysis. <br\/><br\/>The proposed work is a rich training ground for PhD students who will be exposed not only to theoretical computational geometry but also to some of its applications. The students will also be enriched in a broader way by graduate courses on approximation algorithms, computational geometry, and applications.","title":"AF: Small: Some New and Old Frontiers in Geometric Optimization","awardID":"1318996","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":[550204,550205],"PO":["565157"]},"205594":{"abstract":"The last several decades have seen tremendous progress in building program analysis tools to detect bugs early in the software development process. Unfortunately, existing tools neglect an increasingly important category of software systems: those written using frameworks in dynamic languages. Such frameworks are very popular today, e.g., many of the most popular sites on the web are built using them. While the expressiveness, flexibility, and power of dynamic language frameworks have encouraged their rapid and widespread adoption, those features also defeat existing program analysis techniques. This research aims to address this problem by developing practical tools and techniques for detecting and preventing errors early during software development that uses dynamic language frameworks. This research will improve our ability to correctly and rapidly build many important software systems, including those relied upon every day.<br\/><br\/>There are three main components of the project. First, this research will develop specifications to describe the high-level semantic properties of interest. The target dynamic language framework will be Ruby on Rails, one of the most popular web development frameworks today. Specifications to be explored include the Ruby on Rails analogs to type- and memory-safety; specifications supporting domain-specific languages; and specifications that are extensible to application-specific properties. Second, this research will develop a novel run-time wrapping and checking system to perform program analysis while a program is executing. The resulting system will check program properties later than a typical static analysis, but significantly earlier than the last-moment dynamic checks that are the only option today. Finally, this research will explore ways to deploy symbolic execution to amplify run-time property checking even further. The developed technology will be evaluated on open-source Ruby on Rails applications. It is expected that the techniques developed can be applied to many other dynamic language frameworks as well.","title":"SHF: Small: Specifying, Checking, and Analyzing Applications Built with Dynamic Language Frameworks","awardID":"1319666","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["556281"],"PO":["564588"]},"205242":{"abstract":"The goal of this award is to develop theories, methods, and tools to understand the mechanisms of neuromotor adaptation in human-robot physical interaction. Human power-assisting systems, e.g., powered lifting devices that aid human operators in manipulating heavy or bulky loads, require physical contact between the operator and machine, creating a coupled dynamic system. This coupled dynamic has been shown to introduce inherent instabilities and performance degradation due to a change in human stiffness; when instability is encountered, a human operator often attempts to control the oscillation by stiffening their arm, which leads to a stiffer system with more instability. The project will establish control algorithms for robot co-workers that proactively adjust the contact impedance between the operator and robotic manipulator for achieving higher performance and stability. This research will 1) understand the association between neuromuscular adaptations and system performance limits, 2) develop probabilistic methods to classify and predict the transition of operator's cognitive and physical states from physiological measures, and 3) integrate this knowledge into a structure of shared human-robot and demonstrate the efficacy in a powered lifting device with real-world constraints at vehicle assembly facilities. <br\/><br\/>If successful, the research will benefit the communities interested in the adaptive shared control approach for advanced manufacturing and process design, including automobile, aerospace, and military. Such next-generation manufacturing is expected to improve productivity and reduce assembly time as well as physical burden of assembly line workers. Research outcomes will be integrated into current courses at both graduate and undergraduate levels. Students will be recruited from interdisciplinary and multicultural groups including under-represented groups. K-12 outreach will be carried out in conjunction with Georgia Tech Student and Teacher Enhancement Partnership Program and a summer robot camp in a local non-profit association. An online portal is maintained for dissemination.","title":"NRI: Small: Understanding neuromuscular adaptations in human-robot physical interaction for adaptive robot co-workers","awardID":"1317718","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549592,549593,549594],"PO":["563744"]},"205253":{"abstract":"With advances in camera technologies, and as cloud storage, network bandwidth and protocols become available, visual media are becoming ubiquitous. Video recording became de facto universal means of instruction for a wide range of applications such as physical exercise, technology, assembly, or cooking. This project addresses the scientific and technological challenges of video shooting in terms of coverage and optimal views planning while leaving high level aspects including creativity to the video editing and post-production stages. <br\/><br\/>Camera placement and novel view selection challenges are modeled as optimization problems that minimize the uncertainty in the location of actors and objects, maximize coverage and effective appearance resolution, and optimize object detection for the sake of semantic annotation of the scene. New probabilistic models capture long range correlations when the trajectories of actors are only partially observable. Quality of potential novel views is modeled in terms of resolution that is optimized by maximizing the coverage of a 3D orientation histogram while an active view selection process for object detection minimizes a dynamic programming objective function capturing the loss due to classification error as well as the resources spent for each view.<br\/><br\/>The project advances active sensing and perception and provides the technology for further automation on video capturing. Such technology has broader impact on the production of education videos for online courses as well as in telepresence applications. Research results are integrated into robotics and digital media programs addressing K-12 students.","title":"NRI: Small: Collaborative Research: Active Sensing for Robotic Cameramen","awardID":"1317788","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["551321"],"PO":["564316"]},"205495":{"abstract":"The objective of this project is to undertake a comprehensive study of synergistic gains for multi-state communication networks and develop strategies that can exploit them. The project is organized into three complementary thrusts, focusing on 1) dynamic channels, 2) dynamic channel knowledge, and 3) mixed channel knowledge. The first thrust seeks ways to exploit channel variations starting with simple and optimal joint coding schemes over infinite horizons and leading, from a practical perspective, to efficient coding schemes over finite horizons, and, from a theoretical perspective, to alternative canonical models for information-theoretic insights into networks where the static setting has been intractable. The second thrust seeks ways to exploit alternating patterns of channel knowledge, either arising naturally out of channel dynamics or deliberately enforced through alternating feedback from different groups of users, to identify the most efficient forms of channel knowledge spread over time. The third thrust explores ways to exploit the simultaneous availability of distinct forms of channel knowledge, that have previously been studied only in isolation, and are therefore associated with coding schemes that are mutually incompatible.<br\/><br\/>With existing wireless networks struggling to keep up with the exploding data rate demands of an increasingly mobile and information-dependent society, it is imperative to uncover new opportunities for expanding the throughputs of next-generation systems. Any such effort must grapple with the enormous complexity of wireless networks, such as time-varying fading, dynamic network states, and the availability of many forms of network state information across users. The classical approach is to study the essential elements of these networks in isolation: decomposing a fundamentally multi-state network into its constituent states and studying each state individually. The motivation for this reduction typically comes from the conventional wisdom that 1) studying each element in isolation is easier than studying them together, and 2) understanding each element individually will lead to an understanding of their collective behavior. Surprisingly, recent work has shown that this wisdom can be misleading on both counts: multi-state networks are often not only more tractable, practical, and insightful, but also demonstrate significant synergistic gains that are not accessible through isolated studies of the constituent states. By shifting the focus from static to dynamic models, the proposed research will advance our understanding of information-theoretic limits of wireless interference networks where the intractability of the static setting has long stunted progress. Because joint coding across multiple states is often simpler, more insightful, and practical, it will bring the information-theoretic results closer to practice. A joint consideration of multiple states will provide the foundation for proper accounting of the overheads and benefits of channel knowledge. These research efforts are complemented by outreach efforts that include a mixture of traditional activities, such as tutorials at international conferences and summer schools, and novel ventures, such as the publication of an e-book on interference management.","title":"CIF: Small: Collaborative Research: Exploring Synergies of Multi-State Networks","awardID":"1319104","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[550258],"PO":["564924"]},"209972":{"abstract":"With massive quantities of educational materials freely available on the web, the vision of personalized and readily accessible education appears within our grasp. General-purpose search engines are insufficient as they do not focus on educational materials, objectives, pre-requisite relations, etc., nor do they stitch together multiple sources to create customized curricula for students' goals and current knowledge. This exploratory project focuses on establishing fundamental results in: (1) extracting educational units from diverse web sites and representing them in a large directed graph, whose nodes are content descriptors and whose edges encode pre-requisite and other relations; (2) conducting multi-field topic inference via a new family of graphical models to infer relations among educational units; and (3) automated curricular planning, focusing on providing sequences of lessons, courses, exercises and other education units for a student to achieve his or her educational goals, conditioned on current skills. The objective is to develop a data-driven course\/curriculum planner on demand, based on a graph traversal that is enriched with alternate paths, reinforcement options, and conditional branches to match the learner's needs.<br\/><br\/>The broader impact of this research is two-fold: (1) developing methods for mining and traversing web-based educational materials in general, later generalizing to multi-media lessons and courses; and (2) individualized curricular planning, so any student anywhere can be provided with guidance on how to navigate and exploit the vast ocean of massive open online course (MOOC) materials and other educational texts, exercises, etc. in a manner customized to the student's learning objective, capabilities and skills. The resulting system, named TEACHER, can be applied to learning specific job skills, to reinforce classroom instructions, or as stand-alone academic support to address, for instance, the huge percentage of students who attempt taking MOOCs but never complete them due to lack of requisite skills and lack of guidance on how to acquire them. Project web site (http:\/\/nyc.lti.cs.cmu.edu\/teacher\/) will be used to disseminate results.","title":"EAGER: TEACHER: A Pilot Study on Mining the Web for Customized Curriculum Planning","awardID":"1350364","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7625","name":"REESE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[562858,562859],"PO":["563751"]},"205264":{"abstract":"The objective of this research is to enable better training of robots by enabling them to physically communicate via human touch using new compliant multifunctional structures. To achieve this, arrays of conducting polymers will be developed to form a system similar to the human nervous system that can sense shape and forces distributions. This sensor array will be integrated into composite foam structures using a scalable additive manufacturing process. To support development of models and to serve as proof-of-concept for these multifunctional structures on robotic platforms, simulated co-robotics experiments will be conducted using a robotic arm interacting with objects of varying compliance. Experimental details of the associated contact mechanics will be quantified in real-time using Digital Image Correlation and conventional video imaging. Output from the sensor array will then be related to shape and force distributions by solving the nonlinear inverse problem using a novel Singular Value Decomposition method. Research results will be documented and disseminated, and the experiments will be converted to STEM demonstrations targeted at educating young girls.<br\/><br\/>This research will lead to new compliant, scalable, sensing structures that simultaneously monitor in real-time both global and local shapes, as well as force distributions. Since compliant multifunctional sensing structures do not yet exist for robots, it is envisioned that the proposed work will enable realization of new bio-inspired control principles for training robots. This will significantly advance the ability to make safer interactions and decisions in co-robotics by differentiating robotic interactions with humans from other objects in their environment. The proposed integration of research and education will train new mechanical engineers to create multifunctional products that enable new products and new capabilities in existing products in critical areas such as healthcare. The new fabrication methods will enable these structures to be manufactured in the United States in a cost-competitive manner, increasing employment.","title":"NRI: Small: Compliant Multifunctional Robotic Structures for Safety and Communication by Touch","awardID":"1317913","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549647,549648,549649],"PO":["563744"]},"205275":{"abstract":"PI: Dias, M. Bernadine; Steinfeld, Aaron<br\/>Proposal Number: 1317989<br\/><br\/>Summary: As robotics technology evolves to a stage where co-robots become a reality, we need to ensure they are equally capable of interacting with humans with disabilities. The proposed work addresses this challenge by exploring meaningful human-robot interaction in the context of assistive robots for blind travelers. For people with disabilities independent transportation remains a major barrier to employment and quality of life. Furthermore, emergency situations necessitating evacuation is one of the greatest fears they face. The key question we seek to answer in the proposed work is: what role can co-robots play in empowering people with disabilities to safely travel to and navigate unfamiliar environments? We hypothesize that co-robots can enhance the safety and independence of these travelers by assisting them to navigate unfamiliar urban environments effectively and providing support when evacuating. We begin our proposed work with a needs assessment to understand the preferences and challenges of blind travelers. The ultimate objective is to enhance the safety and independence of blind travelers.<br\/><br\/>Intellectual Merit: The proposed work explores three principal research areas applied to three scenarios relevant to assistive robots for blind travelers: (1) information exchange and object manipulation, (2) assistive localization, and (3) urban navigation and emergency building evacuation. The research areas we plan to explore are accessible interfaces, assistive interaction modalities, and effective cooperation mechanisms. Envisioned scenarios include robots assisting humans to localize within necessary resolution and context using a combination of perception, robot localization, and crowdsourcing, robots assisting humans to retrieve lost or fallen objects or locate objects or people of interest, robots assisting humans to interact with other aspects of the environment such as reading notices, and robots assisting humans during emergency evacuation of buildings. We will also explore means of these travelers \"teaching\" the robots to do tasks of interest to them. The robots will have to reason intelligently about task allocation among themselves and coordinating with humans when needed. Overall, the proposed research will significantly advance the knowledge of how assistive robots can meaningfully and effectively interact with travelers with disabilities. The uniqueness of the proposed research is captured in the accessibility of the interfaces, the richness of the interaction modalities, and the flexibility and range of the cooperation mechanisms. The combination of these contributions will significantly advance the state of the art in assistive technology as well as human-robot interaction.<br\/><br\/>Broader Impact: The team has a strong commitment to undergraduate research experience. Over 75% of the students mentored by the Principal Investigator and Co-Principal Investigator have been women, minorities, or people with disabilities. This commitment extends to the team's instructional activities. Team members regularly incorporate research findings into class presentations, guest lectures, and seminars. The team is also committed to community outreach. Both Dias and Steinfeld regularly speak to non-academics and will include aspects of this project in such talks. A final educational outcome will be several planned workshops at our partner organizations and the outcomes from the proposed work are expected to impact operations and methodologies used at these organizations. The assembled team of researchers and partner organizations further enhances the broader impact of this proposal. Principal Investigator Dias is one of the very few female robotics faculty members at the university and in the discipline. She is engaged in many mentoring and leadership activities to encourage and sustain the participation of women in computing and to address the needs of technologically underserved communities. The proposed team of undergraduate students, a graduate student, and a postdoctoral research assistant will also gain significant mentoring and education through their participation in this research. Industry interaction extends beyond regular contact due to faculty involvement in high profile centers. The research and evaluation program is specifically geared towards people with disabilities. Therefore, the contributions of the proposed work will make significant advancements towards realizing the vision of safe and independent travel for people with disabilities. The results of the proposed work will be disseminated broadly through a variety of avenues and all outcomes of the research will be made available in accessible formats to the community partners and their networks.","title":"NRI: Small: Assistive Robots for Blind Travelers","awardID":"1317989","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549679,549680],"PO":["565049"]},"205396":{"abstract":"The amount of information that is becoming available every day is expanding at an exponential rate and this is starting to render ineffective standard techniques for data exploration. Though high-dimensional datasets present great mathematical challenges, in practice the related difficulties are mitigated by the fact that not all the measured variables are important for an understanding of the underlying phenomenon. The \"dimensionality reduction techniques\" considered in this project address this issue. Their principle is to combine the variables into a smaller set onto which the data is projected before attempting a solution of the original problem. <br\/><br\/>The problem of dimension reduction gives rise to many interesting mathematical and algorithmic challenges to linear algebra specialists. In particular, one of the difficulties faced by current techniques is that existing algorithms are often too costly when dealing with very large data sets. For example, a number of methods are based on a form of Principal Component Analysis (PCA) which becomes exceedingly expensive as the number of variables (features) and the number of samples increase. A similar calculation is also required for graph-based approaches such as the Locally Linear Embedding (LLE), or Laplacean eigenmaps. In addition, in many applications data sets are frequently updated, e.g., by adding or deleting data items, a situation for which standard matrix algorithms are not adapted. The proposed work will tackle a few of these challenges. It will focus on the development of computationally efficient dimension reduction methods and related techniques, by exploiting ideas from computational linear algebra. Multilevel or divide and conquer techniques are quite common in other areas of scientific computing but have received relatively little attention in data mining. The proposed work puts methods of this type at the forefront. <br\/><br\/>One of the broader impacts of the proposed work is that it will help promote interest in problems related to the current information revolution because its research theme blends mathematical methods, good algorithmic practices, and applications requiring effective numerical methods. The applications under consideration in this work are all of great relevance to many of the new challenges of society (social networks, commerce, and security). Finally, the software resulting from this research will be broadly disseminated to join an excellent pool of existing web sites that provide tools and repositories related to data exploration.","title":"AF: small: Numerical Linear Algebra Methods for Efficient Data Exploration","awardID":"1318597","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[550000],"PO":["565251"]},"205297":{"abstract":"This project aims to use a combination of elements from dynamic vision, dynamical systems theory, optimization and semi-algebraic geometry to develop a computationally tractable, scalable framework for automatic dynamic scene understanding from multiple, potentially incomplete and corrupted data streams. The long-term vision is to lay the foundations for synthesizing provably robust vision systems, capable of sustained successful operation in complex dynamic scenarios.<br\/><br\/>The core of the project is a unified vision, centered on the use of dynamical invariants as information encapsulators, and emphasizing robustness and computational complexity issues. In this approach, the observed data is treated as the output of an underlying model, typically a difference inclusion, which has associated certain quantities (for example order, embedding manifold, subspace spanned by its trajectories) that are invariant to coordinate transformations, initial conditions, viewpoint changes, synchronization, etc. These invariants compactly capture spatio-temporal information from video data and lead to robust, computationally efficient algorithms for automatic video scene understanding. For instance, in this context video data can be efficiently segmented by detecting changes in these dynamic invariants or clustered according to a suitable defined distance. An application domain directly impacted by this research is aware environments for public space safety, where the co-PIs have been provided access to real data and given a venue for real time testing of the algorithms.","title":"RI: Small: Dynamic Invariants for Video Scenes Understanding","awardID":"1318145","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[549739,549740],"PO":["564316"]},"208575":{"abstract":"This award is for travel support for up to ten students to the 2013 USENIX Workshop on Health Information Technologies (HealthTech13).<br\/><br\/>Previous USENIX workshops on Health Information Technologies (HIT) have traditionally provided a forum for discussion of aggressively innovative and potentially disruptive ideas on all aspects of medical and health security and privacy, including technology and policy. This year's 2013 USENIX Workshop on Health Information Technologies, called HealthTech13, will be held in conjunction with the annual USENIX Security conference in Washington, D.C., on Aug. 12, 2013. Its focus differs from previous USENIX workshops on HIT by its broader scope that is intended to encourage the development of new technologies that generally improve the quality and safety of healthcare, as well as the access to it. The workshop audience is similarly broadened to encourage greater cross-disciplinary engagement by researchers and practitioners in technology, medicine, and policy communities across academia, government and industry. <br\/><br\/>The workshop will offer several means by which participants can interact and engage. In addition to keynote and technical research presentations, there will be a poster session and a best research paper award, which will both be a first for this workshop series, and a continuation of last year's successful session for announcing research progress on hard problems. The topics to be covered in HealthTech13 include architectures for large-scale health information systems, and health information exchanges, medical devices and body area networks, home and assisted living monitoring systems, privacy enhancing technologies, regulatory and policy issues, telemedicine, and mobile health technologies, among others.","title":"Student Travel Support for the 2013 Workshop on Health Information Technologies (HealthTech '13)","awardID":"1340616","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[559051],"PO":["565239"]},"209686":{"abstract":"The Internet is one of the primary communication media for users who are visually impaired. However, the vulnerability of this underserved population of Internet users to various forms of cyber-attacks has deteriorated the effective use of online services offered for general Internet users. The screen reader tools often overwhelm users with visual disability by providing large amounts of text, which may lead those users to ignore that information. An ignored security warning may harm users and make them vulnerable to various forms of cyber-threats. This project translates security cues and warnings into various forms of tones and sounds through sonification. It takes a security cue or warning message and composes the most effective pitch\/sound to represent the security warning and conveys the security information to the users who are visually impaired through sounds instead of text verbalized by screen readers. The sonified security warnings undergo extensive usability testing in order to assess the effectiveness of the sonified security cues. The broader impact of this sonification project includes providing a sonification-based technology to alert Internet users who are visually impaired and assist them to make an informed decision when encountering security warnings and to ensure that those security warnings are not overlooked. The sonification-based technology for transforming security warnings to sounds can be integrated to any popular Internet Web browser.","title":"EAGER: Sonifying Cyber-Security Cues for Internet Users Who Are Visually Impaired","awardID":"1347521","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[562055,562056,562057,562058,562059],"PO":["565327"]},"208366":{"abstract":"Researchers in an expanding number of scientific disciplines, ranging from astronomy to medicine and from economics, to botany, would like to move large volumes of data across institutional and national boundaries. Placing data at different locations expands data processing capacity, facilitates collaborative analysis and accelerates translation of gained insight into scientific discoveries. Yet, we have little understanding of how a complex ensemble of interrelated factors affect the cost, performance, and resource consumption of these data movements. Factors include the number and size distributions of files in a data set, performance characteristics of the source and destination file\/storage systems, time-of-day\/week fluctuations of the network carrying capacity between sites, availability of IPv6 paths. This project will explore models that capture both the characteristics and interplay of facets that are involved in placing data at locations separated by local, national, or international networks. Exploratory work will take place to establish organizational and functional foundations for an international data placement laboratory (iDPL) to support at-scale end-to-end data placement experiments.<br\/><br\/>The iDPL will be constructed with international partners and is intended to be an extensible facility. The UCSD and UW-Madison team bring together extensive experience in software tools (HTCondor, Open Science Grid, Rocks Clustering), high-performance campus-area networks (Quartzite and Prism@UCSD), storage systems (Data Oasis), and engaged science communities. The iDPL core software architecture is to treat data placement experiments as workflows and then use HTCondor?s DAGMan and Metronome continuous testing framework to author, instantiate and manage experiments. Synthetic data sets will be offered to facilitate diagnostics. Whenever possible, international networking providers will be engaged to correlate infrastructure view of the network with user experiences.<br\/><br\/>The broad and long term impact is to lay the groundwork for a persistent, data and information sharing vehicle that can be easily expanded beyond the initial international group of institutions","title":"EAGER: Fundamental Issues in International Data Placement for Data-Intensive Applications, a Laboratory Approach","awardID":"1339508","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7369","name":"INTERNATIONAL RES NET CONNECT"}}],"PIcoPI":["559498"],"PO":["564246"]},"199082":{"abstract":"One core challenge in natural language research is to do robust, wide coverage semantic analysis. Recently, there has been progress towards solving this problem by developing algorithms for learning semantic parsers that map sentences to rich, logical representations of their meaning. State-of-the-art approaches have reached the level where they can, with sufficient training data, be used to learn highly accurate parsers for many different natural languages on a number of benchmark problems. However, the general applicability of this work has been limited by the focus on somewhat idealized conditions, where the application domain is of limited size, sentences are analyzed in isolation, and there is an exclusive focus on database query applications.<br\/><br\/>This CAREER project aims to build a framework for grounded semantic parsing that solves these challenges by reasoning about a sentence's possible meanings given its linguistic and situated context. This type of reasoning is necessary for extending existing learning approaches to fundamentally new applications, such as conversational understanding. However, it is also be crucial for the next generation of semantic parsers that learn from easily gathered data and scale to domains that are several orders of magnitude more complex than previously considered.<br\/><br\/>The project will extend the PI's educational and outreach efforts, including the creation of freely shared online content for introductory and advanced semantics topics. It will also enable new initiatives by the PI to increase diversity in computer science study and research, by supporting efforts to motivate students through early exposure to exciting language understanding problems.","title":"CAREER: Learning Scalable Models for Grounded Semantic Parsing","awardID":"1252835","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["557091"],"PO":["565215"]},"209235":{"abstract":"This EAGER project fosters international collaboration in an area of emerging (nano) technology, the design of novel neuromorphic cellular computing architectures. Led by members of a group at the Notre Dame University, the joint work will engage partners at Arizona State University and three European institutions, the Technical University Munich in Germany, the Hungarian Academy of Sciences, and Spain's Seville University. Each of the five locations currently has strong research activities on various aspects of the topic covered by the collaboration, e.g., hybrid computer architectures, emerging processing, and sensing and enabling integration technologies. All have a distinguished history of ongoing research with support from funding sources in their respective countries. Together, the researchers intend to lay the groundwork for a radically different approach to information processing that is based on brain-inspired spatial-temporal behavior in large-scale, cellular arrays of nanoelectronic processing elements. Specifically, their goal is to identify the computational building blocks for future computing systems. This collaboration will further enable the capacities of these geographically diverse groups to facilitate student exchanges, brainstorming sessions - both physical and virtual - and short exchange visits by junior researchers. Conferences, workshops, summer schools, virtual classrooms to leverage the complementary expertise of various participating centers will be organized, and dissemination of research results and educational products will be undertaken cooperatively by the group. <br\/><br\/>Thus, the broader impact of the project should be significant, especially for the participating U.S. students and junior researchers, first by gaining early career, high level technical experience abroad, some of which is not readily available in the U.S., and secondly, by exposing them to a team-focused research environment in differing disciplinary and cultural contexts. It is also noteworthy that this particular project enjoys the benefits of access to complementary expertise at highly visible research centers for societal impacts of (nano) technology, e.g., the NSF-funded Center for Nanotechnology in Society, at Arizona State University, and the Munich Center for Technology in Society, at the Carl von Linde Akademie in Munich. Such interaction is expected to greatly enhance the societal and educational value of the overall collaborative effort. This EAGER project is co-funded by the NSF's Office of International and Integrative Activities.","title":"EAGER: Computer Architectures for 2020 and Beyond","awardID":"1344531","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":[560914,560915,560916],"PO":["562984"]},"210192":{"abstract":"In this project the PI will explore a novel approach to allowing individuals to monitor their wider environment for potential obstacles and threats while engaged in a task where the eyes are occupied. Specifically, he will focus on mobile device users, who often perform visually-demanding tasks such as composing and reading text messages while ambulatory, so that they may fail to notice the presence of pedestrians, approaching vehicular traffic or other objects which they are at risk of encountering. The PI's approach is to present tactile feedback via a head-mounted interface in order to communicate the presence of obstacles. While situational awareness technologies have been designed to assist ambulatory users, alerts are often presented using visual or auditory feedback. But if the user is engaged with a mobile task precious time may be taken to identify the presence of graphical indicators, whereas auditory alerts may be masked by environmental sounds to that the user misses vital cues. The PI argues that tactile feedback offers considerable advantages when the user's other senses are blocked or restricted, and there is the additional benefit that tactile alerts can be presented discreetly without drawing attention by others. To test these hypotheses, the PI will conduct a sequence of studies to determine whether it is possible to design tactile cues that are effective in supporting informed decisions by the user. Project outcomes will include design of a head-mounted interface prototype using object-recognition and sensor-based technologies to track obstacles in the user's vicinity, along with innovative tactile interface design guidelines. <br\/><br\/>Broader Impacts: This research will advance our understanding of issues relating to situational awareness among mobile device users, and it will also contribute to the body of knowledge on presenting tactile feedback to locations on the head (a field still in its infancy). The development of a library of tactile icons to convey concepts such as the number of obstacles, their location, and their proximity to the user, will have application across diverse domains.","title":"EAGER: Enhancing Mobile Device Users' Levels of Situational Awareness through Tactile Feedback","awardID":"1352924","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563314],"PO":["565227"]},"204780":{"abstract":"Industrial control systems differ significantly from standard, general-purpose computing environments, and they face quite different security challenges. With physical \"air gaps\" now the exception, our critical infrastructure has become vulnerable to a broad range of potential attackers. In this project we develop novel network monitoring approaches that can detect sophisticated semantic attacks: malicious actions that drive a process into an unsafe state without however exhibiting any obvious protocol-level red flags. In one thrust, we conduct a measurement-centric study of ICS network activity, aimed at developing a deep understanding of operational semantics in terms of actors, workloads, dependencies, and state changes over time. In a second thrust, we develop domain-specific behavior models that abstract from low-level protocol activity to their semantic meaning according to the current state of the processes under control. Our goal is to integrate these models into operationally viable, real-time network monitoring that reports unexpected deviations as indicators of attacks or malfunction. A separate \"Transition to Practice\" phase advances our research results into deployment-ready technology by integrating it into the open-source Bro network monitor. Overall, our work will improve security and safety of today's critical infrastructure by providing effective, unobtrusive security monitoring tailored to their specific semantics. In addition, we tie a number of educational activities to the research and involve students at all levels.","title":"TWC: Option: Medium: Collaborative: Semantic Security Monitoring for Industrial Control Systems","awardID":"1314973","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["562329"],"PO":["565327"]},"202251":{"abstract":"Multicore computer chips pose new challenges for hard real-time systems, because of the complex temporal coupling between processing cores' shared last level cache, shared memory driver bandwidth, and shared I\/O bandwidth. The project addresses this challenge by 1) integrating resource partition mechanisms and their implementation, 2) optimizing resource allocation algorithms, and 3) developing schedulability analysis technology to create real time virtual partitions (RTVPs) on each core. The goal is for each RTVP to be able to be analyzed as if it were a standalone single core chip, independent of the workloads in other cores and partitions. In a multicore chip, a task's worst case execution time (WCET) depends on the partition's allocated memory driver bandwidth, the last level cache size, and I\/O bandwidth. To optimize the resources allocated to a RTVP, one needs to know if the tasks in the RTVP are schedulable with a given resource allocation. To perform schedulability analysis, one needs to know tasks' WCETs. To determine tasks' WCETs, one needs to know the resources allocated to the RTVP. The approach taken by this project to break such circular dependency is to first find an initial pessimistic but feasible solution, then apply an interactive optimization method to find a near optimal solution. <br\/><br\/>Our nation has a large body of certified real time safety and mission critical software developed for single core chips, using certification procedures developed for single core chips. Now those chips are becoming obsolete, and newer chips are being designed with slower clock rates but multiple cores. Without a technology like RTVP, the change of workload in one core could adversely impact the schedulability of tasks in other cores, triggering the recertification of applications in other cores. The time and costs of such recertification is economically unsustainable. This problem is especially critical for the aircraft industry. To help ensure the usability of this project's research outcomes, the project team has collaborative contacts with Freescale Semiconductor whose multicore chips are widely used in avionics, Lockheed Martin who develops multicore chip based avionics for the US Department of Defense, and with Rockwell Collins who cooperates with Boeing and the FAA on the development and certification of multicore chip based civilian avionics, as well as the FAA. Key elements of the research are to be incorporated into the educational program of the host institution.","title":"CSR: Medium: Multicore Real Time Virtual Partitions","awardID":"1302563","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[542023,"553633","553686"],"PO":["564778"]},"206750":{"abstract":"This frontier project tackles many of the fundamental research challenges necessary to provide trustworthy information systems for health and wellness, as sensitive information and health-related tasks are increasingly pushed into mobile devices and cloud-based services. The interdisciplinary research team includes expertise from computer science, business, behavioral health, health policy, and healthcare information technology to enable the creation of health & wellness systems that can be trusted by individual citizens to protect their privacy and can be trusted by health professionals to ensure data integrity and security. Although these problems are motivated by a nationally important application domain (health and wellness), the solutions have applications far beyond that domain.<br\/><br\/>This project is developing methods to authenticate clinical staff to tablet computers in a continuous and unobtrusive way, and to provide patients a usable way to control the information that mobile sensors collect about them. One of the goals is to manage security of healthcare devices in the home and in remote clinics, without adding burden on the homeowner or clinical staff; towards this end the investigators are developing methods to verify medical directives issued to remote devices. One approach being investigated is segmenting access to medical records from mobile devices to limit information exposure, and developing methods to audit behavior of this complex ecosystem of devices and systems. The investigators will design tools to handle genomic data in the cloud while enabling patient control over information, detect malware in medical devices through power analysis, and provide contextual information to those who use health data collected in the field.","title":"TWC: Frontier: Collaborative: Enabling Trustworthy Cybersystems for Health and Wellness","awardID":"1329737","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553563,553564,553565],"PO":["564388"]},"205540":{"abstract":"Online community question-and-answer systems such as Yahoo! Answers provide a venue where people can pose questions, provide answers and browse and comment on both. Such sites are now a central part of online interaction on a wide range of sites, including specialized sites for software development, tax preparation, and product support. In this projects, the PIs will experimentally answer a set of causal questions drawn from diverse research literatures about how to improve the performance of Q&A systems, specifically what leads people to provide good quality answers (and questions), how to route questions so they are answered by the best possible member, how referrals and profiles affect questions and answers, and to what extent different kinds of incentives affect behavior on the site.<br\/><br\/>The proposed research is important because online question and answer systems have become an important source of information and advice for individuals and businesses, an important and economical way for businesses to help their customers support each other, as well as an important source of content for web search engines. By understanding these systems, building models of usage and testing designs to support their improved operation, the project will help systems designers understand approaches that can promote beneficial social experiences for users and that can inform the construction of valuable community-contributed repositories of knowledge. Additional benefits include the release of data sets for other researchers and the training of student researchers.","title":"HCC: Small: Experiments in Community Q&A","awardID":"1319382","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[550379,550380],"PO":["565342"]},"205551":{"abstract":"This Cyberlearning: Transforming Education project brings together leading-edge researchers in computational linguistics and computer-supported collaborative learning to explore feasibility issues in designing an intelligent conversational agent that interacts with groups of learners as they are working together and provides cognitive, meta-cognitive, and social advice to enhance their collaborations. The conversational agent is being designed to enhance group dynamics by providing supportive advice as well as promoting awareness of the dynamics. Design of the agent is informed by theories of meta-cognition, collaborative learning, and accountable talk. In addition to investigating how to design such agents and testing their effectiveness, investigators are exploring how learning develops when a conversational agent is available to help with productive talk, the influences of conversational behaviors on developing understanding and actions, and ways of influencing understanding and behaviors.<br\/><br\/>One of the biggest limitations of online education, especially massive online courses (e.g., MOOCs), is personal contact with instructors and peers. While much of formal education focuses on faculty interactions with large groups of students in lectures, much student learning depends on interactions with teachers and teaching assistants and discussions, study sessions, and project work done with peers. Massive online education will be broadly successful only after we know how to provide these kinds of supports to online learners. This project focuses on designing an interactive agent that can provide conversational support for small groups of collaborating students to help them productively solve problems and learn together. The agent is being designed to provide the kinds of advice that a teacher or teaching assistant would provide if such a human mentor were available. The researchers conjecture that with such support available, online students can provide for each other the kinds of social supports for learning that contribute in integral ways to students' learning experiences in higher education. The agent is being deployed and tested in several different online courses. Results are expected to be applicable to massive online education as well as to blended educational environments such as flipped and project-based classrooms.","title":"EXP: Collaborative Research: Fostering Ecologies of Online Learners through Technology Augmented Human Facilitation","awardID":"1319445","effectiveDate":"2013-09-15","expirationDate":"2016-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7511","name":"TUES-Type 2 Project"}}],"PIcoPI":[550405],"PO":["562669"]},"205672":{"abstract":"In this Cyberlearning: Transforming Education EXP project, PIs from computer science and mathematics education are collaborating to investigate the use of gestures by teachers (both human and virtual) and learners in support of mathematics learning. They are investigating the ways teachers' gestures influence learning of mathematics concepts and how to design gestural supports for learning that a computer avatar might use in communicating with a learner. Their conceptual foundations come from embodied cognition, and they are aiming towards understanding the integration of two types of gestures: those that are used to promote understanding of content and those used for social purposes. The project focuses on learning of proportion, and the technological innovation in this project is creation of a gesturing pedagogical agent\/avatar that has a rich repertoire of both types of gestures that it uses while interacting with a learner and helping the learner to deepen his or her understanding of the mathematics of proportion. In a series of design studies, the PIs are designing software and extracting principles for augmenting pedagogical agents with new gesture-enriched capabilities and gleaning insights into the nature, types, and roles of gesture in educational interaction. <br\/><br\/>Despite consistent reform efforts, U.S. students still lag behind their global peers in mathematics understanding and capabilities. Intelligent tutoring systems can be used to provide one-on-one help to students who are struggling as they learn mathematics, but such interactions lack the social cues that help learners maintain their attention and know they are being understood and lack, as well, full means of expressing concepts in ways that learners might need for understanding. Good teachers use gestures for these purposes, and this project focuses on design of pedagogical agents (avatars) that will also be able to use such gestures. Infusing interactive tutoring systems with the ability to gesture in naturalistic and domain-appropriate ways may provide a missing link in making tutorial interactions effective for more learners. At the same time, insights gleaned about the pedagogical roles of gesturing can be leveraged in educating teachers of the future.","title":"EXP: Collaborative Research: Gesture Enhancement of Virtual Agent Mathematics Tutors","awardID":"1320029","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7625","name":"REESE"}}],"PIcoPI":[550684],"PO":["562669"]},"205793":{"abstract":"Assistance dogs have improved the lives of thousands of people with disabilities. Guide dogs, service dogs, and hearing dogs can provide independence and significantly enhance quality of life. However, communication between human and canine partners is currently limited. Handlers give commands, and dogs respond with behaviors, which can sometimes have ambiguous meanings to the handlers. For example, a guide dog stops while walking with his handler, but the dog has no definitive way to tell the handler there is a new barrier on a familiar path. A hearing dog alerts his handler, but can't easily report that the fire alarm has sounded and they need to evacuate. A service dog may need to receive a \"lie down\" command silently in a dark, crowded theater, or from a distance, which would preclude verbal or hand signals. The PI's goal in this research is to explore fundamental aspects of wearable technologies to support two-way communication between assistance dogs and their handlers. To this end, the PI will investigate on-body interfaces for dogs in the form of electronic textiles and computers integrated into assistance dog clothing, such as vests and harnesses. Studies will assess the abilities of dogs to interact with affordances worn on their bodies, and determine what stimuli dogs can sense and comprehend from wearable technology. The work will be divided into three thrusts: Dog-to-handler communication (to determine to what extent dogs can activate affordances on their harnesses, vests, coats, or collars, such as \"touch points\" which can be activated by a dog's nose touch, and \"pull points\" which can be activated by tugging); Handler-to-dog communication (to investigate the sensory capabilities of dogs to respond to simple stimuli incorporated into a dog vest or harness, such as small vibrating motors); Handler feedback and control (the team will implement a head-mounted visual and\/or auditory display to allow handlers to receive input from their dogs, as well as control interfaces that can be integrated into a harness handle, leash, or a piece of clothing worn by the handler). Design and testing of the on-body interfaces for the dogs will be carried out using an iterative approach, and the final designs will be validated with three assistance-trained dogs in laboratory and real-world environments. The PI will also stress-test the designs with dogs at speed on an obstacle course.<br\/><br\/>Broader Impacts: Project outcomes will extend the state of the art in wearable interfaces to animals, and will contribute pioneering work to the nascent fields of Animal Computer Interfaces and Inter-species Interaction, as well as discovering more about the physical and cognitive abilities of dogs. Adapting usability analysis and design techniques from the human realm to apply to animals will expand the body of knowledge in Interactive Computing. In addition to the direct impact of improving the quality of life for people with disabilities, the technologies developed in this project could have application to other working dog teams (e.g., in military or police canine units, and in search and rescue), and they may ultimately allow all pet owners to better communicate with and train their dogs.","title":"HCC: Small: FIDO - Facilitating Interaction for Dogs with Occupations: Wearable computing for two-way communication with assistance dogs","awardID":"1320690","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["554219"],"PO":["565227"]},"202163":{"abstract":"Enterprises rely on specialized network appliances or middleboxes such as load balancers, intrusion detection and prevention systems, and WAN optimizers in order to meet critical performance optimization, security, and policy compliance requirements. With the advent of cloud computing, such middlebox processing will play an increasingly critical role in cloud deployments due to two key factors: 1) As enterprises move their IT infrastructure to the cloud, they want to leverage the same performance and security benefits for applications running in the cloud; and 2) Enterprises want to reduce their infrastructure and management costs by offloading middlebox functionality to cloud providers to leverage the elastic scaling and migration benefits offered by cloud computing. <br\/>Unfortunately, cloud customers and providers today lack the necessary abstractions and mechanisms for enabling this transition. At a high-level, the problem is that these workloads are drastically different from traditional computation and storage services for which cloud computing has been extremely successful. This raises fundamental challenges along several dimensions: the need for flexible composition or chaining of network services; the increased impact of network-level performance on such workloads; the inherent difficulty in identifying bottlenecked resources in multiplexed cloud deployments; and the inability to reason about correct and consistent operation of stateful network processing in dynamic deployment scenarios. <br\/><br\/>This project will bridge this disconnect by addressing foundational issues in the design and implementation of (1) policy frameworks, elastic scaling algorithms, and software-defined controllers for enterprise administrators to translate their requirements into an actual physical realization; (2) algorithms for intelligent network-level placement, traffic engineering, and topology design for cloud providers to support such workloads; and (3) new abstractions for managing and manipulating the middlebox-associated state of the network. <br\/><br\/>Broader Impact: This work will inform the critical industry evolution as enterprises and cloud providers are attempting to realize the benefits of 'network virtualization'. Furthermore, the project will enable new dimensions of flexibility for network deployments that do not exist today---democratizing the benefits of middleboxes to small businesses; providing the ability to elastically scale network-level services to meet application demands; and enabling live migration of entire enterprise deployments across physical infrastructures. The project will generate new course materials on software-defined networking and cloud computing and tightly integrate research with education to help students become experts in these emerging domains. The software tools and benchmark measurement data produced by the research will inform the industry transition and future academic work on such middleboxes-in-the-cloud deployments. Finally, while the project focuses on middleboxes in cloud deployments, the technical foundations developed therein will apply to traditional enterprise and ISP networks as well.","title":"NeTS: Medium Collaborative Research: Enabling Flexible Middlebox Processing in the Cloud","awardID":"1302041","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["553786"],"PO":["565090"]},"205683":{"abstract":"Computing is changing dramatically, particularly for cloud-based service providers such as Facebook, Google, and Amazon. On-line service applications, such as social networking and search, place unique demands on processor memory systems. In particular, these \"big-memory\" applications have working data sizes several orders of magnitude beyond those found in the workloads typically used in computer design research. As a result, these applications place different stresses on processor memory systems. Simultaneously, new, non-volatile memory (NVM) technologies such as Phase Change Memory (PCM), spin-transfer torque random access memory (STT-RAM), and memristors are emerging for use as a replacement for or augmentation to traditional dynamic RAM (DRAM) main memory. These new memory technologies promise higher capacities and fast access times along with non-volatility (data retention when the power is off). As a result, they have the potential to bridge the gaps in current processor memory systems for both data capacity and speed requirements, leading to new usage models, such as storage class memories or combined main memory and storage implementations. These trends together argue for new memory systems architectures, designed for the challenges of big-memory applications, leveraging new memory technologies together with traditional DRAM and emerging process techniques such as 3-D die stacking. <br\/><br\/>This research will characterize big-memory applications in light of future availability of much larger and nonvolatile memories closer to the processor. It will study the implications of these applications on emerging memory architectures in terms of organization, hierarchies, and other structural and management questions. In particular, this research focuses on the development of the following: 1) Memory architectures for big memory applications, leveraging emerging technologies, such as 3-D die stacking and new, byte-addressable, dense non-volatile memories; 2) Deeply speculating instruction and data prefetchers for big-memory applications; 3) Cache policies that proactively manage performance, power, and reliability in memory systems for future big memory applications utilizing NVM; 4) New memory translation microarchitectures to meet the needs of big-memory applications and storage-class main memories; and 5) Quality-of-service policies to manage memory placement based upon usage in future, hybrid, and composite memory systems composed of DRAM and new NVM technologies. The educational impact of this research will include training graduate and undergraduate students with valuable research skills while advancing the state of the art in computer architecture and distributed systems, contributing to the technology workforce.","title":"SHF: Small: Emerging Memory Architectures for Big Memory Applications","awardID":"1320074","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":[550709,550710],"PO":["366560"]},"206783":{"abstract":"Cyber-Physical Systems (CPS) encompass a large variety of systems including for example future energy systems (e.g. smart grid), homeland security and emergency response, smart medical technologies, smart cars and air transportation. One of the most important challenges in the design and deployment of Cyber-Physical Systems is how to formally guarantee that they are amenable to effective human control. This is a challenging problem not only because of the operational changes and increasing complexity of future CPS but also because of the nonlinear nature of the human-CPS system under realistic assumptions. Current state of the art has in general produced simplified models and has not fully considered realistic assumptions about system and environmental constraints or human cognitive abilities and limitations. To overcome current state of the art limitations, our overall research goal is to develop a theoretical framework for complex human-CPS that enables formal analysis and verification to ensure stability of the overall system operation as well as avoidance of unsafe operating states. To analyze a human-CPS involving a human operator(s) with bounded rationality three key questions are identified: (a) Are the inputs available to the operator sufficient to generate desirable behaviors for the CPS? (b) If so, how easy is it for the operator with her cognitive limitations to drive the system towards a desired behavior? (c) How can areas of poor system performance and determine appropriate mitigations be formally identified? The overall technical approach will be to (a) develop and appropriately leverage general cognitive models that incorporate human limitations and capabilities, (b) develop methods to abstract cognitive models to yield tractable analytical human models (c) develop innovative techniques to design the abstract interface between the human and underlying system to reflect mutual constraints, and (d) extend current state-of-the-art reachability and verification algorithms for analysis of abstract interfaces, iin which one of the systems in the feedback loop (i.e., the user) is mostly unknown, uncertain, highly variable or poorly modeled.<br\/><br\/>The research will provide contributions with broad significance in the following areas: (1) fundamental principles and algorithms that would serve as a foundation for provably safe robust hybrid control systems for mixed human-CPS (2) methods for the development of analytical human models that incorporate cognitive abilities and limitations and their consequences in human control of CPS, (3) validated techniques for interface design that enables effective human situation awareness through an interface that ensures minimum information necessary for the human to safely control the CPS, (4) new reachability analysis techniques that are scalable and allow rapid determination of different levels of system safety. The research will help to identify problems (such as automation surprises, inadequate or excessive information contained in the user interface) in safety critical, high-risk, or expensive CPS before they are built, tested and deployed. The research will provide the formal foundations for understanding and developing human-CPS and will have a broad range of applications in the domains of healthcare, energy, air traffic control, transportation systems, homeland security and large-scale emergency response. The research will contribute to the advancement of under-represented students in STEM fields through educational innovation and outreach. The code, benchmarks and data will be released via the project website.<br\/><br\/>Formal descriptions of models of human cognition are in general incompatible with formal models of the Cyber Physical System (CPS) the human operator(s) control. Therefore, it is difficult to determine in a rigorous way whether a CPS controlled by a human operator will be safe or stable and under which circumstances. The objective of this research is to develop an analytic framework of human-CPS systems that encompasses engineering compatible formal models of the human operator that preserve the basic architectural features of human cognition. In this project the team will develop methodologies for building such models as well as techniques for formal verification of the human-CPS system so that performance guarantees can be provided. They will validate models in a variety of domains ranging from air traffic control to large scale emergency response to the administration of anesthesia.","title":"CPS: Synergy: Collaborative Research: Formal Models of Human Control and Interaction with Cyber-Physical Systems","awardID":"1329986","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553660,553661],"PO":["565136"]},"205452":{"abstract":"Computer-mediated communication (CMC) tools and social media have the potential to allow people to interact fluidly across national boundaries but linguistic boundaries still pose challenges to communication. Advances in machine translation (MT) and other technologies have resulted in new tools that could allow people to communicate with one another using their native language, but translation errors create sizeable misunderstandings when MT is used in conversational settings. The goal of the proposed research is to better understand and support communication between people who speak different native languages by enhancing MT output with other information, such as keyword highlighting or pictorial representations. To achieve this goal, the project will (a) explore how the use of MT vs. English affects inter-lingual communication and coordination; (b) iteratively develop and test new tools that provide additional representations of meaning to smooth gaps in MT output; and (c) assess the value of these enhanced MT tools for communication and collaboration in a series of carefully controlled laboratory studies. The results will help delineate the design and technical space for new tools for inter-lingual communication. <br\/><br\/>The project will contribute to the fields of computer-mediated communication and computer-supported cooperative work by developing new theories and understandings of how people use technology to communicate and collaborate across language boundaries. It will contribute to human-computer interaction and related fields through the development of new techniques and tools for enhancing the output of machine translation. The annotated inter-lingual dialogues generated in experiments will also enrich the development of machine translation systems. Finally, the work will explore new technical issues around real-time keyword highlighting and multilingual picture retrieval. The project will also provide new tools and knowledge for communication across language boundaries that will be made widely available to the research community and general public, thus improving people's ability to engage with others across language boundaries, in global organizations and at the personal level, on social network sites and other international forms of new media.","title":"HCC: Small: Understanding and Supporting Communication Across Language Boundaries","awardID":"1318899","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[550147],"PO":["565342"]},"205573":{"abstract":"For over two decades, researchers have shown the potential of augmented reality (AR) to transform computer graphics into an everyday extension of human vision that can enhance such diverse applications as medicine, manufacturing, maintenance, smart offices, and navigation. However, there is virtually no use of augmented reality by the public or industry today. The investigators believe that this discrepancy is due largely to the lack of suitable high performance and widely applicable augmented reality displays on which applications can be deployed. The most capable AR displays available today, optical see-through head-mounted displays (HMDs), generally lack four key qualities that prevent their widespread use: wide field-of-view, non-encumbering, support for mutual occlusion, and preservation of most depth cues. The investigators know of no existing or proposed displays that feature all, or even most, of these capabilities. This project takes a radically different approach to optical see-through design that offers the potential to deliver all four missing qualities in a compact form factor that approaches ordinary glasses. The approach relies on a multi-layer display architecture that follows the principles of the emerging field of computational displays - simple optical devices whose functionality and complexity generally lies in software. This project applies existing multi-layer optimization techniques from desktop 3D displays to optical see-through HMDs, while exploring new approaches such as perceptual error metrics, the use multiple layers for occlusion masks, and field of view zone prioritization. This knowledge will be used to build prototype optical see-through displays while handing such challenges as calibration, tracking, computational complexity, and latency. The performance of this approach will be robustly tested and evaluated in simulation, with calibrated cameras, and with human viewers. The target device will transform augmented reality, allowing society to take advantage of the diverse set of applications that have been studied in AR. The proposed design is a radically different approach to optical see-through displays that uses spatial light modulators and software optimization to replace conventional reflective, refractive, and diffractive optics. The ability to produce a focused image on a display placed closer than the eye can accommodate without the use of lenses will be investigated. Sharing of display components for both image formation and occlusion masking will also be researched. The investigators will also explore the use of multi-layer optimization to create multi-focal imagery, prioritize different areas over the viewer's field of view, and facilitate eye tracking.<br\/><br\/>Broader Impacts: Research and practice have shown the promise of augmented reality to improve such diverse areas as medicine, accessibility, worker efficiency and communications. However, to date there is very little use of augmented reality by the public or industry. This project will lead to a high performance augmented reality display that is badly needed to make the field practical and allow the public to reap the benefits of years of visionary augmented reality research. The science and technology developed in this project will open the use of augmented reality to a wider class of researchers, similar to how the recent development of the commodity depth sensor has permitted new opportunities for scientific inquiry.","title":"HCC: CGV: Small: Eyeglass-Style Multi-Layer Optical See-Through Displays for Augmented Reality","awardID":"1319567","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[550455],"PO":["565362"]},"202185":{"abstract":"It has been widely acknowledged that recognizing objects in images, and human activities in video - the basic problems in computer vision - can be significantly improved by accounting for object (activity) parts, context, and their spatiotemporal relationships. This is because these constraints facilitate resolving ambiguous hypotheses in the face of uncertainty. Since parts and contexts can be efficiently modeled by graphical models (e.g., Conditional Random Field), object and activity recognition are often formulated as probabilistic inference of graphical models. The project develops a new theoretical framework of graphical models that explicitly encodes high-order, spatiotemporal, hierarchical, and contextual interactions among objects (activities) as Quadratic Mutual-Exclusion Constraints (QMCs), for the purposes of object and activity recognition in images and video.<br\/><br\/>The key contributions of the project work include: 1) Approaches to view-invariant object and activity recognition; 2) Formulations of learning and inference of graphical models representing objects and human activities, as finding a maximum weight subgraph (MWS) under the QMCs; 3) Polynomial-time algorithms for solving the MWS problem subject to QMCs; and 4) Explicit performance bounds and theoretical guarantees of tightness and convergence of the proposed learning and inference algorithms. <br\/><br\/>The project framework encodes hard constraints from the domain of interest that have never been used in prior work, and uses principled, polynomial-time algorithms for learning and inference. The research of this project advances the state of the art in object and activity recognition, and enables new applications including video surveillance, retrieval from large datasets, and perception of mobile robots.","title":"RI: Medium: Collaborative Research: Object and Activity Recognition as the Maximum Weight Subgraph Problem with Mutual Exclusion Constraints","awardID":"1302164","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[541851],"PO":["564316"]},"205342":{"abstract":"Database management systems (DBMSs) are ubiquitous in industry and their performance and functionality are crucially important in commercial IT infrastructure. There has been a lot of work on improving the performance of these critical systems. This project adopts a complementary and orthogonal approach to substantially increasing the performance of DBMSs. This is done by exploiting runtime information about the DBMS to specialize the DBMS code on the fly in order to eliminate as much unnecessary work as possible. This dynamic code optimization, which we term \"micro-specialization\", is highly aggressive and goes well beyond what can be achieved using existing compiler optimization technology. This focus on runtime code optimization is also very different from most of the current research on DBMS performance improvement. The project investigates algorithms for automatic identification of code sequences where micro-specialization may profitably be applied; algorithms that can micro-specialize such identified target code sequences; and techniques for ensuring the correctness of the resulting code. Preliminary studies on disparate DBMSes suggest that micro-specialization offers significant performance improvements when applied to mature, high-performance DBMSs and to industry-standard benchmarks, and that there are rich opportunities for other substantial performance gains, with minimal impact on the DBMS architecture.<br\/><br\/>Broader impacts of this project include increased efficiency of the IT infrastructure used by a wide variety of companies, leading to improved overall productivity. Graduate and undergraduate students are involved in all aspects of the research activities as an integral part of the project. The PIs have an established track record of integrating research activities into the undergraduate curriculum and involving undergraduates, including women and members of underrepresented minorities, in research; this project continues this tradition.<br\/><br\/>For further information see the web site at http:\/\/www.cs.arizona.edu\/projects\/microspecialization\/ .","title":"III: Small: Extending and Automating Dynamic Specialization of Database Management Systems","awardID":"1318343","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["560601","550173"],"PO":["563727"]},"205463":{"abstract":"The main objective of this project is to develop scalable algorithms for learning normative patterns and anomalies in graph streams, where the patterns are known, unknown but fixed, or changing over time. The project team is pursuing several techniques, including partitioning the graph over time, processing only the changes to the graph over time, and parallel implementations on high-performance computing platforms. They are evaluating the effectiveness and efficiency of these algorithms in terms of expected data sizes, data rates, and recall\/precision using several real-world, large, dynamic datasets as well as synthetic data. They are also evaluating the discovered patterns and anomalies for their significance in the target domains. This research is advancing the knowledge and understanding of how to efficiently process large, high-rate data streams represented as a graph in order to learn structural patterns and detect structural anomalies in real time. The algorithms developed under this project represent a new level of scalability that is necessary to address today?s massive, dynamic data environments, as well as users' needs to quickly discover actionable intelligence in the form of trends and anomalies. <br\/><br\/>This project impacts the scientific research community by advancing the state-of-the-art in mining graphs for patterns and anomalies in large, dynamic data streams, and disseminating these research results via publications, software tools and data to be provided on the project website. This project also impacts education via the inclusion of research results into existing courses at the teams' institutions, and the dissemination of these curricular materials via the project website. The project supports the research training of two graduate students, utilizing recruiting efforts from underrepresented groups to assist in the selection of these students. The project benefits society by providing efficient and effective tools for detecting patterns and anomalies in data that can lead to new discoveries in a variety of domains where large amounts of dynamic data are available, including national security, cyber-security, and social media.<br\/><br\/>For further information see project website: http:\/\/ailab.wsu.edu\/adgs","title":"III: Small: Collaborative Research: Anomaly Detection in Graph Streams","awardID":"1318957","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550177],"PO":["563727"]},"209940":{"abstract":"CNRI is proposing to develop a framework and a related set of infrastructural tools that will greatly improve the ability of research organizations to register scientific data sets, either those they hold directly or those which they have funded and which are held elsewhere, and expose them for discovery, analysis, and further processing. The project will build the tools and low-level APIs to be suitable for use by data producers as well as organizations that are expert in metadata and data organization.<br\/><br\/>There is no widely adopted infrastructure currently in place for sharing research data. Individual pieces certainly exist, especially within given domains, but transparent and seamless sharing of scientific data requires a level of standardization and acceptance that simply doesn't exist today. To realize the potential of widely available scientific data, it must be discoverable, reference-able, and understandable, and it must be so without the investment of enormous amounts of time and effort on the part of those who are providing the data or those consuming the data. Research institutions currently expose their data through institution-specific web sites and APIs. The PI propose to build a pair of registries that will enable the use of a common API as well as the ability to federate registries across institutions when it makes sense, without requiring the existing underlying storage and management systems to change. We also propose to design basic metadata schemas to be used in those registries. <br\/><br\/>The first of the two registries is a metadata registry in which data sets can be registered and described. A common API will be built both for the registration process as well as for access to the resulting metadata objects. Each metadata object and, if required, each data set, will be given a unique, persistent identifier. These identifiers will resolve to the metadata objects and data sets respectively and their assignment will be part of the deposit API. We will also enable related objects to be associated with each other through the registry and through identifier resolution, depending on the specific cases in hand. This will be transparent to users of the access API. <br\/><br\/>The second of the two registries is a type registry. The metadata objects and data sets will each be typed and the type registry will provide the information needed to decipher those types. The goal is to be able to answer the question of, given a specific identifier or piece of data, what does it represent and how should I interpret it. This interaction will be made as transparent as possible to the access API. The interaction between these two registries is key to the proposed framework. <br\/><br\/>The proposed deliverables will include an open source release of the metadata registry and the type registry software, the basic metadata schemas applicable for those registries, and a prototype service that demonstrates the infrastructure capability by federating research data from at least two sources.","title":"EAGER: Infrastructure for Research Data Registration and Interpretation","awardID":"1349985","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[562790,562791],"PO":["565292"]},"205353":{"abstract":"This project develops robust and persistent algorithms for mapping and localization using low-cost visual\/depth cameras and inertial sensors. New map representations and algorithms are developed to provide computationally efficient long-term 3D mapping and navigation. Topics of investigation include incremental non-Gaussian inference techniques, dense mapping, change detection in dynamic environments, and semantic understanding. A lack of robustness has been a key shortcoming of previous techniques for localization, and thwarted the development of persistently autonomous mobile robot systems. Extension to multimodal distributions poses significant intellectual difficulties. Dense methods are transforming robotic perception, enabling sophisticated physical interaction with objects, traversal of stairs, and safe maneuvering in cluttered and confined spaces. Whereas most past research in robotic mapping has assumed a static world, the approach being developed in this grant exploits the dynamics of the world to discover information about objects and places. These advances are being tested for robotic and man-portable sensing systems operating in indoor, outdoor, and underwater environments. The expected impacts span a broad range of applications, from robotic manufacturing, medical robotics, agriculture, and space and underwater exploration, in which perception is a key requirement. Other potential spin-offs include human-portable mapping applications in real estate, construction, and facility maintenance, health and safety. MIT Online Robotics Education provides a set of online course materials for core topics in robotics, targeted to a broad audience for high school and college education. Open source software modules provide positioning capabilities for low-cost robots for education and service robotics applications.","title":"RI: Small: Robust and Long-Term Visual Mapping and Localization","awardID":"1318392","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[549887],"PO":["564069"]},"205375":{"abstract":"Software engineers use various \"programming languages\" to write the software that runs on our computers, smart phones, cars, and other devices. Modern software is a complex mix of pieces written in many different languages. But cross-language communication is difficult and error-prone, for humans and machines alike. The PI's research will ease the burden of developers working across multiple programming languages by automatically generating the \"glue\" that sticks different fragments together, which will let developers work in more modern, safe, high-level languages while still benefiting from the large existing investment in older, unsafe, low-level code. The result will be faster, cheaper development by productive programmers, leading to more trustworthy software to benefit society as a whole.<br\/><br\/>Specifically, the PI will use a variety of static program analyses to address three recurring challenges of multi-language software development: disallowed values, resource management, and error reporting. Disallowed values, such as unexpected NULL pointers, are a form of API misuse that can lead to failures or undefined behaviors. By identifying these and trapping them in high-level code before they reach low-level libraries, the PI's research will allow safer execution and improved failure diagnostics. Resource management requires uncovering the subtle, sometimes quirky rules by which low-level libraries track memory, file descriptors, and other resources. Static analyses to recover resource management models will let these be managed automatically by the improved mechanisms (e.g., garbage collectors) available in high-level code. Lastly, error reporting analysis will identify ad hoc error propagation mechanisms (such as returned error codes) used in low-level libraries and map these to the structured high-level mechanisms (such as exceptions) that high-level application developers expect. The net effect of these will be improved library bindings that let high-level programmers do their jobs more efficiently, introducing fewer bugs, creating tomorrow's great software for all of us to rely upon.","title":"SHF: Small: Contract Inference for Polyglot Programming","awardID":"1318489","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[549944],"PO":["564588"]},"205397":{"abstract":"Planar complementary metal-oxide-semiconductor (CMOS) technology scaling is coming to an end with the adoption of FinFETs (a type of field-effect transistor or FET) at the 22nm technology node and beyond. As a result of having multiple gates wrapped around the fin body, FinFETs exhibit better control of the channel potential with scaling, thereby alleviating the explosive leakage current problem faced by planar short-channel devices. Since static random access memory (SRAM) bit-cells are often the densest features patterned on an integrated circuit, researchers have begun investigating the design and manufacturability of FinFET SRAMs. Most of these investigations focus on enhancing\/contrasting SRAM direct current (DC) metric targets. However, such metrics are not an accurate guide to FinFET bit-cell designers, as parasitic capacitances for two topologically equivalent bit-cells can be very different (due to differing fin pitches, etc.), resulting in widely varying transient characteristics. Thus, in order to predict array-scale metrics through simulation, capturing transient behavior accurately is absolutely essential. To accomplish the latter, SRAM parasitic capacitances need to be extracted accurately from the layout. Using an accurate parasitic capacitance extraction method that the principal investigator's (PI's) group has developed, the project plans to explore the FinFET SRAM design space from both DC metrics and transient behavior points of view, under process-voltage-temperature variations. <br\/><br\/>Since SRAMs account for more than 50% of the area of modern microprocessors, it is very important to base them on the best SRAM bit-cell design. A successful conclusion of this work, hence, should be very beneficial to the semiconductor industry. The designs\/methodologies\/tools that are to be developed will be disseminated through the web. Technology transfer will be done through various companies the PI interacts with. The material will be included in a course on Design with Nanotechnologies that the PI teaches. Princeton has a tradition of undergraduate independent research. Many seniors are expected to do their research project on this topic. Female and minority students will be attracted to this research through Princeton's Presidential Fellowship Program. The PI has supervised 10 female Ph.D. students so far. Further outreach activities are also planned for high-school students. The PI has supervised the research of four high-school students in the last two years, including a female student.","title":"SHF: Small: Parasitics-aware Exploration of the FinFET SRAM Design Space","awardID":"1318603","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[550002],"PO":["562984"]},"209874":{"abstract":"Future computing systems will allow computations to be performed simultaneously with massive parallelization. However, current practices in scientific simulations cannot utilize the maximum potential of these machines. This is fundamentally due to the need for data synchronization across computing cores, which may cause up to 80% idling of the machines, and there is a need for new methods to overcome this inefficiency.<br\/><br\/>This research is to develop a new framework for high performance computing where synchronization across the processing elements is relaxed. This will eliminate the overhead associated with extreme parallelism and potentially lay the foundation for simulations at scale. The framework is based on asynchronous model of computation for high performance computing to better utilize future systems. The price to pay for asynchrony is poor predictability of the code, resulting in uncertainty in the calculations. The central theme of this project is to accurately quantify this induced uncertainty and develop techniques to mitigate it. Specific research thrusts include: i) study of numerical stability, consistency and accuracy of widely used numerical algorithms, in the context of fluid-flow, under asynchronous conditions; ii) development of new schemes which can maintain its accuracy under asynchronous conditions; iii) determination of efficient implementations of the resulting algorithms on current and future systems. These research goals are addressed in a dynamical systems framework. The behavior of asynchronous numerical algorithms is modeled as Markov jump systems. Issues related to consistency, numerical stability and error control is addressed by using tools for analysis and design of Markov jump systems.","title":"CCF: SHF: EAGER: Collaborative: Asynchronous Algorithms for Exascale Computing Systems","awardID":"1349017","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":[562623],"PO":["565272"]},"209885":{"abstract":"Future computing systems will allow computations to be performed simultaneously with massive parallelization. However, current practices in scientific simulations cannot utilize the maximum potential of these machines. This is fundamentally due to the need for data synchronization across computing cores, which may cause up to 80% idling of the machines, and there is a need for new methods to overcome this inefficiency.<br\/><br\/>This research is to develop a new framework for high performance computing where synchronization across the processing elements is relaxed. This will eliminate the overhead associated with extreme parallelism and potentially lay the foundation for simulations at scale. The framework is based on asynchronous model of computation for high performance computing to better utilize future systems. The price to pay for asynchrony is poor predictability of the code, resulting in uncertainty in the calculations. The central theme of this project is to accurately quantify this induced uncertainty and develop techniques to mitigate it. Specific research thrusts include: i) study of numerical stability, consistency and accuracy of widely used numerical algorithms, in the context of fluid-flow, under asynchronous conditions; ii) development of new schemes which can maintain its accuracy under asynchronous conditions; iii) determination of efficient implementations of the resulting algorithms on current and future systems. These research goals are addressed in a dynamical systems framework. The behavior of asynchronous numerical algorithms is modeled as Markov jump systems. Issues related to consistency, numerical stability and error control is addressed by using tools for analysis and design of Markov jump systems.","title":"CCF: SHF: EAGER:Collaborative:Asynchronous Algorithms for Exascale Computing Systems","awardID":"1349100","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":[562656,562657],"PO":["565272"]},"209203":{"abstract":"This INSPIRE award is partially funded by the Space Weather Research Program in the Division of Atmospheric and Geospace Sciences in the Directorate for Geoscience; the Human Centered Computing Program in the Division of Information & Intelligent Systems in the Directorate for Computer & Information Science & Engineering; and the Advancing Informal STEM Learning Program in the Division of Research on Learning in Formal and Informal Settings in the Directorate for Education and Human Resources. <br\/><br\/>This is a two-year inter-disciplinary project pursuing tightly coupled goals within human centered computing, citizen science, and space weather research. The aurora borealis of the northern hemisphere and its twin, the aurora australis of the southern hemisphere, are among the most beautiful and awe-inspiring of natural phenomena. They are a manifestation of the interaction of solar plasma with the Earth's atmosphere, magnetic field, and surface, the combined effect of which is termed space weather. As the aurora is a visible manifestation of space weather, observations of aurora are potentially a means of forecasting its catastrophic extremes. Capitalizing on public curiosity of normally intangible plasma physics, the objective of this project is to create a system for collecting, analyzing, interpreting, and redistributing data on the dynamics and evolution of auroral events using crowd-sourced ad hoc Tweets and more purposeful postings from citizen scientists. The current solar maximum is the first since the emergence of the ubiquitous use of social media that has changed - and will continue to change - our interactions with computers and the world. Building on a demonstrated prototype system, the project is poised to take advantage of the approach in 2013-2014 of the maximum in the current 11-year solar activity cycle, with several high activity years following. <br\/><br\/>The team combines expertise in space weather science, human-computer interface design, and informal science education to realize each of its intertwined goals. <br\/>1) For space science, the contribution will be a totally new data source for auroral observations and the potential for real-time, higher-resolution space weather forecasts that are a critical step towards transforming our ability to protect and manage critical infrastructure susceptible to interruption and damage. With crowd-sourced data and user contributions, it is possible to achieve the increased density of high quality data needed for improved predictions. State of the art human-computer interfaces, for data upload, analysis, and interpretation that make participation easy, intuitive, and rewarding, will be developed to ensure the high quality data critical to forecasting.<br\/>2) For the field of human-centered computing, the creation of new frameworks will transform our understanding of how the emergent processes of crowd-sourced knowledge and labor come together for scientific discovery under the structures of networked computer platforms. Specifically, the stickiest problem in making crowd-sourced media actionable is the verification of the messages received at a high enough tolerance level for organizational decision-making. A transformative approach will be adopted, employing verification techniques within a community of active participants who will also engage the data, offering human intelligence in collaboration with machine intelligence.<br\/>3) The education approach is innovative and potentially transformative in its use of social media to explore the beautiful and mysterious aurora, through which participants in a dynamic social network will come to understand the relevance of space weather to their lives. Intellectually engaging resources, research projects, and motivational incentives for participation will help build a community of citizen scientists committed to advancing knowledge of space weather.<br\/><br\/>This low-cost, citizen science system for improved forecasting of geomagnetic storms has the potential to transform the way space weather prediction is done and considering the enormous potential cost to society of damage due to such storms would be cost-effective. The project will help enhance public understanding of this little known phenomenon so that citizens are aware and prepared to respond to the effects of space weather. Resulting new understanding of effective approaches to citizen science and the impact of human computer interactions on motivations and success at learning will have value to a wealth of other ongoing citizen science programs.","title":"INSPIRE Track 1: Aurorasaurus - Citizen Scientists Experiencing the Extremes of Space Weather","awardID":"1344296","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["455191",560823,"563414"],"PO":[560825]},"209687":{"abstract":"A basic security concern inherent to outsourced computating services is guaranteeing the integrity of the information received from the cloud. The concern relates both to outsourced data storage and to the results of outsourced computations. There are several aspects to this problem like ensuring software correctness, protecting against intentional deviation and shortcuts, maintaining data provenance, and guaranteeing results of distributed computation from mutually distrusted sources. Moreover, it is not enough to provide separate guarantees on each axis, solutions to these concerns must compose.<br\/><br\/>This proposal concentrates on the design of mechanisms that enable clients to verify the correctness of the results of outsourced computations, authenticity and provenance of remotely stored data, and the combination of the two. Particular emphasis is given to mechanisms that provide composable security guarantees, where one can combine the security properties of several individual mechanisms to a joint, holistic security guarantee, and furthermore make informed and meaningful tradeoffs between performance and verifiability. These new abilities are bound to greatly enhance the acceptance and usefulness of cloud computing, and in particular open it up to new classes of applications and markets.","title":"EAGER: Holistic Security for Cloud Computing: Verifiable Computation","awardID":"1347522","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[562061,562062],"PO":["565264"]},"210160":{"abstract":"In order to develop natural language processing (NLP) technologies for text in a wider range of languages, dialects, genres, and styles, this Early Grant for Exploratory Research investigates a novel methodological approach. Conventionally, linguistic experts are employed to create gold-standard linguistically annotated datasets to which supervised machine learning algorithms are applied. This project frees annotators from the requirement that annotations be complete by moving more of the burden to learning algorithms. Algorithms are developed that are robust to partial evidence, annotator variation, and noise due to errors. As a result, any language enthusiast (not just trained experts) can provide annotations so that NLP can be developed for more kinds of text in more languages for less money. In this exploration, the focus is on dependency parsing, a fundamental NLP component that predicts the grammatical relationships between words in sentences, with experimentation on data in English (two genres), Chinese, and Farsi. The formal basis for the approach is a framework called Graph Fragment Language (GFL). The project assesses the quality of parsers learned from GFL and the productivity of annotators accorded this new flexibility.<br\/><br\/>Beyond documentation and assessment of the new methodology, this project produces open-source software tools for gathering annotated data and constructing NLP tools using the data. It emphasizes the usability of these tools in classrooms, contributing exercises that can be used in NLP and linguistics courses to allow students to engage directly with data, with the models that make use of the data, and with the technological goals that data annotation supports.","title":"EAGER: PARTIAL: An Exploratory Study on Practical Approaches for Robust NLP Tools with Integrated Annotation Languages","awardID":"1352440","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[563245,563246],"PO":["565215"]},"210193":{"abstract":"Discovering and understanding the temporal evolution of events hidden in text corpora is a complex yet critical task for knowledge discovery. Although mining event dynamics has been an important research topic leading to many successful algorithms, researchers, research and development managers, intelligence analysts and the general public are still in dire need of effective tools to explore the evolutionary trends and patterns. This exploratory project focuses on developing and validating a novel idea called narrative animation. Narrative animation uses animated visualizations to narrate, explore, and share event dynamics conveyed in temporally evolving text collections. Film art techniques are employed to leverage the animated visualizations in information organization and change detection, with the goals of enhancing analytical power and user engagement. A prototype system called CityStories is being developed to generate narrative animations of events in cities derived from web-based text. <br\/><br\/>If this novel, risky research is successful, it is expected to yield fundamental results in narrative animation that can advance the current paradigm in information visualization and visual analytics by developing novel techniques in using animations for presenting and analyzing dynamic abstract data at a large scale. The pilot system CityStories system is expected provide a novel network platform for education, entertainment, and data analytics. It will engage general users such as students, teachers, journalists, bloggers, and many others in web information visualization and study. Results of this research will be disseminated through publications, the World Wide Web, and collaborations with researchers and analysts. The project web site (http:\/\/coitweb.uncc.edu\/~jyang13\/narrativeanimation\/narrativeanimation.htm) will include research outcomes, publications, developed software, videos, and datasets for wide dissemination to public.","title":"EAGER: Collaborative Research: Visualizing Event Dynamics with Narrative Animation","awardID":"1352927","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["357887"],"PO":["563751"]},"205980":{"abstract":"Title: Collaborative Research: SAVI- Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field.<br\/><br\/>This award is designated as a Science Across Virtual Institutes (SAVI) award and is being co-funded by NSF?s Office of International Science and Engineering. Four US environmental observatories (National Ecological Observatory Network (NEON), Ocean Observatories Initiative (OOI), Advanced Modular Incoherent Scatter Radar (AMISR) and collectively EarthScope, comprising USArray and the Plate Boundary Observatory (PBO), operated respectively by the Incorporated Research Institutions for Seismology (IRIS) and UNAVCO) are working together with their respective counterparts from the European Union on developing common data policies and standards relevant to global research infrastructures in the environment field. <br\/><br\/>The project goals include developing new understanding through broad harmonization of data and constructing multi- discipline, synergistic data products that have wider societal importance. In addition, these activities will help the scientific community to better develop coupled models that better capture critical feedbacks and interactions of the earth system. These models are needed to help make these types of data more accessible to decision-makers at many levels. <br\/><br\/>Linking these existing programs provides a unique opportunity for early career researchers and students to learn in a multi-disciplinary environment and to benefit not only from the US participants but also their European counterparts. The observatories will be developing graduate and post-graduate courses and the workshops will seek to include early career scientists and students. <br\/><br\/>The proposed activities will focus on addressing societally relevant challenges. Through case studies, ways of harmonizing data will be investigated with the goal of making the data from these observatories easier to include in effective decision-making. Disasters, carbon and water will likely be the first set of issues addressed.<br\/><br\/>This program is creating opportunities for enhancing the career trajectories of a new generation of researchers in Europe and the U.S. The virtual institute is providing mechanisms for exposing early-career scientists to interdisciplinary, multi-institutional activities focused on environmental data and cyberinfrastructure; arming them with new scientific tools to address challenging questions in harmonizing environmental data to help in effective decision making; and showing them how international partnerships can help to solve global problems. It is recruiting a diverse set of US and European students to create collaborative networks of environmental data and cyberinfrastructure experts across these countries.","title":"SAVI: Building a framework between the EU and the USA to harmonize data products relevant to global research infrastructures in the environmental field","awardID":"1321641","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7726","name":"DATANET"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1679","name":"INTERNATIONAL COORDINATION ACT"}}],"PIcoPI":[551467],"PO":["565235"]},"204770":{"abstract":"This research applies anthropological methods to study cybersecurity analysts working in Security Operation Centers (SOC). These analysts process large amounts of data while handling cyber threats. The job requires intelligence and high levels of skills but has many mundane\/repetitive aspects. Adequate tool support is largely lacking and many of the skills and procedures involved are uncodified and undocumented resulting in a large body of \"tacit knowledge.\" This project places researchers trained in both cybersecurity and anthropology into SOCs, working side by side with the analysts. This \"participant observation\" approach provides a means to access the tacit knowledge of the analysts and to convert it into more explicit knowledge, leading to the development of algorithms that can help automate the tasks. The ethnographic fieldwork also provides an opportunity to observe real security operation centers' work processes and identify factors that influence the effectiveness and efficiency with which cybersecurity incidents are handled. This helps to explain why some cybersecurity problems are hard to address in practice, what roles humans and organizational structures play, and where procedures might be inefficient or completely fail for non-technical reasons. The research is carried out through a collaborative effort involving researchers from Kansas State University and two companies. Results from the research will create practical tools that leverage tacit knowledge in security analytics and automate tasks such as incident response and forensic analysis. Research findings also inform the training of cybersecurity professionals by making explicit the tacit knowledge of effective security analytics acquired during participant observation.","title":"TWC SBE TTP: Medium: Bringing Anthropology into Cybersecurity","awardID":"1314925","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[548366,548367,548368,548369,548370],"PO":["565327"]},"204672":{"abstract":"Computerized systems are present in various aspects of modern society. These systems are used to access and share confidential information. Such sharing is achieved through cryptographic protocols which often employ randomization to introduce unpredictability in their behavior to achieve critical security objectives and make it difficult for the malicious adversaries to infer the underlying execution of the participants. It is imperative to ensure that these protocols meet their security objectives such as confidentiality, privacy, fair exchange, anonymity and availability, as serious flaws have often been discovered in widely used cryptographic protocols. Given the ubiquitous role played by these security protocols and the socio-economic-political consequences that incorrect designs of cryptographic protocols may have, reasoning about their correctness is an important social imperative. This task is challenging because of the presence of malicious adversaries on the Internet as well as the subtle interaction between the concurrent nature of Internet and the various features such as cryptography and randomization used by the protocols. Hence, the development of automated techniques to verify their correctness is needed to manage this complexity, and this is the focus of this project.<br\/><br\/>The presence of randomization introduces subtle challenges in verifying the correctness of security protocols. In particular, when reasoning about adversarial behavior, one must only consider those behaviors in which the scheduling of actions of the adversary is independent of the private random choices of the individual participants. This project aims to develop scalable techniques and tools that faithfully, and automatically verify randomized cryptographic protocols by considering only attacks (by an adversary) that are oblivious of the private data and private coin tosses of protocol participants. There are primarily three research tasks identified in this project. First, theoretical completeness results will be established that will reduce the general security problem for unbounded protocol sessions, session identifiers, and messages to the finite bounded cases. The other two tasks will be devoted to making the finite bounded case more amenable to automation. In the second research task, we will develop automated techniques to verify safety properties of protocols based on new symmetry reduction techniques using SMT solvers. The third research task will develop automated techniques for verifying indistinguishability properties of protocols. We will investigate symmetry reduction techniques using SMT solvers for this task as well.","title":"TWC: Medium: Collaborative: Automated Formal Analysis of Security Protocols with Private Coin Tosses","awardID":"1314338","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[548117],"PO":["564388"]},"202252":{"abstract":"The increasingly large amounts of Electronic Medical Record (EMR) data offer unprecedented opportunities for EMR data mining to enhance health care experiences for personalized intervention, improve different diseases risk stratifications, and facilitate understanding about disease and appropriate treatment. To solve the key and challenging problems in mining such large-scale heterogeneous EMRs, the investigators aim to develop: (i) new computational tools to automate the EMRs processing, including new techniques for filling in missing values using a new robust rank-k matrix completion method; (ii) annotation of unstructured free-text EMRs using multi-label multi-instance learning; (iii) a new sparse multi-view learning model to integrate heterogeneous EMRs to predict the readmission risk of Heart Failure (HF) patients and to support personalized intervention; (iv) novel methods for identifying the longitudinal patterns using high-order multi-task learning; (v) a nonparametric Bayesian model for predicting the event time outcomes of the HF patients readmission. <br\/><br\/>The sparse multi-view feature learning and robust multi-task longitudinal pattern finding algorithms have a broad range of applications beyond EMR data mining. Free dissemination of source implementations of the algorithms enable other researchers to further develop and apply the resulting techniques. In particular, the methods and tools are expected to impact other EMR and public health research. This project offers enhanced opportunities for research-based advanced training of students (including members of minorities and under-served populations) and integration of research results into curricula at the University of Texas at Arlington, the University of Texas Southwestern Medical Center at Dallas, and Southern Methodist University. For further information see the web site at: http:\/\/ranger.uta.edu\/~heng\/NSF-III-1302675.html","title":"III: Medium: Collaborative Research: Robust Large-Scale Electronic Medical Record Data Mining Framework to Conduct Risk Stratification for Personalized Intervention","awardID":"1302564","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[542027],"PO":["565136"]},"205530":{"abstract":"The goal of this project is to develop scalable techniques and system architectures for transaction management in cloud computing systems. In the recent years, various data management systems based on key-value storage models have emerged to address the scalability requirements of cloud based applications. However, these systems do not provide rich data management primitives, such as multi-row serializable transactions, often needed in many application domains. This project is investigating scalable transaction management techniques for key-value based data storage systems with the aim of supporting multi-row transactions with ACID (atomicity, isolation, durability, consistency) properties in such systems. For scalability, this investigation is based on utilizing the snapshot isolation model. Additional issues arise in supporting transaction management for geographically replicated data across different cloud datacenters. Considering that weaker consistency models can be adequate in certain applications, this project aims to develop techniques for supporting different transactional consistency models such as snapshot isolation, causal consistency, serializability, and session-level consistency for geo-replicated data in cloud applications. Moreover, the goal of this project is to exploit the semantics of data operations for enhanced performance and availability. <br\/><br\/>This project aims at enabling the use of key-value based cloud data storage models for a large-class of applications with various different data consistency requirements. The lack of transaction support with strong consistency models in cloud computing systems has been an obstacle in adoption of such systems in many critical application domains. A potentially transformative impact of this project will be towards enabling a broader class of applications to utilize cloud computing systems.","title":"CSR:Small: Scalable Transaction Management and Consistency Models for Cloud Data Storage Systems","awardID":"1319333","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["74434"],"PO":["565319"]},"206982":{"abstract":"This project is funded as part of the United States-Israel Collaboration in Computer Science (USICCS) program. Through this program, NSF and the United States - Israel Binational Science Foundation (BSF) jointly support collaborations among US-based researchers and Israel-based researchers.<br\/><br\/>This collaborative reserach project between Duke University and Tel Aviv University aims to study several topics in geometric optimization and related problems that arise in the processing of geometric data in a variety of application areas, such as sensor networks, imaging, surveillance, navigation, geographic information systems, modeling and animation, meshing, computer graphics and vision, bioinformatics, robotics and manufacturing. Processing geometric data in these applications is challenging, because this data is typically huge, measured with uncertainty, obtained in a distributed manner (e.g., in sensor networks), or in an online manner (e.g., in streaming applications), and may involve moving points (e.g., in imaging, animation and modeling). All these traits make the processing a rather demanding task, and call for the design of novel algorithmic techniques for handling it efficiently. For many of these problems, exact algorithms, even polynomial in the input size, are impractical, and approximation algorithms are needed. Even then, making these algorithms depend efficiently on the error parameter is often a difficult and challenging task.<br\/><br\/>The project addresses the above challenges by developing general algorithmic techniques such as computing geometric summaries (including coresets and random samples), handling noisy geometric data under various probabilistic models of uncertainty, handling online data, and handling kinetic data (involving moving objects). Some of the specific problems that are targeted include shape matching, clustering, and geometric searching. Each of these topics raises interesting algorithmic questions, both theoretical and practical, and the project will address as many of them as possible.","title":"BSF:201229:Efficient Algorithms for Geometric Optimization","awardID":"1331133","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":[554177],"PO":["565157"]},"205541":{"abstract":"Pluggable types allow programmers to extend a language's type system to enhance program correctness and program security. Unfortunately, pluggable types require annotations in the program, and therefore, place a burden on programmers. This annotation burden is one reason why pluggable types have not been widely adopted in practice. This project will develop techniques that will allow programmers to realize the benefits of pluggable types without incurring the annotation burden. One concrete application (and thrust of the project) tackles security and privacy of Android apps.<br\/><br\/>Pluggable types will become more important, as JSR 308 (Type Annotation Specification) becomes part of Java 8 in 2014. The PI has developed a framework for inference and checking of context-sensitive pluggable types. The framework is instantiated to nontrivial systems and has inferred and checked close to a million lines of Java code in a modular and compositional manner. The key innovations in the framework are (i) support for context sensitivity, which allows instantiation to precise type systems such as Purity and Ownership, and (ii) a scalable inference engine, which allows type inference with zero or very small number of programmer annotations. The key insight is that viewpoint adaptation, a concept from Universe types, elegantly enables context sensitivity, both in the specification of the type system and in the type inference analysis. The project will advance the framework towards applications in concurrency, sustainable computing and security. Notably, the project will leverage the framework towards modular and compositional information flow analysis for Android; this will help address standing issues such as (i) the large Android library, and (ii) implicit information flow.","title":"SHF: Small: Inference and Checking of Context-sensitive Pluggable Types","awardID":"1319384","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550382],"PO":["564588"]},"204694":{"abstract":"Computerized systems are present in various aspects of modern society. These systems are used to access and share confidential information. Such sharing is achieved through cryptographic protocols which often employ randomization to introduce unpredictability in their behavior to achieve critical security objectives and make it difficult for the malicious adversaries to infer the underlying execution of the participants. It is imperative to ensure that these protocols meet their security objectives such as confidentiality, privacy, fair exchange, anonymity and availability, as serious flaws have often been discovered in widely used cryptographic protocols. Given the ubiquitous role played by these security protocols and the socio-economic-political consequences that incorrect designs of cryptographic protocols may have, reasoning about their correctness is an important social imperative. This task is challenging because of the presence of malicious adversaries on the Internet as well as the subtle interaction between the concurrent nature of Internet and the various features such as cryptography and randomization used by the protocols. Hence, the development of automated techniques to verify their correctness is needed to manage this complexity, and this is the focus of this project.<br\/><br\/>The presence of randomization introduces subtle challenges in verifying the correctness of security protocols. In particular, when reasoning about adversarial behavior, one must only consider those behaviors in which the scheduling of actions of the adversary is independent of the private random choices of the individual participants. This project aims to develop scalable techniques and tools that faithfully, and automatically verify randomized cryptographic protocols by considering only attacks (by an adversary) that are oblivious of the private data and private coin tosses of protocol participants. There are primarily three research tasks identified in this project. First, theoretical completeness results will be established that will reduce the general security problem for unbounded protocol sessions, session identifiers, and messages to the finite bounded cases. The other two tasks will be devoted to making the finite bounded case more amenable to automation. In the second research task, we will develop automated techniques to verify safety properties of protocols based on new symmetry reduction techniques using SMT solvers. The third research task will develop automated techniques for verifying indistinguishability properties of protocols. We will investigate symmetry reduction techniques using SMT solvers for this task as well.","title":"TWC: Medium: Collaborative: Automated Formal Analysis of Security Protocols with Private Coin Tosses","awardID":"1314485","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["550555","553664"],"PO":["564388"]},"202274":{"abstract":"Machine learning (ML) algorithms have become ubiquitous across applications as diverse as science, engineering, business, finance, education and healthcare. However, development of ML software that can scale to massive datasets and that are also easy-to-use remains a challenge in part due to the fact that developing an ML tool currently requires the implementation of a deep software stack, from the actual runtime (i.e., how an ML algorithm is executed) to the API exposed to the users.<br\/><br\/>This project aims to develop DeML, a system to support the authoring and execution of ML tools. Specifically, DeML would allow ML algorithms to be formulated in the form of a declarative query over the training dataset. DeML optimizes the execution of the query over a computing platform (e.g., Amazon EC2 or SQL Azure), taking into account the characteristics of the algorithm, the data, and the available computational resources. Adoption of DeML would greatly reduce the effort required to develop scalable implementations of ML algorithms. The project is organized around three thrusts: (i) Development of a declarative query language, based on extensions of Datalog; (ii) Analysis of runtime of DeML queries; (iii) Optimization of dataflow of DeML queries based on the characteristics of data sources and the capabilities of the underlying execution platform. The resulting open source DeML prototype implementation will be made freely available to the community through the project web page at: http:\/\/deml.cs.ucla.edu.<br\/><br\/>The availability of the DeML could greatly lower the effort needed to author scalable implementations of ML algorithms for analysis of massive datasets, which in turn would increase the availability of such tools to the broader community. Experience gained by implementing and deploying ML algorithms at scale over modern cloud-computing platforms, could help inform critical design choices in the development of future cloud computing platforms for big data analytics, and hence impact a broad range of scientific, engineering, national security, healthcare and business applications of big data analytics. The project offers enhanced opportunities for research-based advanced training of graduate and undergraduate students, including members of groups that are currently under-represented in computer science, in databases, machine learning, and cloud computing.","title":"III: Medium: Collaborative Research: Scaling Machine Learning to Massive Datasets---A Logic Based Approach","awardID":"1302698","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["562969",542091],"PO":["565136"]},"205431":{"abstract":"Variability in software systems is very common and important to tailor<br\/>the system to user needs and to use the system flexibly in different<br\/>contexts without developing a new system from scratch for each context.<br\/>For example, embedded systems with resource constraints,<br\/>high-performance optimizations, and end-user customizable systems all<br\/>exploit variability. However variability in software systems creates a<br\/>fundamental tension between (a) reuse and (b) development and<br\/>maintenance costs. On the one hand, planning variability upfront enables<br\/>systematic reuse and promises to significantly reduce development<br\/>effort, costs, and time to market. On the other hand, variability<br\/>requires an upfront investment and causes long-term maintenance costs,<br\/>because variations give rise to an exponential number of configurations<br\/> with potentially intricate interactions. Especially if introduced in<br\/>an ad-hoc fashion and not managed appropriately, variability can<br\/>increase maintenance costs to a level that outweighs the expected<br\/>benefits. This research develops and integrates reverse-engineering<br\/>techniques for variability to lower upfront investments for variability<br\/>and to reduce the costs of long-term maintenance caused by accidental<br\/>complexity of existing ad-hoc variability implementations.<br\/><br\/>This project revisits how variable and reusable software is implemented<br\/>by encouraging lightweight and incremental implementation mechanisms and<br\/>providing migration mechanisms for all ad-hoc legacy implementations<br\/>common in practice, including conditional compilation (#ifdef),<br\/>branches, clones, textual patches, command-line parameters, and<br\/>plug-ins. It integrates research from different communities, including<br\/>product- line analysis, static analysis, refactoring, concern location,<br\/>and architecture recovery toward a common goal of reverse engineering<br\/>variability implementations . This integration will allow scaling<br\/>research and practice of analyzing and migrating variability from core<br\/>calculi to real-world systems of the size and complexity of the Linux<br\/>kernel.","title":"SHF: Small: Reverse Engineering Variability Implementations","awardID":"1318808","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550097],"PO":["564388"]},"205794":{"abstract":"In this Cyberlearning: Transforming Education EXP project, researchers focus on developing the civic innovators of the future. To become civic innovators, learners must gain experience tackling complex, ill-structures design challenges that are not easily solved by a single individual within a fixed time frame. Such education is challenging, and these researchers take advantage of Web 2.0, crowdsourcing, badges, and social media to design means of supporting the learning of young engineers as they tackle civic problems. Supporting learning from design and complex problem solving activities includes providing support for successfully solving problems and achieving goals as well as providing support for reflecting on those experiences to grasp the collaboration and communication skills and begin to learn strategies and tactics for innovating. The Digital Loft being designed for this purpose includes facilities for identifying peer experts who might collaborate on new problems, assigning expertise badges to make it easy to identify potential experts, extracting key design principles from design and problem solving cases and illustrating and indexing them in a case library that is made available as a resource, helping participants identify when they need instruction, and providing means of finding appropriate mentors to provide necessary help. Research focuses on the identifying important characteristics of the human-machine learning ecosystem that promote successful design and problem solving, successful learning, successful innovation, and sustainability of the infrastructure. The research will produce empirically-grounded principles for designing Digital Lofts for civic innovation education and advance understanding of the roles that digital badges, crowd-sourcing, learning by cases, design practice, and social networking can play in promoting innovation learning. Research and development is being done in the context of Design for America, a multi-university collaborative focused on promoting civic innovation among undergraduate engineering students.<br\/><br\/>There is an urgent need for educating civic innovators who can solve our greatest societal challenges. This project explores the feasibility of a Digital Loft for supporting such education. In such a learning environment, engineering students at the undergraduate level participate in addressing community issues together with both local students and participants in other locations who are solving problems in their own communities. Software tools support the kinds of interactions between student engineers that are needed to promote success and learning and the creation of shared resources that will allow participants to build community knowledge that will help them become better innovators. Research addresses issues in the design and integration of several software functions for successfully promoting innovation education.","title":"EXP: Digital Lofts: Online Learning Environments for Real-World Innovation","awardID":"1320693","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7511","name":"TUES-Type 2 Project"}}],"PIcoPI":[550967,550968],"PO":["562669"]},"205321":{"abstract":"The goals of the Havoc project are to provide (a) foundational results on verified compilation of important concurrency abstractions (e.g., locks, monitors, stacks, queues, hash-tables, etc.) into efficient non-blocking variants, (b) a precise memory model for reasoning about the correctness of program transformations performed by the compiler in a shared-memory concurrency programming model, along with detailed experimental validation on the impact of the model's design on compiler transformations and optimizations, and (c) a methodology to formally reason about complex concurrent interactions between application threads and managed components like modern garbage collectors. The primary artifacts of this effort will be formally certified tools, specifically, compilers, and runtime components found in modern managed languages that can be used to replace existing infrastructure, as well as new language-level memory models that are both conceptually cleaner to reason about and deploy within a verified optimizing compiler framework. <br\/><br\/>These artifacts will dramatically change the safety-critical application landscape, which increasingly contains concurrent components, relieving the need for costly manual inspection of source and binary, and enabling a richer class of optimizations. They will greatly assist engineers in the task of constructing high-assurance, mission-critical software systems, such as avionics, medical systems, and military communications systems.<br\/><br\/>The proposed research focus will be on the specification and verification of optimization passes from Java bytecodes generated from a Java application to a low-level intermediate representation (register transfer language), used in the CompcertTSO certified compiler previously developed by the PIs. In addition, the project will undertake the formalization of salient runtime components, including memory management and threads. While patterned after the Java memory model, the memory abstraction underlying the language semantics will be carefully tailored to facilitate mechanized reasoning about program transformations and will be cognizant of the relaxed memory features of the underlying hardware. This project combines infrastructure engineering and scientific advances in software verification.","title":"SHF: Small: Havoc: Verified Compilation of Concurrent Managed Languages","awardID":"1318227","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[549798,"549800",549800,"558390"],"PO":["564588"]},"205442":{"abstract":"The typical tools of complexity theory and algorithms tend to be biased toward low-level models of computation that refer directly to bit-level representations of data. But when programming in the large, there are layers upon layers of abstraction over any low-level details, and a key feature of modern programming is the ability to compose different abstractions without regard to such details. Compositionality has yielded very reliable software development methodologies, in large part because the tools developed for reasoning about program correctness have developed alongside these notions of abstraction. However, tools for reasoning about complexity have not kept pace. The big picture of the principle investigators' research program is to develop techniques for compositional reasoning about complexity, thereby allowing for reasoning about run-time cost in all its facets of large-scale programming with methods similar to those so successfully deployed for reasoning about correctness. The research project funded by this grant concentrates on characterizing sensible notions of feasibility for programs that use coinductively-defined data such as streams (a program that produces or processes streaming media is just the right model to have in mind) and more generally quantifying, in a machine-checkable manner, resource usage for such programs.<br\/><br\/>Although feasibility has been well-studied in the context of finite structures, extensions to potentially infinite data structures such as streams have been somewhat piecemeal. One facet of this research project is to develop principled notions of feasibility in this setting. To do so, the PIs will extend tools such as logic and programming language formalisms that have previously been used to give resource-free characterizations of complexity classes for finite structures. Such tools are already more closely tied to a compositional view of programming, and they give a jumping-off point for analyzing notions of cost in domains where the very definition of resource usage may not be so obvious. One such tool that they developed in previous work is a framework for compositional cost analysis of higher-order programs. This time-complexity semantics is essentially a translation of target-language programs into a domain of complexities, which encode information about evaluation and usage cost. The translation can be automated, and so this framework not only provides a tool for reasoning compositionally about cost, but provides machine-checkable assertions about the cost of target programs.","title":"SHF: Small: Collaborative research:Complexity and feasibility for programs over coinductively-defined data","awardID":"1318864","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550122],"PO":["565264"]},"205563":{"abstract":"With the ever-increasing size and complexity of microelectronic systems, hardware verification has become a dominating factor of the overall design flow. One promising approach is formal functional verification of arithmetic circuits, which attempts to prove correctness of the design with respect to its intended arithmetic function. This problem is particularly challenging since Boolean techniques, traditionally used in verification of control logic, are not scalable to complex arithmetic designs. Efficient solutions to this problem will contribute to the development of state-of-the-art tools for circuit verification, increase design productivity, and lower the design development cost and consumer prices. <br\/><br\/>The goal of this project is to develop efficient solution to verification of arithmetic circuits without resorting to expensive Boolean techniques. It will be accomplished by modeling the problem as a Network Flow problem, in which the circuit is represented as a network of standard arithmetic components. The computation performed by the circuit is modeled as a flow of binary data and represented as an algebraic, pseudo-Boolean expression. Functional correctness of the circuit is proved by transforming the algebraic flow expression at the primary inputs into an expression at the primary outputs and checking if it matches the binary encoding of the output. The method also offers a way to extract the arithmetic function implemented by the circuit and identify bugs in the design. The technique are applicable to complex arithmetic circuits, such as newly developed adders, large multipliers, arithmetic logic units, and other components of combinational and sequential data paths implementing complex instructions.","title":"SHF: Small: Network Flow Approach to Functional Verification of Arithmetic Circuits","awardID":"1319496","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550433],"PO":["565264"]},"205684":{"abstract":"This collaborative research project, conducted jointly by the investigators from the Michigan State University (MSU) and the University of Michigan at Dearborn (UM-D), investigates the issues and techniques for storing and searching\/querying large scale k-mer data sets (i.e., overlapping k-length subsequences obtained from genome sequences) for sequence analysis in bioinformatics. Efficient k-mer indexing, storage and retrieval are vital to sequence analysis tasks like error correction as sequencing data set sizes increase vastly. Most existing methods for storing and searching k-mers are optimized for exact or range queries. However, this reliance limits the types of sequence analysis that can be done efficiently. Moreover, most existing methods for storing k-mers do not support efficient storage of k-mers at multiple word lengths. For many sequence analysis problems, including error correction, variant detection, and assembly, searches with multiple word lengths enable better sensitivity and specificity. In this project, various techniques for efficiently supporting so-called (discrete) box queries and other related queries (e.g., hybrid queries) on large scale k-mer data sets for sequence analysis are investigated. The approaches to optimizing box queries in solving sequence analysis problems like the error correction are examined. The storage structure and adoption of box queries for supporting searches with multiple word lengths on k-mer data sets are explored. The results from this research will advance the state of knowledge for storage, indexing and retrieval techniques for genome sequence databases. They are expected to significantly impact current practice in bioinformatics by making available new efficient on-disk solutions for sequence analysis. They will also impact a number of other popular application areas including biometrics, image processing, social network, and E-commerce, where processing non-ordered discrete multidimentional data is crucial. <br\/><br\/>This collaborative research project, conducted jointly by the investigators from the Michigan State University (MSU) and the University of Michigan at Dearborn (UM-D), investigates the issues and techniques for storing and searching\/querying large scale k-mer data sets for sequence analysis in bioinformatics. Efficient k-mer indexing, storage and retrieval are vital to sequence analysis tasks like error correction as sequencing data set sizes increase vastly. Most existing methods for storing and searching k-mers are optimized for exact or range queries. However, this reliance limits the types of sequence analysis that can be done efficiently. Moreover, most existing methods for storing k-mers do not support efficient storage of k-mers at multiple word lengths. For many sequence analysis problems, searches with multiple word lengths enable better sensitivity and specificity. In this project, various techniques for efficiently supporting so-called (discrete) box queries and other related queries (e.g., hybrid queries) on large scale k-mer data sets for sequence analysis are investigated. In particular, a new index tree, named the BoND-tree, specially designed for a non-ordered discrete data space characterized by k-mer data sets is developed. The unique properties of the space are exploited to develop new node splitting heuristics for the index tree, and theoretical analysis is performed to show the optimality of the proposed heuristics. Besides the BoND-tree, which is based on data partitioning, space-partitioning based index schemes for box quieres in such a space are also developed. To support a more flexible type of query (i.e., hybrid box and range queries), hybrid index schemes integrating strengths of both box query indexes and range query indexes are studied. To facilitate an efficient index construction for large scale k-mer data sets, bulk loading techniques are also developed for the proposed index trees. In addition, the approaches to optimizing box queries in solving sequence analysis problems like the error correction are examined. The storage structure and adoption of box queries for supporting searches with multiple word lengths on k-mer data sets are also explored. The research in the project will result in the discovery of fundamental properties of the data space for sequence data in bioinformatics, the development of a number of novel storage, indexing and retrieval techniques exploiting the properties of such a data space, and the applications of the proposed techniques for solving important problems in sequence analysis. These results will advance the state of knowledge for storage, indexing and retrieval techniques for genome sequence databases. They are expected to significantly impact current practice in bioinformatics by making available new efficient on-disk solutions for sequence analysis. They will also impact a number of other popular application areas including biometrics, image processing, social network, and","title":"III: Small: Collaborative Research: Supporting Efficient Discrete Box Queries for Sequence Analysis on Large Scale Genome Databases","awardID":"1320078","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8083","name":"Big Data Science &Engineering"}}],"PIcoPI":[550712],"PO":["565136"]},"202175":{"abstract":"Modern software systems are notoriously large and complex, often made up of hundreds and thousands of interconnected modules, and lines of source code that can number in the millions. Consequently, software developers waste a considerable portion of their time trying to find and get to information in that code. To address this problem, this research project will investigate a scientific theory to describe how developers go about seeking this information. The project will then use the expanded theory to identify ways of helping developers to more efficiently find information, and in the process, increase their productivity. This contribution will in turn be put to practical use by providing to builders of today?s software tools new practical ways to apply this scientific theory to their tools, so as to reduce the effort required in today?s software development projects.<br\/><br\/>In particular, this project will (1) expand Information Foraging Theory to describe how learning affects developers? navigation through code and related artifacts, (2) generate theory-grounded design patterns explaining how to design tools that aid developers?<br\/>information foraging, (3) develop a design method enabling tool builders to apply these patterns, and (4) evaluate the validity and effectiveness of (1)-(3). Broader impacts will include (a) helping with teaching and learning for software tool builders, professional software developers, graduate students, undergraduates and high schoolers, (b) broadening participation of students from underrepresented groups, (c) producing publicly available infrastructure consisting of design patterns and training materials,<br\/>(d) disseminating results via publications, software, and workshops, and (e) providing a scientific foundation for future efforts aimed at training developers and designing tools for their use.","title":"SHF: Medium: Collaborative Research: Information Foraging Theory: From Scientific Principles to Engineering Practice","awardID":"1302113","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[541827,"548135"],"PO":["564388"]},"205211":{"abstract":"This project addresses manufacturing tasks that cannot be fully automated because of either the limitations of current algorithms or prohibitive cost and set-up time. Such tasks generally require workers to collaborate in close proximity and adapt to each other's decisions and motions. This project explores accomplishing these tasks through human-robot collaboration. Recent hardware developments in robotics have made human-robot collaboration physically possible, but robots still require new algorithms to ensure safety, efficiency, and fluency when working with people. Creating such algorithms is difficult because there can be high uncertainty in what a person is going to do and how they are going to do it. This project explores the integration of reasoning about how a person moves and how he or she makes decisions into a robot motion planning and decision-making framework. The research centers on the development of new algorithmic frameworks for modeling, simulating, and planning for human-robot collaboration, which requires advances in robot training, task modeling, human motion understanding, high-dimensional motion planning with uncertainty, and metrics to assess human-robot joint action. The results of this project have the potential to signi&#64257;cantly improve American competitiveness in manufacturing; especially for small-batch manufacturing and burst production, where the cost and set-up time of fully-autonomous solutions is prohibitive. The work will be disseminated in research papers and integrated into curricula. The project is guided by an advisory board from the manufacturing industry, which provides another avenue for dissemination.","title":"NRI: Small: Collaborative Research: Adaptive Motion Planning and Decision-Making for Human-Robot Collaboration in Manufacturing","awardID":"1317445","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549490],"PO":["564069"]},"205453":{"abstract":"The main objective of this project is to develop scalable algorithms for learning normative patterns and anomalies in graph streams, where the patterns are known, unknown but fixed, or changing over time. The project team is pursuing several techniques, including partitioning the graph over time, processing only the changes to the graph over time, and parallel implementations on high-performance computing platforms. They are evaluating the effectiveness and efficiency of these algorithms in terms of expected data sizes, data rates, and recall\/precision using several real-world, large, dynamic datasets as well as synthetic data. They are also evaluating the discovered patterns and anomalies for their significance in the target domains. This research is advancing the knowledge and understanding of how to efficiently process large, high-rate data streams represented as a graph in order to learn structural patterns and detect structural anomalies in real time. The algorithms developed under this project represent a new level of scalability that is necessary to address today?s massive, dynamic data environments, as well as users' needs to quickly discover actionable intelligence in the form of trends and anomalies. <br\/><br\/>This project impacts the scientific research community by advancing the state-of-the-art in mining graphs for patterns and anomalies in large, dynamic data streams, and disseminating these research results via publications, software tools and data to be provided on the project website. This project also impacts education via the inclusion of research results into existing courses at the teams' institutions, and the dissemination of these curricular materials via the project website. The project supports the research training of two graduate students, utilizing recruiting efforts from underrepresented groups to assist in the selection of these students. The project benefits society by providing efficient and effective tools for detecting patterns and anomalies in data that can lead to new discoveries in a variety of domains where large amounts of dynamic data are available, including national security, cyber-security, and social media.<br\/><br\/>For further information see project website: http:\/\/ailab.wsu.edu\/adgs","title":"III: Small: Collaborative Research: Anomaly Detection in Graph Streams","awardID":"1318913","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550149],"PO":["563727"]},"205574":{"abstract":"Software plays an increasingly important role in science and engineering. The design of aircraft, sky scrapers, and automobiles; climate modeling and weather prediction; and the development of new pharmaceuticals, are just a few of the many endeavors that use computer programs to simulate natural phenomena. Yet studies have shown that many of these programs are ridden with defects (\"bugs\") that may lead to incorrect results. The same is true in other software domains, but the problems with scientific software are particularly acute for several reasons. Most significantly, much scientific software is \"message-passing\" parallel software---designed to execute on \"supercomputers\" which are networks of many thousands of processors. While there are many methods to help develop verifiably correct sequential programs, few of these have been extended to parallel programs. The \"Design by Contract\" methodology --- which works by decomposing a program into parts that can be specified and verified independently --- is one such approach, and has been successfully applied to sequential programs in a variety of domains. Professor Siegel's project is extending that methodology to apply to message-passing parallel programs, enabling the development of much more reliable scientific and engineering applications.<br\/><br\/>The approach generalizes and extends existing contract specification and verification mechanisms in various ways. As in the sequential case, a procedural decomposition is used, but each procedure can be executed by multiple processes that are not necessarily running in lockstep. The contract pre-conditions and post-conditions are interpreted as \"collective assertions\". These are expressions that can refer to the state of multiple processes and have a special semantics: to evaluate such an expression a snapshot of the local state of each process is taken as it passes through the assertion location; once a snapshot has been obtained from each process they are composed to form a global state in which the expression is evaluated. Contracts must also refer to the state of the message buffers, for example, to express that there are no unreceived messages from one process to another. Symbolic execution and model checking techniques are used to verify a procedure satisfies its contract. These ideas are being realized as an extension to the Toolkit for Accurate Scientific Software (TASS), and applied to programs written in C with the widely-used Message Passing Interface.","title":"SHF: Small: Contracts for Message-Passing Parallel Programs","awardID":"1319571","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["561781"],"PO":["564588"]},"205101":{"abstract":"The overarching scientific goal of this project is two-fold: (1) to develop a robotic architecture endowed with moral emotional control mechanisms, abstract moral reasoning, and a theory of mind that allow corobots to be sensitive to human affective and ethical demands, and (2) to develop a specific instance of the architecture for a co-robot mediator between people with \"facial masking\" due to Parkinson's disease (PD) that reduces their ability to signal emotion, pain, personality and intentions to their family caregivers, and health care providers who often misinterpret the lack of emotional expressions as disinterest and an inability to adhere to treatment regimen, resulting in stigmatization. To tackle these problems, the project brings together two roboticists with extensive prior experience in robot ethics and modeling emotions as well as implementing them in integrated autonomous robotic systems. The robotics expertise is combined with that of an expert in early PD rehabilitation and daily social life. The project will build on extensive software, hardware and data set resources, including complex robotic control architectures with ethical control mechanisms, personality and emotion models, and affect and natural language capabilities.<br\/><br\/>The general expected outcome of the project is an architecture for co-robots that can be adapted to a great variety of health care scenarios in an effort to enrich and dignify already stressed and stigmatized relationships between humans. The project also includes novel educational efforts such as a course in occupational therapy robotics as well as significant K?12 outreach through the Tufts Centers for STEM Diversity and for Engineering Education and Outreach, as well as various important community and public activities such as presentations on health care robotics to focus and patient groups.","title":"NRI: Small: Collaborative Research: Don't Read my Face: Tackling the Challenges of Facial Masking in Parkinson's Disease Rehabilitation through Co-Robot Mediators","awardID":"1316809","effectiveDate":"2013-09-15","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549200,549201],"PO":["564069"]},"205343":{"abstract":"The researchers on this project are developing and studying a web-based multi-media platform that will allow special-temporal annotations intended for collaboration in post-secondary courses. The use of digital web-hosted media is becoming increasingly commonplace and important in postsecondary educational environments. Instructors routinely post large collections of images, audio clips, and videos for student use. As instructors continue to adopt innovative pedagogies using these multi-media materials, the need to deliver multimedia content online, outside of the typical class setting, becomes even more important. Hybrid, distance learning, and massive open online courses place additional demands on the delivery of digitally-mediated content.<br\/><br\/>Current hosting solutions for the large amount of educational media lack tools to foster rich collaborative discourse and social learning. Student-to-student interactions that would be trivial in face-to-face settings are not easily reproduced in online environments. In part this is due to the lack of support for spatial-temporal collaboration within media playback systems. In order to address these limitations, this project explores the implementation of novel techniques to support spatial-temporal collaboration over a variety of web-hosted media including video, audio, and still images. The researchers will design, deploy, and evaluate TrACE - the Transformative Anchored Collaboration Environment.<br\/><br\/>The broader impact of the project lies in the potential transformation of how educational content is addressed in asynchronous environments - in particular massive open online courses (MOOCs). MOOCs have the promise of transforming undergraduate education making it available to traditionally underrepresented populations.","title":"EXP: RUI: Exploring Spatial-Temporal Anchored Collaboration in Asynchronous Learning Experiences","awardID":"1318345","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7511","name":"TUES-Type 2 Project"}}],"PIcoPI":[549860,549861,"558320"],"PO":["564412"]},"205354":{"abstract":"Reliable Internet performance and availability are essential for many existing and future network applications. While the Internet works well enough most of the time for most people, nearly everyone has experienced outages and service degradation that make the network unusable, and we are far from five nines of reliability that critical services require. Improving Internet connectivity requires action against all sources of unavailability and poor performance. <br\/><br\/>The research community has made substantial progress toward understanding and developing technologies to address short-term outages due to BGP (border gateway protocol) routing convergence. However, much less progress has been made at reducing the impact of long-term outages and route misconfiguration. Despite being rare, these events have a large impact on overall network availability because repairs happen on a human timescale. Additionally, many users suffer from the use of sub-optimal (high latency or lossy) paths to network services due to misconfigurations and ineffective route selection. Operators at an affected ISP or service often encounter stumbling blocks at each step: identifying that a problem exists, localizing the root cause of the problem, and affecting a repair. <br\/><br\/>The researchers on this project will develop a system to transform this largely manual troubleshooting process into a fully automated one. The goal of the research is that persistent outages and performance problems can be identified in real-time, rather than today's matter of hours. While automated diagnosis and identification of root cause is fundamentally hard, the project will benefit from dramatic recent progress in Internet measurement technologies, specifically reverse path measurement that provides a much more complete picture of the Internet topology than ever before.<br\/><br\/>Intellectual Merit: The goal of the research project is to change the paradigm of network diagnosis on the Internet -- from blind to informed. The state of art with network troubleshooting is to use ad-hoc techniques. For instance, it is common occurrence on the NANOG (North American Network Operators? Group) mailing list for operators to post requests asking other operators to manually issue traceroutes and report them in order to identify network anomalies. The network could thus benefit from a continuously operated service that can not only detect network problems in realtime but also identify misbehaving network elements at the granularity of routers. There are also a number of challenges to deploying a functional diagnosis system, and the researchers will address them using the following key components. First, the project will produce a scalable measurement system that will synthesize measurements from different techniques to provide snapshots of routing behavior in real-time. Second, the research will focus on developing a general theory of Internet path changes that will help model the propagation of routing events and identify the candidate set of responsible ASes (autonomous systems). Third, the researchers will develop inference techniques that will operate on measured data and identify the origin of failures and path changes in the wide area even when the measurement data is incomplete or subject to transient dynamics.<br\/><br\/>Broader Impact: Our society is increasingly relying on the Internet for critical telecommunications services, such as home health monitoring, e-911, smart grids, and so forth. It is no longer simply an inconvenience when the Internet is unavailable or inefficient. If this project is successful, it will help operators address the major sources of unavailability and misconfigurations in the Internet, benefiting all of its users. In addition, because of a lack of automated tools, operators currently spend huge amounts of time chasing down individual outages and performance misconfigurations; this raises the barrier to entry for small ISPs, ultimately raising the costs of Internet service for everyone.","title":"NeTS: Small: Automated Diagnosis and Root Cause Analysis of Internet Problems","awardID":"1318396","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[549889,549890],"PO":["565090"]},"208632":{"abstract":"The project, Houston-Cyberinfrastructure (HoustonCI), establishes a flow separation and analysis system for the University of Houston campus network while enabling high bandwidth data transfers from domain science laboratories on the University of Houston and Rice University campuses. These laboratories all connect to the regional networks LEARN (Lonestar Education and Research Network, Texas-wide network with over 30 institution members), SETG (SouthEast Texas Gigapop, Houston-based fiber loop with University of Houston, Rice University, Texas Medical Center, and other institutional members in Houston area), GENI (Global Environment for Network Innovation), and Internet2 100 Gbps software-defined network (SDN). In this respect, the project has two objectives: (i) leverage software-defined networking (SDN) to separate flows for better monitoring of where high data transfer needs and network issues are; and (ii) pave the way towards connecting with 100 Gbps cyber-infrastructure of Internet2. The research and development effort is in the creation of a troubleshooting framework that utilizes OpenFlow protocol flow definitions.<br\/><br\/>The impact of the project will be on campus network engineers having a deeper exposure to network utilization by science data transfers. On one hand, such a troubleshooting capability will better the network design and planning processes on campuses. On the other hand, the domain scientists will be able to communicate science data transfer needs with network engineers. SDN will enable effective allocation of bandwidth with savings in campus networking investments by institutions. The research methods and results will be tested and demonstrated by utilizing the GENI infrastructure as an experimentation testbed.","title":"CC-NIE Integration: Houston, We have Troubleshooting for the 100 Gbps Network!","awardID":"1341019","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[559253],"PO":["564246"]},"205255":{"abstract":"The Virtual Robot Test and Integration Laboratory (VRTIL) addresses the need to verify robot software using real data, produced by real sensors, in all its richness and imperfection. The project is embedding robot software in a synthetic environment in order to mimic its sensor interfaces. In an advance over the state of the art, these interfaces are windows into data bases that store the actual sensor data that a robot did read when it was in a particular state.<br\/>The project goal is to reproduce the real world interface as exactly as possible using data. Two basic tools enable such ?virtualization? of a real robot in a real environment:<br\/><br\/>* Virtualized reality provides a means to produce synthetic views from virtual viewpoints that are near to an actual viewpoint for which a real image is available;<br\/><br\/>* High fidelity synthesized motion gathers large amounts of data to calibrate a model that is valid across all of state space.<br\/><br\/>Combining these ideas will result in a software test environment for robot software that has the realism of data logs and the responsiveness of a simulator. This may fundamentally transform the development of robots by lowering the cost and time of testing and debugging.","title":"NRI: Small: Virtualized Robot Test and Integration Laboratory","awardID":"1317803","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549625],"PO":[549626]},"205497":{"abstract":"Stochastic computing is an alternative to binary computing that is inherently fault tolerant. A number is encoded probabilistically as the number of 1s in a stream of N bits. Stochastic representation of numbers is highly redundant, which translates to fault tolerance. In many applications where exactness of computations is less important, stochastic computing may be a desired alternative. Its drawbacks include longer latency and orders of magnitude increase in storage requirements. Nevertheless, in deep submicron technology where fault tolerance is highly desirable, the stochastic computing approach may provide certain advantages. Very little is understood about the performance of Digital Signal Processing (DSP) algorithms implemented using stochastic computing. This project will attempt to understand how various DSP functions can be implemented using stochastic computing. Efforts will be directed towards understanding characteristics such as fault-tolerance, finite precision effects, conversion of two's complement to stochastic numbers, etc. Applications in video coding will be explored to demonstrate the utility of the stochastic computing approach for digital signal processing applications. <br\/><br\/>The approach, if proven successful, may be useful in many applications that require fault tolerance, but can tolerate some error. The impact of the research could then be translated to new products and new markets in the semiconductor industry. The proposed research will train graduate students through related course projects. Graduate research students will transfer technology to industries. Women and minorities will be recruited to participate in this project. Results of the project will be disseminated through publications in journals and conferences, and by talks by the PI in semiconductor industries.","title":"SHF: Small: Digital Signal Processing using Stochastic Computing","awardID":"1319107","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[550262],"PO":["562984"]},"205387":{"abstract":"Many application protocols on the Internet, especially those used by malware, are proprietary and have no publicly released specifications. According to the Internet2 NetFlow weekly reports on backbone traffic, more than 40% of Internet traffic belongs to unidentified application protocols. Therefore, it is critical for network security solutions to understand the specifications of these unknown application protocols. For instance, protocol specifications are needed for parsing unknown application protocol in advanced intrusion prevention systems. Protocol specifications are also useful for many other applications such as vulnerability discovery and system integration. Furthermore, even for some application protocols with known specifications, protocol inference is also needed sometimes for identifying implementation details and bugs that are not unambiguously specified. Inferring protocol specification for unknown application protocols is therefore fundamental to network security.<br\/><br\/>The objective of this project is to develop schemes for automatically inferring the protocol specification of unknown applications from their network traces. The PI proposes a semantics aware approach that takes network traces as the input and automatically outputs the inferred protocol message format. This project represents the first effort towards developing semantics aware approaches to protocol inference, a fundamental building block of many network security solutions. This project is potentially transformative research with high-impact. It will enable a spectrum of new network security applications and solutions. The proposed project is interdisciplinary in nature as it applies natural language processing techniques to network security problems.","title":"TWC: Small: Semantics Aware Approaches to Automated Reverse Engineering Unknown Application Protocols","awardID":"1318563","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["562296"],"PO":["562302"]},"209633":{"abstract":"This collaborative research between Florida State University and Cornell University is to identify language-action features from text-based messages that can be used to dynamically infer a social actor's perceived trustworthiness. The team will investigate using optimal analysis techniques to calibrate trustworthiness reasoning, which can be used to computationally model actors' deceptive behaviors in cyber space and to infer actors' intent based on their words and actions.<br\/><br\/>This research will have a transformative impact in understanding the dynamics of trusting relationships through observing language-action features and psychosocial trustworthiness attribution mechanisms. This study serves as a precursor to a socio-technical schema that will facilitate national security and data protection for the general populace while also protecting the individual's right to privacy. This study will contribute to the science of cyber-security, and will help the cyber-security community to understand and enable trustworthy communication and collaborative information behavior among computer-mediated groups in a systematic way.","title":"EAGER: Collaborative: Language-Action Causal Graphs for Trustworthiness Attribution in Computer-Mediated Communication","awardID":"1347113","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561924,561925,561926],"PO":["565327"]},"209754":{"abstract":"Regular expression matching is the core operation in a wide range of networking and security services (such as malware filtering) on most networking middleboxes and security devices. As each packet is processed by such a device, the packet payload is examined to determine if it matches any of the specified regular expressions, each of which might correspond to a specific security threat. In most regular expression matching solutions, the regular expressions are converted into a finite state automata model which is then used to perform the search. Because each packet must be searched, high speed and low memory regular expression matching solutions are required. Unfortunately, the standard automata models, deterministic finite state automata (DFA) and nondeterministic finite state automata (NFA), are insufficient because neither can achieve both low memory and high speed. This project explores the feasibility of new high speed low memory regular expression matching solutions based on ternary content addressable memory (TCAM). If successful, the result will be a fundamentally new regular expression matching solution that will help make the Internet both faster and more secure.<br\/><br\/>This project will develop new regular expression matching solutions by developing new finite state automata models such as the overlay deterministic finite state automata (ODFA) that account for memory inefficiencies in DFA. Along with developing new automata models, the project will develop new scalable and automated algorithms for efficiently constructing the automata from the input regular expressions as well as efficient algorithms for encoding these automata in TCAM. A key design constraint for the automata models and algorithms will be leveraging the prioritized parallel search and ternary compression capabilities of TCAM to reduce automata size and decrease the time required for each automata lookup.","title":"Exploratory Studies of New Automata Models and Algorithms for TCAM-based Regular Expression Matching","awardID":"1347953","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[562295,562296],"PO":["565251"]},"209534":{"abstract":"Our society is in the midst of a great revolution due to Big Data. This revolution affects every aspect of our lives from online to offline, from business to science. Privacy today is both a major social concern and an intellectually challenging problem; the problem is exacerbated by the Big Data trend. Differential privacy is a principled approach to achieving the goal of privacy which, conceived as it was in the context of large databases, is especially suited for Big Data. This workshop will bring together researchers on differential privacy with the world's leading Big Data theoreticians to congregate at the Simons Institute in the Fall of 2013 to create a rare opportunity for true breakthroughs both in Privacy and in Big Data.<br\/><br\/>The Simons Institute will be hosting a special semester on \"Theoretical foundations of Big Data analysis\" during Fall of 2013, which will bring together several experts in analysis of big data. The purpose of the workshop will be on the one hand to develop new differential-privacy-inspired techniques for allowing large-scale data analysis without threatening the privacy of individuals, and on the other hand for the two communities to explore together the statistical concepts and techniques which are at the basis of both. The four-day workshop on \"Big Data and Differential Privacy\" will take place December 11-14, 2013 at the Simons Institute for the Theory of Computing, located in Berkeley, CA.","title":"Big Data and Differential Privacy","awardID":"1346565","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561682],"PO":["565251"]},"209655":{"abstract":"One of the main impediment to a wider adoption of cloud storage and computing services is the need to keep data private. Recently, several research communities have begun exploring new techniques that allow a client to outsource storage and computation over private data to a potentially untrusted cloud service, while maintaining the privacy of the data. These techniques are highly varied depending on a large number of factors, like the ownership of the input and output data, the type of processing required, the desired level of protection, and the level of trust that the client is willing to put in different components of the outsourcing service. This highly collaborative project combines many different areas of expertise with the aim of substantially changing the way outsourced data and computation are perceived and used in the real world.<br\/><br\/>This project focuses on finding a modular approach to the specification, design and analysis of privacy preserving mechanisms in the context of outsourced computation. The investigators explore a broad solution space that combines algorithmic techniques, limited use of trusted hardware, and distributed computation and trust, and create a common framework that allows one to compare and analyze vastly different solutions in a modular and composable manner. The project will also explore the theoretical and practical limits of what each of these solutions can achieve and hone in on a practical solution that possibly combines many different techniques to achieve the right balance of efficiency and security.","title":"EAGER: Collaborative: Holistic Security for Cloud Computing: Oblivious Computation","awardID":"1347279","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561979],"PO":["565264"]},"209666":{"abstract":"One of the main impediment to a wider adoption of cloud storage and computing services is the need to keep data private. Recently, several research communities have begun exploring new techniques that allow a client to outsource storage and computation over private data to a potentially untrusted cloud service, while maintaining the privacy of the data. These techniques are highly varied depending on a large number of factors, like the ownership of the input and output data, the type of processing required, the desired level of protection, and the level of trust that the client is willing to put in different components of the outsourcing service. This highly collaborative project combines many different areas of expertise with the aim of substantially changing the way outsourced data and computation are perceived and used in the real world.<br\/><br\/>This project focuses on finding a modular approach to the specification, design and analysis of privacy preserving mechanisms in the context of outsourced computation. The investigators explore a broad solution space that combines algorithmic techniques, limited use of trusted hardware, and distributed computation and trust, and create a common framework that allows one to compare and analyze vastly different solutions in a modular and composable manner. The project will also explore the theoretical and practical limits of what each of these solutions can achieve and hone in on a practical solution that possibly combines many different techniques to achieve the right balance of efficiency and security.","title":"EAGER: Collaborative: Holistic Security for Cloud Computing: Oblivious Computation","awardID":"1347350","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[562008],"PO":["565264"]},"209688":{"abstract":"Perhaps the greatest single impediment to the broad adoption of public clouds is concerns about security. Reasoning about all the security aspects of complex modern information systems is enormously difficult. Security is especially problematic in the cloud, where a customer must place their trust in opaque complicated services that they do not understand or control and that are shared with many other customers. Yet, in today's public clouds, the assumption is that providers of cloud services can be fully trusted to provide systems secure against any threat. An architecture that exposes the security properties of the underlying services will enable a community of researchers to innovate in the implementation of secure cloud services. It will also provide customers visibility and control of the security in the services they use. Confidence in the security of the cloud will enable transformational societal benefits as the massive public cloud infrastructure becomes more broadly adopted.<br\/><br\/>This project defines a modular security architecture for cloud services where providers of different services each expose the security properties of their service, and the consumer can construct a compositional service that matches their application requirements and then reason about the security of the compositional service. The investigators extend existing cloud infrastructures to support this architecture, and explore secure modular networking and operating system technologies in the context of this architecture.","title":"EAGER: Holistic Security for Cloud Computing: Architecture for Modular System and Network Design","awardID":"1347525","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["313342",562065,562066,562067],"PO":["565264"]},"209127":{"abstract":"This project addresses the issue of functionality placement in virtualized networks. Functionality placement is the mapping of virtualized functions onto the network substrate and is a key part of using network virtualization to create sophisticated multitenant applications on top of virtualized network \"slices\". Prior work only considered virtualizing of communications links via methodologies such as Software Defined Networking (SDN). This project will demonstrate how one can implement network virtualization by utilizing only the edge of the physical network (which would make deployment much simpler) and how one can apply these ideas to higher-level functions (e.g., firewalls, proxy caches) and not just L2\/L3 switches as are typically considered.<br\/><br\/>Cloud Computing is rapidly changing the face of computing infrastructure and is a major economic driver in the technology sector. This project will benefit the cloud computing industry by informing the design of future cloud computing infrastructure and contributing prototype implementations the research community as open-source code.","title":"EAGER: Network Virtualization for OpenCloud","awardID":"1343947","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[560562],"PO":["564993"]},"205860":{"abstract":"Programming has undergone a paradigm shift: with the abundance of resources available on the web, including open source code repositories, question and answer forums, tutorials, and online books, programmers now routinely search for code. Indeed, it can easily be argued that the modern programmer has to be as versed in finding and interpreting relevant code on the web as they are in actually writing code.<br\/><br\/>Clearly, how effective and efficient developers are in their searches depends on the tool they use. Whether a search concerns a quest for an entire component that performs certain functionality, a hunt for an example of how to use an API, or a quick look for how to implement some algorithm, the success of the search stands or falls with the quality and relevance of the results returned by the chosen search engine.<br\/><br\/>This proposal seeks to explore the concept of social-technical code search, in which the information that is used by the search engine to produce its results is enriched with both social and technical metadata that, together, construct a rich context about the code, its current state, and how it came about. We hypothesize that this context significantly increases the quality and relevance of code search results, and, particularly, too, enables the developer to effectively explore code search results rather than just examining each result one-by-one in isolation.<br\/><br\/>Specific outcomes from this research will include: (1) a novel conceptual approach to code search, (2) new ranking algorithms that take into account both code and its social-technical context when ordering search results, (3) a novel code search engine and associated search interface, (4) new insights into how developers search for code, and (5) an publicly available archive of search queries and behaviors that can be mined and analyzed by other researchers for their purposes.","title":"SHF:Small: Social-Technical Code Search","awardID":"1321112","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[551130],"PO":["564388"]},"204771":{"abstract":"How does web-based malware spread? We use the term web-based malware to describe malware that is distributed through websites, and malicious posts in social networks. We are in an arms race against web-based malware distributors; and as in any war, knowledge is power. The more we know about them, the better we can defend ourselves. Our goal is to understand the dissemination of web-based malware by creating \"MalScope\", a suite of methods and tools that uses cutting-edge approaches to build spatiotemporal models, generators and sampling techniques for malware dissemination. From a scientific point of view, this project brings together two disciplines: Data Mining and Network Security. The outcome is a suite of novel, sophisticated, and scalable techniques and models that will enhance our understanding of malware dissemination at a large scale. We use two types of web-based malware dissemination data: (1) user machines accessing dangerous sites and downloading web-based malware; and (2) Facebook users being exposed to malicious posts. We already have and will continue to obtain more data from our industry partners (e.g. Symantec's WINE project), open-access projects, or collect on our own (e.g MyPageKeeper).<br\/><br\/>The broader impact of our work is that it will enable the development of security solutions for end-users and industry. A 15-minute network outage costs a 200-employee company about $40K, while identity theft costs about $1,500 per person on average. By knowing the enemy better, security researchers and industry can more effectively stop the interconnected manifestations of Internet threats: identity theft, the creation of botnets, and DoS attacks. The PIs have a track record of technology transfer, with collaborators at industrial labs (Yahoo, MSR, Symantec, AT&T, IBM), national labs (LLNL, Sandia), open-source software (``Pegasus''), and spin-off startups (StopTheHacker). Educational impacts include developing a new course, providing publicly available educational material, and open-source software.","title":"TWC: Medium: Collaborative: Know Thy Enemy: Data Mining Meets Networks for Understanding Web-Based Malware Dissemination","awardID":"1314935","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["549003"],"PO":["565327"]},"206960":{"abstract":"Instant by instant, we take in all of the information that we can gather with our senses, and we organize that information into objects and events. This is the foundation for our ability to function in the real world. While we can talk in general terms about how and why we function with such effectiveness and efficiency, a satisfactory scientific explanation requires much more. In particular, it requires a way of relating the things we can measure---specifically, the frequency with which we choose certain to choose certain actions and the time it takes to make those choices---to each other and to our hypotheses for how and why we do these things. Currently, there is no general way of doing this: the work to be accomplished with this support will fundamentally change this state of affairs.<br\/><br\/>The creation of a unified theoretical approach to characterizing the ways in which we organize our perception of the world will open up numerous avenues of research linking behavior and neurobiology. With the aid of the theoretical language that this project will produce, there will be systematic and well-defined ways of testing competing hypotheses for both what we do with perceptual information and how we do it. Critically, because this language will be extremely general and will not be tied to a particular set of ideas or theory, scientists from competing perspectives will be able to frame their ideas using a common vocabulary, something that is not currently possible. In addition, since this language will be mathematical, it will possess an exceptional level of rigor and internal consistency. Finally, this language will demonstrate its utility and power by being applied to a set of thorny issues in the contemporary study of perceptual organization.","title":"Collaborative Research: Building a Unified Theory-Driven Methodology for Identification of Elementary Cognitive Systems","awardID":"1331047","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7252","name":"PERCEPTION, ACTION & COGNITION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[554119],"PO":["564506"]},"205750":{"abstract":"The Java programming language is widely-used and of great commercial and economic significance. It is favored in part because it features automatic management of the computer memory resources it uses, simplifying such management for the programmer. Memory management in Java (and other managed languages) has reached a plateau in cost and effectiveness because most current techniques are tuned based on a small number of coarse-grained measures gathered while programs run. Substantial improvement might be gained from using more accurate estimation of current and near-future memory use to drive better memory management decisions. This would reduce the time, memory, and energy requirements to run Java programs. This is of significance to the full range of Java applications from small embedded systems through laptops and desktops to large servers. There is therefore an urgent need for techniques to derive better online predictors of Java memory use.<br\/><br\/>The long-term goal of the research program this award will support is to substantially improve memory allocation and garbage collection effectiveness by using better online predictors to drive more sophisticated allocator and collector decisions. The objective of this particular project is to develop machine learning techniques that induce accurate and computationally efficient predictors of characteristics of Java memory allocation that influence memory manager performance. Examples include predicting the volume of objects that become \"garbage\" (can be reclaimed and reused for future allocations), as well as objects that will be in use for a long time and will not become garbage soon. The approach is to learn models that predict memory usage based on features compiled from observable run-time events like calls to particular methods or allocations of certain objects. Data to learn models will be obtained from analysis of detailed program execution traces. Features will be selected that are both informative of memory use and computable with low space and time overheads. Programs will then be modified to compute these features as they run, and real-time predictive models will be used to predict future memory usage as programs execute. These predictions will be used to improve memory management performance. This will be accomplished by, for example, improving the timing of garbage collection so that it occurs at points during program execution that result in higher memory reclamation with lower effort.","title":"SHF:Small: Accurate and Computationally Efficient Predictors of Java Memory Resource Consumption","awardID":"1320498","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[550863,550864],"PO":["564588"]},"205520":{"abstract":"Linear algebraic algorithms, and in particular matrix decompositions, have proven extremely successful in the analysis of datasets in the form of matrices. Tools such as the Singular Value Decomposition (SVD) and the related Principal Components Analysis (PCA) have had a profound impact in diverse areas, ranging from web search engines to the physical sciences. Over the last decade, the introduction of randomization provided a new paradigm for the design and analysis of such algorithms. On the other hand, human genetics researchers are now finding out how truly different we are from one another. Large datasets describing the common patterns of human genetic variation may be easily thought of as matrices, with the rows representing individuals and the columns representing loci in the genome that correspond to common polymorphisms. The broader impact of such datasets can not be overemphasized: they are expected to be a key resource for researchers to use to find genes affecting health, disease, and responses to drugs and environmental factors, as well as understanding the evolutionary and biological history of our species.<br\/><br\/>The main objective of this proposal is to bridge the gap between state-of-the-art algorithms for data analysis developed in the theoretical computer science and applied mathematics communities and the application of such algorithms to the analysis of the increasingly larger volume of datasets in the human genetics community. The particular focus of our proposal is, from an algorithmic perspective, the design and analysis of (supervised and unsupervised) randomized algorithms for the so-called CX matrix factorization, and, from a population genetics perspective, the selection of ancestry informative and disorder associated markers, as well as ancestry and affection status prediction. This work will have immediate impact in the analysis of population genetics data. The results will be disseminated to a broad community of applied mathematicians, theoretical computer scientists, and population geneticists","title":"III: Small: Fast and Efficient Algorithms for Matrix Decompositions and Applications to Human Genetics","awardID":"1319280","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550329],"PO":["565136"]},"205531":{"abstract":"A significant amount of sensitive data communicated over wireless media makes wireless communication security an issue of paramount importance. The standard implemented solution to security over wireless networks is based on a modular approach in which transmission and encryption are carried out separately. State-of-the-art encryption algorithms are thus agnostic to the characteristics of the underlying network, but this separation approach is increasingly hard to justify as fast and reliable communications over wireless networks require more effective security architectures. <br\/><br\/>Several theoretical insights have been obtained from information theory in the past few years that suggest that fundamentally secure transmission in wireless interference networks is possible by jointly designing for reliability and security at the outset. This project takes a distinctive approach to accelerate the deployment of information-theoretically secure design of wireless networks by analyzing models with realistic assumptions, developing practical coding schemes and validating with a wireless test-bed. The project investigates two intertwined research tasks: (i) overcoming modeling assumptions, by providing security guarantees irrespective of adversaries' channels, modeling and countering adversaries that can manipulate channel conditions to their advantage, and removing idealized assumptions in implementation; (ii) providing explicit channel code designs that ensure strong secrecy and developing secure codes for multi-terminal wireless networks. <br\/><br\/>Additional activities related to the project include : (i) outreach to the computational security community towards integration of information-theoretic and computational security principles; (ii) dissemination of research results in various forms; (iii) a research exchange program between Georgia Tech and Penn State; (iv) a jointly taught graduate level course by the PIs that incorporates the research results of this project; and (v) recruitment of and mentorship for women in engineering and science. The research conducted in this project will facilitate the implementation of information-theoretic security principles on wireless systems and lead to novel cost-effective security techniques at the physical-layer.","title":"CIF: Small: Collaborative Research: Realizing the Vision of Information-Theoretic Security for Wireless Communications","awardID":"1319338","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550356],"PO":["564924"]},"205652":{"abstract":"This project attacks the pressing research problem of scaling and semi-automating large network experiments. Networking research is technically challenging due to the great scale and heterogeneity of protocols and devices in the Internet that researchers need to experiment with. To address this the project will investigate and tackle the problems of resource limitations and experimental artifacts of the testing platforms and scaling techniques. Work to date has either focused on control plane protocols (e.g., routing simulators), or solely considered the data plane. This project's goal is to conduct experiments that are joint control plane and data plane at a scale not previously possible. <br\/><br\/>The project project includes three complementary efforts to address experimentation scale challenges by designing:<br\/><br\/>(1) Experiment Mapping Tools and Taxonomy: The project will design a general framework, taxonomy, and a set of tools to bridge the current gap between testbed users and large-scale testbed experiments that use multiple scaling techniques. The user can supply hints on desired fidelity of different components, and these will be used to determine a high fidelity mapping for the experiment.<br\/>(2) Experiment Analysis and Partitioning Tools: The project will design methods to model complex dependencies between components of a large-scale experiment to facilitate planning and mapping. These models may also allow partitioning the large experiment into maximally independent smaller experiments that can be sequentially executed to mimic the large experiment.<br\/>(3) Applications to Case Studies: The project will use a range of large experiments as applications, focusing on problematic experiments including (a) experiments to understand the effect of misconfigurations, attacks, and defenses on Internet infrastructure (e.g., scalability of RPKI, effect of worms or DDoS on BGP, BGP policy conflicts), (b) experiments for anomaly detection, and (c) experiments with cloud computing.<br\/><br\/>The research will help identify and deploy scalable protocols that will enable the Internet to securely accommodate increased traffic volumes. Impacts of the research include the development and public dissemination of general-purpose experimental tools, large-scale testing techniques, methodologies for the use of testing frameworks, and related graduate-level courseware. The PI will undertake significant outreach efforts to simulation and testbed teams, e.g., DETER\/Emulab, GENI, AutoNetkit, ns-3, and to industry. The PI will actively involve undergraduate and graduate students from under-represented minority groups in computer science in the research and educational efforts, and will organize a DIMACS workshop on project topics.","title":"NeTS: Small: Meta-Networking Research: Analysis, Partitioning, and Mapping Tools for Large Experiments","awardID":"1319924","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[550639],"PO":["564993"]},"205300":{"abstract":"Today's computing systems that are capable of processing at phenomenal speed and efficiency are based on an architecture that was first proposed by John Von Neumann in 1945, before computers existed. Researchers who have searched for better ways to perform computing have been inspired by the human brain \"architecture\" that represents the ultimate in processing efficiency for applications such as image processing and pattern recognition. Attempts to create neurocomputers and associative memory circuits that mimic the brain via a network of coupled artificial neurons and synapses have been proposed. However, such implementations based on conventional silicon technology have largely been viewed as impractical and inferior to traditional computing hardware in terms of complexity and power consumption. This project will demonstrate a novel neurocomputing system based on the co-design of the system architecture and the nanoscale aluminum nitride (AlN) resonator devices that act as thermally-tunable elements for implementation of artificial neurons and synapses. The goal of this project is to demonstrate the functionality of such a system for a pattern recognition problem.<br\/><br\/>This project combines two exciting technologies, namely, modeling the function of the brain and piezoelectric materials, both of which are of interest to a broad community. Piezoelectric materials, such as crystals, are an exciting demonstration of converting mechanical energy into electrical energy. The same materials used in this project will be incorporated into the CMU undergraduate course 18-220, which is an introduction to circuits, to demonstrate energy harvesting and the important concept of resonance. A lab assignment based on converting vibrational energy to electrical energy will encourage the students to explore techniques for harvesting energy to support future electronic systems. Simpler samples of piezoelectric materials that react to being squeezed and vibrated will be used to light LEDs for demonstration to the C-Mite K through 10th classes at Carnegie Mellon (http:\/\/www.cmu.edu\/cmites\/ ). The brain-inspired electronic systems will be incorporated into graduate course projects for digital integrated circuit design at Carnegie Mellon. The results of this research will also be collaboratively exchanged with researchers from industry, specifically Intel and Qualcomm, to explore the benefits of neurocomputing-based associative memories for use in portable electronic systems.","title":"SHF: Small: Associative Memory based on Ovenized Resonator Exchange","awardID":"1318160","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":["555639",549747],"PO":["562984"]},"206752":{"abstract":"This grant provides funding for the formulation of a data model, and trajectory planning platform and methodology to execute a fully digital 3D, 5-axis machining capability. Research will be performed on methods for utilizing multiple Graphical Processor Units (GPUs), which are readily available, parallel digital processing hardware, in these calculations. The methodology will be implemented in the context of an existing advanced computational framework that has tools for voxelization, variable resolution digital modeling, and parallel computing, integrating the fields of manufacturing and computer science. Experiments involving 5-axis machining will be executed to validate the methodology. Components will be machined and inspected on a coordinate measurement machine to verify that the target geometry has been achieved.<br\/><br\/>If successful, this work will bring classical subtractive manufacturing back into the arsenal of rapid prototyping, providing users of typical CNC machine tools with the ability to rapidly determine if a part can be produced on a specific machine and machine the part. Having such a design and analysis tool will help to reduce the cost, improve the quality and allow rapid deployment of new innovations in components that require machining. This work will contribute to variable resolution digital representations to be employed in next generation digital manufacturing systems. It will also combine state-of-the-art concepts in computing and manufacturing to realize a completely new a cyber-physical approach to manufacturing.","title":"CPS: Synergy: Converting Multi-Axis Machine Tools into Subtractive3D Printers by using Intelligent Discrete Geometry Data Structures designed for Parallel and Distributed Computing","awardID":"1329742","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7918","name":"CYBER-PHYSICAL SYSTEMS (CPS)"}}],"PIcoPI":[553571,"558498"],"PO":["564949"]},"205542":{"abstract":"Geometric reconfiguration problems underlie modern mathematical investigations in robotics, mechanical design and structural engineering. In recent years, challenging questions in computational biology and computational materials science (such as protein folding, viral assembly, flexibility studies of crystalline materials and the rational design of macromolecules with desired functionality) are approached with methods originally developed for abstract, geometrically constraint structures.<br\/><br\/>This project will develop novel algorithms based on mathematically rigorous techniques, by exploiting discrete structures underlying three-dimensional articulated structures (such as linkages, panel-and-hinge chains and polyhedra). Examples include reconfigurations of robot arms within their 3D workspace, with singularity- and collision-avoidance, expansive motions and pseudo-triangulations in a 3D and in periodic settings, and origami foldability. <br\/><br\/>The research project builds upon the PIs' previous work, and extends it in new directions. It seeks to generalize concepts from Rigidity Theory (such as pointed pseudo-triangulations), which were previously applied successfully to 2-dimensional linkage reconfiguration problems. It addresses problems concerning extremal configurations of revolute jointed robotic manipulators and the intrinsic mathematical structure of their 3D workspace. It aims at bringing in novel algebraic and combinatorial rigidity-theoretic methods, and at applying them to periodic and crystalline structures. Recent results relating origami design to properties of piecewise linear surfaces will also be extended to questions concerning origami folding properties to rigidity questions of panel-and-hinge structures.<br\/><br\/>The grant provides funding for the training of graduate and undergraduate students. In particular, REU projects emerging from these topics will involve students from Smith College, an all-women college with a sustained reputation for successfully educating minority undergraduate students in the sciences.","title":"AF: Small: Collaborative:RUI: Mathematical foundations of reconfiguration algorithms for geometrically constraint structures","awardID":"1319389","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":[550384],"PO":["565157"]},"205784":{"abstract":"This interdisciplinary research project seeks to elucidate the computational machinery and algorithms in our brain that enables us to perceive 3-dimensional surfaces of objects in visual scenes based on the 2D images projected on our retinae. The first conjecture of the project is that the neural circuitry underlying perceptual inference in our brain can be predicted by the statistical structures in our natural environment. To prove this conjecture, statistical studies on 3D natural scenes will be carried out and their predictions on neural connectivity will be compared with the functional connectivity and tuning properties of depth-sensitive neurons in the primate visual cortex obtained using large-scale multi-electrode recording techniques. This will provide deeper insights into how the brain represents and builds models of the structures of the external world to enable perceptual inference. To understand what such circuits can compute, the investigator conjectures that neural circuits realize a class of generative models in computer vision and computational neuroscience called Markov random fields and Boltzmann machine. This second conjecture will be evaluated by exploring the theoretical link between the neural circuits and these computational models, by comparing experimental neural observations with behaviors of these computational models, and by evaluating the computational performance of the inferred neural circuits in solving stereo computation and surface interpolation problems in real world data. The research combines techniques in machine and statistical learning, computer vision, neural networks and neurophysiology to dissect neural circuits from a functional perspective. By linking real circuits to a powerful class of computational models in statistical inference, the project will have broader impacts by providing novel evidence and fundamental insight to the neural mechanisms and computational algorithms underlying statistical perceptual inference in the brain. This interdisciplinary project will be a catalyst for the development of educational initiatives to bring bringing computer science and neuroscience together for undergraduates and graduate students, and to promote the awareness and involvement of students from multiple disciplines, including under-represented groups, in the field of computational neuroscience.","title":"RI: Small: Statistical Perceptual Inference in Visual Cortical Neural Circuits","awardID":"1320651","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550942],"PO":["564318"]},"202154":{"abstract":"Crowdsourcing refers to the paradigm of eliciting work, typically in small pieces, from a global population of workers, each making dynamic decisions about which task to complete next, and for whom. Crowdsourcing systems are inherently algorithmic because the coordination task would otherwise be overwhelming. Algorithms take the place of the management and incentive structure of traditional firms, responsible for optimizing workflows, determining payments, matching workers with tasks, and learning models of workers and tasks.<br\/><br\/>With the potential to transform the way in which productive effort is allocated, crowdsourcing is already finding application across a broad range of domains, such as citizen science, the transcription of documents, and collecting training data for use in machine learning for bridging the gap to human-level intelligence.\u00a0\u00a0The goal of this project is to develop a cohesive theory of algorithms and incentives for the design of crowdsourcing systems.<br\/><br\/>The proposed research addresses key challenges in harnessing decentralized resources and capabilities for productive work, and seeks to develop a theoretical framework to guide the design of crowdsourcing systems that align incentives, are adaptive and robust, and achieve high performance for low cost.\u00a0\u00a0The investigators are interested in the roles of matching and pricing, in developing methods for eliciting high quality, unverifiable contributions, and in embracing realistic models of human behavior.<br\/><br\/>The investigators will organize workshops on social computing at upcoming Conferences on Electronic Commerce (EC13 and EC14) and they continue to recruit women and other under-represented groups to join the project.","title":"AF: Medium: Algorithmic Crowdsourcing Systems","awardID":"1301976","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7932","name":"COMPUT GAME THEORY & ECON"}}],"PIcoPI":[541777,541778,541779],"PO":["565251"]},"202275":{"abstract":"It has been widely acknowledged that recognizing objects in images, and human activities in video - the basic problems in computer vision - can be significantly improved by accounting for object (activity) parts, context, and their spatiotemporal relationships. This is because these constraints facilitate resolving ambiguous hypotheses in the face of uncertainty. Since parts and contexts can be efficiently modeled by graphical models (e.g., Conditional Random Field), object and activity recognition are often formulated as probabilistic inference of graphical models. The project develops a new theoretical framework of graphical models that explicitly encodes high-order, spatiotemporal, hierarchical, and contextual interactions among objects (activities) as Quadratic Mutual-Exclusion Constraints (QMCs), for the purposes of object and activity recognition in images and video.<br\/><br\/>The key contributions of the project work include: 1) Approaches to view-invariant object and activity recognition; 2) Formulations of learning and inference of graphical models representing objects and human activities, as finding a maximum weight subgraph (MWS) under the QMCs; 3) Polynomial-time algorithms for solving the MWS problem subject to QMCs; and 4) Explicit performance bounds and theoretical guarantees of tightness and convergence of the proposed learning and inference algorithms. <br\/><br\/>The project framework encodes hard constraints from the domain of interest that have never been used in prior work, and uses principled, polynomial-time algorithms for learning and inference. The research of this project advances the state of the art in object and activity recognition, and enables new applications including video surveillance, retrieval from large datasets, and perception of mobile robots.","title":"RI: Medium: Collaborative Research: Object and Activity Recognition as the Maximum Weight Subgraph Problem with Mutual Exclusion Constraints","awardID":"1302700","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[542093],"PO":["564316"]},"205201":{"abstract":"PI: Sensinger, J. W.; Hargrove, L.; and Kording, K. P.<br\/>Proposal Number: 1317379<br\/><br\/>Problem Description: Better robotic prostheses can dramatically improve the quality of life for the more than 40,000 Americans with an upper limb amputation, many of whom reject existing devices because they have trouble controlling them in the same intuitive, subconscious way that they controlled their intact arms. Prosthesis control is difficult because amputees experience great uncertainty both with respect to whether their device will respond appropriately to their control signals and whether sensory feedback cues accurately reflect the actual movement. Researchers have focused on improving isolated aspects of control, for example by improving filters or mimicking able-bodied sensory cues through haptic devices, but these approaches have minimally reduced the uncertainty of prosthesis control. Human interaction with a prosthesis is a multifaceted, time-varying problem that is difficult to solve. What is missing from robotic prosthesis research are principled methods for optimizing control strategies and sensory cues which take into account behavioral choices people are known to make in the face of high uncertainty.<br\/><br\/>Intellectual Merit: The proposed research is innovative because it poses the co-robot problem in a broader context that incorporates the highly sophisticated behavioral decisions that humans make in optimizing their control strategy and sensory cues. This principled approach is able to integrate multiple effects in ways that were not possible using previous approaches. For example, the proposed approach naturally incorporates the fact that people prefer to use less exerted effort to accomplish a task, but tolerate more effort during portions of movement that require greater precision (e.g. final portion of a trajectory). On the other hand, the approach does not favor high-certainty haptic cues if those cues provide redundant information to existing sensory cues such as vision, or if the haptic information does not reduce the uncertainty of controllable system dynamics. Due to the large sources of control-signal noise present in amputees, the proposed work will lead to improved techniques within the fields of computational motor control and optimal control. This research builds on the team?s extensive experience in the design and control of upper-limb prostheses and in developing the field of computational motor control. Achievement of the proposed aims will contribute to the field of robotic control and to such diverse fields as human-robot interaction, perception, manipulation, and exoskeletons.<br\/><br\/>Broader Impacts: True biomimetic prostheses, exoskeletons, and humanoid robot control will not be possible until there is a firm understanding of how humans integrate with these co-robots in the face of interacting sources of uncertainty. This computational motor project will provide transformative insight into how humans control movement in the presence of large uncertainty and thus fill a critical gap in the knowledge base of this field. The framework developed in this research will be of great interest to the motor-control research community and may be useful in the restoration of other movement disorders such as spinal cord injury and stroke. The lead institution of this proposal, the Rehabilitation Institute of Chicago (RIC), is consistently ranked the top rehabilitation hospital in the country. The close proximity of research and clinical excellence within RIC ensures that the benefits resulting from this work will be quickly disseminated to prosthesis users. The research team will also seek to reach a broader audience?the laboratories at the RIC are regularly visited by students from local high schools and universities, and the RIC also contributes to outreach activities within inner-city Chicago. These outreach programs promote an awareness of rehabilitation research and an enthusiasm for pursuing a career in engineering. Additionally, the team will develop a K-12 educational module based on the template of the successful Summer School in Computational Sensory-Motor Neuroscience developed at Northwestern University and Queen?s University, which will provide a combination of theory and student-driven experimentation using games that will address many of the Illinois Learning Standards in science, math, and English language arts.","title":"NRI: Small: Modeling, Quantification, and Optimization of Prosthesis-User Interface","awardID":"1317379","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["549465",549464,549465,549466],"PO":["565049"]},"206653":{"abstract":"The goal of this project is to transform the efficiency, fidelity, and safety of current critical infrastructure inspection methods by combining human judgment with machine intelligence through the development of an autonomous robotic inspection assistant. The proposed work utilizes small aerial robots, coupled with three-dimensional imaging and the state-of-the-art in planning, modeling, and analysis to develop safe and efficient, high-precision assessment of structures. The key themes of the proposed work are: (1) rapid infrastructure modeling and analysis of large complex structures via a small autonomous aerial robot with 3D mapping capabilities; (2) immersive inspection and structural assessment to combine shape and appearance into an integrated representation amenable to structural health evaluation by an inspector; and (3) adaptive aerial vehicle motion plans that seek to learn from the experience of human inspectors and facilitate as autonomous inspection assistants. The proposed work is exploring the role of humans in the entire cycle from deployment of flying robots to registering data to the assessment. This project brings together members of participating communities and is developing curriculum to engage undergraduate and graduate students from robotics and civil engineering in the proposed research.","title":"NRI: Large: Collaborative Research: Fast and Accurate Infrastructure Modeling and Inspection with Low-Flying Robots","awardID":"1328816","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[553289],"PO":["564069"]},"209920":{"abstract":"Today, systems rely as heavily on data as on the software that manipulates that data. Unlike software, data cannot be easily tested or analyzed for correctness. Part of the problem is that it is difficult to decide whether data is wrong. Typographical errors can change data items by orders of magnitude. For example, the number 1234 might be entered when the correct value is 12.34. Unfortunately, finding this kind of mistake via manual data auditing is onerous, unscalable, and error-prone. Data errors can be costly: Errors in spreadsheet data have led to million dollar losses, and poor data quality has been estimated to cost the US economy more than $600 billion per year. <br\/><br\/>Data debugging is a new approach for locating likely data errors by leveraging the interaction between data and the programs that operate on it. Since it is impossible to know a priori whether data is incorrect, data debugging aims to do the next best thing: identifying data that has an unusual impact on the computation. Intuitively, data with an inordinate impact on the result of a computation is either very important, or it is wrong. By contrast, wrong data whose presence has no particularly unusual effect on the final result does not matter. By calling attention to data with inordinately high impact, data debugging can provide insights into both the data and the computation and reveal errors. Data debugging is especially well-suited for data-intensive programming environments like databases and spreadsheets that intertwine data and programs. Data debugging can dramatically reduce the risks of human data entry errors or data corruption, increase the reliability of computations over data, and potentially save the US economy millions of dollars.","title":"EAGER: Data Debugging","awardID":"1349784","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[562740,562741],"PO":["565264"]},"207995":{"abstract":"Proposal #: 13-37899<br\/>PI(s): Hahn, James K.<br\/> Lee, Taeyoung; Philbeck, John W.; Rickmond, Brian G.; Townsend, Gabe Sibley<br\/>Institution: George Washington University <br\/>Title: MRI\/Dev.: Large-Scale Dense Scene Capture and Tracking Instrument<br\/>Project Proposed:<br\/>This project, developing a large-scale, dense 3D measurement instrument for capturing dynamic environments, integrates devices such as range-and-color sensing devices like depth cameras (RGB-D sensors) by designing and developing key technical methodologies to fuse the data received from remote networked sensors. The instrument will collectively cover a large space at a sampling resolution of at least 1cm with submillimeter resolution in localized regions. These data are then fused into a single underlying representation. The work involves developing a system that possesses both large-scale and real-time dense capture capabilities. Specifically, <br\/>- Experimentally validating perception, planning and control algorithms of agile mobile robots (particularly those that operate with deformable objects) requires ground truth representation of those environments.<br\/>- Validating computational tools for tether dynamics and control for flexible multibody systems requires the capture of their environment in a large environment.<br\/>- Study of human motion for biomechanics, physical therapy, and exercise science applications requires accurate capture of dynamically changing deformable human shapes in a large environment.<br\/>- Image-guided surgical procedures require capture of localized dense patient anatomical surface registered in a larger surgical environment.<br\/>- Human visual perception and navigation require a dense model of the surrounding environments that include object in motion, thus advancing the state of eye movement analysis by enabling fast, automated and objective coding of object analysis by enabling fast, automated, and objective coding of objects people see as they move through the environment.<br\/>- The study of foot deformations enabled by dense shape capture during running and walking on real sediments will shed light on the evolution of gait and human anatomy, and the biomechanics of barefoot walking and running. <br\/>Thus, facilitating new research, the developed system enables rapid capture and construction of large dynamic high-resolution virtual environments that duplicate specific real-world environments, including deformable objects, with unprecedented density of detail.","title":"MRI: Development of Large-Scale Dense Scene Capture and Tracking Instrument","awardID":"1337722","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":[557144,557145,557146,557147,557148],"PO":["557609"]},"205575":{"abstract":"Shape analysis is a fundamental problem in computer graphics and vision. A significant body of research over the past few decades has centered around the medial axis, a classical geometric structure introduced by Blum in 1967. Not only does the medial axis provide a low-dimensional representation that captures both the shape and topology of the object, it also creates derivative shape descriptors that measure useful shape properties, such as thickness. This collaborative project that involves two partner institutions builds on that body of research and extends it in two important and related directions. First, the PIs will generalize the definition of medial axis to systemically define a sequence of medial representations of successively lower dimensionality (e.g., medial curve and medial point of a 3D object) that all inherit the essential properties of the medial axis. These representations, called medial forms, are useful in various applications such as shape matching and decomposition. While algorithms for computing low-dimensional skeletal structures are abundant, sound mathematical definitions have not yet been developed. Second, based on the medial forms, the PIs will introduce a set of shape descriptors that can characterize non-uniform (or anisotropic) expansion of shape. These descriptors will not only be able to differentiate plate-like and tubular parts, but also to measure the amount of plate-like stretching and tube-like elongation. Shape anisotropy is common in 3D objects, and is important for deriving knowledge from digital models, particularly in biological research. However, none of the existing descriptors can characterize this important aspect of 3D shape. The PIs will build on their recent study of 2D objects to develop the mathematical foundation and computational algorithms of medial forms of 3D objects, and will explore their use in analyzing shape anisotropy through several derivative shape descriptors. They will formalize the definitions of medial forms of a 3D object, and will examine their properties including their dimensionality, topology, and sensitivity to boundary perturbations. Based on the definitions, the PIs will develop efficient algorithms that create provably good approximations of the medial forms given discrete samples of an object. The PIs will explore how a variety of derivative anisotropy-aware descriptors, in the forms of either geometric skeletons or surface signature functions, complement existing descriptors in shape modeling applications (e.g., segmentation and matching) and in biological shape analysis.<br\/><br\/>Broader Impacts: The new mathematical formulations and shape descriptors will contribute significantly to both the theoretical and applied side of computer graphics. The ability to characterize shape anisotropy will directly translate to more accurate and efficient analysis of shapes in various application domains, and the PIs will particularly explore their use in understanding biological shapes. The team will develop and distributable online software that implements the new algorithms. Students, both at the undergraduate and graduate level, will be actively recruited with an emphasis on diversity, and in addition project outcomes will be used in outreach programs at local middle- and high-schools.","title":"CGV: Small: Collaborative Research: Theories, algorithms, and applications of medial forms for shape analysis","awardID":"1319573","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[550459],"PO":["565227"]},"205696":{"abstract":"Many studies foresee significant growth in the number of smart phone users, the phone's hardware and software features, and the broadband bandwidth. Therefore, a transformative area of research is to utilize this new platform for various tasks, among which the most promising is spatial crowdsourcing. Spatial crowdsourcing engages individuals, groups, and communities in the act of collecting, analyzing, and disseminating urban, social, and other spatiotemporal information. <br\/><br\/>Two major impediments to the success of spatial crowdsourcing in real-world applications are scalability and trust issues. Therefore, the first objective of this project is to study the issue of scale in spatial crowdsourcing. In particular, given that the task assignment is the main bottleneck of the system, the spatial aspects of the tasks are exploited to reduce the complexity of assignment. In addition, a cloud-based distributed approach to the problem is investigated for better scale-out. The second objective is to extend the framework to incorporate trust by maintaining a reputation score per worker and a confidence level for every spatial task. Consequently, multiple workers can perform a task redundantly in order to satisfy its confidence level. The spatial task assignment solutions are extended to take redundant task assignments and confidence satisfaction into consideration.<br\/><br\/>Spatial crowdsourcing has applications in disaster-response, urban planning, intelligent transportation, journalism and intelligence. As part of this project a spatial crowdsourcing system is being developed that can be used for real-world data collection and evaluation as well as for social studies and educational purposes.<br\/>Results are disseminated through the web: http:\/\/infolab.usc.edu\/projects\/GeoCrowd\/","title":"III: Small: GeoCrowd - A Generic Framework for Trustworthy Spatial Crowdsourcing","awardID":"1320149","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550740],"PO":["565136"]},"205465":{"abstract":"Learning good features is a key to computer vision problems such as recognizing human faces, and understanding scenes. Many computer vision researchers learn features by providing a semantic label for each image in a large database, limiting the amount of information per image to a few bits. Others learn features by identifying common patterns found in images such as lines, blobs, and more complicated shapes, but ignoring semantic information. This project develops algorithms to learn features that are common in images and also predict the semantics of images at various spatial scales using a new type of deep neural network called Heterogeneous Networks. The developed algorithms allow the incorporation of semantic information at intermediate layers. The algorithms developed can not only change how features are learned but also indicate how to scale feature learning to giant datasets of millions of images. The research team addresses challenging problems in human face verification using NCSA's petascale supercomputer, Blue Waters, and two large scale (millions of images) image data sets. <br\/><br\/>The research of this projected is integrated with both undergraduate and graduate education. The results obtained from this project are applicable to a wide range of applications in computer vision and pattern recognition. The research team plans to release algorithms and face data sets collected in this project to research communities once they are finished.","title":"RI: Small: Hierarchical Feature Learning by Heterogeneous Networks with Application to Face Verification","awardID":"1318971","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550184],"PO":["564316"]},"209821":{"abstract":"Although many virtual organizations (VO) are quite effective, not all VO practitioners are effective in each area, and there is no organized body of knowledge or set of ?best practices? among VOs to draw upon for key issues. Therefore centers are likely not as effective as they could be. This proposal involves the creation of an online knowledge exchange. This Virtual Organization Resources and Toolkits Exchange (VORTEX) would provide leaders of virtual organizations with resources about running virtual organizations and access to relevant organizational scientists. VORTEX is intended to aid in building a community among virtual organization leaders so that they can collaborate, share, and learn with and from each other. <br\/><br\/>Specific Objectives of the work include development, evaluation, and improvement of an online Virtual Organization Resources and Toolkits Exchange (VORTEX) environment to aid scientists and engineers to more effectively lead virtual organizations. This type of environment is necessary in order to: <br\/>(1) Connect leaders of virtual organizations with appropriate organization scientists; <br\/>(2) Provide online educational and reference materials for issues associated with managing virtual organizations; and<br\/>(3) Establish a center for leaders of virtual organization to share and collaborate with each other.","title":"EAGER: Collaborative Research: Virtual Organization Resources and Toolkits Exchange (VORTEX)","awardID":"1348463","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7396","name":"NEES RESEARCH"}}],"PIcoPI":[562483],"PO":["565272"]},"208985":{"abstract":"As the digital divide closes and more groups have access to the Internet, cybersafety is an increasingly relevant issue. With the connectivity provided by the Internet comes an inherent interdependency between devices connected to the Internet. Compromised devices are used in various forms of cybercrime (e.g., spam campaigns) that can affect the Internet as a whole. Thus, to increase cybersecurity, it is important to understand the ways and extent to which certain groups may be more susceptible to Internet crime. Previous studies have focused on how factors such as age, gender, occupation, and level of STEM background affect one?s susceptibility to Internet crime (i.e., phishing attacks), however little work has focused on how social class factors into Internet crime susceptibility. <br\/><br\/>This project focuses on understanding new sociological factors that may render particular groups more or less likely to be affected by cybercrime (in particular, phishing attacks that highlight opportunities for economic advancement). <br\/><br\/>The expected results from this project impact both the fields of cybersecurity and sociology. From a cybersecurity perspective, understanding whether particular demographics are disproportionately affected by online attacks can lead to: 1) efforts for prevention (e.g., Internet Service Providers that offer government subsidized broadband access can also provide cybersafety training); and 2) the identification of the spawning point of a cybercrime ecosystem, which if caught and addressed early, can make cyberspace at large more secure. Sociologically speaking, attention to whether social class influences susceptibility to cybercrime helps to highlight some of the previously understudied consequences of economic inequality.","title":"EAGER: Collaborative: Winning the Internet Lottery: Growing Income Inequality, Social Class, and Susceptibility to Cybercrime","awardID":"1343237","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[560149],"PO":["565327"]},"205124":{"abstract":"The objective of this research is to enable the development of teams of robots, equipped with vision and other sensors, capable of working alongside humans in critical missions, such as search and rescue. Key requirements are situational awareness and coordinated action. The approach is to develop mathematical frameworks and algorithms to enable such a team of robots to coordinate their paths, share and analyze their sensor data, maintain communications, and interact effectively and safely with humans. The project brings together experts in computer vision, robotics, estimation theory and controls.<br\/><br\/>Intellectual Merit. Realizing the above goals will require advances in several inter-related domains. Specifically, the sensing, estimation, and trajectory control tasks must seamlessly integrate visual analysis with navigation and control strategies, as well as inputs from humans. Novel distributed estimation strategies must be developed to accommodate difficult and dynamic environments. Efficient human-robot coordination necessitates methodologies for joint exploration and mapping, identifying important visual information, and robots? operation at different levels of autonomy.<br\/><br\/>Broader Impact. The success of this project will be a major step towards the deployment of teams of robots to assist humans in dangerous and complex tasks like disaster response. Search-and-rescue experts will advise the team in developing a prototype system, and evaluating it in situations that mimic operational conditions. The developed software tools will be disseminated to other researchers so they can build on the results. Undergraduates from UCR's highly diverse student population will gain valuable experience working alongside graduate student researchers.","title":"NRI: Small: Multirobot-Human Coordination for Visual Scene Understanding","awardID":"1316934","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["553702",549258,549259],"PO":["563109"]},"205366":{"abstract":"Current petascale platforms can perform large-scale simulations and generate massive amounts of data at unprecedented rates. These rates are expected to increase as exascale platforms are introduced. The generation of more and more data presents new challenges for scientists who struggle with the analysis, sorting, and selection of scientifically meaningful results. When very large amounts of data records are located across a large number of nodes in a distributed memory system, even a small number of comparisons can be costly or even impossible. Therefore, new methodologies are necessary to analyze large scientific datasets at scale.<br\/><br\/>The goal of this project is to develop a transformative analysis method to model the properties of large scientific datasets in a distributed manner on petascale systems today and exascale systems in the future. The research activity includes (1) the design of new algorithms for encoding properties embedded in distributed data in a parallel manner by using space reduction techniques; (2) the design of new algorithms for clustering and classifying these properties by using distributed paradigms such as MapReduce; (3) the deployment of the algorithms for diverse datasets in structural biology and astronomy; and (4) the tuning of the algorithms for both result performance and accuracy on emerging storage technologies. <br\/><br\/>The analysis method will provide the scientific community with infrastructures and instrumentations to identify features that can be used to predict class memberships; find recurrent patterns in datasets; and identify class memberships from a specific feature or property. By effectively and accurately capturing scientific information in a scalable manner, these infrastructures and instrumentations will break the traditional constraint of data centralization and allow scientists to overcome the difficulties associated with the fully distributed nature of the data considered.<br\/><br\/>The project's educational component promotes training and learning in computational modeling and analysis techniques as well as data-intensive algorithms and platforms by involving undergraduate and graduate students in research activities and integrating big data analytics into the undergraduate curriculum at the University of Delaware. The research-based educational materials developed in this project will be made available to the scientific community through the project portal and through tutorials at XSEDE and Supercomputing (SC) conferences.","title":"SHF: Small: Collaborative Research: Modeling and Analyzing Big Data on Peta- and Exascale Distributed Systems supported by MapReduce Methodologies","awardID":"1318445","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":[549918],"PO":["565272"]},"205256":{"abstract":"This project involves a new approach to robot learning from demonstration which allows experimenters to teach multiple robots nontrivial, heterogenous collective behaviors in real-time. Training multiple robots is challenging because while the experimenter may know what emergent collective behavior he wishes to achieve, it is unlikely that he knows what individual robot behaviors, and their interactions, will achieve it. This project decomposes robot teams or swarms into a social hierarchy of leaders and subordinates. Small groups are then trained at the bottom of the hierarchy relatively simple collective behaviors, then groups of leaders are trained progressively with more complex and abstract behaviors, until all trained top-level root behaviors have been learned. The project examines homogeneous behaviors within robot groups, heterogeneous behaviors among the robots, and mixtures of the two at any level in the hierarchy. The method builds on prior work in single-robot behavior training, which used decomposed hierarchies of behaviors consisting of finite-state automata and learned transition functions. Beyond robotics, the project also allows the development of group behaviors for virtual agents found in animation and in emerging agent-based models in the social sciences and biology. The work integrates with a teaching and research robotics laboratory, with agent-based modelers in the social sciences, and with robotics education efforts in K-12 underprivileged schools.","title":"NRI: Small: Online Training of Hierarchical Multirobot Teams","awardID":"1317813","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549627],"PO":["564069"]},"205498":{"abstract":"This project develops the foundations for automating verification of secure and trustworthy systems. It extends the range of analyses that are amenable to automated checking and addresses scalability. Symbolic techniques that represent possibly infinite sets of states by symbolic constraints have become important tools, but many systems of interest fall outside the scope of current techniques. There is a real need to extend and combine the power of symbolic analysis techniques to cover a much wider class of systems in order to develop the foundations for security and trustworthy software and systems.<br\/><br\/>Maude is a language based on rewriting logic. Maude can be used to model a system of any kind -- for example, an algorithm, a database, a hardware system, a programming language, a network protocol, a sensor network, or the molecular biology dynamics of a cell -- using a set of rewrite rules that describe the systems behavior. The current Maude implementation provides a high performance rewrite engine, as well as built-in search, unification, and model checking tools to support execution and analysis of systems specified in Maude. This project will develop general extensibility techniques for symbolic analysis that can simultaneously combine the power of Satisfiability Modulo Theories (SMT) constraint solving, rewriting- and unification-based analysis, and automata-based model checking to analyze a wide variety of systems beyond the scope of each separate technique. Specifically, the goals of the project are to: (i) develop the semantic foundations of extensible symbolic analysis combining SMT solving, rewriting and narrowing, and automata-based model checking; (ii) endow the Maude formal specification system with combined symbolic analysis capabilities based on such foundations; and (iii) demonstrate through case studies the power of such combined and extensible symbolic techniques in analyzing challenging systems in areas such as: model checking, theorem proving, programming languages, cryptographic protocols, and real-time and cyber-physical systems. <br\/><br\/>Maude has a substantial set of users who are doing research on approaches to secure and trustworthy systems. These users are poised to use the new techniques and tools as they develop. For critical systems, where even small security issues can lead to catastrophic failures, this foundational research will support rigorous techniques and tools for automated and scalable checking.","title":"TWC: Small: Collaborative: Extensible Symbolic Analysis Modulo SMT: Combining the Powers of Rewriting, Narrowing, and SMT Solving in Maude","awardID":"1319109","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550264],"PO":["564388"]},"208424":{"abstract":"High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today's architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.<br\/>The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.<br\/>MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities.","title":"SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI","awardID":"1339820","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"8004","name":"Software Institutes"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":[558548],"PO":["565247"]},"206499":{"abstract":"A major factor influencing learning is students' emotions and their general affective state. Given the pivotal role that affect plays in learning activities it is not surprising that there has been a good deal of interest in developing affect-aware technologies. The overwhelming majority of this work, however, has focused on modeling affect, i.e., designing computational models capable of inferring how students are feeling while interacting with an Intelligent Tutoring System (ITS). While modeling of affect is a critical first step in providing adaptive support tailored to students' affective needs, very little work exists on systematically exploring the impact of affective interventions on students' performance, learning, affect and attitudes, i.e., how to respond to students' emotions such as frustration, anxiety, boredom, and hopelessness as they arise. The research fills this gap by analyzing the value of tailoring different types of interventions to negative affective states for individual students and groups of students. <br\/><br\/>The project has two main goals. First, it addresses how to respond to negative student emotion (e.g., frustration, anxiety, boredom) in computer-based learning environments through a variety of interventions. Some correspond to specially-designed digital characters, integrated into the learning environment, which are intended to act like students' learning companions. These agents support students through (a) non-verbal behaviors (e.g., having the characters express empathy in response to student frustration), (b) messages targeting students' cognitive and meta-cognitive skills, as well as motivation and affect. Other interventions involve supporting collaboration between students to mitigate negative emotional states when detected. The impact of these interventions are investigated through a series of eight experiments with a total of 800 students. These experiments help to unveil general prescriptive principles to address student affect. <br\/><br\/>The second goal accomplished by this research is that the experiments provide valuable data to continue to extend and validate existing models of emotion. Specifically, the project triangulates and integrates a complex space of partially overlapping models and constructs of affect in learning (i.e. emotions, attitudes, incoming moods, motivation, engaged use or misuse of software). The project refines several well-established models, in particular the control-value theory of emotions to provide a more stable theoretical framework for the field of emotions in educational software. <br\/><br\/>This research is unique and ground breaking, as few researchers have targeted students' emotion in classrooms, gathered fine grained data on emotions during learning, or assessed the impact of specific affective treatments on a moment-to-moment basis. Students using the tutoring systems have already shown statistically significant gains and learning outcomes, as well as increased positive affect and attitudes. The new affective interventions will greatly increase the broad impact of these systems.<br\/>This research is developing: (1) prescriptive principles about how to respond to student affect; (2) new understanding about the impact of cognitive, affective, and meta-cognitive interventions on emotions and learning; (3) new understanding about individual differences in learning, unveiling the extent to which emotion, cognitive abilities, and gender impact learning; (4) instruction that is sensitive to individual differences; and (5) refined theories of student emotion.<br\/><br\/>This research is also: (1) increasing participation in mathematics of underrepresented populations (women and minorities) who often avoid STEM careers; (2) creating broad access to web-based technologies that help to engage more students by addressing their affective and social needs; and (3) addressing the one-size-fits-all approach to education, by responding to individual student needs with alternative representations of content and pathways through which material is presented.","title":"DIP: Collaborative Research: Impact of Adaptive Interventions on Student Affect, Performance, and Learning","awardID":"1324880","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[552861,552862],"PO":["565121"]},"205289":{"abstract":"The advent of multicore machines has enabled delivery of high performance via parallelism for a wide range of applications. While such machines have become ubiquitous, they pose significant challenges for software developers. One challenge is dealing with the relaxed memory consistency models supported by commercial multicore machines. Simultaneously delivering high performance and ensuring program correctness requires careful introduction of fence instructions in the code. Excessive use of fence instructions leads to poor performance while their omission of can lead to incorrect program behavior.<br\/><br\/>This research will investigate means for constraining the scope of a fence instruction to minimize its impact on performance while preserving desired program behavior. In existing systems the hardware is unaware of the scope and hence fence implementations enforce a strict ordering of memory accesses across a fence that leads to unnecessary stalls. Alternative means for inferring the scope information will be developed for constraining the memory orderings enforced by the hardware. In particular, development of hardware, compiler, and programming support will be carried out. The software and hardware techniques developed in this research will be made available so other researchers are able to experiment with them. The subject of research is relevant to commercial processor manufacturers. The students involved in this research will receive valuable training in the design and programming of multicore systems.","title":"SHF: Small: Memory Consistency -- Hardware, Compiler, and Programming Support","awardID":"1318103","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[549718],"PO":["564588"]},"209887":{"abstract":"In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important research hypotheses. However, the current barrier to entry is often prohibitive and only a few with well-established research infrastructure and deep expertise in mining software repositories can attempt such ultra-large-scale experiments.<br\/>A facility called Boa has been prototyped: a domain-specific language and a BIGDATA repository containing 700,000+ open source projects for analyzing ultra-large scale software repositories to help with such experiments.<br\/><br\/>This experimental research infrastructure is of significant interest to a wide community of software engineering and programming language researchers. The main goal of this EAGER project is to examine the requirements for making Boa broadly available to the software engineering and programming language community, to work with an initial set of researchers to try to fulfill these requirements, and to take preliminary steps toward making Boa a community-sustained, scalable, and extensible research infrastructure. This is an enabling and transformative project. Its success will aid and accelerate scientific research in software engineering, allowing scientists and engineers to focus on the essential tasks. This advance will primarily be achieved by significantly lowering the barrier to entry and thus enabling a larger and more ambitious line of data-intensive scientific discovery in this area.","title":"EAGER: Boa: A Community Research Infrastructure for Mining Software Repositories","awardID":"1349153","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[562662,562663],"PO":["564388"]},"199261":{"abstract":"This project proposes a novel unified model to help software developers license software and (re)use components complying with legal requirements. The solution will investigate novel combinations of information retrieval, internet-scale source code search, repository mining, and static analysis approaches to detect origins of software components. The research will also rely on a feedback-driven hybrid blending of information retrieval and machine learning techniques for identifying components' licenses with high accuracy. In addition, the proposed model will unify these building blocks for license compliance analysis and verification to reason about the given software, components, dependencies, and licenses, as well as their trustworthiness, constraints, and existing or potential legal compliance issues. <br\/><br\/>The proposed research will lead to both theoretical foundations and practical solutions for the comprehensive analysis of complex legal compliance concerns to enable lawful software development and evolution. Among the broader impacts, the project will develop educational course content, involve underrepresented student groups, and produce software tools under open source licenses, collaborating with industry to transfer technology and empirically evaluate proposed research, and conducting K-12 outreach activities.","title":"CAREER: Enabling License Compliance Analysis and Verification for Evolving Software","awardID":"1253837","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[534438],"PO":["564388"]},"209667":{"abstract":"The main impediment to a wide adoption of outsourcing storage and computing services is the need to keep the client's data private. Clients may directly own the data, or it can belong to third parties such as customers of the client in which case there may be a contractual or legal obligation to keep this data private. The data may be outsourced to external services due to it enormity or due to the complexity of the computations to be performed on the data. This project investigates how to design secure and private database services so they can be integrated within a larger design of secure systems. The broad impact of this work is new methods to ensure the private manipulations of large data sets in an efficient and easy to integrate manner, which is of vital importance to enable utilization of cloud services. <br\/><br\/><br\/>The investigators study mechanisms that allow one to privately outsource large amounts of data to the cloud and privately execute various computations over this data. The data may change over time which will require the design of efficient methods to update the data while maintaining its privacy which do not increase dramatically with the size of the database. The project focuses on the composability of the security guarantees provided, so as to ensure that the overall privacy and security guarantees of a service can be deduced from the properties of its components.","title":"EAGER: Holistic Security for Cloud Computing: Computing on Encrypted Data","awardID":"1347364","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[562010,562011,562012],"PO":["565264"]},"209458":{"abstract":"Programming multicore processors is a challenging task due to bugs resulting<br\/>from concurrency and synchronization. A fundamental reason for the difficulty<br\/>is that current programming mechanisms require programmers to manually<br\/>coordinate synchronization between threads running on these systems.<br\/>Consequently, concurrent systems are prone to bugs due to faulty<br\/>synchronization problems such as missing, wrong, or lost notifications for<br\/>threads. Moreover, these bugs are hard to find during testing because they may<br\/>appear only under a rare schedule of events. Current programming mechanisms<br\/>also limit the amount of concurrency in threads to ensure that a shared<br\/>variable is not updated by multiple threads. As a result, many concurrent<br\/>systems slow down due to sequential bottleneck of accessing shared variables.<br\/>This project is expected to fundamentally change the way synchronization<br\/>mechanisms (such as monitors) are written and implemented. It will lead to<br\/>better understanding of how conditions in a multithreaded program can be<br\/>evaluated efficiently. As a result, the concurrent programming systems will<br\/>become more reliable and faster. The impact on society is expected to be large<br\/>because all new systems are based on concurrent programs.<br\/><br\/>The project is developing new synchronization mechanisms with two fundamental<br\/>goals. The first goal is to make synchronization as simple and intuitive as<br\/>possible for programmers. Current (monitor based) synchronization<br\/>mechanisms require programmers to explicitly signal threads that may be waiting<br\/>on certain conditions. In the synchronization mechanism developed in this<br\/>project, there is no notion of condition variables and it is the responsibility<br\/>of the runtime system to automatically signal appropriate threads. Analogous<br\/>to automatic garbage collection in Java, the project is designing efficient<br\/>algorithms and techniques for automatic signaling. The project is<br\/>investigating techniques to deal with fairness and exploitation of runtime<br\/>information such as idle cores for effective and efficient automatic signaling.<br\/>The second goal is to increase the concurrency in implementation of monitor<br\/>without requiring programmers to do any additional work and without violating<br\/>the guarantee provided by the monitor that execution of its methods is<br\/>equivalent to one in which there is at most one thread in the monitor at any<br\/>time. <br\/><br\/>There is also a strong educational component for this project. Synchronization is a fundamental topic<br\/>in computer science education and is taught in all operating system courses. By<br\/>eliminating manual signaling from these courses, students would get more time<br\/>to focus on higher level concurrent programming concerns.","title":"EAGER: Efficient Monitor-Based Synchronization Mechanisms for Concurrent Programs","awardID":"1346245","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[561475],"PO":["565319"]},"209579":{"abstract":"Since 2006, the annual workshop for Women in Machine Learning (WiML) has brought together female researchers in industry and academia, postdoctoral fellows, and graduate students from the machine learning community to exchange research ideas and build mentoring and networking relationships. The one-day workshop has been especially beneficial for junior graduate students, giving them a supportive environment in which to present their research (in many cases, for the first time) and enabling them to meet peers and more senior researchers in the field of machine learning. The networking opportunities provided by the workshop have also helped senior graduate students find jobs following graduation. <br\/><br\/>Intellectual Merit: This workshop will advance machine learning knowledge and foster collaboration within the machine learning community. As invited speakers, established researchers at top universities and research labs will teach workshop participants about cutting-edge ideas from diverse areas of machine learning. Students will present their own research and receive valuable feedback from both senior researchers and their peers. By enabling women at all stages of their careers in machine learning to exchange research ideas and form new relationships, we expect that new connections and research collaborations will be established, thereby advancing the state-of-the-art of the field. <br\/><br\/>Broader Impact: This workshop will provide a forum for female graduate students, postdoctoral fellows, junior and senior faculty, and industry and government research scientists to exchange research ideas and establish networking and mentoring relationships. Undergraduates, particularly those who are interested in pursuing graduate school or industry positions in machine learning, are also welcome to attend. Bringing together women from different stages of their careers gives established researchers the opportunity to act as mentors, and enables junior women to find female role models working in the field of machine learning. The workshop will also benefit the wider machine learning community: Firstly, the WiML website, which lists all previous workshop presenters, serves as a useful resource for organizations looking for female invited speakers. Secondly, co-locating with a major machine learning conference enhances the visibility of female researchers among the wider machine learning community. Thirdly, travel funding provided to workshop participants also facilitates their travel to the co-located conference, which for some participants would otherwise not be possible. Finally, all workshop materials (slides, abstracts, etc.) will be made available on the workshop website in order to ensure broad dissemination.","title":"Workshop for Women in Machine Learning","awardID":"1346800","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[561796],"PO":["562760"]},"209117":{"abstract":"Unexpected deaths of hospitalized patients continue to be common despite evidence that patients who are at risk often show signs of clinical deterioration hours in advance. Existing early warning systems have significant shortcomings because of their poor reliability and the need for monitoring by overburdened clinical staff. Almost 1 out of 5 patients are readmitted within 30 days of hospital discharge with an annual cost to tax payers of $15-17 Billion. Hence, there is an urgent need for automated early warning systems that can provide timely and accurate information. <br\/><br\/>The project seeks to integrate and mine patient data from multiple sources, including routine clinical processes, bedside monitoring, at-home sensing, and existing electronic data sources to facilitate optimized patient-centered decision making. Specifically, the project aims to develop techniques and systems to provide early warning of clinical deterioration and hospital readmission of discharged patients using a novel two-tier system. Tier 1 uses data mining algorithms on existing hospital data records to identify patients who are most at risk of clinical deterioration and readmission. Tier 2 combines clinical data with sensor data to improve the accuracy of predictions on patients who are identified as being at risk by Tier 1. <br\/><br\/>Key innovative aspects of the project include: (1) new data mining algorithms for predicting clinical deterioration and readmission from heterogeneous, multi-scale, and high-dimensional data streams; (2) an alert explanation system to identify the most relevant prognostic factors and suggests possible intervention based on novel feature ranking algorithms; (3) a novel scheme based on cost-sensitive learning to dynamically reconfigure the sensors for achieving good tradeoff between monitoring cost and effectiveness. The resulting advances in healthcare practices that are currently employed in general wards offer several key benefits including (1) reduced workload on clinical staff; (2) capability for continuous monitoring of ward patients that can be used to triage nursing efforts in order to optimize the desired clinical outcomes; (3) capability to extend hospital monitoring to patients at high-risk for hospital readmission with the attendant benefits of reducing readmissions by targeting early preemptive therapeutic interventions.<br\/><br\/>Plans for transitioning the technology to clinical practice include rigorous evaluation of the technology in real-world settings and broad dissemination of the algorithms and their open-source implementations. Some potential broader impacts of the project include improved clinical outcomes, reduced patient mortality rates and healthcare costs, and enhanced opportunities for research-based interdisciplinary training of graduate students in health informatics. Additional information about the project can be found at: http:\/\/www.cse.wustl.edu\/~wenlinchen\/project\/clinical\/","title":"SCH: EXP: Integrated Real-Time Clinical Deterioration Prediction for Hospitalized Patients and Outpatients","awardID":"1343896","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8018","name":"Smart Health & Wellbeing"}}],"PIcoPI":[560533,560534,560535,560536],"PO":["560586"]},"210580":{"abstract":"Attributed-based obligatory access control is a new access control paradigm for achieving fine-grained authorization and assured system accountability. However, access control and obligation policies can be implemented incorrectly for various reasons, such as programming errors and misunderstanding about the policies. It is important to reveal discrepancy between the policy specification and the actual system implementation. The objective of this ?Transition To Practice? project is to develop an open source tool for model-based testing of attribute-based access control and obligation policies. It can build test models by integrating attribute-based access control and obligation rules with functional test models, generate test cases from the test models to meet given coverage criteria, and transform model-level test cases into executable code in a target language and test execution environment. The test code can then be executed with the system under test to exercise the access control and obligation policies. The tool is applicable to a great variety of systems due to the support for various programming languages and test execution environments. It is independent of how access control and obligation policies are implemented in the system under test. The broader impacts of this project include deployment of the tool to various academic and industry projects and involvement of students, particularly undergraduate students, in cutting-edge research.","title":"TTP: Small: Automated Conformance Testing of Access Control and Obligation Policies","awardID":"1359590","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[564245],"PO":[564246]},"209128":{"abstract":"An innovative union of Cognitive Neuroscience, Robotics, and Avatar scientists are coming together for a NSF \"Signing Creatures Workshop\" on November 14-15, 2013 at Gallaudet University (Washington, D.C.). The goal of the Workshop is to explore the feasibility of creating Robot and Avatar creatures that can sign and engage in interactive communication with young deaf and hearing children during critical periods of human development. Language acquisition research has established that children must receive early language exposure to achieve language and reading success. Despite research demonstrating the healthy brain benefits afforded to deaf children with early sign language exposure, this is not available to deaf children born to non-signing parents. Signing Robots and Avatar virtual humans can provide sign language to deaf children in communicative toys and games, similar to learning products for hearing children. The Workshop stems from a NSF grant to Cognitive Neuroscientist, Laura-Ann Petitto, PI, (and Science Director and Co-PI of the NSF Science of Learning Center, VL2, at Gallaudet) and Avatar scientist, David Traum, Co-PI, University of Southern California. Joining them is Robotics scientist Paul Oh of Drexel University. Broader Impact: The Workshop brings together deaf and hearing scientists from a broad range of disciplines to address critically important problems for education, learning, and reading in deaf children. The Workshop gives rise to new cross-disciplinary scientific knowledge and exciting explorations into the development of groundbreaking translational learning and reading products for deaf and hearing children. Additional broad applications include language translation, and high school and adult education.","title":"Signing Creatures Workshop","awardID":"1343948","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"7704","name":"Science of Learning Activities"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1311","name":"LINGUISTICS"}}],"PIcoPI":[560564,560565],"PO":["565215"]},"210250":{"abstract":"This International Conference on Research in Computational Molecular Biology (RECOMB) 2014 is the eighteenth in a series of international conferences in computational molecular biology. Computational biology has become one of the essential tools of modern biological research and RECOMB is one of the oldest and most prestigious conferences in the field. It is arguably the premiere meeting for work on innovative computer science applied to biology. <br\/><br\/>The meeting will be held in Pittsburgh, PA on April 2-5, 2014, jointly hosted by Carnegie Mellon University and the University of Pittsburgh. Funding is requested specifically for student travel awards to support U.S.-based students presenting work at the meeting. While every effort is being made to keep the meeting affordable, cost can nonetheless be an obstacle for potential attendees, especially student attendees. These travel awards will cover registration, hotel, and travel costs for a selected group of student presenters, competitively chosen based on the caliber of their work as assessed by the conference program committee and the individual merit and need of the students. Special attention will be given to women and minority researchers.<br\/><br\/><br\/>The conference series aims at attracting research contributions in all areas of computational molecular biology, including work on analyzing genomes and other molecular sequences, understanding the interaction of complicated systems that underlie biological function, working with molecular structures that make up these systems, making sense of biological image data, and advancing the computer models and algorithms needed for all of these tasks. The meeting will feature a prestigious group of keynote speakers, oral presentations chosen from submitted papers by a program committee of top researchers in the field, and poster presentations selected from submitted abstracts. In addition, the meeting will offer ample opportunities for one-on-one interactions between active researchers and students. The proposed travel awards will particularly extend this opportunity to U.S.-based students who might otherwise lack the opportunity to participate in the meeting.<br\/><br\/>The meeting and proposed travel fellowships will have particular value for educational purposes, creating a unique training opportunity for computational biology students who might not otherwise be able to present work at a prestigious international meeting. Particular invitation will be made to women and minority applicants, who remain underrepresented in computational work in general.","title":"The 18th International Conference on Research in Computational Molecular Biology (RECOMB 2014)","awardID":"1353787","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":[563452],"PO":["565223"]},"210371":{"abstract":"This project compares different techniques for assisting individuals with stroke-related mobility impairments using a robotic ankle orthosis. Several promising assistance techniques have been developed for robotic prostheses and rehabilitation platforms, which have been extended to exoskeletons - worn at the ankle joint, and adapted for individuals with stroke. An ankle exoskeleton test bed is used to emulate each assistance technique, allowing comparisons of efficiency and effectiveness within the same platform. Each technique is first programmed and verified in pilot tests with this emulator, followed by multi-dimensional parameter studies, conducted first on subjects without neurological impairment and then on subjects with hemiparesis following stroke. The results for each technique are used to identify ideal parameters and their settings, which facilitates across-technique comparisons and development of a standardized set of quantitative performance metrics, including measures of effort, preferred speed, and stability. These studies will contribute to a scientific foundation for the design and prescription of robotic ankle-foot orthoses that will benefit the millions of impaired individuals.","title":"NRI: Small: Rapid exploration of robotic ankle exoskeleton control strategies","awardID":"1355716","effectiveDate":"2013-09-15","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["307129"],"PO":[563744]},"210184":{"abstract":"Collaborative Projects: EAGER: A virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Arts and Design (XSEAD)<br\/><br\/>Intellectual Merit<br\/>One of the greatest challenges facing the United States in research and education is how to fundamentally encourage innovation across all sectors and spawn new solutions to address global challenges. Increasing research evidence and industrial innovations (i.e. mobile computing, social media) confirm that broad interdisciplinary collaborations that include both science and art fields have great potential for spawning creativity and innovation in computer science, engineering and the sciences. An emerging hybrid community of scientists, engineers, artists and designers is producing innovative and entrepreneurial research that advances new knowledge and proposes holistic solutions to societal challenges including health, education and environmental change. Yet, this burgeoning interdisciplinary community continues to face problems in its efforts to self-organize among constraints imposed by academic systems and historical biases; it continues to seek a dynamic and synergizing research and outreach exchange.<br\/><br\/>Building upon lessons-learned, a new Virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Art and Design (XSEAD) will be developed. The XSEAD project will address the following urgent needs of the interdisciplinary science-art community: establish a cohesive view of the field and provide a mechanism to attract entrepreneurs and industry; create a venue for multimodal documentation of research outcomes; provide extensive databases of prior and current research; allow rapid dissemination of research outcomes; facilitate forming of collaborations and specialized sub-communities; document and help evolve science-art curricula efforts and evaluation approaches; provide context and support mechanisms for science-arts careers; establish evidence of the societal impact of interdisciplinary science-art integration. The software engineering development components of XSEAD will contribute further knowledge in three technical areas: Content organization (improve the effectiveness of algorithms for dynamic, usage based, organization of large multimedia databases); Recommendation algorithms (promote the use of multi-relational structures for providing effective recommendations); Community dynamics (develop novel algorithms to extract structures that encode meaningful interactions in online social networks).<br\/><br\/>Broader Impact<br\/>XSEAD will expose general non-expert audiences to the evolution and potential of collaborative research across science and arts. It will attract the interest of young people searching for careers that combine the rigor of science and engineering with the creativity and reflection of arts and design. It will serve teachers and informal learning communities seeking exemplars for curricular development, active practitioners looking for further institutional opportunities to present and support their ongoing work, academics developing related interdisciplinary efforts and commercial companies seeking cross-trained expertise. XSEAD will enable rapid research exchange and in-depth peer-reviewed scholarship between the worlds of science and art and provide a unique and deeply engaging inroad to a vast and creative repository. XSEAD will help promote new paradigms for developing human centric solutions to complex societal problems (i.e. cost effective health and wellness, globalization and conflict, adaptive K-12 learning, electronic communication and security). These paradigms will combine knowledge across broad and diverse areas of human knowledge.","title":"Collaborative Research: EAGER: A Virtual eXchange to Support Networks of Creativity and Innovation Amongst Science, Engineering, Arts and Design (XSEAD)","awardID":"1352787","effectiveDate":"2013-09-02","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[563297],"PO":["565342"]},"205850":{"abstract":"In this Cyberlearning: Transforming Education EXP project, PIs from computer science and mathematics education are collaborating to investigate the use of gestures by teachers (both human and virtual) and learners in support of mathematics learning. They are investigating the ways teachers' gestures influence learning of mathematics concepts and how to design gestural supports for learning that a computer avatar might use in communicating with a learner. Their conceptual foundations come from embodied cognition, and they are aiming towards understanding the integration of two types of gestures: those that are used to promote understanding of content and those used for social purposes. The project focuses on learning of proportion, and the technological innovation in this project is creation of a gesturing pedagogical agent\/avatar that has a rich repertoire of both types of gestures that it uses while interacting with a learner and helping the learner to deepen his or her understanding of the mathematics of proportion. In a series of design studies, the PIs are designing software and extracting principles for augmenting pedagogical agents with new gesture-enriched capabilities and gleaning insights into the nature, types, and roles of gesture in educational interaction. <br\/><br\/>Despite consistent reform efforts, U.S. students still lag behind their global peers in mathematics understanding and capabilities. Intelligent tutoring systems can be used to provide one-on-one help to students who are struggling as they learn mathematics, but such interactions lack the social cues that help learners maintain their attention and know they are being understood and lack, as well, full means of expressing concepts in ways that learners might need for understanding. Good teachers use gestures for these purposes, and this project focuses on design of pedagogical agents (avatars) that will also be able to use such gestures. Infusing interactive tutoring systems with the ability to gesture in naturalistic and domain-appropriate ways may provide a missing link in making tutorial interactions effective for more learners. At the same time, insights gleaned about the pedagogical roles of gesturing can be leveraged in educating teachers of the future.","title":"EXP: Collaborative Research: Gesture Enhancement of Virtual Agent Mathematics Tutors","awardID":"1321042","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7625","name":"REESE"}}],"PIcoPI":[551103],"PO":["562669"]},"202242":{"abstract":"The increasingly large amounts of Electronic Medical Record (EMR) data offer unprecedented opportunities for EMR data mining to enhance health care experiences for personalized intervention, improve different diseases risk stratifications, and facilitate understanding about disease and appropriate treatment. To solve the key and challenging problems in mining such large-scale heterogeneous EMRs, the investigators aim to develop: (i) new computational tools to automate the EMRs processing, including new techniques for filling in missing values using a new robust rank-k matrix completion method; (ii) annotation of unstructured free-text EMRs using multi-label multi-instance learning; (iii) a new sparse multi-view learning model to integrate heterogeneous EMRs to predict the readmission risk of Heart Failure (HF) patients and to support personalized intervention; (iv) novel methods for identifying the longitudinal patterns using high-order multi-task learning; (v) a nonparametric Bayesian model for predicting the event time outcomes of the HF patients readmission. <br\/><br\/>The sparse multi-view feature learning and robust multi-task longitudinal pattern finding algorithms have a broad range of applications beyond EMR data mining. Free dissemination of source implementations of the algorithms enable other researchers to further develop and apply the resulting techniques. In particular, the methods and tools are expected to impact other EMR and public health research. This project offers enhanced opportunities for research-based advanced training of students (including members of minorities and under-served populations) and integration of research results into curricula at the University of Texas at Arlington, the University of Texas Southwestern Medical Center at Dallas, and Southern Methodist University. For further information see the web site at: http:\/\/ranger.uta.edu\/~heng\/NSF-III-1302675.html","title":"III: Medium: Collaborative Research: Robust Large-Scale Electronic Medical Record Data Mining Framework to Conduct Risk Stratification for Personalized Intervention","awardID":"1302497","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[541999],"PO":["565136"]},"205642":{"abstract":"The correctness of computer software often relies on its adherence to application-specific invariants during the course of its execution. These invariants are complex, often involving relationships between many different parts of the system, and implicit, often never formally expressed in the text of the program. Violating these invariants, however, leads to numerous bugs and security holes. As a result, programmers employ static type systems to capture these invariants and enable the use of automated tools to check that they are maintained. Algorithms for type inference allow complex types to remain implicit, easing software development and maintenance, while still readily available for documentation and enforcement. <br\/><br\/>While recent type system extensions greatly enrich the expressiveness of statically typed languages, type inference algorithms have not kept pace. This research evaluates the integration of SMT (Satisfiability Modulo Theory) solvers into the type inference algorithm of an industrial-strength functional programming language. In particular, in collaboration with researchers at Microsoft Research Cambridge, it extends the type inference algorithm of the Glasgow Haskell Compiler (GHC). GHC is a mature, open source, Haskell Compiler that is gaining popularity in industry due to its rich type system. The project also includes education and outreach components in the form of an advanced undergraduate\/masters level course and in the direct support of Ph.D. students.","title":"CIF: Small: Rich Type Inference for Functional Programming","awardID":"1319880","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[550615],"PO":["564588"]},"205763":{"abstract":"The analysis of large datasets using computers, a.k.a., big data analytics, is emerging as an important tool in many fields, such as science and discovery, technology, health care, and commerce. The data sets used in such applications are usually dynamic: they change over time as new data becomes available. Such dynamic changes are often small, requiring similarly small but potentially important updates, because new information can be crucial in detecting a pattern or an anomaly. For example, the Internet or a social network changes dynamically as new web pages become available, new links are added, or existing links are removed. As a result of such dynamic changes, two clusters of previously disconnected web sites can become connected by the addition of a single link, indicating for example, an important news item or a security breach. Unfortunately, in many existing big-data systems, absorbing new information involves making one or more passes over the entire dataset. Such batch processing of dynamic data results in slow updates, as well as inefficiencies in the utilization of resources such as hardware and energy, by (unnecessarily) performing many subcomputations that are unaffected by changes. This project aims to lay the groundwork for the programming languages and software systems that can support the development of such applications in the real world. The work has the potential to transform the way the programmers express computations on dynamically changing big data sets, make it possible to derive new information and knowledge from big dynamic data sets by computing with them responsively and efficiently, and transform the way that we teach the design, analysis, and implementations of computations operating for dynamic data sets. The project also includes the development of undergraduate lectures on parallelism.<br\/><br\/>The project aims to enable the user to express the dynamism in large data sets implicitly, without concerning themselves with how exactly the results will be updated when the data changes, e.g., which data depends on which other data, which data may need to be updated, which dependencies need to be reconstructed. Starting with an implicitly dynamic program, a software system automatically and efficiently constructs a record of the computed results and updates it as the dataset changes. To achieve this goal, the project develops abstractions, programming languages, compilers, and run-time systems. Concretely, we expect three sets of contributions: novel, powerful abstractions and cost models for writing programs that operate on dynamically changing large datasets, programming language support in the form of compilers and run-time systems for realizing such abstractions on practical hardware, and efficient algorithms and implementations, to be used to evaluate the proposed and future work.","title":"SHF: Small: Languages and Abstraction for Dynamic Big Data","awardID":"1320563","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[550897],"PO":["564588"]},"205411":{"abstract":"Unlike traditional fast SPICE simulation techniques that rely on a variety of approximation approaches to trade off simulation accuracy for greater speed, SPICE-accurate integrated circuit (IC) simulations can truthfully predict circuit electrical behaviors, and therefore become indispensable for design and verification of nanoscale ICs. However, for post-layout nanoscale circuits, using traditional SPICE-accurate simulation techniques to encapsulate multi-million or even multi-billion devices coupled through complex parasitics can be prohibitively expensive, and thus not applicable to large IC designs, since the runtime and memory cost for solving large sparse matrix problems using direct solution methods will increase quickly with the growing circuit sizes and parasitics densities. To achieve greater simulation efficiency and capacity during post-layout simulations, preconditioned iterative solution techniques have been recently proposed to substitute the direct solution methods. However, existing preconditioned methods for post-layout circuit simulations are typically designed with various assumptions and constraints on the circuit and systems to be analyzed, which therefore cannot be effectively and reliably applied to general-purpose SPICE-accurate circuit simulations. In this research project, the PI will study efficient yet robust circuit-oriented preconditioning approaches for scalable SPICE-accurate post-layout IC simulations by leveraging recent graph sparsification research. By systematically sparsifying linear\/nonlinear dynamic networks originated from dense parasitics components and complex device elements of post-layout circuits, scalable, and more importantly, parallelizable preconditioned iterative algorithms will be investigated and developed by the PI to enable much greater speed and capacity for SPICE-accurate IC simulations in both time and frequency domains.<br\/><br\/>The successful completion of this work will immediately benefit the semiconductor industries. The algorithms and methodologies to be developed through this project will be integrated into undergraduate\/graduate level VLSI design\/CAD courses, while the research results will be broadly disseminated to major semiconductor and EDA companies for potential industrial applications. The CAD tools developed under this research plan will also be exchanged with collaborating industrial partners. The acquired experience in the proposed research plan is also likely to contribute to computing advances in other science and engineering fields, impacting broader research areas that are related to large\/complex system modeling and simulation.","title":"SHF:Small:Graph Sparsification Approach to Scalable Parallel SPICE-Accurate Simulation of Post-layout Integrated Circuits","awardID":"1318694","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[550032],"PO":["562984"]},"206863":{"abstract":"There are at least two key features of the move to cloud computing that introduce the opportunity for significant leaps forward in computer security for tenant services. First, a compute cloud provides a common software, hardware and management basis for rolling out cross-cutting services en masse that have resisted incremental deployment in a one-service-at-a-time fashion. Second, compute clouds offer providers a broad view of activity across an unprecedented diversity of tenant services. This research project leverages these features to develop new approaches to a wide array of fundamental problems in computer security. By convening Cloud Security Horizons summits with industry stakeholders, this project further seeks to both contribute to industry directions in cloud computing and to be informed by them.<br\/><br\/>Particular longstanding security challenges addressed in this project include secure transport, authorization, user and software authentication, security monitoring, and incident analysis. Moreover, since modern clouds are not sufficiently extensible to support the envisioned capabilities, this project is constructing cloud software platforms that enable the flexibility, extensibility and security needed for this research to come to fruition in practice.","title":"TWC: Frontier: Collaborative: Rethinking Security in the Era of Cloud Computing","awardID":"1330553","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["553866",553865,553866],"PO":["565327"]},"202265":{"abstract":"When people make decisions or form beliefs, they often discuss them with others, seeking out others' opinions and sharing their own. Recently such conversations are occurring online, providing a public source of information of interest to companies, the military, the government, public policy bodies, and educators. Moreover these dialogs occur at scale, allowing researchers in natural language processing access to large-scale dialog datasets for the first time. However, automatically processing such dialogs is challenging, because current tools are targeted to traditional language resources such as newspaper articles. This project develops innovative algorithms for automatically processing and identifying important phenomena in such dialogs including: (a) stance - participants' views on a topic; (b) subjective dialog acts - including sarcasm and humor; and (c) central propositions - core ideas in the dialog, by combining methods of crowd-sourced annotation, bootstrapping and machine learning, and cognitive science. A critical project output is a new corpus, including annotations and dialogic summaries. <br\/><br\/>Longer term impacts include public policy, providing government and the military with methods to discover what \"the man on the street\" is saying about current topics. Educators can re-use the corpora and tools to expose children to compelling arguments about important issues. Greater understanding of opinion sharing dialog enables new cognitive experiments and theory: automatically identifying compelling arguments allows political science and social psychology researchers to examine learning and opinion formation. The project trains undergraduate and graduate students in interdisciplinary research combining social media, human computer interaction, computational linguistics and natural language processing.","title":"Collaborative Research: RI: Processing Opinion Sharing Dialogue in Social Media","awardID":"1302668","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[542060,"551128",542062,542063],"PO":["565215"]},"205422":{"abstract":"In many important domains, one must learn the causal structure of a dynamical system in order to design appropriate interventions, policies, and experiments. This project develops a well founded theory and practical algorithms for such learning when scientists cannot measure the system quickly enough and\/or omit causally important variables. Moreover, the theory and algorithms will focus on the most challenging case, when scientists do not know how much information is missing because of lack of either speed or breadth. For example, fMRI measurements in cognitive neuroscience experiments occur roughly every two seconds, but communication between neural regions happens much more quickly (though exactly how much more quickly is unknown). In addition, neuroscientists are almost certainly unable to record all causally significant variables, such as other bodily states. Similarly, many climatological studies omit important variables (e.g., land use) and yield only monthly (or slower) measurements, even though the underlying phenomena presumably proceed on a faster timescale.<br\/><br\/>This project will first focus on the challenge of learning from an undersampled time series (with unknown undersample rate), which will require (a) extending the formal framework of causal graphical models to represent such possibilities; (b) providing a set of theorems characterizing how causal structures change under undersampling; (c) developing algorithms that infer constraints on the \"true\" timescale causal structure from the causal structure learned from the undersampled data; (d) implementing these algorithms in a pre-existing, open-source causal learning environment; (e) testing these algorithms in silico through extensive simulations; and (f) applying them to real world datasets, including large-scale neuroimaging data. This last step is particularly important as it will enable real-world validation of the theory and algorithms developed earlier. In parallel, the project will address the same six challenges for situations in which data are correctly sampled, but causally significant variables are missing. Finally, these two pieces will be merged into an integrated framework and algorithms for situations in which both challenges arise simultaneously. The resulting set of theorems, algorithms, and applications will both extend the current theory of causal modeling and causal structure learning, and also address the practical needs of researchers engaged in causal learning from complex, real-world time series data.","title":"RI: Small: Collaborative Research: Learning Causal Structure from Complex Time Series Data","awardID":"1318759","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[550067],"PO":["564318"]},"205543":{"abstract":"The relational join is central to relational database processing, which is the dominant way data is processed today. The join also models problems in biological and social networks, coding theory, compressed sensing, machine learning, and constraint satisfaction. Recently,\u00a0\u00a0the investigators described the first ever worst-case optimal algorithm (the NPRR algorithm) for join queries.\u00a0\u00a0These new results open a line of new tools to attack a diverse set of fundamental problems related to the join. This project aims to further exploit the new algorithmic techniques developed for NPRR to address the following three classes of problems:<br\/><br\/>(1) Optimal Join algorithms. Developing algorithms that are instance optimal when the data are stored in either traditional database indexes or new indexing structures is a goal of this project. (2) Coping with and Leveraging Noise. This project will extend the latest work to handle and leverage both worst-case and statistical noise models, bridging to coding theory and compressed sensing.\u00a0\u00a0(3) Expressive Query Languages. The project will explore a series of extensions to join queries that will pave the way to overcome challenges in motif finding, search, databases with functional dependencies, and more powerful classes of queries and join operations.<br\/><br\/>If successful, the results of this grant will apply to a variety of pattern extraction problems in modern massive, dynamic, and noisy data sets, which have a wide range of applications in complex network analysis, coding theory, and compressive sensing.","title":"AF:III:Small:Collaborative Research: New Frontiers in Join Algorithms: Optimality, Noise, and Richer Languages","awardID":"1319402","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550386,550387],"PO":["565251"]},"205664":{"abstract":"With the ubiquitous availability of parallel architectures, the burden falls on programmers' shoulders to write correct parallel programs that have high performance and portability across different platforms. Unfortunately, most programmers find it a challenging task. One of the major issues that contribute to this challenge is the intricacies involved with the underlying memory consistency models that define the order of memory operations. The situation is worsened by the fact that the memory model specifications provided by public architecture vendors are often ambiguous, difficult to use, and even incorrect. This project looks at techniques to characterize, detect and avoid bugs caused by memory models. <br\/><br\/>A memory model forms the fundamental basis for writing parallel programs. If a programmer is not careful about the constraints of the underlying memory model, a parallel program might end up having subtle bugs. In one scenario, the bugs might cause a program execution to have unintuitive interleaving of instructions, which eventually leads to incorrect behavior and lack of portability. In other scenarios, the bugs might cause a significant slowdown and poor scalability of the program. Unfortunately these bugs, referred to as Memory Model Bugs, receive very little attention from the research community. Therefore, this work focuses on developing new techniques to deal with these bugs. The work will first characterize different memory model bugs in real world code bases. The findings will be useful in developing hardware and compiler techniques (e.g., new hardware modules, exceptions, static analyzers, etc.) to detect as well as avoid these bugs. Finally, the work will focus on designing new software debugging tools to help programmers get rid of these subtle bugs. The research in this proposal will enable the widespread practice of parallel programming by addressing some of the hardest concurrency bugs. On one hand, it will encourage software and hardware companies to invest in new techniques for debugging and avoiding these bugs. On the other hand, it will help make C++ and Java memory models simple and robust.","title":"SHF: Small: Novel Techniques for Handling Memory Model Bugs","awardID":"1319983","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":[550666],"PO":["366560"]},"202034":{"abstract":"PI: Metin Akay<br\/>Proposal ID: 1301560<br\/><br\/>Neural engineering is an emerging discipline that combines multiple engineering disciplines, including electronic and photonic technologies, computer science, physics, chemistry, mathematics with cellular, molecular, cognitive and behavioral neuroscience to understand the organizational principles and underlying mechanisms of the biology of neural systems and the behavioral dynamics and complexity of neural systems in nature. To highlight this emerging discipline, the 6th International IEEE EMBS Conference on Neural Engineering will be held in San Diego, CA from November 3 through November 6, 2013.<br\/><br\/>Intellectual Merits: The objective of this conference is to highlight progress in Neural and Cognitive Engineering, an emerging field bridging molecular, cellular, systems, cognitive and behavioral neuroscience with engineering, physics, chemistry, mathematics and computer science. We strongly believe that the neural engineering conference further stimulates neural engineering research and education among neuroscientists, chemists, engineers, computer scientists and mathematicians.<br\/><br\/>Broader Impact: In addition to the keynote, plenary, platform and poster sessions, we will also have a panel to focus on the challenges of making a career in the rapidly growing, interdisciplinary fields of neural engineering, from being hired as part of a team and finding<br\/>exciting new research opportunities, to becoming a team leader and what to look for when organizing that team and choosing its members. Options will be discussed for graduate studies and research both internationally and within the U.S., as well as the potential for collaboration among researchers and students internationally.","title":"6th International IEEE EMBS Conference on Neural Engineering, Nov 3-6, 2013, San Diego, CA","awardID":"1301560","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0702","name":"Division of CHEM, BIOENG, ENV, &  TRANSP S","abbr":"CBET"},"pgm":{"id":"5345","name":"BIOMEDICAL ENGINEERING"}}],"PIcoPI":["542495"],"PO":["565045"]},"202276":{"abstract":"This project seeks to identify principles by which information and computer scientists can design solutions for systems that will span periods extending beyond a single human lifespan. The challenge is not simply that hardware, software, and information infrastructure change, but also that culture and social institutions change. Thus the research will develop information systems that can adapt to shifting socio-technical conditions as they unfold; provide a delicate balance among remembering, forgetting, and speaking sensitive to different generations; support public discourse; and link distributed digital heritage across technology, institutions, and time. To achieve these goals, the project will leverage the existing Tribunal Voices testbed within four research strands: (1) multi-lifespan envisioning; (2) tagging and meaning making across generations; (3) supporting public discourse in shifting socio-technical conditions; and (4) constructing multi-lifespan policy and infrastructure.<br\/><br\/>Multi-lifespan systems must be resilient to shifting societal, political, and technological conditions over an extended period of time. A fundamental challenge is to understand what those shifts might be, and how to support such future shifts from a technical perspective in today's designs. This project will explore conceptual, technical, and policy-oriented mechanisms for adapting and supporting such shifts. Another fundamental challenge is determining how to involve the public in interacting with deployed multi-lifespan systems, with sensitivity to multi-generations. This project will develop innovative methods to support public discourse for digital forums and public exhibits with attention to generational perspectives and secure participation. Multi-lifespan systems will need mechanisms for supporting the permanence of data, even as technologies change, as well as the intended impermanence of data. This project will develop integrated technical mechanisms and interaction designs to support a balance among remembering, forgetting, and speaking about difficult topics. <br\/><br\/>This will be the first large-scale research and design investigation from a multi-lifespan design approach. The goals are two-fold: First, achieving technical progress by generating general design knowledge and methods for multi-lifespan information system design. Second, achieving social progress by contributing meaningful information system designs in support of advancing international justice and reconciliation between groups that had been in conflict with each other. In its broadest framing, through the development and refining of the core theory and methods of multi-lifespan information system design, this project seeks to shape the future of human-centered computing such that the next generation of scientists is well positioned to frame and address problems on a longer-term societal level.","title":"HCC: Medium: Multi-lifespan Information System Research and Design","awardID":"1302709","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[542095,"559051"],"PO":["564456"]},"205312":{"abstract":"The modern world runs on software, and software runs on compilers, programs that bridge the gap between human-readable high-level programming languages and the low-level machine code that computers execute. Every year, millions of dollars and countless hours of effort go into ensuring that software is correct and reliable. However, the bulk of this analysis is applied at the high level, leaving the compiler as a potential weak link in the verification chain. Errors in a compiler are particularly insidious because they are difficult to isolate and reproduce, and potentially affect every program processed by that compiler. The VeriF-OPT project aims to make it feasible to verify compilers by providing a user-friendly and reusable framework for constructing rigorous mathematical proofs of compiler correctness, thus removing a source of error that potentially undermines the verification of high-level programs.<br\/><br\/>The VeriF-OPT project will use formal methods tools, including the Isabelle proof assistant and the K Framework for programming language specification, to develop a general, reusable framework for specifying and verifying compilers for any language, lowering the high barrier to entry for compiler verification. The framework will be designed to work particularly well for optimizations for parallel programs, which are often more complex and poorly understood than their sequential counterparts. The core components of the framework are a domain-specific language for the specification of program transformations, an executable semantics for this language that allows compiler designers to test and refine their designs before committing to verification, and a formal semantics for the language that serves as the basis for proofs of correctness. Every verification in VeriF-OPT will produce code fragments, lemmas, and other intermediate results that, thanks to the modular design of the framework, can be reused in future projects. By helping compiler designers, testers, and verifiers work together to create compilers with strong guarantees of correctness, the project will raise the standard for software reliability and help prevent costly and dangerous failures due to undetected and unexpected bugs.","title":"SHF: Small: VeriF-OPT, a Verification Framework for Optimizations and Program Transformations","awardID":"1318191","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[549774,549775],"PO":["565264"]},"205433":{"abstract":"In many important domains, one must learn the causal structure of a dynamical system in order to design appropriate interventions, policies, and experiments. This project develops a well founded theory and practical algorithms for such learning when scientists cannot measure the system quickly enough and\/or omit causally important variables. Moreover, the theory and algorithms will focus on the most challenging case, when scientists do not know how much information is missing because of lack of either speed or breadth. For example, fMRI measurements in cognitive neuroscience experiments occur roughly every two seconds, but communication between neural regions happens much more quickly (though exactly how much more quickly is unknown). In addition, neuroscientists are almost certainly unable to record all causally significant variables, such as other bodily states. Similarly, many climatological studies omit important variables (e.g., land use) and yield only monthly (or slower) measurements, even though the underlying phenomena presumably proceed on a faster timescale.<br\/><br\/>This project will first focus on the challenge of learning from an undersampled time series (with unknown undersample rate), which will require (a) extending the formal framework of causal graphical models to represent such possibilities; (b) providing a set of theorems characterizing how causal structures change under undersampling; (c) developing algorithms that infer constraints on the \"true\" timescale causal structure from the causal structure learned from the undersampled data; (d) implementing these algorithms in a pre-existing, open-source causal learning environment; (e) testing these algorithms in silico through extensive simulations; and (f) applying them to real world datasets, including large-scale neuroimaging data. This last step is particularly important as it will enable real-world validation of the theory and algorithms developed earlier. In parallel, the project will address the same six challenges for situations in which data are correctly sampled, but causally significant variables are missing. Finally, these two pieces will be merged into an integrated framework and algorithms for situations in which both challenges arise simultaneously. The resulting set of theorems, algorithms, and applications will both extend the current theory of causal modeling and causal structure learning, and also address the practical needs of researchers engaged in causal learning from complex, real-world time series data.","title":"RI: Small: Collaborative Research: Learning Causal Structure from Complex Time Series Data","awardID":"1318815","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550101],"PO":["564318"]},"205444":{"abstract":"The modernized Smart Grid (SG) is expected to enable several new applications such as dynamic pricing, demand response and fraud detection; however, collection of such fine-grained data raises privacy issues. This project aims to design and implement several novel mechanisms for securing data collection and communication in SG Advanced Metering Infrastructure applications while preserving user privacy when the data are to be accessed. The underlying communication infrastructure, namely Neighborhood Area Networks, is to be built with wireless mesh networks using the IEEE 802.11s, an IEEE 802.11 amendment for mesh networking. The project investigates user privacy preservation mechanisms using partially and fully homomorphic encryption during data collection in the Neighborhood Area Networks. For the collected data at the data repository, attribute-based access control mechanisms are studied. As part of these access control mechanisms, novel scalable key establishment and group key management schemes are investigated. A testbed consisting of IEEE 802.11 Linux routers is part of the project to assess the overhead of privacy mechanisms under quality of service constraints. <br\/><br\/>The testbed will be a community resource for researchers, educators and students as well as utility companies interested in SG communications and privacy. Ensuring practical user privacy solutions will increase the participation of customers in SG applications saving cost and energy. The testbed will be a great help to utilities, researchers and educators who work on SG communications. The software and technical manuals will be made available as open-source documents. New graduate courses on the Smart Grid will be introduced into the academic curriculum.","title":"TWC TTP: Small: Collaborative: Privacy-Preserving Data Collection and Access for IEEE 802.11s-Based Smart Grid Applications","awardID":"1318872","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["560587"],"PO":["565239"]},"202177":{"abstract":"Modern software systems are notoriously large and complex, often made up of hundreds and thousands of interconnected modules, and lines of source code that can number in the millions. Consequently, software developers waste a considerable portion of their time trying to find and get to information in that code. To address this problem, this research project will investigate a scientific theory to describe how developers go about seeking this information. The project will then use the expanded theory to identify ways of helping developers to more efficiently find information, and in the process, increase their productivity. This contribution will in turn be put to practical use by providing to builders of today?s software tools new practical ways to apply this scientific theory to their tools, so as to reduce the effort required in today?s software development projects.<br\/><br\/>In particular, this project will (1) expand Information Foraging Theory to describe how learning affects developers? navigation through code and related artifacts, (2) generate theory-grounded design patterns explaining how to design tools that aid developers' information foraging, (3) develop a design method enabling tool builders to apply these patterns, and (4) evaluate the validity and effectiveness of (1)-(3). Broader impacts will include (a) helping with teaching and learning for software tool builders, professional software developers, graduate students, undergraduates and high schoolers, (b) broadening participation of students from underrepresented groups, (c) producing publicly available infrastructure consisting of design patterns and training materials, (d) disseminating results via publications, software, and workshops, and (e) providing a scientific foundation for future efforts aimed at training developers and designing tools for their use.","title":"SHF: Medium: Collaborative Research: Information Foraging Theory: From Scientific Principles to Engineering Practice","awardID":"1302117","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["549767"],"PO":["564388"]},"205455":{"abstract":"Cloud computing is growing at exponential rates due to its great benefits to virtually all companies relying on IT systems. The biggest concern preventing further cloud adoption is data security and privacy. The main security principle in the design of cloud servers has been virtual isolation which ignores information leakage through subtle channels shared by the processes running on the same physical hardware. <br\/><br\/>The goal of this project is to explore side-channel leakage on virtualized machines that form the cloud. By utilizing a cloud testbed, this project explores interactions of the underlying hardware, virtualization platforms, and cryptographic software. A better understanding of side-channel leakages is aiding the design of effective countermeasures. To facilitate effective transition to practice the project is taking a pragmatic approach by focusing on commonly used cryptographic software libraries which lie at the heart of virtually any security solution. To address identified weaknesses in existing crypto libraries, countermeasures in the form of patches will be released, ensuring the security of cloud servers.<br\/><br\/>The project resolves weaknesses in cryptographic software by issuing updates with immediate benefits to virtually all cloud customers. The impact is further amplified by the continuing rapid growth of cloud adoption. By understanding the vulnerabilities in virtualized systems, the project raises awareness in the software security community. By integration of results into existing curricula, the project aids training future cybersecurity experts at the undergraduate and graduate levels.","title":"TWC TTP: Small: RAIN: Analyzing Information Leakage in the Cloud","awardID":"1318919","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["550273",550154],"PO":["565327"]},"205576":{"abstract":"The Integrated Digital Event Archive and Library (IDEAL) system addresses the need for combining the best of digital library and archive technologies in support of stakeholders who are remembering and\/or studying important events. It extends the work at Virginia Tech on the Crisis, Tragedy, and Recovery network (see http:\/\/www.ctrnet.net) to handle government and community events, in addition to a range of significant natural or manmade disasters. It addresses needs of those interested in emergency preparedness\/response, digital government, and the social sciences. It proves the effectiveness of the 5S (Societies, Scenarios, Spaces, Structures, Streams) approach to intelligent information systems by crawling and archiving events of broad interest. It leverages and extends the capabilities of the Internet Archive to develop spontaneous event collections that can be permanently archived as well as searched and accessed, and of the LucidWorks Big Data software that supports scalable indexing, analyzing, and accessing of very large collections. Through a new model-based approach to intelligent focused crawling, it improves the quality (e.g., accuracy, coverage, and elimination of noise) of collections of webpages so as to ensure comprehensiveness, balance, and low bias, as is needed for scholarly study of historically important events by social scientists. It incorporates a range of visualization capabilities in support of key stakeholder communities, including archivists, librarians, researchers, scholars, and the general public. IDEAL connects the processing of tweets and webpages, combining informal and formal media, to automatically detect important events, as well as to support building collections on chosen general or specific topics. It supports integration of multiple types and at multiple levels, including key models about the event it is crawling (event models), the sources of information about the event (source models), the mechanisms used for disseminating information about the event (publishing venue models), and the entities related to the event (society \/organization models). Integrated services include topic identification, categorization (building upon special ontologies being devised), sentiment analysis, and visualization of data, information, and context.<br\/><br\/>The IDEAL website (http:\/\/www.eventsarchive.org) supports searching, browsing, analyzing, and visualizing of event collections (of both tweets and webpages), as well as access to project software, methods, findings, publications, and other results. Usage is encouraged of the integrated system along with a growing number of collections, as well as of particular tools such as for focused crawling, which should aid curators to avoid non-relevant content while including a broader range of sources, improving significantly upon current crawling and archiving methods. Important data and information on events of interest are saved rather than lost, helping preserve our history and culture, in support of public interest, education, policy making, historical analyses, and comparative studies. Students studying sociology, human-computer interaction, digital libraries, information retrieval, computational linguistics, multimedia, and hypertext are gaining experience and contributing in scholarly studies, algorithms, software, interfaces, and big data handling.","title":"III: Small: Integrated Digital Event Archiving and Library (IDEAL)","awardID":"1319578","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[550461,550462,550463,550464,550465],"PO":["563751"]},"205587":{"abstract":"This project will tackle quantifier elimination by using group analysis. Quantifier elimination is a problem of taking a complex formula (with quantifiers) and producing an equivalent but simpler formula (without quantifiers). Numerous non-trivial and important problems in science and engineering can be reduced to quantifier elimination problems. Hence any progress in quantifier elimination can have significant and broad impact on science and engineering. Group analysis is an approach of applying group theory to simplify or elucidate a difficult problem, as manifested in algebraic and differential Galois theories, theories of Lie groups and Lie algebras, among many others. In particular, the group analysis significantly extended the practical solvability of various differential equations arising in science and engineering. <br\/><br\/>This project aims to make further progress in quantifier elimination by using group analysis. The main idea is to simplify a quantified formula via group analysis before carrying quantifier elimination. Preliminary investigation shows that group analysis could provide dramatic speed-up, when there is a hidden group structure in the problem (which is often the case due to various intrinsic symmetries in the nature). <br\/><br\/>The scientific goal of the project is to develop theory, algorithms, and software packages that will be used by scientists and engineers for solving ever more non-trivial problems. The educational goal of the project is to train future researchers (PhD students) and to inspire undergraduate students via REU programs and independent studies.","title":"AF: Small: Quantifier elimination by group analysis","awardID":"1319632","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[550490,550491],"PO":["565157"]},"209943":{"abstract":"The objective of this project is to overcome the limitations of sensor artifacts (noise), false detection, and energy\/power constraints by combining the analysis of multiple physiological signals through specialized hardware which implements a multi-layer classification technique comprised of a unique sequence of signal processing and machine learning functions to distill time series data. The hypothesis is that a hybrid architecture can leverage common operations and communication patterns between DSP and machine learning to support these computations more efficiently than traditional digital signal processors and general purpose processors. This hypothesis is explored in the context of wearable seizure detection, using traces of EEG and other physiological sensor data obtained from the Epilepsy Center at University of Maryland Medical Center. <br\/><br\/>Success of this exploratory research could have a significant impact for robust and efficient monitoring and use of continuous multi-physiological data for patients. Just in the context of epilepsy, it could enable seizure detection and caregiver alerts, which is important at night when seizures can happen without someone to help nearby. Longer term potential impacts extend to human-centered cyber-physical systems, cyber-security, and unmanned vehicles.","title":"CSR: EAGER: Multi-physiological Signal Processing Architectures for Seizure Detection","awardID":"1350035","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[562797],"PO":["564778"]},"208986":{"abstract":"This project designs and deploys a multi-disciplinary framework to model spatial, temporal and social dynamics of cyber criminals. The framework fuses theories in both computer science and criminology. Specifically, project objectives are a) Apply and validate existing theories in the realm of general criminology (in particular Akers? social learning theory and Gottfredson and Hirschi?s general theory of crime) to study cyber crimes; b) Derive novel Internet usage features as fingerprints for cyber crimes; c) Design classification algorithms (based on multi-fractal analysis and petri-net designs) to subsequently model multiple dynamics of cyber criminals by integrating theoretical and practical outcomes from the above two objectives; and d) Extensively test and validate project outcomes. The core novelty of this project is in using real Internet data from subjects (initially a cyber savvy college sample) that is collected continuously, unobtrusively, while still preserving a high degree of privacy.<br\/><br\/><br\/>Outcomes of this project will have far reaching impacts. It lays a foundation for fusing expertise in social sciences (specifically criminology) and cyber security, as a result of which existing theories in general criminology can be empirically tested for practical validity in studying cyber crimes. The identification of unique Internet fingerprints associating with cyber crimes will provide new insights into human centered aspects of cyber crimes, which is lacking today. The classification algorithms designed will provide cyber defenders with new tools to combat cyber crimes from multiple perspectives including prevention, detection, forensic investigations and prosecution.","title":"EAGER: Collaborative: A Multi-Disciplinary Framework for Modeling Spatial, Temporal and Social Dynamics of Cyber Criminals","awardID":"1343245","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[560151],"PO":["565327"]},"205477":{"abstract":"Users today have access to a broad range of free, web-based social services. All of these services operate under a similar model: Users entrust the service provider with their personal information and content, and in return, the service provider makes their service available for free by monetizing the user-provided information and selling the results to third parties (e.g., advertisers). In essence, users pay for these services by providing their data (i.e., giving up their privacy) to the provider.<br\/><br\/>This project is using cloud computing to re-architect web-based services in order to enable end users to regain privacy and control over their data. In this approach---a confederated architecture---each user provides the computing resources necessary to support her use of the service via cloud providers. All user data is encrypted and not exposed to any third-parties, users retain control over their information, and users access the service via a web browser as normal.<br\/><br\/>The incredible popularity of today's web-based services has lead to significant concerns over privacy and user control over data. Addressing these concerns requires a re-thinking of the current popular web-based business models, and, unfortunately, existing providers are dis-incentivized from doing so. The impact of this project will potentially be felt by the millions of users who use today's popular services, who will be provided with an alternative to the business models of today.","title":"CSR: Small: Towards Confederated Web-Based Services","awardID":"1319019","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[550216,"564627"],"PO":["565319"]},"205246":{"abstract":"The research objective of this award is to leverage recent advances in 3D-printing technology and in the robotics communities: open-source tools to enable rapid, on-site design and manufacture of soft, human-friendly robots. These soft robots can be customized to the geometry of a particular task by creating 3D molds from the object geometry where embedded chambers for fluid can be molded into the elastomer. Actuation of the devices can be achieved by pressurizing the chambers, leading to induced strain in certain parts of the elastomeric material. The research will enable 3D simulations of soft robot designs to be performed via a finite element approach that takes into account the complex geometry and non-linear material properties. Deliverables include a catalog of design and fabrication rules for soft robots, guidelines for simulating fluid-structure interaction with finite element modeling, demonstration and validation via hardware, documentation of research results, engineering student education, and the development of an open-source platform for sharing the research with the broader educational and research communities. <br\/><br\/>If successful, the results of this research will provide an opportunity to create custom soft robots that are inherently low-cost and safe for interacting with humans and fragile objects. This will lead the way for a transition from conventional machines to a new generation of versatile, multifunctional soft systems customized for specification applications where safe interaction with humans and the environment is critical. Example applications include soft robotic braces that can be customized for patients for physical rehabilitation and soft robotic grippers for that can be customized to particular objects on manufacturing assembly lines. For dissemination, an open-source platform will be developed that will consist of 3D CAD models, simulation files, fabrication instructions that can all be adapted for other applications. This information will enable high-school students, undergraduate design teams, and academic and industrial researchers to create their own soft robots.","title":"NRI: Small: Integrated modeling and manufacturing framework for soft fluidic robotics","awardID":"1317744","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549604,"553417"],"PO":["563744"]},"206357":{"abstract":"In this Cyberlearning: Transforming Education DIP (Development and Implementation) project, the PIs are investigating how embodied play among elementary school students can be used to help them understand scientific phenomena (e.g., the working of forces, complex behaviors of bees). They are instrumenting elementary school classrooms with advanced tracking. The PIs are building upon the ways that young children engage in socio-dramatic play. Through role play, children explore and reflect upon the complex rules that govern the world, and in this project, they are asked to play roles in a natural system (e.g., bees in a beehive) and identify the rules that would make their role play match the workings of the natural system. Motion capture technology is used to record their interactions, and they reflect together as a class after role-playing experiences. Research focuses on the qualities of reflection and subsequent learning afforded by two aspects of such play -- when children interact to plan their role play (equivalent to modeling) and when they act out the system or phenomenon (equivalent to simulation) and then have a chance to examine their interactions. To understand the affordances and importance of embodied play, reflection and learning outcomes are also compared across conditions of acting out a system or phenomenon (1st person embodied simulation) and running a computer simulation of that same system or phenomenon (3rd person virtual simulation).<br\/><br\/>There is currently little understanding of how to teach science productively in early elementary school. As suits this age group, these PIs are going after how to take best advantage of the kinds of role playing children naturally engage in at this age. Much of their role play discussion has to do with coming up with the rules of the role play; the PIs' insight is that this could be seen as the equivalent of creating a model. This research is developing a model of how to involve kids in grades K-3 in learning science through modeling and, in addition, focusing on the the kinds of reflection on that play that result in learning and how to affect such reflection and subsequent learning with the help of technology.","title":"DIP: The Science Through Technology Enhanced Play (STEP)","awardID":"1323767","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[552477,552478,552479,"552599"],"PO":["562669"]},"209866":{"abstract":"The goal of this travel grant is to broaden the audience attending the premier network protocol conference, the IEEE International Conference on Network Protocols (ICNP), and as a result, raise the level of interaction and the potential for new collaborations, new investigations, and higher quality research. Support from the National Science Foundation is requested on behalf of graduate students attending institutions in the United States. <br\/><br\/>Intellectual Merit: By supporting graduate students to attend ICNP 2013, this grant will increase the dissemination of the conference research results to a larger and a more diverse audience, which otherwise would not have been able to attend. In so doing, this proposal will contribute to the strength and inclusiveness of the research community, and will lead to the long-term health and vitalization of ICNP. In addition, preference in grant awards to women and under-represented students will hopefully increase the participation among these groups. By advertising to a wide range of colleges and universities, participants from a more diverse set of institutions should be able to attend and benefit from the conference. A broad, as well as diverse population is key to the health of the research community and the discipline. <br\/><br\/>Broader Impacts: Conference attendance is a crucial part of the life of a researcher. By creating new opportunities for students, especially those from under-represented groups to attend a high-quality conference, this project will benefit the research community in several ways. The students themselves benefit from the opportunity to meet and interact with many other researchers in a favorable setting, and from seeing research presented that may be related to what they are working on, or may inspire them to try a new direction. The research community benefits from the improvement of the students in the pipeline, and the introduction of new researcher perspectives. Additionally, everybody benefits from increased diversity of participants attending the conference.","title":"Student Travel Support for the 21st IEEE International Conference on Network Protocols (ICNP)","awardID":"1348891","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["446168"],"PO":["565090"]},"206489":{"abstract":"In this Cyberlearning: Transforming Education DIP (Development and Implementation) Project, the PIs focus on how to address the English-language learning needs of young English-language learners (ELLs). They are enhancing the Moved by Reading (MbR) computer program that has been used to increase the reading comprehension of native English speakers. In MbR, a child actively simulates a sentence or a set of sentences being read by moving pictures on a computer screen to illustrate what he or she understands. When they become competent at that, learners transition to imagining moving pictures as they are reading so that they become independent and accomplished readers. In this project, the PIs are enhancing MbR in several ways to help ELLs learn English and learn to read in English. The new approach, called EMBRACE (Enhanced Moved by Reading to Accelerate Comprehension in English), includes MbR as well as facilities for vocabulary support in learners' first language (in this case, Spanish), personalizing choice of next texts to read based on learners' syntax and vocabulary capabilities and needs, game-like facilities for sustaining engagement, and support for collaboration. Research is addressing the roles and effects of each of these supports for learning so as to identify good practices for promoting English-language literacy in ELLs, how to use technology to support those practices, and trajectories of development of English-language competencies in such learners.<br\/><br\/>Because reading serves as a gateway to academic success, graduation, and employment, there is a great national need to address the English-language learning and reading needs of those whose first language is something other than English. This project addresses this issue through the development of an approach called EMBRACE -- Enhanced Moved by Reading to Accelerate Comprehension in English, an interactive software system to be used in elementary school classrooms, supplementary programs, and at home. Using EMBRACE, children learn English vocabulary, syntax, and reading through manipulating pictures representing the content of what they are reading. Choice of next texts to read is personalized to individual learners. Learners also are supported in working on language issues with peers.","title":"DIP: EMBRACEing English Language Learners with Technology","awardID":"1324807","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}}],"PIcoPI":[552833,552834,552835],"PO":["562669"]},"209635":{"abstract":"This collaborative research between Florida State University and Cornell University is to identify language-action features from text-based messages that can be used to dynamically infer a social actor's perceived trustworthiness. The team will investigate using optimal analysis techniques to calibrate trustworthiness reasoning, which can be used to computationally model actors' deceptive behaviors in cyber space and to infer actors' intent based on their words and actions.<br\/><br\/>This research will have a transformative impact in understanding the dynamics of trusting relationships through observing language-action features and psychosocial trustworthiness attribution mechanisms. This study serves as a precursor to a socio-technical schema that will facilitate national security and data protection for the general populace while also protecting the individual's right to privacy. This study will contribute to the science of cyber-security, and will help the cyber-security community to understand and enable trustworthy communication and collaborative information behavior among computer-mediated groups in a systematic way.","title":"EAGER: Collaborative: Language-Action Causal Graphs for Trustworthiness Attribution in Computer-Mediated Communication","awardID":"1347120","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561930],"PO":["565327"]},"199163":{"abstract":"Progress in understanding the individual and group maneuvers of flying animals has been slow to date, hampered by technical limitations in quantifying flight behavior and performance in the field. However, the diversity of form and function apparent in living bird, bat and insect species is related to the ability to maneuver, particularly to avoid or enable prey capture and prevail in contests with others of the same species - activities which cannot be adequately replicated in a laboratory. This research will take advantage of recent advances in low power, high-resolution, high-speed video technology to develop flexible recording techniques suitable for precisely measuring individual and group animal flight maneuvers in natural environments over a wide range of sizes and time scales. These capabilities will be used to address hypotheses on the fundamental relationships between shape, biomechanics and maneuverability in flying animals and on their group behavior. Similarities in basic grouping parameters will be examined among different bird or bat species known for varying degrees of coordinated behavior. In addition to providing insight into the biology of a range of animal species, discoveries in this area will also help speed development of micro-air vehicles, animal scale flying machines. Finally, this proposal will lead to rich, freely available data sets and ready-to-use software for other researchers. <br\/><br\/>As part of the education and outreach activities funded by this proposal, a partnership with UNC's Morehead Planetarium and Science Center will incorporate findings from this research and other recent studies of animal flight behavior and performance into a fulldome planetarium show and associated educational materials to be viewed by visiting North Carolina primary school students. The show and materials will also be shared with other planetariums worldwide.","title":"CAREER: Individual and Group Animal Flight Dynamics","awardID":"1253276","effectiveDate":"2013-09-15","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}}],"PIcoPI":[534241],"PO":["376407"]},"209228":{"abstract":"Semantic technologies, Linked Open Data, and knowledge representation formalisms such as OWL are beginning to play increasingly important roles across a broad range of applications.Ontology modeling languages, e.g., RDF, OWL, and RIF, enable the inference of implicit knowledge from knowledge that is explicitly encoded in the knowledge base. Web Reasoning and allied fields focus on the design, analysis, and development of ontology languages, study of their theoretical properties, and the design and implementation of effective inference algorithms. There is an urgent need for advanced training graduate students to conduct research in this area and prepare for academic or industrial careers. Participation in premier research conferences in the area is an essential element of such training. <br\/><br\/>The Web Reasoning and Rule Systems conference series, established in 2007, attracts the leading researchers in the field of Web Reasoning, and offers a venue for presentation of the latest advances in the field. The co-location of the conference with the Reasoning Web Summer School makes it also an excellent venue for training graduate students. This project provides travel grants for U.S. based graduate students to attend the 2013 Web Reasoning and Rule Systems Conference scheduled to take place during July 27-29, in Mannheim, Germany. The conference is co-located with the 2013 Reasoning Web Summer School, and also with the 26th International Workshop on Description Logics. In addition to attending the invited and contributed talks at the conference, and the Summer School, the students will benefit from individualized mentoring by established researchers. <br\/><br\/>Broader impacts of the project include the training of a new generation of researchers and technologists in the emerging area of Web Reasoning which is currently seeing substantial industrial interest and uptake, and enhanced opportunties for broadening the participation of groups that are currently under-represented in Computer Science in general, and Semantic Technologies, Web Reasoning, and related areas in particular.","title":"Student Travel Fellowships: 2013 Web Reasoning and Rule Systems Conference","awardID":"1344437","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[560895,560896,560897,"563585","561219"],"PO":["565136"]},"202793":{"abstract":"Real-time motion-capture and interactive 3D computer-generated environments are rapidly emerging as an integrated and powerful human-computer interaction context with the ability to revolutionize applications in many areas. Recent advances in sensor and display technologies can now be seen in both low-cost consumer-oriented and in high-end industrial-oriented human-centered computer systems. This proposal aims to support and enhance research and educational activities in new research and application areas by strengthening and expanding the available research infrastructure at the University of California - Merced, specifically by enhancing an existing visualization and motion capture facility with the acquisition of (a) a high-end data glove, (b) an occlusion-free motion capture suit, and (c) portable interactive displays integrated with Kinect sensors. The proposed equipment enhancement will allow the development of new research projects on the following topics: (a) gesture synthesis models with coordinated hand-arm motions, (b) remote virtual collaborative sessions for upper body motion rehabilitation assessment and remote therapy delivery, (c) automated progress monitoring of hand motor rehabilitation in respect to given motion protocols, and (d) human-like full-body motion planning in tight spaces for training applications. The projects will target interactive training and therapy applications that can benefit from new human-computer interfaces that are situated in virtual environments.<br\/><br\/>Intellectual Merit<br\/><br\/>An interdisciplinary team from computer science and cognitive science will explore novel human-centered computing research with a focus on the channels of communication that are used in parallel during physical training and therapy. The proposed portable and occlusion-free high-end motion sensing equipment will allow the development of human-like motion models for synthesis of gestures as well as full-body task-oriented motions. These models will enable, during immersive remote interactions, the discovery of new interaction paradigms, which will involve autonomously synthesized human-like motions and human motions that will be captured and streamed back to the system. A gesture analysis tool will be developed to provide algorithmic infrastructure to automatically analyze and extract the components of the gestures. The infrastructure will permit rehabilitation and therapy to be evaluated in actual practice. The scientific and practical research projects will advance the effectiveness of virtual humans in diverse educational, training, and therapeutic applications, and will help make such 3D computer-generated environments more accessible to the common user.<br\/><br\/>Broader Impact<br\/><br\/>The infrastructure will support research projects that relate to motion rehabilitation, which has the potential to benefit the many Americans who are in need of physical therapy such as for ailments in the back, neck, and shoulders. The research will advance an understanding of a principled approach for using virtual humans for a broad range of practical training and therapy applications. The instrumentation and research will provide unique learning opportunities for graduate and undergraduate students. The research outcomes will be disseminated broadly in cognitive science, computer science, and rehabilitation therapy publications. The infrastructure and research will be demonstrated to local high school and community college students to motivate interest in science and engineering.","title":"II-EN: Acquisition of Sensors and Displays for Research on Motion Synthesis and Rehabilitation","awardID":"1305196","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543484,543485],"PO":["565362"]},"210185":{"abstract":"This student travel grant supports graduate students enrolled in U.S. institutions to attend the premier conference in the multimedia area, ACM Multimedia 2013. It is a top scientific conference in the area of multimedia, covering latest technical trends, system demos, grand challenges, open source software competition, innovative new ideas, doctoral symposium, and many other innovative activities. Participation in such an event allows students to interact with the leading experts from around the world, learn cutting edge research, develop professional networks, and interact with industry leaders involved in practical applications and technology transfer. The opportunity is broadcast to the broad community and students from underrepresented groups are particularly encouraged to apply.","title":"ACM Multimedia 2013 Student Travel Grant","awardID":"1352869","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[563299],"PO":["564316"]},"205840":{"abstract":"As virtualization spreads throughout the data center, the communication endpoints within the data center are becoming virtual machines (VMs), not physical servers. Therefore, the network facilities for packet processing and security must operate all the way to the VM. This last hop switching among virtual machines within the physical server has become a critically important component of the data center network. This project will explore the design space of server\/switch integration, in which software and hardware based switching are more efficiently integrated directly into the server. More specifically, it will decouple the switching operations and distribute their implementation throughout the virtualized system. This project will be comprised of the following thrusts: <br\/> - Efficient Software Switching via a decoupled software switching architecture using a flow-based approach that eliminates these<br\/> overheads and that enables efficient switching in software<br\/> - Judicious choice and use of hardware acceleration. <br\/> - The Rack Becomes the Server via having the network control plane learning the hardware topology, monitoring the network traffic,<br\/> instructing the hypervisor to move communicating VMs closer to each other. <br\/><br\/>As the demand for data center capacity continues to increase at an incredible rate, it is becoming critical to minimize the number of physical machines. This research will transform the way in which networking is implemented on future systems, enabling the effective use of 100s of virtual machines per physical machine. This reduction in physical machines can have significant societal and economic impacts. If U.S. data centers used virtualization technology to achieve just half of the power reductions predicted by Intel, we would save $3.5 billion of electricity and prevent the release of 322 million pounds of CO2 per year.","title":"NeTS: Small: Scalable Network Virtualization for Multi-Core Systems","awardID":"1320965","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[551076],"PO":["564993"]},"210075":{"abstract":"The PI is proposing work that will begin with a consideration of government data but will be extended through the Research Data Alliance (RDA) thereby bolstering the prospects of a common set of data models, APIs and registries for both government and scientific data. Given the global connections of RDA,the proposed work offers the potential to advance common data infrastructure even beyond the US landscape. Johns Hopkins University will develop a set of implementation-independent data models and application programming interface (API) specifications to support semantically useful sharing and machine action over metadata aggregated from heterogeneous data sources. Support for aligning and reasoning over common concepts within these data will be provided through a system of types and properties (fields, concepts, etc.) associated with these types. The initial specification will include a type (and associated properties) that defines a set of core metadata to which participating data providers can map each registered dataset. Allowing multiple types will support extending these core properties and permitting each metadata record to enumerate the set of types to which it conforms. Properties may describe high-level attributes of a dataset and more detailed features. The PI and his team will develop and clarify the initial set of core metadata and to develop an exemplar set of extension types. Intellectual Merit : Seamless and effortless transfer of government and research data across time, geographies, and scientific domains is a difficult problem and this small and admittedly risky project can only hope to provide one piece of the puzzle, but the potential rewards of success are significant and well worth pursuing. Broader Impacts: The proposed work would address a range of diverse data types from government and scientific sources, making them generally available for adoption without encumbrance. The coordination of outreach and adoption through the Research Data Alliance would amplify the results of this proposed to a wide range of communities, data producers and data consumers. Additionally, since the proposal team works within a research library, there is a natural venue for outreach to a community that provides capacity for additional outreach, adoption and sustainability. Finally, it is worth noting that one of the team members (DiLauro) is African-American.","title":"EAGER: Framework for Exposure of Government and Scientific Data","awardID":"1351528","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[563069,563070],"PO":["565292"]},"205851":{"abstract":"Washington State University will design and study the educational affordances of a social programming environment (SPE), a technology-rich learning environment centered around a social networking-style activity stream of learners' problem-solving activities and progress. Through an iterative, participatory design process involving students and instructors at Washington State University, Pacific University, and Mesa Community College, this project will develop (a) an SPE to support learners as they work outside-of-class on the kinds of individual programming assignments that are the centerpiece of early computing courses; and (b) sets of practical guidelines and best practices to help instructors implement social programming assignments in their courses. This work will contribute new, theoretically-grounded accounts of how learning and participation proceed in social problem-solving environments, as well as empirical findings related to their impact on student learning outcomes, problem-solving processes, attitudes, and retention.<br\/><br\/>Building upon previous work adapting the studio-based learning model for computing education, the project staff aim to address low retention in computing degree programs by facilitating effective online learning communities for introductory computing courses. This project will result in two new open source technologies?a ?social plug-in? for a computer programming environment, and a learning management system tailored for ?social programming??that can be readily used in both K-12 and undergraduate computing education. While this project focuses on computing education, the novel social problem-solving technologies and the empirical findings of the research studies will be applicable to many STEM disciplines in which learners benefit by working individually on problems within a social environment.","title":"EXP: Exploring Social Programming Environments in Early Computing Courses","awardID":"1321045","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7511","name":"TUES-Type 2 Project"}}],"PIcoPI":[551105,551106],"PO":["560704"]},"204762":{"abstract":"Industrial control systems differ significantly from standard, general-purpose computing environments, and they face quite different security challenges. With physical \"air gaps\" now the exception, our critical infrastructure has become vulnerable to a broad range of potential attackers. In this project we develop novel network monitoring approaches that can detect sophisticated semantic attacks: malicious actions that drive a process into an unsafe state without however exhibiting any obvious protocol-level red flags. In one thrust, we conduct a measurement-centric study of ICS network activity, aimed at developing a deep understanding of operational semantics in terms of actors, workloads, dependencies, and state changes over time. In a second thrust, we develop domain-specific behavior models that abstract from low-level protocol activity to their semantic meaning according to the current state of the processes under control. Our goal is to integrate these models into operationally viable, real-time network monitoring that reports unexpected deviations as indicators of attacks or malfunction. A separate \"Transition to Practice\" phase advances our research results into deployment-ready technology by integrating it into the open-source Bro network monitor. Overall, our work will improve security and safety of today's critical infrastructure by providing effective, unobtrusive security monitoring tailored to their specific semantics. In addition, we tie a number of educational activities to the research and involve students at all levels.","title":"TWC: Option: Medium: Collaborative: Semantic Security Monitoring for Industrial Control Systems","awardID":"1314891","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["557160","562328"],"PO":["565327"]},"202232":{"abstract":"Enterprises rely on specialized network appliances or middleboxes such as load balancers, intrusion detection and prevention systems, and WAN optimizers in order to meet critical performance optimization, security, and policy compliance requirements. With the advent of cloud computing, such middlebox processing will play an increasingly critical role in cloud deployments due to two key factors: 1) As enterprises move their IT infrastructure to the cloud, they want to leverage the same performance and security benefits for applications running in the cloud; and 2) Enterprises want to reduce their infrastructure and management costs by offloading middlebox functionality to cloud providers to leverage the elastic scaling and migration benefits offered by cloud computing. <br\/>Unfortunately, cloud customers and providers today lack the necessary abstractions and mechanisms for enabling this transition. At a high-level, the problem is that these workloads are drastically different from traditional computation and storage services for which cloud computing has been extremely successful. This raises fundamental challenges along several dimensions: the need for flexible composition or chaining of network services; the increased impact of network-level performance on such workloads; the inherent difficulty in identifying bottlenecked resources in multiplexed cloud deployments; and the inability to reason about correct and consistent operation of stateful network processing in dynamic deployment scenarios.<br\/><br\/>This project will bridge this disconnect by addressing foundational issues in the design and implementation of (1) policy frameworks, elastic scaling algorithms, and software-defined controllers for enterprise administrators to translate their requirements into an actual physical realization; (2) algorithms for intelligent network-level placement, traffic engineering, and topology design for cloud providers to support such workloads; and (3) new abstractions for managing and manipulating the middlebox-associated state of the network. <br\/><br\/>Broader Impact: This work will inform the critical industry evolution as enterprises and cloud providers are attempting to realize the benefits of ?network virtualization?. Furthermore, the project will enable new dimensions of flexibility for network deployments that do not exist today---democratizing the benefits of middleboxes to small businesses; providing the ability to elastically scale network-level services to meet application demands; and enabling live migration of entire enterprise deployments across physical infrastructures. The project will generate new course materials on software-defined networking and cloud computing and tightly integrate research with education to help students become experts in these emerging domains. The software tools and benchmark measurement data produced by the research will inform the industry transition and future academic work on such middleboxes-in-the-cloud deployments. Finally, while the project focuses on middleboxes in cloud deployments, the technical foundations developed therein will apply to traditional enterprise and ISP networks as well.","title":"NeTS: Medium: Collaborative Research: Enabling Flexible Middlebox Processing in the Cloud","awardID":"1302412","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["561112"],"PO":["565090"]},"205631":{"abstract":"This project focuses on research and teaching activities in the general area of approximation and online algorithms. Many optimization problems are intractable, so it is natural to approximate the optimum instead of computing an exact solution. With this insight, the past two decades have seen tremendous activity in approximation algorithms, to the point where the approximability of some basic problems is well-understood. In addition to this enhanced understanding of the computational complexity of optimization problems, an ever-increasing set of techniques and tools to attack problems old and new have been proposed. Given this situation, it is natural to take a two-pronged plan of research: while continuing to resolve some of the long-standing open problems of interest, the investigator will concurrently extend the scope of the new techniques, and also investigate more expressive models and problem abstractions that attempt to capture the rich diversity of optimization problems that arise in practice.<br\/><br\/>Along these lines, a major theme of this research is to investigate how to solve optimization problems in the presence of partial information, and hence uncertainty.\u00a0This is something often considered in practice: based only on probabilities of various events happening in the future, and subject to constraints on resources (time\/money), a system must make decisions. In the spirit of computational thinking, this research is aimed at formalizing some of these problems so that efficient solutions can be found for more realistic models than the traditional worst-case model. Research progress on these questions will advance the state-of-the-art in decision-making under uncertainty, an area lying at the intersection of computer science, operations research, and decision theory. This research will also be instrumental in training of graduate and undergraduate students.","title":"AF: Small: Approximation Algorithms for Uncertain Environments and Graph Partitioning","awardID":"1319811","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0706","name":"Division of DESIGN & MANUFACTURING INNOV","abbr":"DMI"},"pgm":{"id":"5514","name":"OPERATIONS RESEARCH"}}],"PIcoPI":[550590],"PO":["565251"]},"205401":{"abstract":"Searching for objects in physical space is one of the most common tasks for robots. Transient targets refer to a class of objects which are not identifiable unless momentary sensing and\/or signaling conditions are satisfied. The transient property is often introduced by target attributes, privacy concerns, environment constraints, and\/or sensing limitations. For example, searching for a black box on the ocean floor after an airplane crash is a typical transient target search (TTS) task because the search relies on the transient radio\/sonar signals emitted from the black box. This project develops the theoretical foundations for TTS problems that will impact a large group of real world applications. Due to their stochastic nature, TTS problems are challenging because the transient property is often coupled with factors such as sensing range limits, various coverage functions, constrained mobility, signal correspondence, limited number of searchers, and a vast searching region. Extensive modeling and analysis will be performed to understand the various factors in the searching process. As the result, a new computation framework consisting of models, metrics, and algorithms that integrate the resource to improve TTS performance will be developed with three focuses: searching time analysis that addresses the critical performance issue, multi-target search that focuses on unique signal correspondence and collaborative sensing issues, and planning for coordination of robots which includes both centralized and decentralized approaches. This project also develops new course materials for undergrad and graduate students to better understand algorithms and robust intelligence in robotic searching applications.","title":"RI: Small: Robotic Search of Transient Objects","awardID":"1318638","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550010],"PO":["564069"]},"205643":{"abstract":"As nano-scale field-effect devices quickly approach their physical limits in feature size, their stochastic device characteristics will pose severe challenges to constructing robust digital circuitry. Unlike transistor defects due to fabrication imperfection, quantum-related switching uncertainties will seriously increase their susceptibility to noise, thus rendering the traditional thinking and logic design techniques inadequate. This work aims at developing a new logic design paradigm that achieves circuit robustness for stochastically imperfect transistors and interconnects. To this end, the PI reformulates the traditional boolean-based digital design problem as a probabilistic logic-labeling problem and attempts to solve it with two approaches: a graph-theoretic approach, and a field-theoretic approach. The project also studies two unconventional logic design methodologies: a bio-inspired logic scaffolding and a self-correcting logic design. <br\/><br\/>The PI will disseminate findings by developing new curricula and creating compelling interactive learning materials e.g., hardware-based emulations, software simulations, and interdisciplinary study opportunities to expose students to the new area of stochastic logic design. The PI will also create mentoring and outreach programs specifically targeted to attract underrepresented groups, thus preparing a new diverse workforce for future IC industry.","title":"SHF: Small: Bio-Inspired Logic Design with Graph and Field Theory","awardID":"1319884","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":["559716"],"PO":["562984"]},"206743":{"abstract":"This frontier project tackles many of the fundamental research challenges necessary to provide trustworthy information systems for health and wellness, as sensitive information and health-related tasks are increasingly pushed into mobile devices and cloud-based services. The interdisciplinary research team includes expertise from computer science, business, behavioral health, health policy, and healthcare information technology to enable the creation of health & wellness systems that can be trusted by individual citizens to protect their privacy and can be trusted by health professionals to ensure data integrity and security. Although these problems are motivated by a nationally important application domain (health and wellness), the solutions have applications far beyond that domain.<br\/><br\/>This project is developing methods to authenticate clinical staff to tablet computers in a continuous and unobtrusive way, and to provide patients a usable way to control the information that mobile sensors collect about them. One of the goals is to manage security of healthcare devices in the home and in remote clinics, without adding burden on the homeowner or clinical staff; towards this end the investigators are developing methods to verify medical directives issued to remote devices. One approach being investigated is segmenting access to medical records from mobile devices to limit information exposure, and developing methods to audit behavior of this complex ecosystem of devices and systems. The investigators will design tools to handle genomic data in the cloud while enabling patient control over information, detect malware in medical devices through power analysis, and provide contextual information to those who use health data collected in the field.","title":"TWC: Frontier: Collaborative: Enabling Trustworthy Cybersystems for Health and Wellness","awardID":"1329686","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553542,553543,553544],"PO":["564388"]},"202266":{"abstract":"","title":"Collaborative Research: RI: Processing Opinion Sharing Dialogue in Social Media","awardID":"1302672","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[542065],"PO":["565215"]},"206875":{"abstract":"There are at least two key features of the move to cloud computing that introduce the opportunity for significant leaps forward in computer security for tenant services. First, a compute cloud provides a common software, hardware and management basis for rolling out cross-cutting services en masse that have resisted incremental deployment in a one-service-at-a-time fashion. Second, compute clouds offer providers a broad view of activity across an unprecedented diversity of tenant services. This research project leverages these features to develop new approaches to a wide array of fundamental problems in computer security. By convening Cloud Security Horizons summits with industry stakeholders, this project further seeks to both contribute to industry directions in cloud computing and to be informed by them.<br\/><br\/>Particular longstanding security challenges addressed in this project include secure transport, authorization, user and software authentication, security monitoring, and incident analysis. Moreover, since modern clouds are not sufficiently extensible to support the envisioned capabilities, this project is constructing cloud software platforms that enable the flexibility, extensibility and security needed for this research to come to fruition in practice.","title":"TWC: Frontier: Collaborative: Rethinking Security in the Era of Cloud Computing","awardID":"1330659","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553906],"PO":["565327"]},"205544":{"abstract":"Research and development in cloud-based distributed systems is currently<br\/>hindered due to the incredible difficulty of running such systems at<br\/>scale. The Wisconsin Pocket Datacenter (PoD) project directly addresses this<br\/>dilemma by enabling researchers and developers to run large-scale distributed<br\/>systems on modest hardware resources. The key novel idea behind PoD is data<br\/>elision, which removes actual application data from the underlying disks,<br\/>memories, and network transfers of the distributed system under test. PoD thus<br\/>enables many virtual instances of such a system to be run upon just a few<br\/>physical machines, thus making testing, debugging, and in general development<br\/>simpler, faster, and less resource-intensive.<br\/><br\/>Cloud-based distributed systems represent a critically-important piece of<br\/>modern computing infrastructure. As users and corporations migrate their data<br\/>and computation into such facilities, it is increasingly important to ensure<br\/>that such systems operate correctly and efficiently. PoD enables developers to<br\/>debug existing systems and test new designs without excessive hardware<br\/>resources; with such a tool in their workbench, system developers will be able<br\/>to more readily build and test the next generation of cloud-based systems to<br\/>support the needs of a growing percentage of society.","title":"CSR: Small: The Pocket Datacenter","awardID":"1319405","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[550389,550390],"PO":["565319"]},"205665":{"abstract":"Many distributed applications in the current Internet are massively replicated to ensure unsurpassed data robustness and scalability; however, constant data churn (i.e., update of the source) and delayed synchronization lead to staleness and thus lower performance in these systems. The goal of this project is to pioneer a stochastic theory of data replication that can tackle non-trivial dependency issues in synchronization of general non-Poisson point processes, design more accurate sampling and prediction algorithms for measuring data churn, solve novel multi-source and multi-replica staleness-optimization problems, establish new fundamental understanding of cooperative and multi-hop replication, and model non-stationary update processes of real sources.<br\/><br\/>The now omnipresent cloud technology has become a vast consumer and generator of data that must be stored, replicated, and streamed to a variety of clients. This project focuses on understanding theoretical and experimental properties of data evolution and staleness in such systems, whose outcomes are likely to impact Internet computing through creation of insight that leads to better content-distribution mechanisms, more accurate search results, and ultimately higher satisfaction among everyday users. Furthermore, this project blends a variety of inter-disciplinary scientific areas, reaches out to the student population at Texas A&M to engage them in research activities from early stages of their careers, trains well-rounded PhD students knowledgeable in both theoretical and experimental aspects of large-scale networked systems, engages under-represented student groups in STEM fields, disseminates information through two new seminars at Texas A&M, and shares data models and experimental results with the public.","title":"CSR: Small: Yesterday's News: Theory of Staleness under Data Churn","awardID":"1319984","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[550668,550669],"PO":["565319"]},"205786":{"abstract":"The overall goal of this project is to advance the computational theory and algorithms for equilibria and fixed points. Many problems from different areas can be formulated as the problem of computing a fixed point of a suitable function F. Examples include the computation of Nash equilibria of games, price equilibria in markets, the value and optimal strategies for stochastic and other dynamic games, the analysis of various basic stochastic models (branching processes, stochastic context-free grammars, recursive Markov chains, and others) that arise in many areas. In some cases (e.g., Nash and market equilibria) one wishes to compute any fixed point, while in several others (e.g., stochastic models and games), the function F is monotone and one wishes to compute a specific fixed point, the least fixed point.<br\/><br\/>The project will build on recent progress to advance the theory and algorithms on two fronts: market equilibria, and least fixed point problems. In the first area, it will seek to develop a more systematic methodology and show general results that characterize what features make the market equilibrium problem hard and what features make it easy; it will advance our understanding of the computation of equilibria, both on the hardness side and on the algorithmic side; and it will try to resolve open questions regarding specific types of markets, and limitations of price adjustment schemes. In the second area, the project will leverage recent powerful positive results to address and solve in a unified way basic problems on stochastic context-free grammars, quasi-birth-death processes, and other stochastic models that require the solution of more general classes of monotone fixed point equations.<br\/><br\/>The problems and models studied in this project are fundamental in various disciplines (including economics, game theory, biology, and various areas of computer science such as verification and natural language processing), and they have been studied and are used extensively. The research of the project will provide a systematic, unified treatment of the underlying fundamental questions, and will result in algorithms and insights that are useful in the various relevant areas.","title":"AF: Small: Computational Aspects of Markets, Equilibria, and Fixed Points","awardID":"1320654","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550946],"PO":["565251"]},"206402":{"abstract":"In this Cyberlearning: Transforming Education DIP (Development and Implementation) Project, the PIs focus on better promoting science learning in early elementary school (grades K-3). They focus in the discipline of life sciences, with specific focus on complex biological systems. The approach to learning about complex systems is through participatory simulation augmented by wearable computers. Children act out the roles of agents in complex biological systems (e.g., bees gathering honey, predators and preys) together, and with the help of electronic puppets that they can wear on one hand, they watch changes in the characteristics of animals they are simulating (e.g., energy, hunger, thirst, need for sleep) as they interact in the environment the way those animals would. Research focuses on how learning happens in the context of 1st person participation in a simulated system and how to best facilitate that learning, how learning about one complex system readies children to learn about other complex systems, how the understanding of complex systems builds over time with exposure to a variety of such systems, and best ways of using technology to affect such learning.<br\/><br\/>There is little understanding currently of how to teach science productively in elementary school. As is appropriate for this age group, these PIs take a playful approach to immersing youngsters (in grades K through 3) in the lives of animals and organisms and the ecosystems they live in. Children participate in simulations of natural ecosystems, taking on the roles of animals or organisms in those systems. To promote the kinds of reflection on experience that will lead to learning, children are equipped with electronic puppets that help them experience how those animals? lives are affected by their interactions with animal and plant life in their environment.","title":"DIP: BioSim: Developing a Wearable Toolkit for Teaching Complex Science Through Embodied Play","awardID":"1324047","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[552597,552598,552599],"PO":["562669"]},"209911":{"abstract":"Even as the availability of computing power, network bandwidth and other resources increases at a rapid rate, users and applications increase their demand for computation and their use of resources at roughly the same pace. \u00a0Thus, no matter how much progress is made \u00a0on hardware efficiency, we will always need efficient algorithms to manage these resources. The philosophy of managing additional resources in an intelligent manner goes beyond computer systems, and is relevant to many other \u00a0scientific and industrial areas. In various real-life systems, processing times may be controllable by allocating resources, such as additional money, overtime, energy, fuel, catalysts, subcontracting, or additional manpower, to the job operations. In such systems, job scheduling and resource allocation decisions should be coordinated carefully to achieve the most efficient system performance. Applications arise in many industrial areas.<br\/><br\/>The PI plans to study several algorithmic problems that arise in scheduling with additional resource constraints. \u00a0 While this field has received much attention over the past ten years and beyond, the PI will focus on two areas that have received very little attention in the computer science community -- scheduling when the benefit of the schedule and the cost of energy or other resources are monetized, and scheduling in models that go beyond the typically studied computer systems setting. \u00a0Because these have received very little attention, this work is somewhat speculative. \u00a0If successful, this work could have a high impact, with applications into many new areas. The PI will design simple, low overhead algorithms.","title":"AF: EAGER: Scheduling with Resource Contraints","awardID":"1349602","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[562721],"PO":["565251"]},"208833":{"abstract":"Computing devices and electronics have become increasingly numerous on a per-capita basis with people owning multiple personal computing devices such as cell phones, tablets, and laptops along with embedded electronic devices such as remote controls and calculators. Also, electronics advance and become obsolete at a very rapid pace. While this advancement has caused great strides in human productivity, it has come at a cost in terms of pollution to our environment, the creation of electronic waste (e-waste), and harm to electronics recycling workers. This project addresses this challenge by studying how to create computer architectures for biodegradable electronics built out of organic semiconductors. By embracing biodegradable and compostable electronics, the computing and electronics industry can switch from an industry that harms the environment to one that is sustainable and does not harm the environment.<br\/><br\/>This project tackles the challenging problem of designing computer architectures in organic semiconductors. This is a brand new field as most computer architecture research has targeted (non-biodegradable) silicon-based substrates. Many challenges exist in designing for organic semiconductors including, low mobility (low-speed), large transistor-to-transistor variability, and a different interconnect to transistor speed ratio than is found in silicon. This project takes a bottom up approach by first laying the groundwork including the creation of a model logic cell library for organic semiconductors, then it uses that library as a synthesis target for different computer architectures, and finally an architectural design tradeoff study to understand pipeline length and machine architectures.<br\/><br\/>This research can broadly impact humanity by laying the groundwork for electronics and computers that can be manufactured and disposed of without adversely impacting the environment. This can ultimately lead to electronics such as a fully biodegradable cell phone, thereby removing the need to properly recycle electronics, removing e-waste from our landfills, and reducing dangers to electronics.","title":"EAGER: Architectures for Biodegradable Processors","awardID":"1342487","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[559770],"PO":["565319"]},"206655":{"abstract":"The goal of this project is to transform the efficiency, fidelity, and safety of current critical infrastructure inspection methods by combining human judgment with machine intelligence through the development of an autonomous robotic inspection assistant. The proposed work utilizes small aerial robots, coupled with three-dimensional imaging and the state-of-the-art in planning, modeling, and analysis to develop safe and efficient, high-precision assessment of structures. The key themes of the proposed work are: (1) rapid infrastructure modeling and analysis of large complex structures via a small autonomous aerial robot with 3D mapping capabilities; (2) immersive inspection and structural assessment to combine shape and appearance into an integrated representation amenable to structural health evaluation by an inspector; and (3) adaptive aerial vehicle motion plans that seek to learn from the experience of human inspectors and facilitate as autonomous inspection assistants. The proposed work is exploring the role of humans in the entire cycle from deployment of flying robots to registering data to the assessment. This project brings together members of participating communities and is developing curriculum to engage undergraduate and graduate students from robotics and civil engineering in the proposed research.","title":"NRI: Large: Collaborative Research: Fast and Accurate Infrastructure Modeling and Inspection with Low-Flying Robots","awardID":"1328930","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[553297,"561623",553299,553300],"PO":["564069"]},"205687":{"abstract":"More than one-third of all program code exists to implement user interfaces and this code contains an even larger portion of all defects. This is no surprise; user interface code is seldom reusable and current event-handling-based approaches to user interface programming require programmers to manage a large number of details; too many to consistently produce correct and rich user interface behaviors. The impact is experienced by all computer users: user interfaces get stuck, behave illogically, lose information, lack crucial or useful functionality, or are simply confusing. The result is user frustration and wasted effort, a very significant cost when aggregated over all users, and significant even to one user when aggregated over all of his or her computer use. This project seeks advances in user interface programming, studying a declarative approach where many behaviors of user interfaces can be expressed as reusable algorithms encoded in software libraries. This drastically reduces the details that an application programmer needs to be concerned with; application-specific \"event handling\" code is not necessary at all. The goal is to significantly reduce the cost of producing high quality user interfaces, and, indirectly through better user interfaces, to make our interactions with computers less frustrating and more productive.<br\/><br\/>The proposed declarative programming approach is based on multi-way data-flow constraint systems, which are applied to model the data directly manipulated by user interfaces and the dependencies within that data. The project seeks to show that once an application programmer specifies such a constraint system and connects it to the visual elements of a user interface, a large class of the interface's behavior can be derived from reusable algorithms parametrized over the constraint system. Examples of such behaviors include updating values of elements based on user interaction on other user interface elements, enabling and disabling user interface elements, keeping the user interface maximally responsive by concurrently executing computations triggered by users' interactions, managing changes in the structure of the user interface, and correctly orchestrating all of the above.","title":"SHF: Small: Foundations for User Interface Programming","awardID":"1320092","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[550718],"PO":["564588"]},"205214":{"abstract":"This project addresses manufacturing tasks that cannot be fully automated because of either the limitations of current algorithms or prohibitive cost and set-up time. Such tasks generally require workers to collaborate in close proximity and adapt to each other's decisions and motions. This project explores accomplishing these tasks through human-robot collaboration. Recent hardware developments in robotics have made human-robot collaboration physically possible, but robots still require new algorithms to ensure safety, efficiency, and fluency when working with people. Creating such algorithms is difficult because there can be high uncertainty in what a person is going to do and how they are going to do it. This project explores the integration of reasoning about how a person moves and how he or she makes decisions into a robot motion planning and decision-making framework. The research centers on the development of new algorithmic frameworks for modeling, simulating, and planning for human-robot collaboration, which requires advances in robot training, task modeling, human motion understanding, high-dimensional motion planning with uncertainty, and metrics to assess human-robot joint action. The results of this project have the potential to signi&#64257;cantly improve American competitiveness in manufacturing; especially for small-batch manufacturing and burst production, where the cost and set-up time of fully-autonomous solutions is prohibitive. The work will be disseminated in research papers and integrated into curricula. The project is guided by an advisory board from the manufacturing industry, which provides another avenue for dissemination.","title":"NRI: Small: Collaborative Research: Adaptive Motion Planning and Decision-Making for Human-Robot Collaboration in Manufacturing","awardID":"1317462","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549499],"PO":["564069"]},"205577":{"abstract":"Verification is currently a major bottleneck in the design of the<br\/>hardware and software systems society depends on. Design errors<br\/>can lead to recalls, software crashes, cyber attacks, and even<br\/>the loss of life. A fundamental approach for identifying and<br\/>fixing design errors is to use formal verification, which checks<br\/>whether a set of system properties holds across all possible<br\/>system behaviors. Unfortunately, current algorithms suffer from<br\/>poor scalability and cannot be directly applied to modern<br\/>designs. This drawback is currently addressed with the use of<br\/>manual abstractions that in essence hide irrelevant<br\/>implementation details, thereby reducing the size of formulas to<br\/>be analyzed. Besides the manual effort involved, another<br\/>disadvantage of this approach is that it is very hard to<br\/>guarantee that an abstraction is correct. The goal of the proposed<br\/>research is to develop efficient methods for algorithmically<br\/>building provably correct abstractions, thereby drastically<br\/>improving the scalability of formal verification algorithms.<br\/><br\/>Our approach to finding correct abstractions is based on the<br\/>following three ideas. First, building an abstraction that is<br\/>correct in a small subspace of the search space is easy. Second,<br\/>one can stitch together abstractions found for subspaces to<br\/>produce an abstraction for the entire subspace explored so far.<br\/>Third, abstractions built for explored subspaces may hold in many<br\/>subspaces that have not been visited yet. This re-usability of<br\/>abstractions makes our approach extremely powerful in identifying<br\/>irrelevant parts of formulas. From a theoretical point of view,<br\/>the proposed research will lead to a better understanding of<br\/>abstractions and to the development of new formal verification<br\/>algorithms. From a practical point of view, it will result in the<br\/>creation of various kinds of tools including SAT-solvers and<br\/> model checkers that will boost the scalability of formal<br\/>verification.","title":"SHF: Small: Dynamic Abstractions for Verification","awardID":"1319580","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[550467,550468],"PO":["565264"]},"205698":{"abstract":"Cascades, also known as epidemic processes, are network phenomena where the activation of one node increases the likelihood of activation of its neighbors; this results in an event starting at one node eventually affecting a much larger part of the network via successive spread. Cascade processes serve as flexible yet coherent models for several phenomena: spread of viruses and malware in mobile phones, diseases in human society, opinions and actions in online social networks. This project focuses on using cascades as an inference and learning tool; the aim is to ascertain important network structure and properties, from partial and very noisy observations of cascade progressions on it. This runs counter to the vast majority of work on cascades, which is focused on the \"forward\" problem (of predicting how a cascade will spread given network properties). <br\/><br\/>The project will develop an analytical and algorithmic framework that achieves the following three aims: <br\/><br\/>1. Inferring Graph Structure: What graph best explains observed cascades? From noisy samples of multiple cascade progressions, the project will formulate graph-learning as non-parametric statistical inference, and propose an algorithmic approach that leverages recent break-throughs in regularized convex optimization and iterative (forward-backward) methods. Conversely, the project will develop lower-bounds on sample complexity using statistical minimax theory. Applications abound - for instance, learning the true Twitter interest graph from observation of cascades over the follower network. <br\/> 2. Detecting and Identifying the Causative Network: Is it possible to detect if a cascade is progressing; if so which network is it evolving on? Interactions occur over multiple possible networks in many different domains (mobile forensics, epidemiology, online social networks), pointing to the broad applicability of this thrust. <br\/>3. Learning Node Opinions: Users often need to be active participate for cascades to progress (e.g., retweet on social media). By correlating user decisions with user actions, is it possible to learn individual user opinions? <br\/><br\/>For validation the project will test the algorithms both on both synthetic data and real data. Public data sets to be leveraged include Texas hospital records along with online blog and search records (Spinn3r, Twitter, Google Flu Trends, infochimps).<br\/><br\/>Cascade processes are widely prevalent in modern networks. The project's algorithms and understanding of inverse problems will further the state of the art in diverse fields including biological and human disease networks, societal networks of self-interested agents, and mobile and malware networks. In addition, this project will continue and broaden the PI's emphasis on recruiting and mentoring students from under-represented communities. The industrial affiliates program of the Wireless Networking and Communications Group at The University of Texas at Austin will facilitate technology transfer to industry.","title":"NeTS: Small: Inverse Problems from Cascades: Structure, Causation and Opinions","awardID":"1320175","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["560222",550745],"PO":["564993"]},"202189":{"abstract":"This project aims at addressing the fundamental problem of how to maximize the efficiency of access to the wireless medium. In recent years, new information-theoretic solutions have emerged, some of which can now be implemented due to technological advances. Also, long engrained notions, such as half duplex, are being challenged. It has been shown that through a combination of strategies at the physical layer and signal processing, full duplex systems are indeed feasible. These developments have radically altered the very notion of what a spatio-temporal resource is. These developments have necessitated a thorough redefinition and appraisal of the problem of efficient access as addressed in this project.<br\/><br\/>Over the years, developments have taken place in parallel and somewhat in isolation in the physical layer community and in the protocol community. By bringing together a broad-spectrum of expertise from both areas, this project envisages fundamental advances in wireless networking at the access layer. The core challenge is how to use the scarce resources of 'space' and 'time' as efficiently as possible. In addition, distributed operation, channel fading, time-varying channel conditions, and fast time-scales of transmission opportunities and decisions, exacerbate the problem. This project has the potential of making a transformative advancement in the science of medium access.","title":"NeTS: Medium: Collaborative Research: Leveraging Physical Layer Advances for the Next Generation Distributed Wireless Channel Access Protocols","awardID":"1302182","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[541861],"PO":["557315"]},"207887":{"abstract":"Tasks which must complete by specific deadlines (known as real-time tasks) appear in many systems where computers interact with humans or the physical environment such as autonomous vehicles, traffic management, robotics, industrial process management, video surveillance, radar tracking, and hybrid structural testing. With a growing number of application domains where this kind of interaction occurs, there is an increasing need for systems that can run complex tasks within stringent timing constraints. In a separate, but related trend, processor clock speeds have largely stagnated, and most modern computers are parallel computers with multiple cores or processors on each platform. Both to keep up with the demands of emerging embedded systems, and to exploit the capacity of multicore computers effectively, real-time applications must harness parallelism more effectively than has been possible to date. \u00a0This research will enable these important applications by conducting both theoretical and empirical research on how to implement and execute parallel real-time tasks efficiently.<br\/><br\/>This research intends to develop provably good algorithms for parallel real-time tasks. \u00a0These algorithms must provide guarantees of both correctness and performance. The research focuses on three specific directions: (1) Scheduling foundations: Design and analysis of efficient scheduling algorithms for parallel real time tasks that take the complex characteristics of modern parallel platforms into consideration. \u00a0(2) Synchronization mechanisms: Design of effective synchronization techniques in order to allow coordination and resource sharing between different tasks as well as different threads of the same parallel task. \u00a0(3) Concurrency platform: Implementation of a modular and extensible concurrency platform for real-time parallel tasks that will be used to develop, test and validate the scheduling and synchronization mechanisms required to run these tasks. \u00a0This platform will be made available under a maximally permissible open source license to practitioners who wish to parallelize their real-time applications or to extend the platform itself to validate their own scheduling solutions. \u00a0","title":"XPS: FP: Real-Time Scheduling of Parallel Tasks","awardID":"1337218","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[556687,"560533",556689],"PO":["565251"]},"205467":{"abstract":"In this Cyberlearning: Transforming Education EXP project, reserachers are addressing how and for what purposes Augmented Reality (AR) technologies can be used to support learning of critical inquiry strategies and processes; the question is being explored in the context of history education and the SCIM-C (Summarizing, Contextualizing, Inferring, Monitoring, and Corroborating) framework developed for inquiry history education. A combined hardware\/software platform is being designed and developed to support SCIM-C pedagogy. Students use a mobile device with augmented reality to augment their \"field\" experience at a local historical site. In addition to experiencing the site as it exists, AR technology is being built to allow them to view and experience the site from several social perspectives and to view its structure and uses over several time periods. Students also use the mobile device to collect, annotate, and organize data at the local site, and the apps organize that data according to where it is collected at the site. Additional desktop and laptop software supports exploration, manipulation, and analysis of the data and writing and discussion in support of telling the site's story. Research focuses on the design of AR applications and associated technologies to support inquiry based in field work for the variety of disciplines where analysis of change over time is important (e.g., historical-cultural studies, geosciences, eco-sciences) and on understanding how to use the perspectives augmented reality can provide to promote inquiry processes and to promote understanding of how very small changes over long periods of time may add up to very large changes.<br\/><br\/>Learning how to think critically, analyze sources, and develop an evidence-based account is central to a variety of disciplines. However, learners struggle to grasp the importance of such inquiry processes, to understand how experts engage in inquiry, and to master the strategies needed to practice such inquiry themselves. The purpose of this project is to explore how augmented reality (AR) technology can be used to give learners concrete experiences of life during several historical periods and support their reasoning as they make sense of the story of the place and what influenced its history. The researchers posit that the same types of concrete experiences with the past and the same types of help with making sense of changes over time are necessary to understand many sciences as well, and they expect that what is learned about making the historical past concrete enough for high schoolers to grasp will also be applicable to making the history of the natural world concrete enough for high schoolers to support their understanding of such concepts as geologic time, geologic processes, and evolutionary processes.","title":"EXP: Exploring the potential of mobile augmented reality for scaffolding historical inquiry learning","awardID":"1318977","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}}],"PIcoPI":["210005",550189,550190,550191],"PO":["562669"]},"208624":{"abstract":"This CC-NIE Integration project performed at the Louisiana State University is developing a cyber-infrastructure integrating six different large scale scientific research groups with high performance computing (HPC) clusters at LSU using software defined network technology and Hadoop and MPI (Message Passing Interface) based parallel frameworks.<br\/><br\/>The project consists of three objectives: (i) Building 10Gbps software-defined network (SDN) with OpenFlow switches and controllers to provide multiple virtual network slices to each group; (ii) Transferring Big Data with automatically tuned operation through multiple optical paths over the SDN network achieving at most 20Gbps of aggregated disk-to-disk transfer rate; and (iii) Analyzing Big Data by developing data-intensive distributed computing frameworks with Hadoop and MPI technologies parallelizing large number of jobs over HPC clusters.<br\/><br\/>Those three components are integrated with a web portal service and a GENI-enabled network management system. To achieve high disk-to-disk transfer rate at 20Gbps, the project has an industrial partnership with Samsung Electronics that contributes 70TB Solid State Drive (SSD) storage and optimizes the I\/O bandwidth.<br\/><br\/>The cyber-infrastructure accelerates the current Big Data research projects spanning a wide range of research areas including gene sequencing research at Biology and Vet School, computational chemistry, big data mining at Computer Science, coastal hazard simulation research at civil and environmental engineering. In the end, the project will establish a methodology to build integrated cyber-infrastructures consisting of high speed networks, high performance computing, and high speed storage for the Big Data Science and Engineering.","title":"CC-NIE Integration: Bridging, Transferring and Analyzing Big Data over 10Gbps Campus-Wide Software Defined Networks","awardID":"1341008","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[559222,559223,559224,559225,559226],"PO":["564246"]},"205247":{"abstract":"Robots are potential tools for life saving in underground rescue operations like mine disasters. Human rescuers are thwarted by roof falls, explosion dangers, quality of air, visibility through smoke and dust, mental stress and physical endurance. The inability of human rescuers to cover sufficient distance in a short time often has fatal consequences for accident victims. Robots which autonomously scout ahead of human rescuers and summarize environmental information can greatly enhance situational awareness, enabling teams to push forward without delay. This proposal envisions immersive, robotically-created 3D models, fused from many sensor sources and created through smoke, dust, mud, flood and fire. These expansive models provide rescuers with visual clarity that are uncorrupted by environmental condition. Illumination artifacts, sensor noise and errors are removed by fusion and intelligent view planning of radical new modalities including LIDAR, RADAR, multispectral imaging and actively-illuminated RGB sensing. <br\/><br\/>While the world has often been captivated by high-visibility mine accidents, robots are tangible tools with visible results that have the opportunity to become the centerpiece of any rescue effort. The disrupting effect of the first trapped miner found and human life saved by a rescue robot would inspire countless people to appreciate the evolving role of science and technology in our lives. Prior underground robotics work by this team have generated considerable press and initiated acceptance of robotic technology in even the most change-adverse industries. The investigators will continue to push for robots through media appearances, lab and mine tours for school children and live demonstrations at science museums. This project will support education at all levels, which includes supporting postdoctoral and graduate research in robotics at CMU, employing undergraduate REU interns, and developing curriculum for the Mobile Robot Design course at CMU. Contributions from this research will impact automation in industries ranging from underground production to civic inspection, and defense.","title":"NRI: Small: Robotic Scouts: Augmenting Human Perception for Underground Rescue","awardID":"1317749","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[549607,549608],"PO":["564069"]},"205269":{"abstract":"With advances in camera technologies, and as cloud storage, network bandwidth and protocols become available, visual media are becoming ubiquitous. Video recording became de facto universal means of instruction for a wide range of applications such as physical exercise, technology, assembly, or cooking. This project addresses the scientific and technological challenges of video shooting in terms of coverage and optimal views planning while leaving high level aspects including creativity to the video editing and post-production stages. <br\/><br\/>Camera placement and novel view selection challenges are modeled as optimization problems that minimize the uncertainty in the location of actors and objects, maximize coverage and effective appearance resolution, and optimize object detection for the sake of semantic annotation of the scene. New probabilistic models capture long range correlations when the trajectories of actors are only partially observable. Quality of potential novel views is modeled in terms of resolution that is optimized by maximizing the coverage of a 3D orientation histogram while an active view selection process for object detection minimizes a dynamic programming objective function capturing the loss due to classification error as well as the resources spent for each view.<br\/><br\/>The project advances active sensing and perception and provides the technology for further automation on video capturing. Such technology has broader impact on the production of education videos for online courses as well as in telepresence applications. Research results are integrated into robotics and digital media programs addressing K-12 students.","title":"NRI: Small: Collaborative Research: Active Sensing for Robotic Cameramen","awardID":"1317947","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["382346",549664],"PO":["564316"]},"209658":{"abstract":"The Principal Investigator (PI) formulates the information procuration problem of converting unstructured data into structured information as one of using limited resources (such as processing time and collection costs) among several available strategies for information acquisition, extraction, collation and aggregation in a sequential and adaptive manner. The proposal aims to build a Markov decision process (MDP) for which both the states and the rewards will be learned, and from which an optimal adaptive strategy for effective information procuration will be extracted.<br\/><br\/>Recent methods for designing adaptive strategies for multi-armed bandit problems and budgeted learning approaches by the PI will be extended for this purpose, as well as techniques from inverse reinforcement learning. Moreover, given the intended size of the application data sets, the focus will be on on scalable algorithms for these problems. Due to the centrality of the problem, new approaches to making better sense of unstructured data will have much impact both in terms of developing new methods and in practice. The proposed synthesis of methods from Operations Research, Approximation Algorithms and Machine Learning is novel in this context. This proposal will increase the cross-fertilization of ideas between Operations Research and Machine Learning, via a collaboration team formed at this intersection.","title":"Information Procuration via Adaptive Algorithms","awardID":"1347308","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[561985],"PO":["565251"]},"209669":{"abstract":"Scripting languages such as Python, Javascript, and Ruby have carved out an important niche: rapid development of solutions to small and medium sized problems. Unfortunately, scripting languages today also have significant shortcomings: type errors arise only at runtime, making it harder to isolate bugs; there are no declared type interfaces, forcing libraries to use informally-described interfaces; and runtime performance is poor compared to statically-typed languages. The main cause of these problems is the lack of a static type system.<br\/><br\/>This project is preliminary work on a new scripting language that addresses this problem by developing a statically-typed scripting language from scratch, aiming to retain the flexibility of existing scripting languages while also solving the fundamental problems of difficult debugging and poor runtime performance. The PI's approach is based on subtype constraint type inference algorithms, which are very flexible and do not require type declarations.","title":"EAGER: Reconceiving Scripting Language Design","awardID":"1347405","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[562016],"PO":["564588"]},"207018":{"abstract":"This project is funded as part of the United States-Israel Collaboration in Computer Science (USICCS) program. Through this program, NSF and the United States - Israel Binational Science Foundation (BSF) jointly support collaborations among US-based researchers and Israel-based researchers. The availability of fast and cheap computers coupled with massive storage devices has enabled the collection and mining of data on a scale previously unimaginable. This opens the door to potential abuse regarding individuals' information. This work explores the fundamental tradeoffs between privacy for individuals' data and the usefulness of the information one can obtain from these large datasets.<br\/><br\/>Differential privacy is a well-established paradigm aimed at mitigating the drawbacks of traditional anonymization techniques, as it provides a rigorous guarantee for the added risk to an individual in participating in a database. This research addresses fundamental questions clarifying the boundaries of what is possible under differential privacy and its relaxations, by exploring the fundamental conflicts between privacy and utility and the additional tensions introduced by computational efficiency. This work expands the potential impact of differentially private algorithms on real-world applications, and also ensures broad impact via curriculum development, pedagogical development, and outreach activities.","title":"BSF:2012348:The Boundaries of Privacy","awardID":"1331343","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[554263],"PO":["565264"]},"199065":{"abstract":"Social networking and sensor-rich devices such as smartphones are becoming increasingly pervasive in today's society. People can share information concerning their location, activity, fitness, and health with their friends and family while benefiting from applications that leverage such information. Yet, users already find managing their privacy to be challenging, and the complexity involved in doing so is bound to increase. Usable techniques are required to enable people to effectively manage the dissemination of their private information as sensed by their mobile devices and sensors in their environment. <br\/><br\/>This project explores `sensible' privacy controls and feedback mechanisms that allow people (or automated mechanisms acting on their behalf) to respond to unanticipated patterns and actual uses of their information in a way that is usable and intuitive. This approach has two advantages: 1) people need only care about the subset of data and usage scenarios that have the potential to violate their privacy, thus reducing the amount of data to which they must regulate access; and 2) people make better decisions concerning such access when these decisions are made in a context where they know how their data is being used. <br\/><br\/>Sensible privacy mechanisms can have a profound and positive societal impact by not only helping people control their privacy, but also potentially increasing their participation in sensor-enabled computing because of this added control. This project firmly integrates education into the research through research experiences for underrepresented groups and the development of course modules on privacy at the undergraduate and graduate levels.","title":"CAREER: Sensible Privacy: Pragmatic Privacy Controls in an Era of Sensor-Enabled Computing","awardID":"1252697","effectiveDate":"2013-09-01","expirationDate":"2018-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[534031],"PO":["562974"]},"209339":{"abstract":"Semantic technologies, are beginning to play increasingly important roles across a broad range of applications. There is an urgent need for advanced training graduate students to conduct research in this area and prepare for academic or industrial careers.<br\/><br\/>Participation in premier research conferences in the area is an essential element of such training. This project provides funds to subsidize the travel expenses of 10-15 students at U.S. universities to attend the 2013 International Semantic Web Conference (ISWC) which will be held October 21-25, 2013 in Sydney, Australia. <br\/><br\/>ISWC is a premier international conference which offers a venue for presentation of rigorously peer-reviewed research results in Semantic Web and allied areas. The conference includes two events specially targeted to graduate students: The ISWC doctoral consortium offers an opportunity for doctoral students to present their work and receive feedback and mentoring. The ISWC Career Mentoring lunch provides an informal setting for students to discuss all issues pertaining to research careers with senior researchers in the community, and to establish long-term mentoring ties. <br\/><br\/>Broader impacts of the project include: Enhanced opportunities for training and mentoring of US-based graduate students in Semantic Web and related areas, broadening the particiation of students from groups (women and minorities) that are currently under-represented in Computer Science in general, and Semantic Web in particular.","title":"III: Travel Fellowships for Students from U.S. Universities to Attend ISWC 2013","awardID":"1345449","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["563585",561219],"PO":["565136"]},"210362":{"abstract":"Genome data enable scientists to pose a host of compelling questions spanning diverse disciplines. However, relational databases are inefficient at modeling the complex relationships between genes and the proteins they encode. The PI will enable biologists to answer these questions efficiently and automatically by developing a computational infrastructure that models the inherent structure of biological data, by creating a graphical database of genome and proteome data for the human genome and related eukaryotic genomes to model relationships (evolutionary, interaction, regulatory) that cannot be represented effectively in relational databases. Nodes will represent different biological entities - genes, proteins, species - and edges between nodes will represent different relationships between these entities. For example, edges between genes and proteins can represent \"Gene G encodes protein P\" or \"Gene G is regulated by protein P\". Edges between proteins can represent physical interaction or homology. Different types of features for these entities at each node will be stored and the team will use the network structure and statistical modeling methods to enable precise predictions of various aspects of \"function\" -- molecular function, metabolic pathway, biological process, cellular localization, inter-molecular interactions, protein 3D structure, etc. Functional annotation will be automated, with results produced in both machine-readable and human-readable formats. Intuitive web-based interfaces will be provided for navigation and interpretation of data by experimental biologists. Provenance of predicted functions will be provided, allowing biologists to drill down to examine the underlying support and evidence. All core software tools will be provided in open source, and data will be downloadable. This project will contribute curriculum materials suitable for inclusion in undergraduate and graduate courses in bioinformatics, genomics, phylogenomics and evolutionary biology and provide a resource for researchers in vertebrate genomes.This project will contribute curriculum materials suitable for inclusion in undergraduate and graduate courses in bioinformatics, genomics, phylogenomics and evolutionary biology,<br\/>and provide a resource for researchers in vertebrate genomes.","title":"EAGER: Towards a self-organizing map and hyper-dimensional information network for the human genome","awardID":"1355632","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[563723],"PO":["565136"]},"202772":{"abstract":"There are both harmful and benign bacteria and viruses all around us. Scientists use a technique called metagenomics to characterize these organisms by extracting DNA from a sample, and deciphering the genetic code of all of the DNA sequences. The scientific approach typified by metagenomics is to generate data and rely on large, fast, computers to solve complex problems. The Computational Enhancement of Analytical Metagenomics Systems award supports this work with computational infrastructure and training.<br\/><br\/>Intellectual Merit: The team working on this project are world-leaders in the development of tools and analyses for metagenomics data. The computational infrastructure enabled by this award continues their leadership in this area for the coming decade. The computing infrastructure is also used by researchers at both San Diego State and other Universities in California, the United States, and around the world. The infrastructure enables the researchers to tackle really difficult problems that they couldn't approach before. An ~500 core computer cluster including machines with lots of memory and disk storage space allows the scientists to run their massive computations required to analyze this data to protect our nation, develop new therapeutics and understand our world.<br\/><br\/>Broader Impacts: This proposal directly supports an NSF Transforming Undergraduate Education in Science program that teaches genomics and metagenomics to undergraduate students at San Diego State University, an Hispanic-Serving Institution. Over the last six years SDSU has improved the graduation rate of all students by providing access to cutting edge research equipment like that proposed here.","title":"II-EN: Computational Enhancement of Analytical Metagenomics Systems","awardID":"1305112","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["553952","561674",543427,543428],"PO":["562974"]},"202783":{"abstract":"Computing power consumption is the most difficult and pervasive challenge towards improving the performance of computing systems. This project seeks the acquisition of a new class of cluster computing systems based on low-power System-on-a-Chip (SoC) processor nodes. Compared to traditional servers, this new class of servers promises large improvements in throughput per Watt. Using the acquired infrastructure, the project explores the ramifications of this new class of servers on the performance, energy consumption and thermal characteristics of application workloads. The results from different types of parallel workloads are analyzed to understand the interactions between application characteristics and system architecture and the impact of these interactions on performance and power consumption. The project also investigates the impact of this new class of servers on the effectiveness of power and thermal management techniques. <br\/><br\/>The proposed infrastructure enables the pursuit of new research directions in energy-efficiency of computing systems. The results of this project will be disseminated in premier research venues, enabling the computing community to better understand the impact of emerging low-power SoC-based systems on the energy efficiency of computing clusters. The broader impacts of this project include outreach to local high school students in Providence, RI, and research experiences for undergraduate students at Brown University.","title":"II-NEW: A Platform to Advance Research in Energy-Efficient Computing","awardID":"1305148","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[543459],"PO":["565272"]},"210285":{"abstract":"The goal of this project is to vastly improve the way musculosketal modeling is performed and utilized by creating a data driven model that builds on the product of exponentials formulation for joints. By using parameter fitting, a kinematic chain of joints can be fitted the individual rather than scaling a generic template which does not take into account the large diversity in body shapes and sizes. The team will look at a structured, yet data driven approach to modeling a person, making it possible to compare joint ranges and muscular limitations both contralaterally, as well as against their peers. It is also possible to compare a patient and against their past history allowing for better understanding and diagnosis within a specified patient groups (scoliosis, the elderly, hip replacement recovery) as well as with the general population. The PI proposes a hybrid optimal control approach for determining muscular activation based on segmenting different dynamical modes. These modes can take into account changes in mass or geometric constraints such as assistive devices (e.g. crutches walkers or exoskeletons). They propose to apply these methods of musculoskeletal modeling to the upper limbs in the elderly group who experiences muscle weakness, joint damage and may have artificial prostheses.<br\/><br\/>The tools developed under this proposal can be used to analyze any number of biological creatures by modeling their joints in a similar manner. It expands to a wider robotic community where robotic kinematic chains can be directly compared to biological chains. This can be used for teleoperation of robotic devices where particular joints can be mapped between each other. It also adds to the tools that can be used to design assistive exoskeletons and prosthetic devices, as it allows biological and mechanical joints to be modeled in together- potentially improving the methods of controlling these devices. The project team consists of a PI from Computer Science and two consultants, one from Mechanical Engineering (UC Berkeley) and a MD from UCDMC who have extensive experience in workspace assessment techniques, human modeling, human-machine interaction, and control. Research findings will also be outreached to K-12 students and their parents though various official events at the University. The Center for Information Technology Research in the Interest of Society (CITRIS) at UC Berkeley provides a unique environment and opportunity for the investigators to interact and share research findings with other researchers, students, and broader public.","title":"EAGER: Individualized Musculoskeletal Modeling for Diagnosis, Rehabilitation and Real-time Feedback","awardID":"1354321","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[563532],"PO":["564069"]},"210054":{"abstract":"This award will support attendance by students and postdocs affiliated with US institutions at the upcoming thematic trimester on Semantics of proofs and certified mathematics at the Institut Henri Poincare in Paris. The thematic trimester will run from April 22, 2014 until July 11, 2014. The purpose of this thematic trimester is to provide a forum for the extended community of researchers and students in computer science and mathematics interested in proof assistants, and more generally, in the mathematics of formal proofs. Supporting student travel to attend professional conferences and workshops is a very important mission of the NSF. Broader impacts include training the next generation of researchers in this important research area.","title":"Semantics of Proofs and Certified Mathematics - Participant Support","awardID":"1351344","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1268","name":"FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[563027],"PO":["565264"]},"202200":{"abstract":"There is a growing need for effective approaches to mining very large, i.e., petabyte scale data sets in many areas of science, engineering, and business.<br\/><br\/>The project aims to design, analyze, and implement a number of fundamental matrix-mining and graph-mining operations that are scalable to petabyte-sized inputs. Such efforts guarantee the continuation of the phenomenal growth in analyzing, visualizing, and extracting information from massive matrices and graphs. Project leverages Rensselaer's unique computing platform in the form of a massively parallel machine (a Blue Gene\/Q) with access to approximately 1.2 petabytes of storage, as well as a data-staging layer, named the RAM Storage Accelerator (RSA) with 512 computational nodes and a a total of 8TBs of fast RAM. The platform is configurable to allow the computational nodes at the RSA level to be used to pre-process data from the secondary storage in a cloud-like fashion. The project aims design and analyze approximation algorithms for matrix and graph mining tasks that follow an iterative, two-step approach: given petabytescale data, first, using computationally inexpensive approaches to obtain compact data sketches using the RSA layer as a \"cloud\" in order to reduce their size from the petabyte scale to the terabyte scale. The resulting data sketches are processed using computationally demanding approaches on the Blue Gene\/Q. This process is iterated using the approximate solutions in order to improve the quality of the sketches and the approximation guarantees. <br\/><br\/>The research team expects to release software and libraries for matrix and graph mining algorithms that implement our two-phase approaches for PB-scale matrices and graphs. The resulting tools will be applied to the analysis of petabytes of data from computer simulations of the dynamics of biomolecular systems. The investigators plan to involve students and researchers from other institutions in the design, analysis, and development of the proposed methods through an internship program. The project also offers increased opportunities for research-based training in Data Analytics and High Performance Computing to graduate and undergraduate students at RPI. The results of the research will be made available to the academic community through the project web site.","title":"III: Medium: Mining petabytes of data using cloud computing and a massively parallel cyberinstrument","awardID":"1302231","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["550329",541892,541893,541894,541895],"PO":["565136"]},"205720":{"abstract":"The rapid advance of program analysis greatly benefits many applications, including security vulnerability detection, software fault localization, performance optimization, to name a few. However, handling library functions and system calls (also referred to as environmental functions) presents a pervasive and critical challenge in program analysis. Even though these environmental functions are not written by developers, they are an intrinsic part of program semantics and consequently it would be ideal for a program analysis to co-analyze the program and its execution environment. Despite its importance, achieving program-environment co-analysis in practice is challenging. First, the difficulty to acquire the source code of some environmental functions precludes source code based analysis. Moreover, even if source code is available, the code base is often prohibitively large and complex, making analysis difficult.<br\/><br\/>In this project, the goal is to develop a highly automated technique that can construct models for environmental functions from their binary implementations and a set of initial inputs. The models are essentially programs that provide the same functionality of the functions being modeled, yet substantially simplified. Such programs can be included as part of the application, enabling program-environment co-analysis. The proposed technique will lead to a highly automated solution that will largely offload the onus of manually crafting models from program analysis developers' shoulders. Moreover, it will make program environment co-analysis feasible and more precise, enabling detection of security vulnerability and software defects that are otherwise undetectable. Additionally, the PIs expect the proposed research to foster learnings in both program analysis and operating systems, as well as providing many opportunities to incorporate findings to relevant courses in computer science.","title":"SHF: Small: Collaborative Research: Towards Automated Model Synthesis of Library and System Functions for Program-Environment Co-Analysis","awardID":"1320326","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["550848"],"PO":["565264"]},"202211":{"abstract":"Workload characterization is central to development of new computer architectures. The rise of the mobile-cloud paradigm has increased the diversity and rate at which applications are created thus challenging computer architects' ability to build optimized systems for them. In the past, architects have been able to examine software codes of interest (often through slow laborious manual inspection if necessary) when releases were far and few in between to derive intuition necessary to make architectural and microarchitectural discoveries. But this method does not scale to emerging applications that are literally hammered out in the hundreds by the day. Further, new languages and platforms have behaviors that are quite different from legacy codes and there is an urgent need for intuition on these applications. Without new methods to characterize emerging workloads, computer architects risk running into an intuition wall. This risk might prove calamitous if unmitigated, given the added reliance on (micro)architects to develop more energy efficient designs to compensate for the losses due to slowdowns in Dennard's scaling.<br\/><br\/>Advances in machine learning provide an opportunity to overcome the intuition wall. In the last decade there have been many major advances in machine learning on graphs motivated by need\/benefits of mining behaviors in social networks and enabled by cheap commodity computing. In this project, the PIs plan to leverage these advances to discover and program new computer architectures. By viewing program execution as a graph, clustering these graphs, and mining them for similarities, the PIs plan to discover new behaviors that architects and microarchitects can use to develop new on-chip acceleration structures. The PIs also plan to study how legacy code can semi-automatically be converted to execute on the architectures with the new accelerators.","title":"SHF:Medium:Overcoming the Intuition Wall: Automatic Graphical Analysis of Programs to Discover and Program New Computer Architectures","awardID":"1302269","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":[541922,541923,541924],"PO":["366560"]},"210197":{"abstract":"This EArly-concept Grants for Exploratory Research analyzes the role of discourse and dialog context in the generation of believable, human-like behaviors for conversational agent (CA), i.e., a virtual agent that interacts with a user. CAs aim to engage the users by displaying human-like behaviors not only through speech by also through facial gestures. One useful modality to drive facial behaviors is speech. Spoken language carries important information beyond the verbal message that a CA engine should capitalize on. A challenge in speech-driven animation is to generate behaviors that respond to the discourse context. This proposal presents a top-down approach to explore the importance of considering contextual information in the modeling of speech-driven facial gestures. The project starts with speech-driven models, based on dynamic Bayesian networks, which do not capture the specific discourse context, responding only to the properties of the acoustic features. Then, the study considers discourse-specific models in which the intent of the gestures is known. The study defines a specific, controlled domain as testbed, recording multiple human interactions. Similar speech-driven models are trained constrained by the specific discourse function. The study evaluates the differences in the perceived naturalness, appropriateness and rapport of generated facial gestures. <br\/><br\/>The study explores which discourse aspects affect the facial animation models, and which are more domain specific or independent. By incorporating the intrinsic discourse information, the proposed models generate behaviors that respond to conversational functions, addressing one of the limitations in speech-driven facial animations. The findings have a longterm impact in variety of health care applications, such as helping hearing impaired individuals and teaching social skills to autistic children. Likewise, discourse-dependent speech-driven models can play a key role in better tutoring systems that display human-like behaviors to communicate and engage with the students.","title":"EAGER: Investigating the Role of Discourse Context in Speech-Driven Facial Animations","awardID":"1352950","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[563325,563326],"PO":["565215"]},"206820":{"abstract":"Natural language privacy policies have become a de facto standard to address expectations of notice and choice on the Web. Yet, there is ample evidence that users generally do not read these policies and that those who occasionally do struggle to understand what they read. Initiatives aimed at addressing this problem through the development of machine implementable standards or other solutions that require website operators to adhere to more stringent requirements have run into obstacles, with many website operators showing reluctance to commit to anything more than what they currently do. This project offers the prospect of overcoming the limitations of current natural language privacy policies without imposing new requirements on website operators.<br\/><br\/>This frontier project builds on recent advances in natural language processing, privacy preference modeling, crowdsourcing, formal methods, and privacy interfaces to overcome this situation. It combines fundamental research with the development of scalable technologies to semi-automatically extract key privacy policy features from natural language website privacy policies and present these features to users in an easy-to-digest format that enables them to make more informed privacy decisions as they interact with different websites. Work in this project also involves the systematic collection and analysis of website privacy policies, looking for trends and deficiencies both in the wording and content of these policies across different sectors and using this analysis to inform ongoing public policy debates. An important part of this project is to work closely with stake holders in industry to enable the transfer of these technologies to industry for large-scale deployment.","title":"TWC SBE: Option: Frontier: Collaborative: Towards Effective Web Privacy Notice and Choice: A Multi-Disciplinary Prospective","awardID":"1330214","effectiveDate":"2013-09-01","expirationDate":"2017-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[553752],"PO":["562974"]},"205610":{"abstract":"A \"rectangular layout\" L is a partition of a rectangle into a set of disjoint smaller rectangles by vertical and horizontal line segments. L is said to \"represent a graph G\" if the smaller rectangles correspond to the vertices of G one-to-one, and two vertices are adjacent in G if and only if their corresponding rectangles share a common boundary in L. An \"area assignment function\" of L is a specification of the area of each rectangle in L. L is called a \"rectangular cartogram\" if the areas of the rectangles in L equal to their specified areas. L is called \"area-universal\" if it can realize any area-assignment function.<br\/><br\/>Rectangular layouts have applications in VLSI, architectural design, computational geometry, geographic information system, and other practical fields. Extensive research works have been done in this area. But many interesting problems remain open. The PI will study several open algorithmic problems related to these structures:<br\/>(1) How to decide if a given graph G has an area-universal layout?<br\/>(2) Given an area-universal layout L and an area-assignment function a, how to compute the coordinates of the cartogram for a by using a combinatorial algorithm?<br\/>(3) Not all plane graphs have rectangular layouts. If G does not have one, how to find a representation of G by using recti-linear polygons of more complex shapes?<br\/>(4) Another useful subclass of rectangular layouts are \"slicing floorplans\". Many optimization problems are NP-complete for general floorplans, but polynomial time solvable for slicing floorplans. How to decide if a given graph G has slicing floorplans or not?<br\/><br\/>The PI aims to develop efficient algorithms for solving these problems. This research will make both theoretical and practical contributions. On the practical side, many problems discussed above are motivated by real-world applications. On the theoretical side, the algorithmic tools developed in this project include several important graph-theoretic constructions, and are related to some open questions in graph theory and combinatorics. While the four problems outlined above are interesting and important, they are by no means the only problems in this field. The techniques developed in this research are likely useful in solving related problems. Two graduate students will work on this project under PI's supervision. The topics covered in this research will be taught in an advanced algorithm course.","title":"AF: Small: Efficient Algorithms for Rectangular Layouts and Cartograms","awardID":"1319732","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":[550545],"PO":["565157"]},"205731":{"abstract":"This project will use new technologies for measuring brain activity to understand in detail how human listeners are able to separate competing, overlapping voices, and thereby to help design automatic systems capable of the same feat. Natural environments are full of overlapping sounds, and successful audio processing by both humans and machines relies on a fundamental ability to separate out sound sources of interest. This is commonly referred to as the \"cocktail party effect,\" based on the ability of people to hear what a single person is saying despite the noisy background audio from other speakers. Despite the long history of research in hearing, this exceptional human capability for sound source separation is still poorly understood, and efforts to automatically separate overlapping voices by machine are correspondingly crude: although great advances have been made in robust processing of noisy speech by machine, separation of complex natural sounds (such as overlapping voices) remains a challenge. Advances in sensor technology now enable the modeling of this function in humans, giving an unprecedented, detailed view of sound representation processing in the brain. This project works specifically with measurements of neuroelectric response made directly on the surface of the human cortex (currently with a 256-electrode sensor array) for patients awaiting neurosurgery. Using such measurements made for controlled mixtures of voices, the project will endeavor to both develop models of voice separation in the human cortex by reconstructing an approximation to the acoustic stimulus from the neural population response, and in the process learning the linear mapping between the neural response back to a spectrogram measure of the stimulus. To attempt to significantly improve the ability of machine algorithms to mimic human source separation capability, the project will also focus on a signal processing framework that supports experiments with different combinations of cues and strategies to optimize agreement with the recordings of neural activity. The engineering model is based on the Computational Auditory Scene Analysis (CASA) framework, a family of approaches that have shown competitive results for handling sound mixtures.","title":"RI: Small: Collaborative Research: Towards Modeling Source Separation from Measured Cortical Responses","awardID":"1320366","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550824],"PO":["564318"]},"202222":{"abstract":"This project considers cellular systems based in the millimeter wave (mmWave) bands, between 30 and 300 GHz, where the available bandwidths are much wider than today?s cellular networks [4-6]. Indeed, available spectrum at these frequencies can be easily 200 times greater than all cellular allocations today under 3 GHz. Moreover,the very small wavelengths of mmWave signals combined with advances in low-power CMOS RF<br\/>circuits enable large numbers of miniaturized antennas to be placed in small dimensions. These multiple antenna systems can be used to form very high gain, electrically steerable arrays, fabricated at the base station, in the skin of a cellphone, or even within a chip. Due to the limited range of mmWave signals, the proposal envisions cellular systems based on large numbers of mmWave ?picocells? (100 to 200m radius), each using highly directional antennas for improved range and spatial separation. Combining dramatically increased bandwidths with spatial multiplexing gains from the high-dimensional multiple antenna transmissions such mmWave picocellular systems offer the possibility of of 1000 times more capacity than current commercial networks.<br\/><br\/>This project will provide a basis for bringing mmWave technologies to a multiuser, multi-cellular setting and enable mmWave systems for wide-area networks with mobility. With partnerships from leading device vendors in this space, the PIs aim to drive the innovation forward in terms of device and protocol development, and train a new generation of students at the graduate and undergraduate level in this emerging area of wireless communications. The enhanced throughput gains that could approach 1000x current wireless cellular network throughputs would revolutionize the wireless broadband industry and lead to improved delivery of network services to users.","title":"NeTS: Medium: Massive Mobile Broadband Communications with Millimeter Wave Picocellular Networks","awardID":"1302336","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["559524",541951,"550857"],"PO":["565303"]},"205621":{"abstract":"In this research, a new vertically integrated cross-layer timing variation resilience methodology at the algorithm, microarchitecture and circuit levels, with \"hardware-assistance\" from the latter two levels is proposed. This addresses the effects of process variations and random delay defects in modern deeply scaled technologies as well as the effects of electrical bugs. At the highest level of the layer stack, the project considers algorithmic level workload adaptation as well as adaptation to intermittent errors in the underlying hardware due to low-power\/high-speed pipeline and arithmetic unit operation. A key contribution of this research is a novel way to accurately determine when the logic and arithmetic units of pipeline stages have finished computation. This uses concepts from wave pipelined operation of logic circuits and allows activity completion detection within nearly a single or a few gate delays. Such completion sensing allows pipeline stages to \"borrow\" or \"lend\" time much more effectively than currently used synchronously clocked pipelines. In addition, backup error detection mechanisms allow the processor pipeline to operate reliably even when there is some kind of malfunction in the completion sensing circuitry. A vertically integrated control algorithm is used to modulate power vs. performance vs. timing error resilience at the circuit, microarchitecture and algorithm (video compression) levels to deliver the desired quality of service at the required video throughput with minimum power consumption. <br\/><br\/>Through this research effort, the development of courses at GaTech in embedded DSP design and test, and yield management research under extreme process variations will be greatly facilitated. The PIs will develop a set of teaching materials on power management and error resilience in real-time digital signal processing systems. The PIs will make maximum effort to involve undergraduate students from the Summer Undergraduate Research Experience for minorities (SURE) program in the proposed research. It will also be possible to involve senior undergraduate project students in this research through targeted advisement. They will participate in H.O.T. Days@ Georgia Tech, a one-week long summer program designed to introduce high school students to electrical and computer engineering concepts. The key involvement will be in working with robots (LEGO Mindstorm, simple functions). Both Georgia Tech and Auburn University aggressively encourage participation of undergraduate students, as well as women and minorities in research. Auburn's participation in the project will also improve the research capabilities of Alabama, an EPSCOR state. Additionally, several master's students from the Historically Black Tuskegee University located near Auburn will take graduate courses at Auburn University. The best prepared among these students will be encouraged to join the project and Ph.D. programs at the participating universities. Thus, funding for this project will support the goals of recruiting more U.S. citizens, women and minorities to graduate programs.","title":"Ccf:small:timing Variation Resilient Signal Processing: Hardware-assisted Cross-layer Adaptation","awardID":"1319783","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":["550643"],"PO":["565272"]},"205863":{"abstract":"This project aims to advance our fundamental understanding of secrecy over arbitrary wireless networks. Over the last decade we have significantly deepened our fundamental understanding on how to send information over wireless networks, while our understanding on how to securely send this information has not reached the same depth as yet. This project aims to develop a unifying theory that enhances wireless secrecy by exploiting wireless properties such as: the existence of feedback (today part of all wireless standards); the possibility of selecting and using multiple network communication paths; the smart use of wireless jamming and the wireless channel variability and unpredictability. We enable this by \"building on erasures:\" through appropriate coding and smart wireless jamming, we convert (Gaussian) wireless networks to erasure networks. We then develop protocols that use interaction and feedback to enable provable security against active and passive adversaries, even if they are computationally unbounded. <br\/><br\/>The project also promotes the training of research engineers: we will integrate the research into the curriculum via the creation of novel coursework combining the underlying concepts in wireless communication, network coding\/protocols, and information theory. The research in this project, if successful, will contribute to the fundamental sciences of information theoretic network security and secure network coding. Given our increasing dependence on wireless devices as a portal for socio-economic activities, wireless security will have broad implications in mobile commerce.","title":"CIF: Small: Wireless Network Security: Building on Erasures","awardID":"1321120","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[551137,551138],"PO":["564924"]},"204774":{"abstract":"The use of cloud computing has revolutionized the way in which cyber infrastructure is used and managed. The on-demand access to seemingly infinite resources provided by this paradigm has enabled technical innovation and indeed innovative business models and practices. This rosy picture is threatened, however, by increasing nefarious interest in cloud platforms. Specifically, the shared tenant, shared resource nature of cloud platforms, as well as the natural accrual of valuable information in cloud platforms, provide both the incentive and the possible means of exploitation. <br\/><br\/>To address these concerns we are developing a self-defending, self-evolving, and self-accounting trustworthy cloud platform, the TCloud. Our approach in realizing TCloud holds to the following five tenets. First, defense-in-depth through innate containment, separation and diversification at the architectural level. Second, least authority by clear separation of functionality and associated privilege within the architecture. Third, explicit orchestration of security functions based on cloud-derived and external intelligence. Fourth, moving-target-defense through deception and dynamic evolution of the platform. Fifth, verifiable accountability through light weight validation and auditable monitoring, record keeping and analysis.<br\/><br\/>Our approach to fundamentally refactor the cloud architecture to explicitly enable security related functionality lays the foundation for truly trustworthy cloud computing. Given the unrelenting push towards the use of cloud technologies our work has broad applicability across industry, healthcare, government and academia. All software we develop will be released to the community in open source form.","title":"TWC: Medium: TCloud: A Self-Defending, Self-Evolving and Self-Accounting Trustworthy Cloud Platform","awardID":"1314945","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["557575","559299","560436",548382],"PO":["565327"]},"205765":{"abstract":"Software has become a critical infrastructure in our society with its<br\/>pervasive use in all kinds of products from home appliances to<br\/>automobiles, from individual pacemakers and smart phones to air<br\/>traffic control systems and high-end computers in large-scale power<br\/>grids. During software development, the build process is vital to make<br\/>sure that a software product is correctly and reliably constructed and<br\/>configured in accordance with different users' operation environments,<br\/>operating systems, platforms, and devices. To reliably build a<br\/>software product, a build tool is used to execute build code to<br\/>generate the executable and deliverable files according to the rules<br\/>specified in the build files. As a software project evolves over time,<br\/>build code continually evolves and is defect-prone due to the high<br\/>rate of changes and complexity of the build files. The complexity of<br\/>build code leads to two important issues. First, the maintenance of<br\/>complex build code has imposed much extra effort on software<br\/>developers. Second, its complexity is a source for configuration and<br\/>build errors in software development.<br\/><br\/>This project aims to improve software reliability with a comprehensive<br\/>approach to develop a scientific foundation for build code analysis to<br\/>provide automatic supports for 1) build code maintenance and 2)<br\/>detecting, testing, and locating configuration and build errors. We<br\/>will first examine configuration and build code errors and related maintenance <br\/>tasks in real-world projects. Second, we will develop a scientific foundation<br\/>with novel concepts, techniques, and algorithms for build code static<br\/>analysis, build code smell and configuration error detection,<br\/>refactoring, and change analysis. Third, we will investigate a<br\/>foundation for build code dynamic analysis and leverage it to support<br\/>fault localization in build code given a reported build<br\/>failure. Finally, a theoretical foundation and tool supports<br\/>(e.g. semi-automatic test generation) for build code testing will be<br\/>developed. Our research will fundamentally advance the body of<br\/>knowledge in theoretical foundation for software building, build code<br\/>analysis and maintenance. Our results will lead to more reliable<br\/>software and improvement in code quality as well as in the developers'<br\/>productivity. Our validation efforts involve students and<br\/>professionals, promoting teaching and training in information<br\/>assurance and software reliability.","title":"SHF:Small: Build Code Maintenance and Detecting, Testing, Locating Configuration and Build Errors","awardID":"1320578","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["562663"],"PO":["564388"]},"202267":{"abstract":"The increasingly large amounts of Electronic Medical Record (EMR) data offer unprecedented opportunities for EMR data mining to enhance health care experiences for personalized intervention, improve different diseases risk stratifications, and facilitate understanding about disease and appropriate treatment. To solve the key and challenging problems in mining such large-scale heterogeneous EMRs, the investigators aim to develop: (i) new computational tools to automate the EMRs processing, including new techniques for filling in missing values using a new robust rank-k matrix completion method; (ii) annotation of unstructured free-text EMRs using multi-label multi-instance learning; (iii) a new sparse multi-view learning model to integrate heterogeneous EMRs to predict the readmission risk of Heart Failure (HF) patients and to support personalized intervention; (iv) novel methods for identifying the longitudinal patterns using high-order multi-task learning; (v) a nonparametric Bayesian model for predicting the event time outcomes of the HF patients readmission. <br\/><br\/>The sparse multi-view feature learning and robust multi-task longitudinal pattern finding algorithms have a broad range of applications beyond EMR data mining. Free dissemination of source implementations of the algorithms enable other researchers to further develop and apply the resulting techniques. In particular, the methods and tools are expected to impact other EMR and public health research. This project offers enhanced opportunities for research-based advanced training of students (including members of minorities and under-served populations) and integration of research results into curricula at the University of Texas at Arlington, the University of Texas Southwestern Medical Center at Dallas, and Southern Methodist University. For further information see the web site at: http:\/\/ranger.uta.edu\/~heng\/NSF-III-1302675.html","title":"III: Medium: Collaborative Research: Robust Large-Scale Electronic Medical Record Data Mining Framework to Conduct Risk Stratification for Personalized Intervention","awardID":"1302675","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["560636"],"PO":["565136"]},"206513":{"abstract":"In this Cyberlearning: Transforming Education DIP (Development and Implementation) project, the PIs are addressing challenges of knowledge community pedagogy, where students are given a high level of agency and responsibility for developing questions, exchanging and critiquing ideas with peers, and advancing collective understanding. The project addresses four main challenges in supporting such pedagogy: (1) making student ideas visible and accessible, (2) supporting discourse and knowledge building with digital posters and other summative representations, (3) scaffolding complex collaborative inquiry through technology, and (4) supporting a sense of collective epistemology. Knowledge building is well researched from a conceptual perspective; until now, however, there has not been a concerted effort to develop technological infrastructure that would support its pedagogy well. In a series of design-oriented studies, researchers are working together with veteran teachers to co-design elementary biology curriculum that takes a knowledge community approach, develop the suite of tools, and carry out research investigating ways of sustaining a knowledge community pedagogy over long periods of time.<br\/><br\/>Scholars have argued that the demands of a \"knowledge society\" require new models of collaborative and inquiry-oriented learning that engage learners in sustained investigations and promote agency and autonomy. Several sophisticated pedagogical approaches have been developed to achieve these goals, but up until now, the technological infrastructure for supporting teachers and students in such approaches has not been at the same level of sophistication as the pedagogical approaches themselves. This proposal aims to narrow that gap through design of a suite of hardware and software tools that supports representing personal and collective knowledge in both private and public forms, tangible and embodied interactions as well as verbal interactions, and the orchestration of activities. The aims are to support student work and meta-cognition, provide to teachers the supports they need to carry out the pedagogies, and to examine the interactions between teacher, students, technology, and pedagogy that lead to sustained agency and community engagement among learners.","title":"DIP: Community knowledge construction in the instrumented classroom","awardID":"1324977","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[552897,552898,552899],"PO":["562669"]},"198780":{"abstract":"Many research and commercial endeavors are experiencing dramatic transformations through the use of Big Data, wherein large data repositories are collected and analyzed to reveal trends, correlation, and information that may not be apparent in smaller samples. Current approaches assume centralizing the repository, which may be a poor fit in environments where the data generation rate exceeds the network capabilities. In this project, the PIs investigate system architectures for both real-time and historical analysis of geographically distributed data, combined with research in adaptively reducing data volumes to optimize bandwidth capabilities. This combination allows better use of the computation and storage associated with smarter end devices, including, but not limited to, distributed sensors, smart meters, and even full servers, without requiring network upgrades. Given the historical trends of the growth of computation and storage versus the capacity limits of wide-area networks, this research enables more data collection and analysis to be performed at a lower overall system cost. Further, the ability to dynamically adapt data precision and fidelity to available network bandwidth allows systems to gracefully and automatically improve performance in the presence of higher-capacity networks. The research enables the collection and analysis of data that is currently left unanalyzed because of network constraints. Such data can include finer-granularity usage data, which could indicate actionable steps to reducing household energy consumption, or it could include a greater olume of debugging and monitoring data, which could better predict system failures or provide greater insight than with current methods.","title":"BIGDATA: Small: DCM: JetStream: A Flexible Distributed System for Online and In-Place Data Analysis","awardID":"1250990","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8083","name":"Big Data Science &Engineering"}}],"PIcoPI":[533299,533300],"PO":["565136"]},"205435":{"abstract":"Rapid technology scaling has fueled an unprecedented growth in the semiconductor<br\/>industry, transforming the face of modern society. Commodity systems have<br\/>undergone a sea change from the uniprocessor era of past decades to the current<br\/>many-core era. With numerous on-chip processing cores, the communication fabric<br\/>in many-core systems becomes a critical design component. Network-on-Chip (NoC)<br\/>architectures are widely regarded as the most promising design for the<br\/>communication platform for many-core systems, primarily due to their<br\/>scalability. Many-core systems such as Intel 80-core and Tilera have already<br\/>used NoCs as the backbone of communication between their on-chip processors. To<br\/>maintain fault-free execution in these many-core systems, it is imperative to<br\/>ensure sustained lifetime in both NoCs and processing cores. This research<br\/>embarks on boosting sustainability in NoC architectures through a proactive<br\/>design paradigm.<br\/><br\/><br\/>Using a cross-layer collaborative venture, spanning from the device layer to the<br\/>architecture layer, this project establishes a transformative framework to<br\/>design sustainable NoC architectures. Existing techniques for NoCs tackle the<br\/>sustainability design challenge in a reactive way by triggering corrective<br\/>mechanisms after a component failure. The investigators explore an orthogonal<br\/>proactive strategy, recognizing the need for device level aging awareness during<br\/>the entire life span of an NoC. This project demonstrates that sustainability<br\/>can be improved in NoCs without sacrificing power-performance, by considering<br\/>the criticality of packet transmission. By playing a central role to facilitate<br\/>long term sustainability in NoCs, this research can substantially improve the<br\/>cost efficiency in building large data centers supporting our modern compute<br\/>intensive society. Major research insights will be disseminated through teaching<br\/>to develop relevant skill sets, lead to more sustainable future computer<br\/>systems, and benefit the community at large.","title":"SHF:Small:Boosting Sustainability in NoC Architectures Through a Proactive Approach","awardID":"1318826","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[550105,550106],"PO":["366560"]},"205556":{"abstract":"Model checking is a technique for verifying the correctness of computer <br\/>systems. Different implementations are in wide industrial usage today. <br\/>There are still, however, many large gaps in our understanding of the <br\/>algorithmic issues involved in model checking, and the technology is still <br\/>greatly challenged by industrial designs. For example, in hardware-design <br\/>verification, it is rarely possible to apply existing tools to complete units <br\/>with clear functionality. This project is pushing the frontier of this <br\/>technology with the goal of scaling its applicability to functional system <br\/>units by developing novel scalable algorithms for model checking. The <br\/>result will be increased reliability of computer systems. <br\/><br\/>This project will explore the mathematical approach to design <br\/>verification that uses automata theory as a unifying paradigm for design <br\/>specification and verification. The automata-theoretic approach separates the <br\/>logical and the combinatorial aspects of reasoning about systems. The translation <br\/>of specifications to automata handles the logic and shifts all the combinatorial <br\/>difficulties to questions about automata, yielding clean and asymptotically optimal <br\/>algorithms. While the fundamental theory is well understood, there are <br\/>still many challenging gaps and improved algorithms can enhance the <br\/>scalability of this approach significantly. This project investigates ways of improving <br\/>automata-theoretic algorithms so they are more suitable for model checking <br\/>at scale.","title":"SHF: Small: Pushing the Frontier of Linear-Time Model-Checking Technology","awardID":"1319459","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["565263"],"PO":["565264"]},"205798":{"abstract":"Massive graphs arise in any application where there is data about both basic entities and the relationships between these entities, e.g., web-pages and hyperlinks; neurons and synapses; papers and citations; IP addresses and network flows; people and their friendships. Graphs have become the de facto standard for representing many types of highly structured data. However, the size of these massive graphs is such that existing graph algorithms in traditional computational models are typically not applicable. <br\/><br\/>In this project, the PI investigates the utility of random linear projections that compress massive graphs into a lower-dimensional space where computation becomes more tractable. An advantage of this approach is its widespread applicability; the linearity of the projection leads to algorithms appropriate for parallel and distributed computation, online processing, and various compressed sensing models. A rich body of analytic and empirical work exists on using linear projections in the context of numerical data such as feature vectors and frequency counts. For example, such projections have been studied in the context of research on locality sensitive hashing and nearest neighbors, fingerprinting, sparse signal recovery, metric embeddings, spatial partition trees, and low-rank matrix approximation. The goal of this project is to extend this powerful technique to highly structured graph data.<br\/><br\/>The main research components are: A) Investigating the types of graph structure that can be measured linearly. This includes designing new projections and proving bounds on the dimensionality required to preserve graph properties such as distances, eigenvalues, the size of cuts and matchings, and the frequency of induced subgraphs. B) Developing new applications of the framework including fast algorithms for processing dynamic graphs, graph sampling and property testing, graph fingerprinting, and MapReduce-style distributed computing. C) Taking steps towards a theory of homomorphic compression. Linear projections are homomorphic with respect to linear operations and the main challenge in designing projections for graph compression is recasting the relevant graph operations as linear operations. The PI will develop this idea further and explore compression schemes where it is possible to compute directly on the compressed data without the need to first uncompress the data.<br\/><br\/>In conjunction with these research goals, the project includes educational and broader impact initiatives that are designed to ensure a wide dissemination of research results and to train graduate and undergraduate students.","title":"AF: Small: Massive Graph Analysis via Linear Measurements: Towards a Theory of Homomorphic Co","awardID":"1320719","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[550976],"PO":["565157"]},"205567":{"abstract":"In this research, a new vertically integrated cross-layer timing variation resilience methodology at the algorithm, microarchitecture and circuit levels, with \"hardware-assistance\" from the latter two levels is proposed. This addresses the effects of process variations and random delay defects in modern deeply scaled technologies as well as the effects of electrical bugs. At the highest level of the layer stack, the project considers algorithmic level workload adaptation as well as adaptation to intermittent errors in the underlying hardware due to low-power\/high-speed pipeline and arithmetic unit operation. A key contribution of this research is a novel way to accurately determine when the logic and arithmetic units of pipeline stages have finished computation. This uses concepts from wave pipelined operation of logic circuits and allows activity completion detection within nearly a single or a few gate delays. Such completion sensing allows pipeline stages to \"borrow\" or \"lend\" time much more effectively than currently used synchronously clocked pipelines. In addition, backup error detection mechanisms allow the processor pipeline to operate reliably even when there is some kind of malfunction in the completion sensing circuitry. A vertically integrated control algorithm is used to modulate power vs. performance vs. timing error resilience at the circuit, microarchitecture and algorithm (video compression) levels to deliver the desired quality of service at the required video throughput with minimum power consumption. <br\/><br\/>Through this research effort, the development of courses at GaTech in embedded DSP design and test, and yield management research under extreme process variations will be greatly facilitated. The PIs will develop a set of teaching materials on power management and error resilience in real-time digital signal processing systems. The PIs will make maximum effort to involve undergraduate students from the Summer Undergraduate Research Experience for minorities (SURE) program in the proposed research. It will also be possible to involve senior undergraduate project students in this research through targeted advisement. They will participate in H.O.T. Days@ Georgia Tech, a one-week long summer program designed to introduce high school students to electrical and computer engineering concepts. The key involvement will be in working with robots (LEGO Mindstorm, simple functions). Both Georgia Tech and Auburn University aggressively encourage participation of undergraduate students, as well as women and minorities in research. Auburn's participation in the project will also improve the research capabilities of Alabama, an EPSCOR state. Additionally, several master's students from the Historically Black Tuskegee University located near Auburn will take graduate courses at Auburn University. The best prepared among these students will be encouraged to join the project and Ph.D. programs at the participating universities. Thus, funding for this project will support the goals of recruiting more U.S. citizens, women and minorities to graduate programs.","title":"Collaborative Research: Timing Variation Resilient Signal Processing: Hardware-assisted Cross-layer Adaptation","awardID":"1319529","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[550443],"PO":["565272"]},"202179":{"abstract":"A framework will be developed to help scientists and engineers create brain-inspired, brain-sized networks that can carry out practical applications. Large-scale spiking neural networks, which follow the brain's architecture and activity, have been used to successfully model phenomena such as learning and memory, vision, auditory processing, neural oscillations, and many other important aspects of neural function. Additionally, spiking neural networks are particularly well suited to run on neuromorphic hardware, state of the art computers that emulate the brain?s structure and dynamics. These neuromorphic systems depend on the binary nature of spikes to lower communication bandwidth and energy consumption. Although significant progress has been made towards the specification and simulation of large-scale spiking neural networks on a variety of hardware platforms, many challenges remain before these neurobiologically inspired algorithms can be used in practical applications. While biology does provide increasingly abundant empirical data that can constrain these systems, many parameter values must be chosen manually by the designer to achieve appropriate neuronal dynamics, a task that is extremely tedious and often error-prone. To meet this challenge, an automated parametertuning framework will be developed that is capable of quickly and efficiently tuning large-scale spiking neural networks. The framework will leverage recent progress in evolutionary algorithms and optimization techniques for off-the-shelf graphics processing units (GPUs). The parameter search will be guided by the idea in neuroscience that biological networks adapt their responses to increase the amount of transmitted information, reduce redundancies, and span the stimulus space. This notion of efficient coding will guide the tuning process of the artificial spiking neural networks. Computer scientists and engineers will be able to use the resulting automated parameter-tuning framework to create brain inspired applications, such as vision and memory systems, on neuromorphic hardware. Moreover, the resulting framework will allow neuroscientists to more readily create models that better describe their empirical data and generate new quantitative hypotheses that can be tested in the laboratory.","title":"RI: Medium: Collaborative Research: BCSP: Automated Parameter Tuning of Large-Scale Spiking Neural Networks","awardID":"1302125","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7275","name":"CROSS-EF ACTIVITIES"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7713","name":"ACTIVATION"}}],"PIcoPI":[541836,541837],"PO":["564318"]},"207998":{"abstract":"Proposal #: 13-37732<br\/>PI(s): Lumetta, Steven S.<br\/> Iyer, Ravishankar; Jongeneel, Cornelis Victor; Robinson, Gene E.; Sinha, Saurabh<br\/>Institution: University of Illinois - Urbana-Champaign<br\/>Title: MRI\/Dev.: Novel Computing Instrument for Big Data <br\/>Project Proposed:<br\/>This project, developing CompGen, an instrument that adopts a hardware-software co-design approach, aims to provide a<br\/>- Vehicle for biologists and computer scientists to collaborate and develop new algorithms that are significantly faster and more accurate at a scale essential for handling the data deluge; <br\/>- Software framework and tool set for algorithm development that support diverse data analysis and visualization; <br\/>- Framework for developing accelerators and mapping to heterogeneous computational resources and hierarchical database storage. <br\/>Promising technologies include emerging die-stacked and non-volatile memory technologies as well as accelerators (GPUs, FPGAs, APUs). <br\/>The project brings together a multidisciplinary team of geneticists, bioinformatics specialists, computer and algorithms designers, and data mining experts. The research to be enabled includes a wide and eclectic variety of problems with direct impact on health and social issues. Some directions include understanding the impact of climate change on gene expression and ecosystems, bringing genetic analysis into medical clinics, identifying effective antibiotics, and exploring socio-genomics relations between stress, depression, and genetics among low-income African-American mothers. <br\/>CompGen provides an environment that enables managing and processing genomic information and developing new algorithms. The instrument brings disruptive computing architectures and algorithmic techniques to facilitate analysis of genomic data while providing high accuracy results, resilience to errors, and scalability with growing volumes of data. It enables addressing the challenges of scale and diversity in genomic data through the development of new algorithms, models, and statistical methods. The instrument development focuses on reduction of data volume, optimization of storage hierarchy, identification and implementation of computational primitives, data visualization, mathematical toolkit optimization, and performance and reliability assessment. These developments are expected to lead to new computational structures and hardware\/software architectures that can be incorporated into hierarchical databases as well as heterogeneous processors for data analysis, compression, and optimization. <br\/>Broader Impacts: <br\/>In addition to serving many areas, CompGen will serve as a tool for educating students and professionals in efficient ways to process and analyze genomic data and for handling big data in general. The instrument will serve multidisciplinary classes in which students gain hands-on research experience and introductory classes that expose students to applications and tools. Existing outreach and education programs will be utilized to expose the instrument. Plans include Open House events attracting thousands of visitors, Coursera courses, and minority outreach workshops. A mentoring tool, Mytri, will be used for networking among female students. Moreover, the CompGen design will be made available to others by fundamentally changing the methods by which big datasets are handled in genomics research. To this effect, an R&D consortium of hospitals, companies, and universities has been established to help identify needs, provide sources of data, act as early adopters, and ensure that new technologies are transferred smoothly into widespread use.","title":"MRI: Development of a Novel Computing Instrument for Big Data in Genomics","awardID":"1337732","effectiveDate":"2013-09-15","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}}],"PIcoPI":[557160,557161,557162,557163,557164],"PO":["557609"]},"205336":{"abstract":"This project addresses several key emerging security challenges that arise due to the wildly successful large-scale adoption of mobile devices with diverse network capabilities. The novel approach focuses on to understanding how various information that are legitimately and willingly provided by smartphone users due to the requested permissions of downloaded applications can be potentially abused. The second research focus is to identify improvements in the design of cellular network middlebox (e.g., firewall) policies by detailed exposure and explicitly defining the key requirements. Using insights from the two research thrusts, support to detect abuse of network resources by mobile devices is explored, leveraging both existing mechanisms and new improvements for evaluation. There are three key areas of technical contributions from this project: (1) Investigation of the construction of cellular botnets from non-traditional means, (2) Discovery of the deficiency of cellular network policies to resource abuse and targeted attacks, and (3) Misbehavior detection of mobile devices in abusing infrastructure resources to gain unfair advantages in network and energy usage.<br\/><br\/>This project will have significant and broad impact on both the networking security community and our society, as effective reliance on mobile devices for information access is rapidly growing and security assurance of cellular data network infrastructure is critical to achieving robust and highly available mobile networks and network services. Any software developed in this work will be available to the research community and the network operator community. Associated experimental tools and specific experimental configurations will be released through web venues.","title":"TWC: Small: Exposing Attack Vectors and Identifying Defense Solutions for Data Cellular Networks","awardID":"1318306","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[549845],"PO":["562302"]},"205347":{"abstract":"Sampling problems are fundamental in many areas of science and engineering, for example, statistics, artificial intelligence (machine learning and vision), and biology (phylogeny). Efficiency of sampling has impact on other important algorithms, for example, the speed of statistical tests and the reliability of estimators and classifiers. This project seeks to characterize combinatorial problems that admit efficient sampling algorithms. The research under this award addresses both sides of the characterization for a broad class of ubiquitous graphical models: 1) complexity theoretic obstacles to efficient sampling and 2) efficient sampling algorithms.<br\/><br\/>Recent results established a useful characterization for antiferromagnetic 2-spin models (for example, weighted independent sets and Ising model) in terms of the behavior of the model on regular trees. This created a connection to problems and techniques studied in the statistical physics community. This project aims to use the connection to the statistical physics and its techniques (for example, belief propagation recurrences) to generalize the characterization to models with more than two spins (multi-spin models, such as the Potts model, frequently occur in the applications). As a first step towards this generalization a relation between the efficiency of commonly used Markov chains (Glauber dynamics, Swendsen-Wang algorithm) and the behavior of the models on regular trees will be explored. Beside Markov chains the PI will also investigate other approaches to sampling problems (for example, dynamic programming and utilizing strong spatial mixing).<br\/><br\/>The product of the research will be efficient sampling algorithms and an improved understanding of obstacles to efficient sampling. The algorithmic problems, concepts, and techniques will be transferred to both undergraduate and graduate teaching (in the form of guided problems and implementation challenges). The award will support the training of two PhD students in the area of sampling and counting algorithms at the University of Rochester.","title":"AF: Small: Identifying sampling problems with efficient algorithms","awardID":"1318374","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[549871],"PO":["565157"]},"208405":{"abstract":"Modern cloud computing systems use virtual machine technologies to deliver unprecedented flexibility to users, enabling businesses and individuals to cost-effectively deploy computing and storage capacity on-demand, at scale, across multiple infrastructures distributed geographically. While the ability to deploy virtual machines for cloud computing is widely supported, researchers face increasing challenges in prototyping and deploying experimental research systems that span across multiple cloud providers. In particular, providing end-to-end network connectivity among distributed virtual machines in today's Internet environment (where nodes are often constrained by firewalls and network address translators) requires significant investment of time in development, testing and maintenance of code needed solely to provide connectivity. This project addresses these connectivity challenges in cloud computing by developing an open-source scientific software element that allows researchers and users of clouds to seamlessly create virtual networks on demand for distributed virtual machines. To this end, the project creates software-defined virtual networks that support the standard Internet Protocol (IP) and use tunneling of virtual network packets over Peer-to-Peer (P2P) links among virtual machines for scalable and resilient messaging. In addition to the core IP-over-P2P virtual networking, the software provides a framework for configuration, management and monitoring that enables easy deployment of user-defined overlays for inter-cloud research experiments. <br\/>The open-source software developed in this project enables advances in the state-of-the-art of research of cloud computing systems and applications. Complementary to research and development activities, this project delivers educational modules, tutorials, software packages, and pre-configured virtual machine images that allow non-expert users to deploy their own virtual networks over private, commercial and public clouds. Because cloud computing technologies are increasingly pervasive and of growing importance to the economy and society, the broader impacts of this project can reach Internet users at large who benefit from the ability to seamlessly interconnect cloud virtual machines across multiple providers. In particular, leveraging online social networking technologies, the virtual network software software enables individuals and small groups to easily create social virtual private networks connecting personal computers and multiple cloud resources.","title":"SI2-SSE: Peer-to-Peer Overlay Virtual Network for Cloud Computing Research","awardID":"1339737","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"8004","name":"Software Institutes"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":[558495],"PO":["565247"]},"209626":{"abstract":"Cybersecurity is paramount to protecting national interests in domains that include but extend well beyond defense and finance. However, current state-of-the-art cyber-defenses have severely limited predictive and attribution capabilities. Detecting and understanding cyber attacks is not sufficient--we can liken that to \"studying symptoms instead of a disease.\"<br\/><br\/>This research carries out a synergistic approach to integrating cyber data forensics with human-centric social network analysis under a common framework. The three major activities of this project are as follows: (a) comprehensive models of cyber-attack characteristics are developed using feature extraction techniques on diverse data sources, (b) adversarial groups are classified according to their feature similarities, and (c) group classification is enhanced using analytic techniques from social network science.<br\/><br\/>To accomplish these activities: (1) different models for constructing joint representations of computer and social networks are investigated within a multi-mode graph framework; (2) data reduction and feature extraction techniques are developed for associating large datasets with this unified graph model; (3) an existing system is leveraged to discover invisible and missing links between adversarial networks of individuals; and (4) social network models and tools, as well as case studies, are applied to infer adversarial group typology.<br\/><br\/>This research is expected to benefit computer science, cyber security, and social sciences by improving detection methods for cyber attacks. Applying time-series analysis in creative ways to multi-mode networks should contribute to the field of artificial intelligence. This joint work should apply also in fields such as health care, marketing, or forecasting technology adoption trends.","title":"EAGER: Human-Centric Predictive Analytics of Cyber-Threats: a Temporal Dynamics Approach","awardID":"1347075","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561902,561903,561904,561905],"PO":["565327"]},"209637":{"abstract":"This project examines three properties of underground cybercrime communities: 1) profitability, 2) connectivity, 3) and sustainability. It identifies qualitative and quantitative metrics for these properties as well as discusses the relative effectiveness of distinct operationalization of these metrics under different levels of data granularity. The goal is to develop metrics that provide meaning indicators even when data is limited. for example, if public posts are available but not private messages between individual cybercriminals. The analysis targets five underground forums: AntiChat, BadHacke, BlackhatWorld, Carders, and L33tCrew. Finally, the project combines linguistic techniques, e.g. topic modeling, social network analysis, and analysis to provide a repeatable, verifiable, and systematic framework that enables a scientific exploration of these forums and the impact of distinct interventions at mitigating underground forums. <br\/><br\/>This project assumes that cybercrime is made economically feasible by collaboration between criminals with specialized skills, as facilitated by underground forums. Thus, it argues to change the fundamental focus of anti-cybercrime efforts from making individual incidence of cybercrime unprofitable to making cybercrime communities unsustainable. The findings inform defender efforts, both for academic researchers as well as practitioners. The project builds a scientific framework to evaluate the effectiveness of such interventions. This framework is being used to develop tools for law enforcement as well as pedagogical material to educate students in cyber-security.","title":"EAGER: Cybercrime Science","awardID":"1347151","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[561935,561936],"PO":["565327"]},"209648":{"abstract":"The project conducts formal, basic research into the modeling of \"intention''. This under-researched and ill-understood concept underlies many applications, including online search, calendars, intelligent dialog systems, security applications, self-driving cars, military applications, and many more. This particular project is concerned with domain-independent, formal models underlying all such applications. 'informational attitudes' such as knowledge and belief which capture the information available to the agent, and 'motivational attitudes' such as goals, preference and utility which capture what the agent cares about, have been studied extensively. In contrast, and despite their importance, \"action attitudes\" which capture the agent's attitude towards different actions she might take in light of her motivations and the information available to her, have been poorly studied. This is true in particular of intention, perhaps the most basic action attitude, and the focus of this project.<br\/><br\/> The project is grounded in the Principal Investigator's prior work, in which he laid out a computational point of view, dubbed the \"database perspective''. That work, which offered an axiomatic theory of the joint revision of belief and simple ('atomic') action intention, is being extended along multiple dimensions:<br\/><br\/>-- Modeling complex intentions, using Dynamic Epistemic Logic (DEL).<br\/>-- Modeling achievement intentions.<br\/>-- Modeling teleology.<br\/>-- Developing a quantitative (\"probabilistic\") theory of intention.","title":"EAGER: Formal models of intention","awardID":"1347214","effectiveDate":"2013-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[561961],"PO":["565035"]},"210660":{"abstract":"Word meanings are central to the semantic interpretation of texts.<br\/>Although much work to date has focused on statistical approaches that often ignore the explicit understanding of the text, recent research work has begun to challenge this simplification, demonstrating that semantic interpretation is indeed essential for a number of language processing applications.<br\/><br\/>The key observation underlying this CAREER project is that word meaning distinctions differ from one lexical resource to another and that the optimality of word meaning representations should be dictated by the target application. The project is exploring rich and flexible word meaning representations that combine the benefits of multiple monolingual and cross-lingual lexical resources and that can be adapted to the context and to the target application. In particular, the multilingual nature of these representations allows for an effective exploitation of the knowledge and resources available in different languages. The project also explores the role played by these word meaning representations and the corresponding monolingual and cross-lingual knowledge sources in several natural language processing tasks including lexical substitution, word and text translation, and text-to-text semantic similarity.<br\/><br\/>Another aim of the project is to integrate natural language processing into educational applications, and explore the use of the word meaning interpretation models to build a comprehension-assistant tool for students of English as a second language (ESL) and English as a foreign language (EFL). The educational program also fosters increased awareness about research in multilingual natural language processing among college, undergraduate, and graduate students, through a college outreach program and a new course on multilingual computational linguistics, as well as increased exposure of students to international experiences through international collaborations.","title":"CAREER: Semantic Interpretation with Monolingual and Cross-lingual Evidence","awardID":"1361274","effectiveDate":"2013-09-01","expirationDate":"2014-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1311","name":"LINGUISTICS"}}],"PIcoPI":[564432],"PO":["565215"]},"209109":{"abstract":"This award will support student travel to 22nd International Conference on Parallel Architecture and Compilation Techniques (PACT 2013). The conference is being held in Edinburgh, Scotland in September 2013. PACT is a leading conference on parallel architectures, compilers, languages, algorithms, and applications. Supporting student travel to attend professional conferences and workshops is a very important mission of the NSF. Broader impacts include building the next generation of researchers in this research area, as well as providing international experiences to build a globally-aware workforce.","title":"Student Travel Support: 22nd International Conference on Parallel Architectures andCompilation Techniques (PACT 2013)","awardID":"1343823","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[560516],"PO":["564588"]},"210231":{"abstract":"Given a graph, like a social\/computer network or the blogosphere, in which an infection (or meme or virus) has been spreading for some time, how does one select the k best nodes for immunization\/quarantining immediately? This team was the first to show that the propagation (specifically, the so-called \"epidemic threshold\") depends on a single number, the first eigenvalue of the adjacency matrix of the network, for any graph and almost any propagation model in the literature. This team also gave linear-time provably near-optimal algorithms for static pre-emptive node\/edge removal, by minimizing the eigenvalue on arbitrary graphs. They were also the first to give a a linear-time algorithm to automatically detect the number and identity of possible culprits under perfect information, carefully using the Minimum Description Length principle, again on arbitrary graphs. <br\/><br\/>The major thrust of this proposal is: Given a graph, a virus model (SIR, SIS etc.), a set of already infected nodes, and a fixed<br\/>budget of k nodes\/edges to immunize or quarantine, can one quickly find an optimal or near-optimal solution to best contain the virus?<br\/><br\/>Technical Merit: This is the first to study the short-term immunization problem on arbitrary graphs. The problem has received limited attention in past literature: the few current results (except the PI's past work, see related work) all are on specific graphs like random graphs, and not arbitrary graphs. The focus of this work is on scalable techniques (linear or sub-quadratic on nodes\/edges) which can be applied to large graphs.<br\/><br\/>Impact: The work has numerous immediate applications in public health and epidemiology, e.g., designing dynamic \"what to do next\" policies etc. Leveraging state-of-the-art simulators from the Virginia Bio-Informatics Institute, this work helps in realistic simulations, as well as in making more informed choices and policy decisions for future. The work also has high broader impact, as propagation-style processes on networks appear in many other settings like viral marketing, cyber security, social media like Twitter and blogs etc.<br\/><br\/>Education: The PI will incorporate research findings in graduate level classes, give tutorials at conferences, and aim to engage undergraduate students from underrepresented groups into this exciting area of research through programs like NSF REU and MAOP\/VTURCS (Minority Academic Opportunities Program and VT Undergraduate Research in CS) at VT.<br\/><br\/>For further information, please see the project web page: <br\/>URL: http:\/\/www.cs.vt.edu\/~badityap\/NSF-PROJECTS\/EAGER-13\/","title":"EAGER: Immunization in Influence and Virus Propagation on Large Networks","awardID":"1353346","effectiveDate":"2013-09-15","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[563409],"PO":["563727"]},"210363":{"abstract":"The goal of this Early-concept Grant for Exploratory Research is to explore a new generation of computational tools for joint modeling of physiological and linguistic signals of human behavior. The project is the first to investigate physio-linguistic models for deception analysis. To achieve this goal, the following three research objectives are pursued. First, a novel physio-linguistic dataset of deceit is built, covering several different domains. Second, rule-based classifiers for deception detection are explored, using physiological features (e.g., heart rate, respiration rate, galvanic skin response, skin temperature), as well as linguistic features. Third, data-driven learning approaches for multimodal deception detection are developed, taking advantage of the recent progress in early, late, and temporal fusion models. <br\/><br\/>The project is exploratory in nature, and acts as a catalyst for novel research problems. First, it explores rich sets of multimodal features extracted from physiological and linguistic modalities, analyzing their effectiveness in the recognition of deceit. Second, it also explores the integration of multiple physio-linguistic modalities, through experiments with rule-based and data-driven techniques that fuse multimodal features into joint deception analysis models. To address the challenges of multimodal research work, the team working on this project brings together experts from the fields of bio-sensors, computational linguistics, and physiology and behavioral sciences.<br\/><br\/>The project has high potential payoffs, as models of deception detection have broad applicability, including: the development of critical tools for various applications in fields such as criminal justice, intelligence, and security; the enhancement of applications that can be negatively affected by the presence of deceit, such as opinion analysis or modeling of human communication; and a deeper understanding of fundamental aspects of human behavior, which can positively impact medical applications in psychiatry and psychology. The tools and datasets produced during this project will be made freely available for the research community.<br\/><br\/>For further information see the project web site at: http:\/\/web.eecs.umich.edu\/~mihalcea\/deceptiondetection\/","title":"EAGER: Physio-linguistic Models of Deception Detection","awardID":"1355633","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["564432",563726],"PO":[563727]},"210374":{"abstract":"Analysts in all areas of human knowledge, from science and engineering<br\/>to economics, social science and journalism are drowning in data. The<br\/>proliferation of digital information requires tools and techniques for<br\/>exploring, analyzing and communicating data in a manner that scales as<br\/>both the data and the organizations analyzing it grow in<br\/>size. Throughout the data life-cycle, sensemaking is often a<br\/>collaborative process. As different analysts each contribute to data<br\/>acquisition, cleaning, analysis, and interpretation they contribute<br\/>contextual knowledge that deepens understanding. Analysts may disagree<br\/>on how to interpret data, but then work together to reach<br\/>consensus. Many data sets are so large that thorough exploration by a<br\/>single person is unlikely. In short, social cognition plays a critical<br\/>role in the process of scalable data analysis. New analysis tools that<br\/>address human cognitive characteristics, social interaction and data<br\/>analytics in an integrated fashion can improve our ability to turn<br\/>data into knowledge.<br\/><br\/>Scalable data analysis requires social interaction and therefore<br\/>social context must be embedded in data analysis tools. The goals of<br\/>this project are (1) to understand how social interaction and social<br\/>context can facilitate successful data analysis, (2) to develop models<br\/>and tools for representing and annotating data transformations,<br\/>visualizations, and social activity (e.g., textual and graphical<br\/>annotations, discussions, links, tags), and (3) to design and test<br\/>visual interfaces that leverage our tools to support collaborative<br\/>analysis practices, including data entry, transformation,<br\/>visualization, and interpretation. Central concerns include (a) a<br\/>focus on enabling social interaction throughout the data life-cycle<br\/>and (b) the use of scalable data transformation routines that can<br\/>return results in a time frame concordant with interactive,<br\/>exploratory data transformation and analysis.<br\/><br\/><br\/>Further information on this project can be found at: <br\/>http:\/\/vis.berkeley.edu\/projects\/scalable_social_data_analysis\/","title":"DC: Medium: Collaborative Research: Data Intensive Computing: Scalable, Social Data Analysis","awardID":"1355723","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7793","name":"DATA-INTENSIVE COMPUTING"}}],"PIcoPI":[563750],"PO":[563751]},"210143":{"abstract":"This project will develop and evaluate the potential of a new human-computer system that bridges the roles of virtual student and virtual teacher to allow humans and computers to take turns teaching and learning from each other. The key insight is that reading comprehension activities (e.g., vocabulary building, summarizing, question generation, concept mapping) closely parallel the knowledge engineering required to create virtual teachers for intelligent tutoring systems (ITSs). The system links these activities so that when students read online, they engage a virtual student in educational tasks that both improve their reading comprehension and simultaneously contribute to the creation of ITSs for future students. An important aspect of the proposed research is to find the optimum balance between student learning (which benefits the individual) and the creation of ITS knowledge representations (which benefits many). Specific research objectives are: (1) to develop a baseline platform (called BrainTrust) such that students can create ITS knowledge representations by teaching a virtual student; (2) to study the relationship between the student's ability, the virtual student's ability, the student's learning outcomes, and the quality of knowledge representations produced. A distinctive characteristic of the proposed research is the study of these questions in ecologically valid conditions, as students engage in authentic study, while also participating in randomized experiments.<br\/><br\/>The research may lead to the development of systems that improve reading comprehension, which may have broad benefits given the centrality of reading comprehension to all learning. In particular, problems with reading comprehension have been linked to first-year college student dropout that disproportionately affects African-American students. The research will also enhance infrastructure for research and education through the development and dissemination of the BrainTrust platform, a next-generation computing infrastructure to rapidly create and deploy ITSs tailored to specific needs. If this exploratory project demonstrates that the dual outcomes of human learning and high-quality knowledge representations can be achieved, it will open a new area of research that brings teaching these virtual students full circle with learning from their derived intelligent tutoring systems.","title":"EAGER: Using Crowdsourced Virtual Students to Create Intelligent Tutors","awardID":"1352207","effectiveDate":"2013-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}}],"PIcoPI":[563209],"PO":["565342"]},"210286":{"abstract":"Massive spatiotemporal datasets are often collected using global positioning systems (GPS) and other location-aware devices. Spatiotemporal data mining and analysis have become increasingly important as continued growth in geographic information science and technology enables scientific investigations and decision-making support in a plethora of fields. Big data and extensive computational capabilities are needed to mine and analyze the massive quantities of complex spatiotemporal data collected across multiple scales and used for diverse applications. However, conventional methods and tools for spatiotemporal data mining and analysis are developed primarily using sequential computing, and cannot adequately handle this increasing data intensity, complexity, and diversity of applications. Only by seamlessly harnessing heterogeneous and advanced computing and information infrastructure - cyberinfrastructure - can large and complex spatiotemporal data be efficiently analyzed on a wide scale.<br\/><br\/>This project creates a unified cyberinfrastructure framework by adapting and integrating heterogeneous modalities of computing and information infrastructure (e.g., cloud, high-performance computing, and high-throughput computing) for scalable spatiotemporal data analytics. The framework encompasses two types of novel and complementary capabilities: 1) a suite of methods and algorithms for scalable spatiotemporal data analytics through synthesis of data mining, information network analysis, and parallel and cloud computing; and 2) a geographic information system (GIS) based on advanced cyberinfrastructure (i.e., cyberGIS) to facilitate the use of the methods and algorithms by a large number of users. These novel capabilities help overcome many current limitations in geographic and social science research involving huge amount of spatiotemporal data, and bring forth useful insights for formulating new policies. The framework is designed to gain new fundamental understanding about individual activity patterns and spaces in the domain of environmental health through scalable analysis of massive space-time trajectory data that depict the movement of individuals over space and time. By the ubiquitous use of spatiotemporal data, the project will lead to both transformative and broad impacts on almost all disciplines that employ geospatial technologies for scientific problem solving and decision-making support.","title":"EAGER: CISSDA: A Unified Cyberinfrastructure Framework for Scalable Spatiotemporal Data Analytics","awardID":"1354329","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8083","name":"Big Data Science &Engineering"}}],"PIcoPI":[563534,563535,563536,563537,563538],"PO":["565272"]},"205820":{"abstract":"This project seeks to develop new algorithmic techniques for the<br\/>design of and routing in data networks. Classical network optimization<br\/>problems address transportation networks that carry physical<br\/>commodities. Data networks are fundamentally different --- data can be<br\/>modified, compressed, and replicated at little cost. Incorporating<br\/>this flexibility in altering traffic into network optimization<br\/>problems can bring about a considerable improvement in the capacities<br\/>of communication networks.<br\/><br\/>This project aims to revisit classical network optimization problems<br\/>within a new model where the cost of carrying data is non-linear and<br\/>depends on its compressibility. It aims to provide theoretical<br\/>underpinnings for new WAN optimization approaches proposed in the<br\/>networking literature, including, for example, traffic redundancy<br\/>elimination.<br\/><br\/>The PI considers a model where links in the network can compress data and<br\/>remove duplicate packets. For example, if two traffic streams contain<br\/>identical data and use overlapping routes, edges carrying both streams<br\/>need only transmit the common packets once; these can then be<br\/>replicated at routers where the streams diverge. Accordingly, the load<br\/>on an edge is a submodular function of the traffic streams that use<br\/>the edge. Unlike previous work on network optimization with non-linear<br\/>costs, in this model, the cost of routing depends on the identities of<br\/>the traffic streams involved, and not just on the total traffic<br\/>volume. Most of the problems considered are computationally<br\/>intractable and the PI aims to develop approximation algorithms.<br\/><br\/>Several graduate and undergraduate courses will benefit from this project.","title":"Approximation Algorithms for Data Networks","awardID":"1320854","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[551023],"PO":["565251"]},"202795":{"abstract":"The Speech Recognition Virtual Kitchen <br\/><br\/>Performing successful research on end-to-end speech processing problems requires the integration of many individual tools (e.g. for data cleaning, acoustic model training, language modeling, data analysis, real-time audio, decoding, parsing, synthesis, etc.). It is difficult for new researchers to get started in the field, simply because a typical lab environment consists of a hodgepodge of tools suited to a particular computing set-ups. This environment is hard to recreate, because few people are experts in the theory and practice of all these fields, and can debug and replicate experiments from scratch. <br\/><br\/>This research infrastructure project creates a \"kitchen\" environment based on Virtual Machines (VMs) to promote community sharing of research techniques, and provides solid reference systems as a tool for education, research, and evaluation. We liken VMs to a \"kitchen\" because they provide an environment into which one can install \"appliances\" (e.g., toolkits), \"recipes\" (scripts for creating state-of-the art systems using these tools), and \"ingredients\" (spoken language data). The kitchen even holds \"reference dishes\" in the form of complete experiments with baseline runs, log-files, etc., together with all that is needed to recreate and modify them. <br\/><br\/>The project is developing a community and repository by (a) building pilot VMs, (b) engaging the community in using and continuing to develop them on its own, and (c) evaluating the impact of providing VMs for education and research. We envision researchers as well as students downloading a VM, reproducing the baseline experiment, implementing changes, posting their results in the community, discussing with other users who have worked on the same VM, merging improvements back into the VM, which get re-distributed, and finally publishing easily reproducible results. Work with curriculum and project development will support the creation of engaging activities to specifically encourage students at undergraduate and graduate levels.","title":"CI-ADDO-NEW: Collaborative Research: The Speech Recognition Virtual Kitchen","awardID":"1305215","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["548330"],"PO":["565215"]},"210297":{"abstract":"The proposal requests student travel support for The ACM SIGPLAN International Conference on Functional Programming (ICFP 2013) and associated workshops. The conference is being held in Boston MA during the period of September 22-28. ICFP is the premier conference on all topics related to functional and higher-order programming and the meeting includes eight related workshops and symposia. ICFP is the premier conference on all topics related to functional and higher-order programming. Supporting student travel to attend professional conferences and workshops is a very important mission of the NSF. Broader impacts include building the next generation of researchers in this research area, as well as providing international experiences to build a globally-aware workforce.","title":"Application for NSF Travel Grant for Student Attendance at ICFP 2013","awardID":"1354611","effectiveDate":"2013-09-01","expirationDate":"2014-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[563565],"PO":["564588"]},"205710":{"abstract":"This project will use new technologies for measuring brain activity to understand in detail how human listeners are able to separate competing, overlapping voices, and thereby to help design automatic systems capable of the same feat. Natural environments are full of overlapping sounds, and successful audio processing by both humans and machines relies on a fundamental ability to separate out sound sources of interest. This is commonly referred to as the \"cocktail party effect,\" based on the ability of people to hear what a single person is saying despite the noisy background audio from other speakers. Despite the long history of research in hearing, this exceptional human capability for sound source separation is still poorly understood, and efforts to automatically separate overlapping voices by machine are correspondingly crude: although great advances have been made in robust processing of noisy speech by machine, separation of complex natural sounds (such as overlapping voices) remains a challenge. Advances in sensor technology now enable the modeling of this function in humans, giving an unprecedented, detailed view of sound representation processing in the brain. This project works specifically with measurements of neuroelectric response made directly on the surface of the human cortex (currently with a 256-electrode sensor array) for patients awaiting neurosurgery. Using such measurements made for controlled mixtures of voices, the project will endeavor to both develop models of voice separation in the human cortex by reconstructing an approximation to the acoustic stimulus from the neural population response, and in the process learning the linear mapping between the neural response back to a spectrogram measure of the stimulus. To attempt to significantly improve the ability of machine algorithms to mimic human source separation capability, the project will also focus on a signal processing framework that supports experiments with different combinations of cues and strategies to optimize agreement with the recordings of neural activity. The engineering model is based on the Computational Auditory Scene Analysis (CASA) framework, a family of approaches that have shown competitive results for handling sound mixtures.","title":"RI: Small: Collaborative Research: Towards Modeling Source Separation from Measured Cortical Responses","awardID":"1320260","effectiveDate":"2013-09-15","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[550775,"554699"],"PO":["564318"]},"205501":{"abstract":"Computer users can be distinguished from one another based on differences in their typing rhythms. Our research extends this idea to ask whether a user's level of anxiety or stress can also be determined, but based on *changes* in typing rhythms. Thus, our primary research objective is to answer a single scientific question in a laboratory study: Do typing rhythms change when a typist is under stress, such that the change can be measured and detected with a standard keyboard?<br\/><br\/>Why would anyone want to do this? There are at least two reasons: (1) people under stress tend to make more errors at the keyboard\/console, which can affect job accuracy and reliability; and (2) insiders who abuse their legitimate access to sensitive computer systems are likely to show signs of stress in the execution of a nefarious act, such as cyber espionage. In each of these examples, remote detection of typist stress, through the use of the common keyboard, would alert supervisory personnel to a potential problem that could be averted.<br\/><br\/>In addition to having a major effect on error prevention in national critical infrastructure controls, and on insider detection in cyber space, the impact of this research in health care could also be substantial. For example, subtle changes in typing rhythms could indicate early onset dementia, episodes of cognitive decline, or even a tendency for musculoskeletal disorders such as carpal tunnel syndrome or digital flexor tendinitis.<br\/><br\/>As this research develops, so will the range of beneficial uses.","title":"TWC: Small: Safer Computing through Biometric Stress Detection","awardID":"1319117","effectiveDate":"2013-09-01","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[550270],"PO":["565327"]},"205633":{"abstract":"Computation is omnipresent in society, and randomness plays an important role, both as a liability and as a commodity. In particular, the ability to flip fair coins seems surprisingly useful in a plethora of computational settings, and a central line of research in the theory of computing tries to determine its actual power. In that context researchers develop deterministic simulations of randomized processes that are as efficient as possible. The canonical approach entails the construction of pseudo-random generators, which are efficient deterministic procedures that stretch a short random coin flip sequence to a much longer sequence that still looks random to the process under investigation. The driving question of the project is whether this canonical approach is omnipotent or whether there exist better ways to obtain deterministic simulations.<br\/><br\/>The project focuses on the relationships between derandomization, pseudo-random generators, and lower bounds. The existence of efficient pseudo-random generators is known to be equivalent to certain types of circuit lower bounds (which remain open). There are also a number of results showing that derandomization implies circuit lower bounds of some sort, but the lower bounds are typically not strong enough so as to imply back the same derandomization. A major thrust of the project is to establish equivalences between circuit lower bounds and derandomization, implying that canonical derandomization through pseudo-random generators is omnipotent.<br\/><br\/>The PI and his coworkers have developed a framework for deriving such results, and intend to apply it to large classes of randomized processes, including efficient decision procedures and efficient verification processes known as Arthur-Merlin games. The main focus lies on the standard notion of derandomization, in which the simulation needs to be correct everywhere, but the PI will as well consider weaker notions in which the deterministic simulation is allowed to err on some inputs.<br\/><br\/>Apart from furthering our knowledge about the power of randomness in computation, the project aims to provide graduate training on that topic and in the broader area of computational complexity.","title":"AF:Small: Derandomization and Lower Bounds","awardID":"1319822","effectiveDate":"2013-09-15","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":["565082"],"PO":["565157"]},"205754":{"abstract":"Modern computing systems require manipulation of persistent<br\/>data. However, the mechanisms to access persistent data are separated<br\/>from the mechanisms to manipulate such data in a traditional two-level<br\/>storage model: persistent data is stored in non-volatile storage, such<br\/>as hard disks or solid-state disks, whereas it is manipulated in<br\/>volatile memory, such as DRAM. Unfortunately, this decoupled model of<br\/>system design leads to large amounts of inefficiency in terms of both<br\/>performance and energy.<br\/><br\/>This research undertakes a rethinking of the memory and storage<br\/>systems in the presence of new and emerging non-volatile memory<br\/>technologies such as phase-change memory , spin-transfer torque RAM,<br\/>and memristors. These technologies promise large capacities at low<br\/>latency, making them prime candidates for the design of a single-level<br\/>coordinated memory and storage system where the manipulation and<br\/>storage of all data are unified. Yet, there are significant<br\/>application-level and system-level efficiency and correctness<br\/>challenges in devising such a single-level store with emerging<br\/>technologies. The central goal of this project is to develop<br\/>hardware\/software cooperative techniques to enable high-performance<br\/>and energy-efficient single-level stores in computer systems targeting<br\/>a wide range of applications in cloud computing, client systems, and<br\/>mobile systems.<br\/><br\/>It is expected that research performed in this project will enable new<br\/>applications and much more robust and efficient data storage systems,<br\/>making our daily lives that increasingly depend on data processing<br\/>better and more productive. Enabling unified memory and storage for<br\/>modern computing systems can largely reduce wasted work and take<br\/>advantage of emerging low-power technologies, thereby taking a large<br\/>step in making computing more energy efficient.","title":"CSR: Small: High-Performance and Energy-Efficient Single-Level Stores: Efficient Coordinated Management of Storage and Memory","awardID":"1320531","effectiveDate":"2013-09-01","expirationDate":"2016-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[550874],"PO":["565319"]},"202245":{"abstract":"Many areas of machine learning (ML) currently rely on heuristic algorithms with no performance guarantees, and, in fact, the underlying problems are computationally intractable. The proposed project will lead to new machine learning algorithms with provable guarantees. The PIs will leverage theoretical ideas from average case analysis, semi-random models, approximation, solution stability, and approximate linear algebra, and develop new tools and techniques. Benefits from this endeavor will occur across fields. Theoretical computer science will benefit by gaining new problems and research agendas, and further development of algorithmic and mathematical tools. Machine learning will benefit by gaining new models (hopefully, more tractable than the ones currently in use), new modes of thinking, and new algorithms. The task of proving bounds on algorithms will provide insight into when they do or do not work, as well as suggest modifications to existing heuristics.<br\/><br\/>The project will involve training a new generation of graduate students and postdocs who will be fluent both in theoretical algorithms and machine learning. The PIs gained experience in delivering this kind of training and mentoring during the past couple of years and will continue this, including working with the undergraduate students. The PIs will disseminate the ideas of this project by lecturing, teaching, and creating new course materials, including videos. One specialized workshop will also be organized, devoted to the topics of study. Open-source software implementation will be released based upon any new learning algorithms that are discovered.","title":"AF: Medium: Towards Provable Bounds for Machine Learning","awardID":"1302518","effectiveDate":"2013-09-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["549429",542007],"PO":["565157"]}}