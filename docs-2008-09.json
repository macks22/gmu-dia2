{"139458":{"abstract":"Abstract:<br\/><br\/>The PIs have recently developed a discriminatively trained deformable part based model for detecting objects in images. This model achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. Our entry in the 2007 object detection challenge won in six classes. Modifications since the 2007 challenge have improved our results so that we now outperform the best 2007 challenge results in ten of twenty classes including the person class. These accomplishments rely on a newly developed approach to discriminative training for latent variable models which we call a latent SVM (LSVM). We propose to extend this methodology in a variety of ways. The research will focus on deeper latent information such as subclassification (mixture models), three dimensional pose, and figure\/ground segmentations. They will also use class hierarchies, visual words, and hierarchical object models with parts and sub-parts. We also propose a general methodology for using SVM training to train models, such as geometry-based 3D models, which are highly nonlinear in model parameters. All aspects of this research are strongly tied to empirical performance -- no method will be adopted unless it actually improves the state of the art. The goal has been, and will continue to be, to improve the state of the art through the use of semantically deeper models and improved general purpose machine learning methods.<br\/><br\/>Progress on this project can be found at http:\/ttic.uchicago.edu\/ ~dmcallester\/objects.html","title":"Collaborative Proposal: Discriminative Latent Variable Object Detection","awardID":"0811340","effectiveDate":"2008-09-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[370702],"PO":["564316"]},"139469":{"abstract":"For virtually the entire history of computing, improvements in the speed of sequential computations, which execute only a single task at a time, have been the primary source of increased computing power. But the computing field is now encountering fundamental limits on the underlying computing substrate that eliminate this source of performance improvement. The field must instead work with parallel computers, which obtain increased performance by performing multiple tasks at the same time. A key challenge to obtaining these performance benefits is the difficulty of developing parallel software that can correctly coordinate the activities of multiple tasks that execute at the same time. The research addresses this difficulty by investigating the development of compilation techniques designed to automatically translate sequential software that performs a single task at a time into parallel software that automatically performs multiple tasks at the same time.<br\/><br\/>The research focuses on modern object-oriented computations that manipulate linked data structures such as lists, graphs, and trees. It builds on the recent availability of verified implementations of these data structures to reason with the more general abstract data structure state as opposed to the concrete objects and references that the data structure implementations manipulate when they run. The developer can then use the abstract data structure state to specify an equivalence condition that any parallel computation must satisfy. The expected result is that the analysis techniques will be able to use the equivalence condition to automatically generate parallel software that may produce a different but equivalent result as the corresponding sequential software. This additional freedom promises to substantially broaden the range of computations that are amenable to automatic analysis for faster parallel execution.","title":"CPA-CPL: Automatic Parallelization Using Semantic Commutativity Analysis","awardID":"0811397","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["497068"],"PO":["565272"]},"139359":{"abstract":"PI: Zhang, Tong & Parhi, Keshab<br\/>Proposal No: 0810992 & 0811456<br\/>Title: Collaborative Research: CPA-DA: Noise-Aware VLSI Signal Processing: A New Paradigm for Signal Processing Integrated Circuit Design in Nanoscale Era<br\/>Institution: Rensselaer Polytechnic Institute & University of Minnesota<br\/><br\/><br\/>ABSTRACT<br\/>The objective of this proposal is to develop a new noise-aware design methodology that can maximize the error resilience of signal processing integrated circuits. As CMOS technology approaches its end-of-roadmap physical limit, there have been increasing levels of environmental and process variations, and susceptibility to noise, which make it a challenge to maintain the historical yield and reliability. This proposal will develop methodology and approaches that tackle this grand challenge in the context of signal processing integrated circuits implementation. The intellectual merit of this proposal lies in the research theme of leveraging the unique characteristics of signal processing functions to substantially improve the tolerance to noise. There are two major parts to this project: developing noise analysis techniques for signal processing integrated circuits, and exploring design space for noise-aware VLSI signal processing. In particular, this research will develop analysis techniques that can quantitatively estimate how variations in signal processing integrated circuits may affect the signal processing performance. This research will further explore the design space for noise-aware VLSI signal processing where the objective is to minimize the noise-induced signal processing performance degradation at minimal energy consumption and\/or silicon cost. <br\/><br\/>The proposed research program represents the first step towards exploring a new research area. If successful, it will have broad impact on the semiconductor industry and national economy in both the near term and long term: In the near term, it will generate considerable economic benefit by improving the noise tolerance and the effective yield of signal processing integrated circuits. From another perspective, it will enable more aggressive CMOS scaling for implementing signal processing integrated circuits, which will be greatly beneficial since the signal processing functions are typically very hardware resource demanding. In the long term, this research will shed light on signal processing system implementation using post-silicon nanotechnology, such as molecular electronics, where a significant degree of noise is presumably inevitable. The education objective of this proposal is to promote the education of VLSI signal processing, the inter-disciplinary area linking semiconductor and signal processing\/communication, to a wider spectrum of students.","title":"Collaborative Research: CPA-DA: Noise-Aware VLSI Signal Processing: A New Paradigm for Signal Processing Integrated Circuit Design in Nanoscale Era","awardID":"0810992","effectiveDate":"2008-09-01","expirationDate":"2011-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["360575"],"PO":["562984"]},"144870":{"abstract":"To solve the most critical intellectual and social problems, teams need to be made up of the best possible people, linked to the best possible resources. While there is growing awareness of the socio-economic consequences of team collaborations, there is little socio-technical understanding of how teams are assembled or how a given mode of assembly impacts its effectiveness. <br\/><br\/>This project seeks to address this limitation by developing a theoretical framework to understand the socio-technical dynamics shaping the assembly of teams in virtual communities. These multidimensional networks include a variety of links that exist not only among individuals, but also with documents, datasets, workflows, analytic tools, and concepts. With these new configurations in mind, this project addresses two main research questions: First, what are the socio-technical motivations that explain the assembly of teams in virtual communities? Second, to what extent do the assembly mechanisms of teams influence their effectiveness? Empirically testing such models poses formidable data collection challenges. However, this project has access to six major initiatives serving diverse scientific virtual communities including nanoscience, environmental engineering, earthquake engineering, chemical sciences, media research and tobacco research.<br\/><br\/>Intellectual merit: This effort is uniquely positioned to usher a new generation of theories and methods focused on explaining an important precursor to all collaborations - the socio-technical assembly mechanisms used to generate effective teams in virtual communities. Methodologically, this project will significantly extend network analytic techniques for statistically modeling high-dimensionality multimodal networks. <br\/><br\/>Broader impacts: The proposed research will have broad impacts on three stakeholder communities. First, individual researchers, especially students and those not in elite institutions will learn strategies to assemble effective teams in virtual communities. Second, leaders of virtual communities will gain a dashboard to assess and steer strategies within their initiatives. Third, funding agencies will get new insights in assessing their portfolios and directing science policy.","title":"Collaborative: VOSS: Understanding and enabling network dynamics in virtual communities","awardID":"0838564","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"5376","name":"INNOVATION & ORG SCIENCES(IOS)"}}],"PIcoPI":[386473,386474,"532398"],"PO":["519302"]},"143781":{"abstract":"Virtually every human endeavor in a broad variety of disciplines in science, engineering, and finance, is encountering the need to make new discoveries through the analysis of massive datasets. Yet, the task of creating the necessary scalable analysis tools for modern high-performance parallel hardware is a daunting task, due to the complexity of developing efficient algorithms and subsequently having to parallelize and tune these algorithms for real systems. This research project aims to address this problem by developing a new domain-specific programming model, called the Tree-based High-Order Reduce (THOR) model, that enables rapid automatic implementation of customized parallel data analysis and mining tasks with minimal coding effort.<br\/><br\/>The key enabling insight behind THOR is the generalized n-body problem (GNP) theory, a mathematical formalism that unifies the expression of seemingly disparate statistical data analysis tasks, including n-point correlation, hierarchical clustering, k-nearest neighbors classification, and kernel density estimation, among numerous others. As its name suggests, a GNP elegantly generalizes the classical n-body problem from physics to a much broader class of problems. Most importantly, the GNP form permits the development of asymptotically fast solutions, e.g., generalized versions of the fast multipole method. The THOR model enables the data analyst to specify a GNP, from which the THOR program generator can automatically produce a highly tuned parallel implementation. In short, this project aims to show how a programming model, which is bound to an appropriately high-level mathematical formalism while having the simplicity of a model like MapReduce, can lead to both scalable data analysis algorithms and their efficient implementation.","title":"THOR: A New Programming Model for Data Analysis and Mining","awardID":"0833136","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["518447","558498"],"PO":["565272"]},"143792":{"abstract":"High-end distributed and distributed shared memory platforms with millions of processors will be deployed in the near future to solve the toughest technical problems. Their individual nodes will be heterogeneous multithreading, multicore systems, capable of executing many threads of control, but with relatively little memory per thread, low bandwidth to main memory and deep memory hierarchies. <br\/>A programming model that supports productive, portable, efficient parallel programming both within and across the nodes of these petascale systems is essential if their potential is to be realized. Since it is easier for application developers and tool vendors to extend existing software rather than adopt a new programming language, a programming model based upon a familiar paradigm is highly desirable. <br\/><br\/><br\/>OpenMP is a widely supported shared memory programming model that provides ease of maintenance. It is suitable for programming multicore nodes, but does not address the needs of distributed memory platforms. This research will significantly extend OpenMP so that it can be used to program all levels of a high-end petascale system. In order to accomplish this, the investigators will enhance its existing mechanisms for describing multiple levels of parallelism, provide additional features for specifying synchronization and for achieving high levels of locality, as well as develop a novel I\/O interface. Moreover, they will substantially improve the state of the art in OpenMP implementation technology, enabling high performance between nodes as well as within them. Results will be demonstrated via a state-of-the-art Fortran\/C\/C++ OpenMP compiler, a highly optimized communications library, and a range of large scale applications.","title":"COLLABORATIVE RESEARCH: Extreme OpenMP: A Programming Model for Productive High End Computing","awardID":"0833163","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["529083"],"PO":["565272"]},"143462":{"abstract":"This project builds a holistic model of reliability and performance for large-scale networking system (LSNS) and develops model-based self-improvement (MBSI) technology for high reliability, high performance, and smooth network communication in LSNS. This project addresses a number of problems, including the precise formulation of a holistic model of the LSNS reliability and performance, the analytical evaluation of the reliability and performance, the scheme to reduce the jams of network traffics, and further uses the self-improvement technology to automatically glean data, build models, evaluate designs, and optimize tasks. The methods, like graph theory, Bayesian approach, maximum entropy principle, universal generating function, and Monte Carlo simulation, are mainly adopted in modeling and evaluating the LSNS, while autonomic computing technologies are implemented for the self-improvement functions. As a result, this research further advances the theory, algorithm and technology in reliability, and fills the gap of reliability modeling and self-improvement in LSNS. <br\/><br\/>Expected results: (1) Novel formulations will be designed to effectively model a LSNS with all essential components; (2) An innovative HSA (Hybrid Stochastic Algorithm) will be developed to evaluate the task reliability and performance for LSNS; (3) New optimization schemes for LSNS will be designed; (4) A novel MBSI technology will be developed for self-improving the reliability, performance, and network communication; (5) Research outcomes will be applied in a variety of LSNSs, such as NASA's outer-space exploration, tele-medicines, grid computing, etc; (6) A set of software tools will be developed. (7) Research results will be disseminated through journal\/conference publications and PIs' websites.","title":"(NECO) Collaborative Research: Reliability Modeling for Large-Scale Networking System (LSNS), and Self-Improvement in LSNS","awardID":"0831609","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[382015],"PO":["565090"]},"145530":{"abstract":"In traditional spreadsheet systems, such as Excel, cells are <br\/>partitioned into \"directly specified\" cells<br\/>and \"computed\" cells and the formulas used to specify the values of <br\/>computed cells are \"functional\",<br\/>i.e. for every combination of values of the directly specified cells, <br\/>the formulas specify unique values<br\/>for the computed cells. Logical Spreadsheets expand the utility of <br\/>traditional spreadsheets by<br\/>dispensing with the distinction between directly specified cells and <br\/>computed cells and generalizing<br\/>from functional definitions to logical constraints.<br\/><br\/>One problem with logical spreadsheets is the potential for <br\/>inconsistency. In using logical spreadsheets,<br\/>users can select values for cells that are inconsistent with the <br\/>values of other cells, given the constraint<br\/>defining the spreadsheet. Simply eliminating inconsistent values is <br\/>not the answer - (1) rejecting new<br\/>values that are inconsistent with past values limits the user's <br\/>freedom and (2) dropping past values that<br\/>are inconsistent with new values restores consistency but loses <br\/>information. On the other hand, retaining<br\/>inconsistencies is problematic when the spreadsheet tries to derive <br\/>consequences of updates, since<br\/>inconsistent logical theories entail everything!<br\/><br\/>Unlike logical entailment, existential entailment behaves well in the <br\/>face of inconsistencies and, therefore,<br\/>is more useful in this setting. Work in this project includes (1) a <br\/>study of the behavioral properties of existential entailment in the <br\/>context of logical spreadsheets, (2) the development of efficient <br\/>algorithms<br\/>for computing existential entailment (with special attention to rule <br\/>transformation, reformulation, and<br\/>differential logic), (3) the development of an open-source Javascript <br\/>implementation incorporating these<br\/>algorithms, and (4) an evaluation of this technology in the form of <br\/>user surveys, comparative studies, and<br\/>ablation studies.<br\/><br\/>Further information on the project can be found at the project web <br\/>site at http:\/\/logicalc.stanford.edu.","title":"SGER: Managing Inconsistency in Logical Spreadsheets","awardID":"0841152","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[388248],"PO":["565136"]},"143473":{"abstract":"Most of the Internet's complexity resides in software running on Internet routers. Bugs in this software are a highly critical problem, leading to a number of recent high-profile attacks and outages, and are increasingly becoming a bottleneck in building highly reliable networks. The PIs are designing and evaluating techniques to make the Internet resilient to software bugs. Their approach consists of two key components. First, they are building a highly reliable single instance of a network router. This involves performing a characteristic study of bugs in router software, by using static and dynamic code analysis and by taxonomizing publicly disclosed vulnerabilities. They also apply and extend techniques such as rollback, reordering inputs, microreboots, and automated debugging to construct a software router resilient to implementation bugs. Second, the PIs are developing and building an architecture for highly-available bug-resistant networks. Their design leverages the principle of \"control and data diversity\", which simultaneously runs multiple functionally-equivalent instances of a piece of software or data. Each instance is changed from the others in a way that makes it unlikely multiple copies will simultaneously undergo the same bug, for example by randomizing the execution environment, having each instance be responsible for a subset of routes, or by having different programmers implement each instance. In addition to producing designs and algorithms that enable these networks, the PIs will also make available tools and implementations to enable their use. <br\/>Successful completion of this project will significantly improve the Internet's ability to avoid and recover from failures.","title":"NeTS-NECO: Collaborative Research: Fixing the Reliability Problem in Network Software From its Root","awardID":"0831653","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["551097","548463"],"PO":["565303"]},"146993":{"abstract":"The goal of this project is to provide support for query processing in a mobile P2P database by using a paradigm of data-item store-and-forward. It achieves this goal by 1) developing a ranking function for data-items that incorporates factors such as the supply of a data item in the mobile P2P system, its demand in the system, the size of the data-item, and its reliability; 2) enabling the query processing system to take advantage of a cellular infrastructure when such is available; and 3) developing an incentive mechanism that will motivate users to participate in the query processing system even when they do not have a personal query to process. The research results will enable discovery of local, and possibly temporary, resources in many application domains, including social networks, transportation, mobile electronic commerce, emergency response, and homeland security. This project supports a Ph.D. student to pursue research in the area of mobile P2P databases. Publications, technical reports, software, and experimental data from this research will be disseminated via the project web site(http:\/\/www.cs.uic.edu\/~boxu\/mobidik).","title":"SGER: Feasibility of Decentralized Search in Mobile P2P Databases","awardID":"0847680","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["557651","523591"],"PO":["469867"]},"134420":{"abstract":"Machine Learning and Data Mining have gained importance across many areas of science and other industries. A critical challenge is to design learning algorithms that work on real-world data sets, which are often noisy and almost never perfectly fit any particular model. Truly robust and noise-tolerant learning algorithms are necessary for improved predictions, compression, medical diagnoses, and automation. The potential applications of such algorithms are as wide-spread and varied as those of the field of Statistics.<br\/><br\/>More specifically, the project entails designing machine learning algorithms that are provably robust: ones that optimally fit noisy data. The project has several novel algorithmic and analytical ideas for designing provably noise-tolerant learning algorithms. As with all such ?agnostic? learning algorithms, these algorithms are also computationally efficient. Partly due to the wide variety of applications, theoretically-inspired algorithms have long been the state-of-the-art for machine learning and data mining. The research results of this project have broader impacts across a number of scientific, medical, and industrial fields. The project?s impact extends to academia through educational efforts, including graduate and undergraduate training, curriculum development, novel educational endeavors, and seminars.","title":"CAREER: New Approaches to Agnostic Learning","awardID":"0746550","effectiveDate":"2008-09-01","expirationDate":"2010-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[357394],"PO":["562944"]},"144100":{"abstract":"The purpose of this project is to develop and evaluate user interface tools that enable middle school students to effectively teach themselves the basics of computer programming using programs written by peers. The proposed tools will enable middle school users to 1) identify interesting functionality within programs written by peers and 2) re-create that functionality for use in their own programs through automatically generated custom tutorials. As users complete these tutorials to re-create functionality selected from programs their peers have written, they will be introduced to a broad range of programming concepts and constructs. The project will include user testing to both guide and evaluate the development of 1) user interface tools that enable novice programmers to find code of interest in an unfamiliar codebase and 2) technology to automatically generate customized tutorials based on a recorded history of the sequence of user interface actions used to construct a program.<br\/><br\/>Computer programming has become a fundamental tool that enables progress across a broad range of disciplines including basic science, communications, and medicine. Yet, Computer Science is failing to attract the number of students necessary to sustain progress both within the discipline and in those disciplines supported by computer science. Opportunities to study computer science during middle school (when many students begin to opt out of math and science based careers) are rare. This project will enable middle school children to teach themselves computer programming using programs created by their peers. The project will be implemented within Storytelling Alice, an environment that enables middle school children to write programs to create 3D animated movies. Users will be able to identify parts of movies created by other users that interest them and follow automatically generated tutorials to learn how to create the selected parts of those movies. By building tools that help users to learn effectively from programs written by peers within an appealing programming environment, we enable middle school students across the country to develop skills in computer programming at a time when formal opportunities to study computer science are decreasing and the need for computer scientists is increasing. While this project targets middle school students in the context of learning computer programming, there are many other audiences (e.g. adults learning a new piece of computer software) would benefit from software technologies that enable self-teaching.","title":"Collaborative Research: Enabling Independent Learning of Computer Programming Using Programs Written by Peers","awardID":"0835438","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7625","name":"REESE"}}],"PIcoPI":["565193"],"PO":["564456"]},"143011":{"abstract":"Emerging computing technologies have made it possible to manufacture<br\/>large-scale embedded multi-agent systems, from vast sensor networks to<br\/>modular robots and smart materials. A key challenge is understanding<br\/>how to program such systems at the individual agent level in order to<br\/>achieve system-level behavior that is complex, fault-tolerant, and<br\/>adapts to the environment. One source of inspiration is multicellular<br\/>biological systems (tissues, organs, and simple organisms) that<br\/>achieve complex self-adaptation in changing environments through the<br\/>distributed cooperation and sensing of vast numbers of cells. Such<br\/>systems can provide novel bio-inspired principles for the design and<br\/>programming of multi-agent systems.<br\/><br\/>This research investigates new computational paradigms for programming<br\/>multi-agent robotic systems to achieve complex self-adaptation in<br\/>response to the environment. The two main thrusts are: (a) the<br\/>development of a global-to-local programming methodology for<br\/>describing complex global adaptation goals and automatically deriving<br\/>provably robust multi-agent control (b) the development of a<br\/>tissue-inspired modular robotic system that can demonstrate<br\/>self-adaptive structures. A key source of inspiration is the<br\/>decentralized control strategies that cells use to achieve<br\/>environment-responsive structures and functions, e.g. shape adaptation<br\/>in plants and vascular networks, and locomotion in simple animals. The<br\/>aim is to harness these bio-inspired principles to create multi-agent<br\/>robotic systems that replicate the adaptability and responsiveness of<br\/>living systems. This work has broad application, from robust control<br\/>in distributed sensor-actuator networks to the development of<br\/>self-adapting architectural structures and prosthetics. This research<br\/>significantly advances our understanding of how to design autonomous<br\/>multi-agent computing systems that can self-organize, self-repair, and<br\/>respond to the environment.","title":"EMT\/BSSE Programmable Self-Adaptation: A Bio-inspired Approach To Multi-agent Robotic Systems","awardID":"0829745","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["563386"],"PO":["565223"]},"143374":{"abstract":"Encryption is an important tool for security, particularly in wireless systems, where the signal is not physically constrained. However, in extremely low-power applications (e.g. RFID and sensor nets), the power required for nodes to run standard cryptographic algorithms at a high level of security can be prohibitive. In this exploratory project, the hypothesis that encryption at the physical layer can supplant encryption done digitally at higher layers in extremely low-power wireless applications is being evaluated. A novel low-power ultra-wideband (UWB) radio framework is being developed and its cryptographic security for different system parameters considered. Team expertise in digital and RF circuits allows accurate calculation of both trends and fundamental lower bounds on the power required under both standard digital cryptographic and the physical layer cryptography scheme. The research team consists of UMass faculty with expertise in ultra-wideband system design and characterization, low-power digital hardware, and low-power RF hardware, and (external) contributions from an expert in cryptography at RSA Labs. Expected results of the project include (1) the development and cryptographic assessment of the proposed UWB framework for encryption, and (2) the development of trends and lower bounds on the power required for both standard (digital) approaches and the UWB framework. The establishment of the hypothesis would have a substantial impact by providing a completely new view of how to provide encryption in low-power wireless systems. The project is also providing opportunities for graduate and undergraduate students to perform research crossing the boundaries of analog and digital circuits, communication theory and cryptography.","title":"CT-ER: Ultra-wideband Radio for Low-Power Security","awardID":"0831133","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["532505","545656","438750"],"PO":["543481"]},"143495":{"abstract":"This project centers on the Featherlight Information Network with Delay-Endurable RFID Support (FINDERS), composed of passive RFID tags which are ultra light, durable, and flexible, without power supply for long-lasting applications under strict weight constraints and harsh environments. It expands the use of RFID gear for wireless network construction, aiming to find events of interest and gather aggregate information. FINDERS faces unprecedented challenges in communication and networking, due to its sporadic wireless links, unique asymmetric communication paradigm, intermittent computation capability, and extremely small memory of tags. The objectives of this project include the establishment of fundamentals and principles, system design and optimization, mathematical analysis and modeling, and prototyping and experimental evaluation of the FINDERS system. The success of this project will not only advance the scientific study of wireless sensor networks and RFID systems but also serve as a new paradigm for pervasive data acquisition under strict weight constraints and harsh environments. The technologies and the prototype of FINDERS to be developed in this project will immediately benefit wildlife and biological research projects. In the long term, it will foster multi-disciplinary research opportunities for future relevant projects, effectively strengthening the research competitive edge of UL Lafayette. The research results will be disseminated through conference presentations, journal publications, a dedicated website, and frequent lectures at our graduate seminars. The research problems and findings and the testbed developed in this project will enrich course materials, provide students with hands-on experience, and attract more graduate students into this emerging area.","title":"NEDG: Featherlight Information Network with Delay-Endurable RFID Support (FINDERS)","awardID":"0831823","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["409472","551058"],"PO":["565303"]},"143143":{"abstract":"Abstract<br\/>_____________________________________________________________________________________<br\/>Collaborative Research: Data Communication via Particle Velocity Channels - A Paradigm Shift in Underwater Acoustic Communication<br\/><br\/>Over 75% of the earth?s surface is covered with water, with many resources upon which human life depends. High speed wireless data communication with acoustic waves among underwater sensors, deepwater moored instruments, autonomous underwater vehicles, and surface vessels is of crucial importance in many applications of national interest. Examples include offshore oil industry, environmental and ocean monitoring to predict natural disasters such as hurricanes, and so on. However, the underwater acoustic channel is a complex and highly bandlimited environment, and the achievable data rates by current systems are much smaller than the needs.<br\/><br\/>The core novel idea of the research is to transform the foundation of underwater acoustic communication, by communicating over the unexplored degrees of freedom of the acoustic field, i.e., the acoustic particle velocity channels. Over the past few decades, only the pressure channel of the acoustic field has been used for underwater communication. The key concept in this research is to take advantage of the vector components of the acoustic field, such as the three components of acoustic particle velocity. Particle velocity channels are promising for high speed communication, due to their possibly smaller delay spreads. The small size of particle velocity transceivers is another advantage over large pressure-only arrays traditionally used for underwater communication. In this research the investigators develop a cohesive framework for high rate underwater communication via acoustic particle velocity channels. The research objectives fall into two closely-related categories: channel modeling and transceiver design. Channel modeling objectives aim at understanding and characterization of particle velocity channels, whereas transceiver design objectives address new issues encountered at the channel modeling stage.","title":"Collaborative Research: Data Communication via Particle Velocity Channels-A Paradigm Shift in Underwater Acoustic Communication","awardID":"0830204","effectiveDate":"2008-09-01","expirationDate":"2012-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7750","name":"CDI TYPE I"}}],"PIcoPI":["560747","560746"],"PO":["564898"]},"143264":{"abstract":"Collaborative Research:<br\/>New Directions in Graph-Based Code Design<br\/><br\/>Abstract<br\/><br\/>This collaborative research focuses on the physical layer of digital communication system design ? in particular on the analysis, design, and implementation of capacity-approaching low-density parity-check (LDPC) codes for practical communication environments. In the last ten years, the area of channel coding has undergone a revolutionary change with the growing popularity of graph-based codes and iterative decoding algorithms. These coding methods, which include both turbo codes and LDPC codes, approach the limits of channel coding performance promised by Shannon in his landmark 1948 paper. Currently, these codes are in the process of replacing conventional error control techniques in numerous digital communication and storage standards, including, among others, deep-space communication, next-generation wireless transmission, last-mile cable transmission, digital video broadcasting, and high-density digital magnetic recording.<br\/><br\/>The research addresses several issues related to graph-based codes. In particular, it focuses on the analysis, design, and implementation of LDPC convolutional codes, which have several advantages compared to LDPC block codes, but have not received much attention from the research community. Conventional convolutional codes, on the other hand, have had a transformative effect in numerous practical communication environments, and the same is likely to be true in the capacity-approaching world of LDPC codes. The project emphasizes bridging the gap between advanced theoretical research and realistic practical implementations. In particular, it is concerned with adapting LDPC convolutional code designs to various industry standards that require flexibility in both frame length and code rate and with developing VLSI implementations of hardware decoders that can be tested under real operating conditions.","title":"Collaborative Research: New Directions in Graph-Based Code Design","awardID":"0830650","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["508186","40607"],"PO":["564924"]},"143385":{"abstract":"Modern organizations, such as businesses, non-profits, government <br\/>agencies, and universities, collect and use personal information from <br\/>a range of sources, shared with specific expectations about how it <br\/>will be managed and used. Accordingly, they must find ways to comply <br\/>with expectations, which may be complex and varied, as well as with <br\/>relevant privacy laws and regulations, while they minimize <br\/>operational risk and carry out core functions of the organization <br\/>efficiently and effectively. Designing organizational processes to <br\/>manage personal information is one of the greatest challenges facing <br\/>organizations (see, e.g. a recent survey by Deloitte and the Ponemon <br\/>Institute [TI07]), with far-reaching implications for every <br\/>individual whose personal information is available to modern <br\/>organizations, i.e. all of us.<br\/><br\/>This project responds to these challenges by developing methods, <br\/>algorithms and prototype tools for integrating privacy, compliance, <br\/>and risk evaluation into complex organizational processes. It <br\/>explores, articulates and characterizes formally the scope and nature <br\/>of privacy-expectations of stakeholders as well as those of key <br\/>regulations, such as HIPAA, GLBA, COPPA, BASEL 2, and Sarbanes-Oxley <br\/>(SOX). It incorporates the diverse perspectives and areas of <br\/>expertise of its multidisciplinary research team, which includes <br\/>three computer scientists, one philosopher, and collaborating <br\/>researchers from IBM. This industry connection facilitates <br\/>interaction with product teams that have served complex organizations <br\/>concerned with business process integrity, information security, <br\/>privacy, and information risk management. The research builds on <br\/>\"contextual integrity\" (a philosophical account of privacy) as well <br\/>as language and risk-based methods for privacy policy specification <br\/>and enforcement. Extensive training and educational opportunities are <br\/>provided to undergraduate and graduate students and research results <br\/>integrated into courses at CMU, NYU, Stanford, and UPenn.","title":"Collaborative Research: CT-M: Privacy, Compliance and Information Risk in Complex Organizational Processes","awardID":"0831199","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[381805],"PO":["565327"]},"134442":{"abstract":"The proliferation of embedded devices and significant advances of wireless network technologies have led to the emergence of Open Distributed Real-time and Embedded (ODRE) systems and applications which further the expansion of our society's digital backbone. These applications involve an increasingly large number of small dynamic concurrent objects that must together satisfy multiple types of QoS requirements. As such, the need for a new paradigm to reduce the complexity and ease the development of these systems is growing.<br\/><br\/>Viewing ODRE systems as compositions of coordination and concurrent computation decouples the two concerns and allows higher levels of abstractions. However, these advantages can only be fully realized if the following fundamental requirements are met. First, it is essential to have a coordination model that focuses on coordination under QoS constraints, and is decentralized, exogenous, scalable and stable.<br\/>Second, in order to reason about QoS constraints, a formal model that uniformly represents these different types of constraints must be provided. Third, tools that support coordination abstractions must be available to facilitate the development of ODRE applications. This project is devoted to meeting these requirements.<br\/><br\/>Collaborating with industry and laboratories and progressively evaluating research results in real-world application settings are two additional key facets of this project. This ensures that the results are relevant and usable in improving the robustness of critical software. In addition, the collaboration and role models from industry enrich the students? learning environment and provide them the support needed for successful careers in real-time embedded computing.","title":"CAREER: Behavior-Based Coordination for Open Distributed Real-Time and Embedded Computing","awardID":"0746643","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["460574"],"PO":["561889"]},"143033":{"abstract":"EMT: Collaborative Research: Primate-inspired Heterogeneous Mobile and Static Sensor Networks <br\/><br\/>Although previous bio-inspired models have concentrated on invertebrates (such as ants), mammals such as primates with higher cognitive function are valuable for modeling the increasingly complex problems in engineering. Understanding primates? social and communication systems, and applying what is learned from them to engineering domains is likely to inspire solutions to a number of problems.<br\/>This research involves studying and modeling modes of group behavior and communication of coppery titi monkeys, rhesus macaques, and other primate models, and applying what the investigators learn to the distributed control of heterogeneous mobile and static sensor networks. The investigators will model the social and communication behavior of these primates, which will provide biological inspiration for solving problems in communication and networking. The phases of this research include: 1) identification, interpretation, and translation of primate behavioral models, 2) assessment of the effectiveness of small and large group formations based on primate grouping models in heterogeneous mobile and static sensor networks, 3) development of bio-inspired message-based communications, and 4) development of bio-inspired behavior-based communications. This research aims to achieve a deeper understanding of effectiveness of bio-inspired communications and networking by studying primates, and to establish interdisciplinary research and education in the fields of biological modeling, sensor networking, and robots control.","title":"EMT: Collaborative Research: Primate-inspired Heterogeneous Mobile and Static Sensor Networks","awardID":"0829827","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7946","name":"BIO COMPUTING"}}],"PIcoPI":["472085","513708","504300","542301"],"PO":["565223"]},"144364":{"abstract":"Collaborative Research:<br\/>0836656 (Peter Doerschuk, Cornell University)<br\/>0836649 (Bud Mishra, NYU)<br\/>0836720 (Sanjoy Mitter and Emery Brown, MIT)<br\/>Title: Discovery of Succinct Dynamical Relationships in Large-Scale Biological Data Sets<br\/><br\/><br\/>ABSTRACT:<br\/><br\/>Many types of information in neuroscience and molecular biology can be described as a set of measurements taken repeatedly as some index changes its value. In some situations, such as transcriptomic data measuring gene activities, the index is time while in other situations, such as in genetics association study, the index is position in a genomic DNA sequence and, in any case, the complete collection of data is referred to as a time series. Inference is the process of taking such time series, probably corrupted by errors, and computing answers to the following sorts of questions: (1) What is the system that generated the time series? For instance, if the system is known to be a differential equation of a specific type, what are the parameter values in the differential equation? (2) Given a completely specified system and a time series, did that system generate that time series? For instance, if a biologist has hypothesized a system that describes gene expression for a particular set of genes and then measures expression data, is the data compatible with the system, or equivalently, the hypothesis? (3) Given two time series, were they generated by the same system? For instance, if the pattern of nerve firings in a neural system is recorded in two different experimental situations, is the pattern the same or is it different? The four Principal Investigators are focused on three different biological application domains at three different biological scales: (1) the phenotyping of animal and human ethanol-consumption behavior (whole organism scale), (2) the pattern of action potentials measured on ensembles of neurons (cell-population scale), and (3) the time course of gene expressions as governed by the regulatory circuits of the cell (cellular scale). The types of challenges that are encountered in these applications include the following characteristics: the information is distributed over long periods of time rather than concentrated in time; the systems include delays and feedback paths; and the systems are highly nonlinear, including switching behavior, rather than linear. The major methodologies that will be developed and combined to solve inference problems in these application areas are: (a) information theory and stochastic control, (b) multi-scale approaches to learning the geometry of the data, and (c) computer algebra and symbolic computation. For example, to deal with the presence of delay and feedback in neuroscience systems, especially in the context of the interaction between information and stochastic control, requires a fundamental rethinking of classical information theory as it is employed in technology-based communication systems.<br\/><br\/>As the cost of computing decreases, computing becomes increasingly pervasive. A major purpose of pervasive computing is the real-time collection of high-dimensional time series of very diverse types of data including biological, medical, financial, communication systems status, power systems status, etc. The project will provide computational algorithms and software to analyze this data in more sophisticated ways and thereby extract more sophisticated information. Action taken upon this more sophisticated information, e.g., personalized medicine based on individualized genomic information or more accurate and flexible control of power systems thereby avoiding blackouts, will have important human and economic benefits to society. An important component of the project is educational, e.g., three graduate students working on the project will receive tuition and stipend and an unrestricted number of undergraduates will participate through a variety of ways, e.g., project courses. By attracting talented students to science and technology and providing challenging research experiences, the project will have important work force benefits to society.","title":"Collaborative Research: CDI-Type II: Discovery of Succinct Dynamical Relationships in Large-Scale Biological Data Sets","awardID":"0836656","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7751","name":"CDI TYPE II"}}],"PIcoPI":["518003"],"PO":["562984"]},"144012":{"abstract":"This project is developing a new operating system, called Corey, intended to help systems software and applications achieve high performance on multicore systems. Current multiprocessor operating system kernels typically provide abstractions whose semantics implicitly require sharing data among processors, and which are implemented with shared data structures protected by locks. This approach may cause the kernel to be a bottleneck due to contention for data and locks, even for applications that do not require sharing semantics.<br\/><br\/>Corey is based on the principle that applications should control all sharing: all kernel data structures should be local to a processor core unless directed otherwise by the application. Corey exposes a new set of low-level abstractions (shares, address trees, and kernel cores) that allow applications to control all inter-core sharing. Corey takes advantage of the likely abundance of cores by allowing applications and kernel sub-systems to dedicate cores to handling specific functions and data. The intended result is that applications can tailor their use of processors and shared memory to achieve maximum performance.<br\/><br\/>These ideas are being evaluated by implementing applications such Web services and MapReduce processing, and comparing both ease of implementation and performance to implementations built for more traditional operating systems such as Linux. Corey will be released as open source, and specific successful techniques will be ported to Linux when possible. The ideas and software will be used in MIT's undergraduate operating systems course, to give students experience with multicore programming. At a higher level, the work is intended to help shrink the gap between available compute power and software's ability to harness it, potentially helping many users of computers.","title":"CSR-PSCE, SM: An Operating System for Multi-core Processors","awardID":"0834415","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["562011","541752"],"PO":["535244"]},"143286":{"abstract":"The rapid rise of overlay networks -- as for example, in the form of social networks and other peer-to-peer systems, sensor networks, or mobile ad hoc networks -- is revolutionizing the way we group and exchange information. However, not much is known about self-stabilization mechanisms for these highly dynamic networks. Minimum requirements for overlay network protocols to be useful in practice are that they be local, simple, and self-stabilizing. Locality is important for fast response times and for minimizing the impact of topology changes on the overlay network properties, simplicity is important so that the protocols can be used in a wide range of systems and for a formal verification of their effectiveness, and self-stabilization is important for automatic recovery from any illegal state since protocols requiring human intervention will not scale to systems potentially spanning millions of sites. <br\/><br\/>This project provides mechanisms that allow overlay networks to self-stabilize from an arbitrary connected state in an efficient and robust way. Moreover, our mechanisms will self-stabilize from an arbitrary state even under adversarial behavior of some of the nodes. Since overlay networks and self-stabilization are used in many contexts, this project benefits a number of research communities within and outside of computer science. Moreover, it consolidates strong international collaboration with the Tech. U. of Munich, Germany, while advancing education and enhancing diversity at Arizona State University.<br\/><br\/>This award is co-funded in part by NSF's Office of International Science and Engineering (OISE).","title":"Theory of Self-Stabilizing Overlay Networks","awardID":"0830704","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["563358"],"PO":["565251"]},"144023":{"abstract":"One of the main challenges in multi-core processor resource management is that<br\/>existing operating systems (either conventional OS for single core or OS for SMP) are<br\/>not able to effectively handle the new complexities in multi-core processors.<br\/>In order to address this challenge, the collaborators will conduct three closely related<br\/>projects. (1) A hybrid system design and implementation for OS-based cache<br\/>partitioning will provide efficient software management of shared caches with minimum<br\/>hardware complexity, and will well define hardware\/software interface of shared cache<br\/>management. (2) The collaborators will design and implement scheduling algorithms in<br\/>OS kernels to effectively allocate CPU, caches and memory bandwidth resources to<br\/>multiprogramming jobs in multi-core processors. (3) A data object locality-aware cache<br\/>partitioning design and implementation will distinguish the locality strengths of objects<br\/>and make effective cache allocation decisions.<br\/>The intellectual challenges of this project are threefold: (1) Hybrid system design<br\/>involves complex interactions between hardware and the underlined operating system,<br\/>and demands insightful understanding of existing system structures and innovation to<br\/>enhance both architecture and the OS kernels. (2) OS-based scheduling in multi-core<br\/>processors is a fundamental and complex problem in system research. (3) System<br\/>implementation of proposed algorithms for scheduling and object coordination demands<br\/>a lot of creative ideas for their seamless integration in the kernels. The broader impact<br\/>of this project is expected to be significant. Solutions to address critical issues for<br\/>significant performance improvement in multi-core processors are timely demanded in<br\/>many application areas. The research training to both undergraduate and graduate<br\/>students will address the concerns of lacking strong system professionals in IT and<br\/>computer industries.","title":"Collaborative Research: CSR-PSCE, TM: Effective Resource Sharing and Coordination inside Multicore Processors for High Throughput Computing","awardID":"0834476","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["486357"],"PO":["535244"]},"143055":{"abstract":"EMT\/QIT -- New Designs and Decoding Algorithms for Quantum Error Correction<br\/><br\/>Abstract<br\/>Jing Li (Tiffany)<br\/><br\/>Quantum computers will likely provide the ultimate cure for the explosive growth of information data and the increasingly pressing demand to solve complex and comprehensive problems (such as factoring and discrete problems). Of critical importance to quantum computation and communication is quantum error correction, the mathematical-physical mechanism that protests the fragile quantum states from unwanted evolutions, thus enabling robust implementation of quantum processing devices and reliable transmission over noisy quantum channels. Proof-of-concept and the first constructive quantum error correction code did not appear until 1995, and the research is still at its infancy. <br\/><br\/>This research entails serious theoretic and algorithmic study to advance the state of quantum error correction. New methods will be investigated to provide a refreshing angle to the systematic construction of better families of quantum stabilizer codes and especially unrestricted low-density parity-check (LDPC) stabilizer codes. New and efficient decoding algorithms will be designed to revive several existing stabilizer codes. New concepts and tools will be developed to analyze and evaluate quantum codes. The tools used in the research is highly mathematical-physical, and involve quantum mechanics, quantum information theory, linear algebra, Euclidean\/projective geometry, matrix theory, graph theory and convex optimization.","title":"EMt\/QIS New Designs and Algorithms for Quantum Error Correction","awardID":"0829888","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":["545094"],"PO":["565157"]},"143297":{"abstract":"Many aspects of computation can be viewed as communication processes. Communication complexity is the mathematical theory aimed at estimating the amount of communication necessary for such processes. Communication complexity arguments can be used to provide estimates on various other resources needed for computation, including time and space (memory) and circuit size. Communication complexity has many applications in different areas including computer networks, VLSI circuits, data structures, cryptography, learning theory and distributed computing.<br\/><br\/>This project focuses on exploring the connections between communication complexity and the complexity of computational problems in other models of computation. The main objectives of this research are developing new techniques for proving lower bounds on communication complexity, and using these methods to obtain lower bounds on resources in other models. The project addresses problems of randomized and multiparty communication complexity, private information retrieval, and estimating the space requirements of data stream algorithms.<br\/><br\/>Proving lower bounds on the complexity of specific functions with respect to various resources has been one of the most challenging areas in complexity theory. Communication complexity arguments and techniques originating from communication complexity have been at the core of several lower bound results that are at the boundary of what is achievable by current techniques. The project can potentially lead to new methods for attacking fundamental problems in complexity theory.","title":"Communication Complexity and Applications","awardID":"0830756","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":["451556"],"PO":["565157"]},"146333":{"abstract":"ABSTRACT (250 words):<br\/><br\/>NSF Long-Term Bridge Monitoring and Prognostication Workshop<br\/> <br\/>As the nation's infrastructure ages and as demands on the infrastructure intensify, it has become increasingly evident that there is an urgent need to be able to accurately assess the health and safety of bridge systems to ensure life safety and to prioritize the allocation of limited resources to retrofit and replace systems. There is increasing interest in incorporation of smart bridge technology within bridge systems to flag abnormalities that require immediate attention. Current systems have severe limitations associated with limited knowledge obtained from discrete sensors, transmission and storage of large amounts of information, interpretation of the data, and prioritization of decisions to be made based on the results. This workshop will bring together 20 participants and up to 20 observers from a broad range of disciplines with individual expertise in one or more of the following areas: sensors, networking\/communication, data interpretation\/decision making, with the purpose of identifying technical challenges and research needs regarding integration issues across these fields as well as research needs within the individual topic areas. The workshop will result in a document which will provide a clear list of priorities which can be used as a resource to inform the associated programs at NSF and broader research community of the short and long-term research needs and grand challenges to be addressed associated with bridge sensing and prognostication which might guide the cross-disciplinary resources required to support the effort. Particular emphasis will be placed on the research needs associated with the integration across the major topic areas.","title":"Bridge Monitoring and Prognostication Workshop; Minneapolis, MN; September 2008","awardID":"0844960","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["498443"],"PO":["561889"]},"144034":{"abstract":"Modern embedded critical systems are generally interactive and reactive, i.e. they are programs that interact with mechanical or electronic devices such as sensors and actuators. Errors in safety-critical and mission-critical programs (such as in the aviation arena) can be catastrophic. This research proposal identifies several areas in which the static analysis of critical systems using abstract interpretation can ensure the correctness of various aspects of critical systems. There are important issues in the analysis of critical systems for which abstract interpretation provides a promising approach. This research pursues analysis of liveness and responsiveness properties of critical systems. Most present-day static analyzers analyze safety properties, stating that no unwanted error can ever appear during any program execution. Recent work on termination has shown that abstract interpretation can also be useful in proving liveness properties like termination. Such liveness properties state that something good must eventually happen when the program is executed. In the context of reactive critical systems, an important liveness property is responsiveness: the program should react to an external event in a timely fashion. Abstract interpretation has proven to be a powerful method for verifying the safety properties of programs. On the other hand, temporal logic and its related methods provide ways for dealing with liveness properties of programs. An important aspect of the work proposed here is the integration of the two approaches in order to reason formally about a wide class of properties incorporating both safety and liveness. Furthermore, existing verification methods using abstraction are often restricted to finitary abstractions, i.e. abstractions into a finite domain. As another aspect of the proposed work, these verification methods will be extended to infinite domains. The research results are expected to have impact in a number of diverse areas. First, the new methodologies developed in the area of abstract interpretation as part of this research will provide the theoretical basis for future uses of static analysis to ensure the correctness of critical and embedded systems. Second, we anticipate the construction of software will be performed to extend the capability of current analyzers used in industry, such as the ASTR\u00c9E system, to check increasingly important properties ? such as liveness and responsiveness of critical systems. Additionally, the proposed research will involve students at both the graduate and senior undergraduate level. Finally, the techniques developed will become part of the topics covered graduate courses in abstraction interpretation and verification.","title":"CSR - EHCS(EHS), TM: Abstract Interpretation-Based Analysis and Verification for Critical Systems","awardID":"0834535","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["409234","417591","417592"],"PO":["561889"]},"144155":{"abstract":"Understanding the dynamical behavior of the Internet and other large networks is a daunting problem. Traditional approaches try to capture the Internet?s detailed behavior at a granular level, but then struggle with the system?s overwhelming complexity. At the other extreme, models that emphasize the large scale while ignoring fine details are more tractable, but frequently neglect features needed for practical engineering issues, especially when dynamical rather than static behaviors are an important concern. This research develops a happy medium through a novel fusion of analytical and computational techniques. Three important problems are explored. The first concerns the overall efficiency and stability of the Internet; the second develops methods for improving the functioning of sensor networks (large arrays of small sensors that can be used to monitor environmental changes); and the third focuses on improving the quality of large cooperative computing systems by providing incentives for users to behave in ways that benefit the whole.<br\/><br\/>The potentially transformative aspect of this research lies in its ?computational-analytic? approach to creating effective models for a wide range of complex networks. In this strategy, computational techniques are interlaced with analytical ones, using such methods as ?equation-free modeling?? from chemical engineering and numerical renormalization from statistical physics. The approach is radically interdisciplinary, with the power to unify and extend multi-scale analytical methods adapted from such diverse fields as dynamical systems theory, mathematical physics and theoretical biology. Spin-offs of the techniques created here are expected and will be investigated for other important problems involving complex networks, including routing on the Internet, synchronization in neural networks, and stability in global financial networks.","title":"CDI Type II: Complex Dynamics in the Internet: A Computational Analytic Approach","awardID":"0835706","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7751","name":"CDI TYPE II"}}],"PIcoPI":[384275,"516972","516972","562348","562348"],"PO":["565272"]},"143066":{"abstract":"Collaborative Research: EMT\/QIS:<br\/>Quantum Algorithms and Post-Quantum Cryptography<br\/>Project Summary<br\/><br\/>Cryptography is the basic infrastructural element enabling privacy and <br\/>security for electronic transactions. However, when a large-scale <br\/>quantum computer is finally built, it will force us to abandon <br\/>established methods of cryptography, such as RSA and Diffie-Hellman, <br\/>which are in common use today. The proposed research will further <br\/>this line of disruptive quantum algorithmic research; but it also aims <br\/>to erect a new framework of secure post-quantum cryptography, in order <br\/>to maintain this societally critical infrastructure.<br\/><br\/>The most attractive approach for salvaging modern cryptography would <br\/>be to develop classical cryptosystems for which we have compelling <br\/>evidence of security even in the face of quantum adversaries. Recent <br\/>work by the PIs and their collaborators has shown that certain <br\/>algebraic problems possess hardness properties relevant even for <br\/>quantum algorithms. We propose to strengthen and leverage these <br\/>results in order to develop cryptographic schemes which can be carried <br\/>out by today's computers, but which will remain secure even against <br\/>quantum attack in the future.<br\/><br\/>In tandem with this effort, we propose to develop new quantum <br\/>algorithms for breaking cryptosystems based on conjugacy in the braid <br\/>group. This is one of the few remaining classical cryptosystems which <br\/>has not already been shown to be vulnerable to quantum attack.<br\/><br\/>Our research program is also directly integrated with graduate student <br\/>training at all four institutions, undergraduate educational <br\/>innovation, educational outreach, and broad scientific dissemination.","title":"Collaborative Research: EMT\/QIS: Quantum Algorithms and Post-Quantum Cryptography","awardID":"0829909","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["550549"],"PO":["565157"]},"144045":{"abstract":"Serious new technical challenges are barriers to advances in microelectronics technology as technology scaling comes up against fundamental limits of material properties and lithography. Large process variations and other random performance and power constraining imperfections are now being observed as microelectronic devices are scaled down to atomic dimensions. This requires that module level performance in electronic designs be pessimistically guardbanded to ensure proper overall system level functionality, forcing systems to operate at performance levels far below the inherent capability of the underlying design fabric. In addition, embedded DSP systems must be designed to work under worst case operating conditions resulting from ill-conditioned input signals. Wider guardbands from increasing performance and input signal variability in future technologies can negate most of the performance benefits of scaling, stalling a key payoff from Moore?s Law for embedded systems. In such an environment, introduction of new scaled devices will be cost-effective only if the guardbands can be controlled down to acceptable margins, despite the presence of these uncertainties. This remains a major unsolved challenge, especially for embedded DSP processors that must be concurrently optimized for system level power, performance and reliability. To address these problems, this project is developing the concept of test, diagnosis and continuous signal monitoring enabled dynamic circuit-architecture-algorithm co-modulation (or co-tuning) for both static (procees) and dynamic (input signal) uncertainties. Under this new design paradigm, feedback driven reconfiguration control mechanisms involving circuitry and software (?tuning knobs?) are designed into the IC to support power-performance trade-off and reliability recovery post manufacture. The research pursues vertically integrated circuit-architecture-algorithm tuning methods that offer 10X benefits over optimizations performed at a single level of the design hierarchy. The diagnostic information generated is used to dynamically optimize (post-manufacture) individual module level behavior to optimize system level performance, power and reliability metrics via specially designed hardware and software control mechanisms. In this way, each instantiation of a design adapts to the maximum performance, power, and reliability levels it is capable of in the presence of process variations and adverse operating conditions. <br\/><br\/>Graduate students working on the project receive a unique kind of training in this multidisciplinary research, which together the fields of digital design and test, control systems, embedded digital signal processing architectures, and algorithms. The students participate in summer internship programs with industry. Through joint efforts at Georgia Tech, Auburn University, and Tuskegee University, this project also actively supports the goals of recruiting more U.S. citizens, women and minorities to graduate programs.","title":"EHCS: Dynamic Vertically Integrated Power-Performance-Reliability Modulation in Embedded Digital Signal Processors","awardID":"0834620","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550443"],"PO":["561889"]},"143077":{"abstract":"Theoretical approaches to quantum computing frequently model the state of a<br\/>quantum system as something that changes discretely in time. However, an<br\/>unmeasured quantum system fundamentally evolves continuously in time as<br\/>described by Schroedinger's equation. Even the quantum measurement process<br\/>can be described by a continuous-time stochastic differential equation.<br\/>This research explores quantum computation in its complete continuous-time<br\/>formulation, with the goal of uncovering new paradigms for processing<br\/>quantum information.<br\/><br\/>The research focuses on three key areas: continuous-time quantum computing<br\/>architectures, continuous-time quantum error correction, and continuous-time<br\/>quantum algorithms. These areas were selected because examples highlighting<br\/>the differences between continuous-time and discrete-time quantum computing<br\/>formulations are already known in each of them. For example,<br\/>continuous-time quantum walk architectures require no external dynamical<br\/>controls, continuous-time quantum feedback can mediate quantum error<br\/>correction, and continuous-time quantum scattering can calculate Boolean<br\/>formulas with fewer queries than any classical algorithm can. A major<br\/>objective of this research is to expand upon these directions and develop<br\/>new ones. The strategy will be to focus on problems that are sufficiently<br\/>well-defined and tractable that progress can be made.<br\/><br\/>This project not only supports graduate student training in quantum<br\/>information science but also supports education and outreach activities<br\/>through the Southwest Quantum Information and Technology (SQuInT) Network.<br\/>The PI will seek out exceptional students outside his home institution,<br\/>especially encouraging those in underrepresented groups, and sponsor their<br\/>attendance at SQuInT meetings to further enrich this growing field.","title":"EMT\/QIS: Continuous-time quantum computation","awardID":"0829944","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[380945],"PO":["565223"]},"143198":{"abstract":"NUMBER: 0830403<br\/>INSTITUTION: University of California-Irvine<br\/>PI: Goodrich, Michael T. & Eppstein, David A.<br\/>TITLE: Collaborative Research: Algorithms for Graphs on Surfaces<br\/><br\/>Collaborative with:<br\/>NUMBER: 0830149<br\/>INSTITUTION: Brown University<br\/>PI: Tamassia, Roberto<br\/>TITLE: Collaborative Research: Algorithms for Graphs on Surfaces<br\/><br\/>ABSTRACT<br\/>This project addresses fundamental questions on geometric and spatial aspects of graphs and networks, with applications to road networks, sensor networks, computer networks, and social networks. In particular, methods will be developed for constructing effective geometric layouts of networks on surfaces and in three-dimensional space and for analyzing properties of networks by exploiting their geometric structure. <br\/><br\/>The focus of the project is the design and analysis of algorithms for graphs on surfaces in the following three thrust areas: (1) algorithms for embedding graphs on surfaces, including methods for greedy embeddings of graphs to facilitate geometric routing, algorithms for manifold triangulation for a set of points sampled from an embedded surface, and techniques for drawing trees in the plane; (2) algorithms for graphs embedded on surfaces, including the study of connections between partial cubes and integer lattices, the development of algorithms for graphs embedded in non-Euclidean spaces, and the design of methods for solving graph problems on quadrilateral meshes; (3) applications of algorithms for graphs on surfaces, including applications of geometric graphs to computer security and algorithms for road networks.","title":"Collaborative Research: Algorithms for Graphs on Surfaces","awardID":"0830403","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["521484","517727"],"PO":["565157"]},"144056":{"abstract":"Recording and deterministically replaying execution gives system designers the ability to travel backward in time. Time travel in computer systems means the ability to recreate arbitrary past states and events on the computer system. In general, recreating past states and events is achieved by logging key events when the software runs, then restoring to a previous checkpoint and replaying the recorded log to force the software down the same execution path deterministically. This alluring mechanism has enabled a wide range of applications in modern systems including debugging programs, performing post-hoc security analysis, and improving fault tolerance. To maximize effectiveness, replay systems should (i) record at production-run speeds, (ii) keep logging requirements minute, (iii) replay at a speed similar to that of the initial execution. Software-only systems for deterministic replay achieve these goals for uni-processor systems, but suffer from poor performance on multiprocessor systems. Hardware-only systems record and replay efficiently on multiprocessor systems, but current proposals for hardware-only systems are largely impractical since they place too much functionality within the hardware and because they do not mix recording, replaying, and traditional execution on the same system concurrently. This research will focus on the design and implementation of Capo, a hybrid hardware\/software replay system that will record and replay execution on multiprocessor systems efficiently. The key contribution of this research will be designing and implementing the first hardware\/software interface for combining hardware and software level replay systems. This interface will serve as the foundation for a new generation of replay systems that will achieve both the flexibility of software-level replay systems and the efficiency of hardware-level replay systems.<br\/><br\/>If successful, this research will have substantial impact on industry, by enabling the effective use of deterministic replay of parallel codes. The team will release all software artifacts as open source, which will help researchers and educators in many institutions.<br\/><br\/>In addition to their research contributions, this team will enhance a series of courses and expand their course offerings in parallel systems - especially at the undergraduate level. The PIs have a long-standing commitment to undergraduate education, routinely involving undergraduates in their research and exposing them to parallel software and hardware. The PIs will continue to involve undergraduate and graduate students in their research groups. This project is compelling to undergraduates because it involves the interaction of two different system layers.","title":"CSR-PSCE, SM: Recording and Deterministically Replaying Shared-memory Multiprocessor Execution Efficiently","awardID":"0834738","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["485665","548692"],"PO":["565255"]},"144067":{"abstract":"The future of information technology industry depends on designing computer systems that are tolerant of errors caused by variations in device characteristics. Traditionally system reliability is achieved by replicating critical system components. Since variability induced errors occur slowly over time, replication for the sole purpose of providing reliability is prohibitively expensive for low cost computing platforms. This research explores using 3D stacking to implement redundant components and variability monitoring circuitry on a 3D stacked die. Using 3D stacking the redundant computation blocks can be built using a variation resilient process technology that may be slower than the process technology used for building the primary processor. This research takes a holistic approach to designing the 3D stacked monitoring spanning from innovative microarchitecture solutions to exploiting application's inherent error tolerance. On the microarchitecture front, this research explores the potential for seamlessly reconfiguring the monitoring layer to act in three modes: performance assists, when variability induced errors are rare, or as guard processors, when variability induced errors begin to appear, or as backup processors, when device aging may result in irreparable errors on the primary processing substrate. On the architecture front, this research explores a new exception class called Reliability Aware Exceptions that allow microarchitecture blocks to raise an exception in response to a variability induced error. These software visible exceptions can then be exploited by application classes that are inherently error tolerant and can customized exception handling mechanisms.","title":"CSR-PSCE,SM: A Holistic Design Approach to Reliability Using 3D Stacked","awardID":"0834798","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["518642"],"PO":["535244"]},"137654":{"abstract":"Abstract<br\/><br\/><br\/>IIS - 0803410<br\/>Tools to Mine and Index Trajectories of Physical Artifacts<br\/>Keogh, Eamonn<br\/>University of California-Riverside<br\/><br\/><br\/>The project proposes to develop computational methods and tools for the discovery of spatio-temporal patterns in the distribution and historical development of physical artifacts important to anthropology including the development of a shape recognition system that allows researchers to compare numerous projectile points and petroglyphs according to several criteria. This will involve creation of a set of definitions\/ data representations\/predicates\/algorithms and intuitive and usable software tools to enable the study of the spatio-temporal spread of physical objects. The proposal uses innovative technology to apply to questions central to archaeology. but which also has broad applicability to research in other, diverse domans.","title":"Tools to Mine and Index Trajectories of Physical Artifacts","awardID":"0803410","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["543515","430635","508296",365895],"PO":["543481"]},"146014":{"abstract":"This is funding to support a research conference to develop guidelines for 21st Century pedagogical middleware: an integrated set of conceptual, methodological, and technological tools for teaching complex innovative and creative thinking in a digital world. The focus area of interest will be to model the sustainabilty of urban areas, particularly in very large or megacities. The conference will be held in Arizona so that researchers can parter with NSF's Long Term Ecologial Research Network (LTER) at Arizona State. Select expert researchers will participate, drawing from the following interdisciplinary fields: (a) theories of learning, (b) technology, (c) virtual learning organizations, (d) research methodology, and (e) measurement. For each theme, a lead discussant will prepare working documents prior to the meeting, lead discussions during the meeting, and prepare the appropriate section of the resulting white paper. <br\/><br\/>Funding for this initiative is important because it is address the problem of how to best guide research and development of technologically complex learning communities and organizations. This colloquium is innovative in its goal to link theories of how people think creatively in cross-functional teams to inform the design of collaborative technologies. Further, by considering assessment throughout the process, the resulting integrative framework (whitepaper) will be well-grounded and useful to a wide interdisciplinary audience.","title":"Distributed Learning and Collaboration: Pedagogical Middleware and Computing","awardID":"0843651","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["125813","562148"],"PO":["565227"]},"139601":{"abstract":"This project addresses the task of automatically detecting the influence structure and flow of ideas in document corpora that have grown over time (e.g. scientific literature, political debates, news, email, wikis, blogs) in order to trace the origin and development of ideas over time. The intellectual merit of this project lies in the development of statistically well-founded methods for discovering and analyzing the influence structure in evolving archives. In particular, this project will focus on the development of statistical tests for two relations that are central to understanding the structure of an archive and how ideas developed - namely originality and influence. The ability to detect influence and origin of ideas will be of substantial help in understanding, exploring, interpreting, visualizing, and aggregating the rapidly growing body of historical text available online. The project will also evaluate in how these methods augment traditional citation analysis in hyperlinked collections, and whether they allow similar functionality even in non-hyperlinked archives. The new capability will benefit a number of widely used applications, including search engines for internet content.","title":"III-COR:Small: Information Genealogy","awardID":"0812091","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["531625"],"PO":["563751"]},"139612":{"abstract":"Image data is of immense practical importance in medical informatics, and a subject of strong interest to researchers in industry and academia. While digital image databases are now prevalent in clinical and educational settings, and traditional means for interacting with and querying such collections can provide some level of useful functionality, there are few examples of systems that attempt to bridge the ?semantic gap.? The work proposed in this grant is a multi-institutional collaboration combining research in medical image processing, machine learning and pattern recognition, knowledge representation and querying, and evaluation by domain experts in the field, is intended to advance the state-of-the-art in this direction. The archive of 60,000 cervigram images assembled by the National Library of Medicine and National Cancer Institute is an ideal collection for this purpose. The NLM cervigram archive forms a narrow image domain that has a limited and predictable variability. In such cases, explicit representation of domain knowledge alleviates the semantic gap between the low-level sensory recordings of a scene (raw image data), and objects and processes implied from images (semantic interpretation). This research will follow an information hierarchy that proceeds from raw image data to low-level image features, recognition of objects and tissue types, knowledge-based reasoning about disease processes, and, finally, tools and visualizations to support diagnosis decisions by clinical and NLM\/NCI collaborators. The research team will employ an underlying paradigm known as Computer-Assisted Visual Interactive Recognition, or CAVIAR, which considers the domain expert an integral part of the equation and attempts to optimize the performance of the complete human-machine system. <br\/><br\/>Intellectual Merit <br\/><br\/>Image content understanding is still considered a vexing open problem at the same time databases are growing rapidly in size and complexity. It is anticipated that this work will have a positive impact in areas relating to medical image analysis, including information extraction, organization, representation, and querying, as well as in training. <br\/><br\/>Broader Impact <br\/><br\/>Through the focus on the NLM\/NCI cervigram archive, this research may help advance the role of cervicography as a more cost-effective procedure than pap smears and colposcopy in screening for cervical cancer. Results from this targeted-domain project could also illuminate gaps and help establish new priorities for research in broader domains such as multimedia content structuring, understanding, indexing, and retrieval.","title":"III-CXT-Small: Collaborative Research: Structuring, Reasoning, and Querying","awardID":"0812124","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[371109],"PO":["565136"]},"147235":{"abstract":"Objectives of the Project<br\/>We propose a workshop for brainstorming and planning for the Computational Thinking Olympiad (CTO). The goal of this olympiad is to increase the awareness of Computational Thinking at the high school level.<br\/>The proposed workshop will be focused on deciding on the desired content of the Olympiad. We have generated (though an NSF-sponsored REU supplemental grant) a private Wiki that contains lists of example problems from related programs, such as COMAP, ACM Programming Contest, ICPC and IOI. The Wiki contains over 40 example problems. In this proposed workshop, we will<br\/>discuss these related programs, determine the niche into which the CTO would t, and determine what sorts of problems t best therein.<br\/>At the conclusion of the workshop, our aim is to have de ned the sort of problem to be solved at the CTO, decided the roll of programming projects in the CTO, and placed ourselves within the context of other, related, Olympiads.","title":"Computational Thinking Olympiad: Brainstorming Workshop","awardID":"0848473","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550878"],"PO":["565215"]},"139634":{"abstract":"The goal of this project is to develop a declarative language and system for specifying, implementing, analyzing and auditing large-scale secure information systems. The project unifies three bodies of work: logic-based trust management languages, database query languages for declarative networks, and database techniques for analyzing data computations via the concept of provenance or lineage. The first part of the project investigates the design of the Secure Network Datalog (SeNDlog) language that unifies the declarative networking and logic-based access control languages. A declarative networking system with security extensions is developed to execute SeNDlog programs to implement a variety of secure networked information systems. The second part of the project applies the use of data provenance in networks for performing real-time network diagnostics and forensics. This project studies optimization techniques for computing network provenance efficiently, and analyzes opportunities presented in the unified framework across network protocols, security policies, and distributed information systems. The broader impact of this work is a unified extensible platform for developing and analyzing next-generation secure information-centric networks, including authenticated Internet-scale routing infrastructures, scalable content-based networks, and secure distributed data management systems. A graduate course titled Networking-meets-Databases is created to explore synergistic topics at the intersection of these two areas, drawing practical experiences and use cases from the project. Publications, technical reports, software and experimental data from this research are<br\/>disseminated via the project web site (http:\/\/www.cis.upenn.edu\/~boonloo\/decsec\/).","title":"NGNI-Small: Declarative Secure Networked Information Systems","awardID":"0812270","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["518109"],"PO":["565136"]},"137698":{"abstract":"We are entering an Industrial Revolution in the production of information. While in the past data was \"handmade\" by typing on keyboards, today data are increasingly manufactured by machines: sensors, cameras, software logs, etc. When harnessed in a timely manner, these data can have significant positive impact in many contexts, including early warning and rapid response in natural disasters, air quality monitoring, and improved Internet security. To provide useful information in these contexts, computers in multiple locations must coordinate over networks, because the data are both widely distributed and massive, and cannot be \"warehoused\" at a single location in a timely manner. Worse, sensor data is typical \"noisy\" or erroneous in various ways, so statistical methods must be employed to convert the raw \"evidence\" data into probabilistically reliable information.<br\/><br\/>In this project we develop new techniques to integrate statistical inference methods from AI with overlay network algorithms developed for peer-to-peer and wireless settings. We design new overlay network algorithms customized for distributed inference. We also develop network-aware inference algorithms that can trade off inference approximation quality for communication efficiency and robustness to network failure. Finally, we explore the use of a high-level declarative language for programming both the networking and inference logic. The high-level language enables us to investigate compilation techniques to co-optimize the inference and overlay network tasks for maximal utility. We prototype and evaluate our ideas via open-source implementations deployed on testbeds like Emulab and Planetlab. Software and research papers are disseminated at http:\/\/declarativity.net.","title":"NGNI-Medium: Collaborative Research: MUNDO: Managing Uncertainty in Networks with Declarative Overlays","awardID":"0803690","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["451018"],"PO":["543481"]},"139645":{"abstract":"The goal of this project is to develop a stream processing system that captures data uncertainty from data collection to query processing to final result generation. This project focuses on data that is naturally modeled as continuous random variables. For such data, it employs a principled approach grounded in probability and statistical theory to capture data uncertainty and integrates this approach into high-volume stream processing. The first contribution of the project is to capture uncertainty of raw data streams from sensing devices. Since the raw streams may not present data in a format suitable for query processing and can be highly noisy, this project employs probabilistic models of the underlying data generation process and machine learning techniques to efficiently transform raw data into a desired representation with an uncertainty metric. The second contribution is to capture uncertainty as data propagates through various query operators. To efficiently quantify result uncertainty of a query operator, this project explores various techniques based on probability and statistical theory to reduce statistics that input streams need to carry and to expedite the computation of result distributions. This project integrates research and education through curriculum development and enables broader participation of women in research through college outreach and CRA's distributed mentor program. This project also includes software release and real-world deployments in domains such as object tracking and monitoring and hazardous weather monitoring, resulting in significant scientific and social impacts. Results of this project are disseminated at the project web site (http:\/\/avid.cs.umass.edu\/projects\/uncertain-streams\/).","title":"III-COR-Small: Capturing Data Uncertainty in High-Volume Stream Processing","awardID":"0812347","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["518346","518347"],"PO":["565136"]},"139656":{"abstract":"Last Modified Date: 07\/23\/08 Last Modified By: Tatiana D. Korelsky <br\/><br\/>Abstract <br\/>In crisis response domains, emotional manifestations are very complex and extreme emotions are common. While speech technologies have shown significant progress over the years, recognizing and understanding emotional speech in noisy environments is still a big challenge. Understanding such language is daunting given fragmented and ungrammatical utterances in addition to errors <br\/>from automatic speech recognition (ASR). Furthermore, there is little <br\/>research on the analysis of the relationship between emotion detection and language understanding which have traditionally been viewed as parallel independent tasks. Even when the output of one of these tasks is used as an input feature to the other, typically, during training a \"true\" classification is used instead of the \"estimated\" classes (as would be the case if the system were to be used in a real-setting) resulting in a mismatch and degraded performance. This project attempts to overcome such a limitation of current approaches by focusing on analyzing the degree of the dependencies between emotion and intent and investigating joint classification methods via multitask learning for language understanding and emotion detection tasks. <br\/><br\/><br\/>The primary intellectual merit of the project is integrated research on <br\/>developing an end-to-end information processing system that has the potential to very significantly impact the crisis response process. For speech processing, communucation between individuals and emergency dispatch personnel <br\/>as well as communications during responders during response pose a big challenge since callers are typically very emotional and the language used reflects that. Processing such speech requires significant research, and this project can is a first step toward this genre.","title":"RI-Small: Collaborative Research: Dispatcher's Assistant for Emergency First Response","awardID":"0812440","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["434792"],"PO":["565215"]},"137599":{"abstract":"Participants in human-human conversation often entrain to one another, adopting the<br\/>vocabulary and other behaviors of their partners.<br\/>Evidence of this has been found from laboratory studies and observations of real life<br\/>situations. We are investigating many types of entrainment in two large corpora of<br\/>human-human conversations to improve system behavior in Spoken Dialogue Systems<br\/>(SDS). We want to discover which types of entrainment occur generally across<br\/>speakers and which seem to be speaker-specific, which types of entrainment can be<br\/>reliably linked to task success and perceived naturalness, and which types of<br\/>entrainment can be automatically modeled in SDS.<br\/>Our research has importance for the construction of better SDS.<br\/>Currently, research SDS have attempted to entrain users to system vocabularies to<br\/>improve speech recognition accuracy: Since users are likely to employ the same<br\/>vocabulary in their answers that systems use in their queries, systems have a better<br\/>chance of recognizing user input correctly if they can predict word usage. However,<br\/>there has been little attempt to create SDS that entrain to user behavior, despite<br\/>evidence that human beings rate humans and systems that behave more like them<br\/>more highly than those that do not. Our work focuses on determining which types of<br\/>system entrainment to users will be most important to users and most feasible for SDS.<br\/>Our results will be disseminated through papers and presentations at speech and<br\/>language conferences. We will also provide publicly available annotated corpora for<br\/>future research by others.","title":"RI-Medium: Collaborative Research : Corpus-based Studies of Lexical, Acoustic, And Discourse Entrainment in Spoken Dialogue","awardID":"0803159","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["519134"],"PO":["565215"]},"147059":{"abstract":"The Workshop on GeoSpatial and GeoTemporal Informatics (GSTI'08) focuses on geo-spatial and geo-temporal informatics. The motivation for the organization of this workshop comes from the substantial challenges posed to traditional geoinformatics by the emergence of novel technological devices, artifacts and methodologies, including (but not limited to) sensors and sensor deployment techniques (e.g., geosensor networks, video cameras on-board unmanned aerial vehicles, hand-held devices and more). These innovations represent the rapid evolution of the traditional static and centralized geocomputational paradigm. They are drastically changing the types of data that are collected, analyzed, and disseminated, they are broadening the applications in which these datasets are used, and they are also expanding the corresponding user community. <br\/><br\/>In order to address this evolution and its impact on the future of science and engineering research and ultimately on the needs of society, the workshop will bring together a number of leading researchers from the US and overseas. The objective is to identify the emerging major challenges in geospatial and geotemporal informatics, and to make relevant recommendations to the scientific community, anticipating future needs and demands. <br\/><br\/>The research challenges addressed by GSTI'08 extend the frontiers of traditional geospatial research, addressing computer science issues such as spatiotemporal queries over diverse data feeds, mobile computing, location-based services, streaming data integration and mining, automated updating of geospatial databases, VR modeling, and computer vision. <br\/><br\/>The GSTI'08 Workshop is to be held in the Washington, DC metropolitan area in the fall of 2008. The workshop is expected to have significant impact by identifying research areas that need to be pursued in order to support and respond to emerging and predicted societal needs. The workshop results, including a report, will be widely disseminated via the Web site (http:\/\/www.ggs.cos.gmu.edu\/workshops\/GSTI-08\/) and other venues.","title":"Workshop on GeoSpatial and GeoTemporal Informatics","awardID":"0847881","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[391834],"PO":["563751"]},"139569":{"abstract":"Project Abstract<br\/><br\/>Multi-core (parallel) processors are becoming ubiquitous. The use of such systems is key to science, engineering, finance, and other major areas of the economy. However, increased applications performance on such systems can only be achieved with advances in mapping such applications to multi-core machines. This task is made more difficult by the presence of complex memory organizations which is perhaps the key bottleneck to efficient execution, and which was not previously addressed effectively. This research involves making the mapping of the program to the machine aware of the complexities of the memory-hierarchy in all phases of the compilation process. This will ensure a good fit between the application code and the actual machine and thereby guarantee much more effective utilization of the hardware (and thus efficient\/fast execution) than was previously possible. <br\/><br\/>Modern processors (multi-cores) employ increasingly complex memory hierarchies. Management of such hierarchies is becoming critical to the overall success of the compilation process since effective utilization of the memory hierarchy dominates overall performance. This research develops a new cache-hierarchy-aware compilation and runtime system (i.e., including compilation, scheduling, and static\/dynamic processor mapping of parallel programs). These tasks have one thing in common: they all need accurate estimates of data element (iteration, task) computation and memory access times which are currently beyond the (cache-oblivious) state-of-the-art. This research thus develops new techniques for iteration space partitioning, scheduling, and synchronization which capture the variability due to cache, memory, and conditional statement behavior and their interaction. This research will have a broad impact on the computer industry as it will allow the ubiquitous multi-core systems of the future to be efficiently exploited by parallel programs.","title":"CPA-CPL: Cache-Aware Synchronization and Scheduling of Data-Parallel Programs for Multi-Core Processors","awardID":"0811882","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["532654","532656"],"PO":["565272"]},"150151":{"abstract":"As separate sovereign governments, Indian tribes, just like state and local governments, have an obligation to improve the lives of their citizens. However, tribal governments are much more limited in their ability to access capital. Many tribal communities are often burdened with extremely low socio-economic factors, including low educational achievement, high unemployment, high poverty, and low per capita income. Upwards of $50 billion of unmet capital needs go unfunded each year in Indian Country, in large part due to impediments to tribal access to the capital markets. There are numerous reasons tribes are at a disadvantage, but one little studied is information asymmetry. Relatively poor market awareness is both a cause of tribal underdevelopment and an impediment to any solution. Because most tribes confront information asymmetries for any debt financing strategy they might consider, an empirical study of successful information sharing and integration strategies for tax-exempt bonds will help identify suitable strategies to induce sharing in other domains where tribes operate at an informational disadvantage.<br\/><br\/>Based on preliminary work (IIS 0534905), this Tribal Finance Information Clearinghouse (TFIC) project will make three unique contributions to information science. First it will collect, aggregate, analyze, and disseminate new digital content consisting of access to market-actionable data on tribal tax-exempt bonds that is not currently available. Second, it will generate data about the factors that impede or induce contributions of private information to a collective information good. (Broadly, insight into how to design information systems that resolve the social dilemmas of information collaboration is an increasingly important.) Finally, the TFIC will be an exercise in informational empowerment. By presenting original financial data as part of a larger infrastructure that includes updated content regarding regulatory, legal, and legislative changes, industry news, and a forum to contribute lessons learned, the TFIC will add significant informational value to transactional data. The TFIC will also support multidisciplinary research on the design and use of information technologies and the resulting impact on government institutions and citizens.<br\/><br\/>Broader Impact<br\/>Without empirical data about tribal interactions with capital markets, many research questions remain unanswerable. With this project, for the first time, these may be answerable as the TFIC will offer researchers and policy-makers an unparalleled opportunity to analyze a complete data set related to a significant minority group and to determine whether information asymmetries, legal regulations, or other variables impact capital markets access for tribes. Not only will the outcomes directly impact Native Americans, but the results may be generalizable to other minority enterprises that also face impeded market access. The relatively small number of actors (i.e., 88 tribal issuers since 2001) with durable presence in the data makes study of capital access questions more manageable than might be the case with non-tribal entities.<br\/><br\/>The TFIC may also impact the marketplace itself, given that a recent study of non-tribal tax-exempt bonds found that the \"most critical consequence\" of information asymmetry is higher borrowing costs, but that \"alleviating information asymmetry\" reliably eliminates the risk premium and reduces borrowing costs, particularly for new or infrequent issuers such as tribes.","title":"HCC: Digital Tribal Government: The Tribal Finance Information Clearinghouse (TFIC)","awardID":"0902426","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[400168],"PO":["565342"]},"143782":{"abstract":"Most of the traditional HEC applications and current petascale applications are written using the Message Passing Interface (MPI) programming model. The MPI-1 standard provides communication semantics for two-sided operations. The MPI-2 standard added new one-sided communication semantics. However, most of the current candidate petascale applications continue to use the MPI-1 semantics.<br\/>These applications find the available MPI one-sided communication semantics and their implementations in existing MPI-2 libraries very restrictive to exploit performance, scalability and fault-tolerance. The investigators, involving computer scientists from Ohio State University (OSU) and computational scientists from Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), will study and analyze the current restrictions in the MPI one-sided communication semantics, their implementations and usages. Novel solutions will be proposed to alleviate these restrictions so that the next generation ultra-scale systems and applications can be scaled to hundreds of thousands of cores.<br\/><br\/>The investigators will specifically address the following challenges: 1) What are the limitations of using MPI one-sided operations in petascale applications? 2) What extensions are possible to the current MPI one-sided operations to alleviate such limitations? 3) How to design and implement these extensions in an MPI library for emerging ultra-scale HEC systems? 4) How to redesign petascale applications to take advantage of proposed one-sided extensions and their implementations? and 5) What kind of benefits (performance, scalability and fault tolerance) can be achieved by the proposed extensions for petascale applications on the next generation ultra-scale systems? The research will be driven by a set of petascale applications (ENZO, AWM-Olsen, PSDNS and MPCUGLES) from established NSF computational science researchers running large scale simulations on the TACC Ranger and other NSF HEC systems.","title":"Collaborative Research: Extending One-Sided Communication MPI Programming Model for Next -Generation Ultra-Scale HEC","awardID":"0833139","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["515803"],"PO":["565272"]},"143430":{"abstract":"When systems rely on a \"human in the loop\" to carry out a security-critical function, cyber trust indicators are often employed to communicate when and how to perform that function. Indicators typically serve as warnings or status indicators that communicate information, remind users of information previously communicated, and influence behavior. They include a variety of security- and privacy-related symbols in the operating system status bar or browser chrome, pop-up alerts, security control panels, or symbols embedded in web content. However, a growing body of literature has found the effectiveness of many of these indicators to be rather disappointing. This research is systematically studying the effectiveness of cyber trust indicators and developing approaches to making these indicators most effective and usable. The researchers are using cognitive psychology's \"mental models\" approach to study how both expert and non-expert personal computer users understand common cyber trust indicators. They are also using the \"Communication-Human Information Processing\" (C-HIP) model from warnings science to structure their evaluation and provide insights into the human information processing steps at which a warning is mostly likely to fail. Using an iterative design process, they are developing improved indicators for several common applications and evaluating the effectiveness of these indicators. The expected results include a set of specific recommendations for improving a set of common cyber trust indicators, a set of design patterns for designing effective cyber trust indicators, and a curriculum module for teaching students how to design effective and usable cyber trust indicators.","title":"CT-ISG: Usable Cyber Trust Indicators","awardID":"0831428","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["553878","423630"],"PO":["543481"]},"143793":{"abstract":"The High End Computing (HEC) field is at a major crossroads due to the advent of many-core technology --- integration of tens and hundred processors (cores) on a single chip, heading to 1000 cores per chip in Exascale systems in the 2015-2020 timeframe. Unlike previous generations of hardware evolution, this shift in the hardware road-map will have a profound impact on future HEC software. First, the programmer will be faced with the scalability challenge of expressing and managing parallelism at the scale of millions to one billion threads in a single system. Second, the programmer will be faced with the locality challenge of optimizing data movement in a highly non-uniform system structure with order-of-magnitude gaps in bandwidth and latency between adjacent levels of the memory hierarchy.<br\/><br\/>The specific focus of this project is on developing programming models, compilers, and runtimes to address the above challenges of future HEC systems, guided by a specific many-core architecture to ensure practicality. The research will deliver results in the following areas: 1) new parallel programming constructs for many-core architectures such as asynchronous activities, places, and phasers that build on past experiences with the X10 language, but will be manifested in C\/C++ instead of Java in this research; 2) new parallel intermediate representations (PIR?s) and compiler optimizations for parallelism and locality; 3) a new thread virtual machine with two levels of parallelism, Synchronous Coarse-Grain Threads (SCTs) and Asynchronous Fine-Grain Threads (AFTs); and 4) evaluation of the programming model, compiler, and runtime on a Cyclops C64 many-core system that directly exemplifies the parallelism and locality challenges facing future HEC systems.","title":"Collaborative Research: Programming Models, Compilers, and Runtimes for High-End Computing on Manycore Processors","awardID":"0833166","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["557772"],"PO":["565272"]},"146840":{"abstract":"Situational awareness is increasingly reliant on unmanned vehicle teams to provide information by constant wide area surveillance. Issues of how to best coordinate the actions of many autonomous agents, individually or in teams, in order to accomplish the search goal of complete and timely coverage are essential for the robust information. Unfortunately, the application of robot teams in coverage tasks has many unaddressed facets of complexity. Specifically, the propagation constraints of networks limit the ability for robot teams to facilitate cooperation using digital networks. In this project, we investigate the cooperation problem with a focus on modeling and experimentation in the presence of real world network constraints. This research project aims to use bio-inspired cooperation paradigms to: 1) improve large-scale search and surveillance by actively predicting and managing network constraints based on role assignment, proximity-based sub-team formation, and communication through observation, 2) study of message age and impact upon performance, and 3) study of proximity to message importance. It is expected that results will be in line with previous work that shows that less communication can result in better cooperation in certain areas and team sizes.","title":"SGER: Impact of Intermittent Networks on Team-based Coverage","awardID":"0846976","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["542301"],"PO":["403839"]},"145630":{"abstract":"ATIP has reported on HPC activities for more than a decade and is well known throughout the Asian S&T community. ATIP has already conducted three China HPC Workshops. The current proposal is for support for the first ATIP workshop on HPC in India. As distinct from China, India has an established reputation for software development, both as an outsourcing site as well as for indigenous software for large applications.<br\/>The First ATIP Workshop on HPC in India will include a significant set of presentations, posters, and panels from a delegation of Indian academic, research laboratory, industry experts, and graduate students, addressing topics that include Government Plans, university Research, Infrastructure, and industrial applications. This component will include both Indian and international vendors to get their perspectives on the status of computing in India. Another important component will be a panel in which panelists and the audience identify topics that are suitable for collaborative (US-India) research and the mechanisms for developing those collaborations. A key aspect of the proposed Workshop will be the unique opportunity for members of the US research community to interact and have direct discussions with top Indian scientists who will travel to the US to participate. A specific goal of the Workshop is to motivate the preparation of joint research proposals with researchers from both US and India.<br\/>This activity is jointly funded by CISE\/CCF and the Office of International Science and Engineering.","title":"ATIP First Workshop on High Performance Computing (HPC) in India: Indigenous Hardware, Software, & Infrastructure Research","awardID":"0841558","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0406","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"7299","name":"Catalyzing New Intl Collab"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["496762"],"PO":["565272"]},"143210":{"abstract":"In 1953, Claude Shannon, the founder of information theory, pointed out that there is no theory via which information embodied in structure can be quantified; this situation remains in effect today. The need for such a theory has become pressing in recent years with the proliferation of structured data sets arising from diverse applications. We have yet to answer fundamental questions such as:<br\/>What are fundamental limits on storage and processing of structural information? What are fundamental bounds on extraction of information from large biological databases? Lack of understanding of such questions threatens to raise severe impediments to further advances in science and engineering of complex systems. The main goal of this work is to search for measures and algorithms to appraise the amount of organization and structure embodied in artifacts and natural objects.<br\/>We propose to make headway in information theory of data structures.<br\/><br\/>Data is increasingly available in various forms (e.g., sequences, expressions, interactions, structures) and in exponentially increasing amounts. Most of such data is multidimensional and context dependent; thus it necessitates novel theory and efficient algorithms to extract meaningful information from non-conventional data structures. In compressing such a data structure, one must take into account two types of information: the information conveyed by the structure itself, and then the information conveyed by the data labels implanted in the structure. The specific goals of this project are: (i) characterization of the total amount of information conveyed by a data structure (and how this decomposes into the two types of information mentioned above), and (ii) the design of efficient compression algorithms based upon the total amount of information conveyed in (i).","title":"Collaborative Research: Information Theory of Data Structures","awardID":"0830457","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["480652"],"PO":["564924"]},"143221":{"abstract":"Anomaly detection is essential for a broad range of security, surveillance, and monitoring problems in areas ranging from health care and environmental protection to homeland security and manufacturing. However, because of the increasing complexity of systems and data, as well as the increasing sophistication of adversaries, traditional methods of anomaly detection are no longer sufficient. These methods assume that anomalies look substantially different from normal measurements, or that their characteristics remain constant over time. In many practical applications of interest, however, anomalies are only distinguished by subtle spatio-temporal characteristics. For example, a network security event may involve a sequence of traffic patterns, each of which is innocuous in its own right, but which occupy a localized or lower-dimensional spatial domain when viewed together. Alternatively, anomalies may exhibit temporal structure caused by a slow but steady drift from normal to abnormal behavior. This project develops new theoretical and algorithmic approaches to detecting such spatio-temporal anomalies. The research will impact the monitoring of a wide range of critical infrastructures and application domains, including environmental systems, health care networks, power grids, and communication networks.<br\/><br\/>The project focuses on spatio-temporal anomalies that (1) have significant spatial overlap with the nominal distribution, (2) are distributed on a manifold of lower dimension than nominal measurements, and (3) have time-varying distributional characteristics. The research applies and extends techniques from statistical machine learning, including transductive, manifold-adaptive, and online learning, to the anomaly detection setting. <br\/>This framework allows the investigators to address several difficult and long-standing challenges such as optimal tracking of drifting anomaly distributions and efficient relaxations of combinatorial graph-based inference algorithms.","title":"Learning and Adapting to Spatio-Temporal Anomalies","awardID":"0830490","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"I312","name":"Central Intelligence Agency"}}],"PIcoPI":["518010","518011"],"PO":["564898"]},"143001":{"abstract":"The United States continues to face a crisis in the development of highly-skilled workers in science, engineering, and technology. Computing has deeply permeated every aspect of our society, and every field has been transformed by the recent arrival of affordable high-performance computing. Despite this pervasive growth of computation throughout engineering and science, university level computing education is still largely separated from other disciplines. It is crucial to develop a scientific and engineering workforce with a thorough understanding of the fundamental conceptual ideas from computer science and software engineering that pertain to a specific domain and the skills necessary to apply them to their field of expertise. The University of Texas at El Paso proposes a Conceptual and Development Planning (CDP) project that will support the institutional groundwork to develop synergistic multidisciplinary curricula combinations across the departments of Computer Science, Biological Sciences and Economics and Finance that will provide students with substantive content in domains not typically provided by traditional academic degrees. The overarching goal of this project is to support the conceptual design and planning for the creation of a computing-centric, interdisciplinary, and cross-fertilizing model that spans the institution?s academic structures. This proposal addresses these needs by developing a comprehensive approach to developing skills in relevant aspects of computation and algorithm design in students in a wide variety of scientific disciplines. The team will develop a new computational curriculum suitable for students whose careers will overlap. The principal focus is the development of multidisciplinary academic programs, built from collaborative work between the involved constituents, rather than the creation of new academic units. These multidisciplinary academic programs will convey sufficient depth to prepare students to make significant contributions in their respective fields. This project will permit the investigation of the assertion that collaborative work between computer science, and disciplines where the participation of women is more reflective of the population, e.g., biology and social sciences, can help reverse the trend of a small proportion of women graduating with a computing related degree or an engineering degree.","title":"CPATH CDP: An Integrated, Multidisciplinary and Cross-Fertilizing Model for Computing Education","awardID":"0829683","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":[380750,"440750","440750","429793",380754],"PO":["565136"]},"143364":{"abstract":"Proposal Title: Collaborative Research: CT-M: Unification Laboratory for<br\/>Cryptographic Protocol Analysis<br\/>Institution: SUNY at Albany<br\/>Abstract Date: 08\/13\/08<br\/>The ability to reason about different equational theories is very<br\/>important to the analysis of cryptographic protocols. An understanding of<br\/>how equational properties of a function used by a protocol can be<br\/>exploited by an intruder can be invaluable in finding flaws that might<br\/>otherwise be missed. Numerous examples exist in the literature and even<br\/>in fielded protocols. One very powerful tool for cryptographic protocol<br\/>analysis with equational theories is equational unification. Equational<br\/>Unification was the basis of the NRL Protocol Analyzer (Maude-NPA). Its<br\/>use of these theories allowed it to both reproduce existing flaws and find<br\/>new ones at a level of precision way beyond that available to other tools<br\/>at its time. This suggests that equational unification if properly<br\/>extended, can give support in a similar fashion to analysis of<br\/>cryptographic protocols that use functions that obey more expressive<br\/>equational theories. The aim of this project is to provide a<br\/>laboratory for equational unification that will develop the algorithms<br\/>and techniques that can be used to support the use of equational<br\/>unification in cryptographic protocol analysis. This effort will<br\/>consist of two parts: the development of unification algorithms for<br\/>theories of interest to cryptographic protocol analysis, and the development<br\/>of new techniques for employing unification in cryptographic protocol<br\/>analysis. The project will help in the design and implementation of next<br\/>generation tools for protocol analysis. Tools developed in the project<br\/>will be made available to other researchers working on formal protocol<br\/>analysis methods as well as to protocol designers for experimentation. The<br\/>educational component of the project will involve undergraduate and<br\/>graduate students at both institutions.","title":"Collaborative Research: CT-M: Unification Laboratory for Cryptographic Protocol Analysis","awardID":"0831064","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["550264"],"PO":["565264"]},"143485":{"abstract":"ANET: Mobius: A Multi-Tier Socially-Aware Network Infrastructure<br\/><br\/>Abstract<br\/><br\/>This research explores the benefits of embedding social knowledge in network protocols and services to support mobile social computing. Specifically, it investigates 1) which mobile social computing problems can be solved with socially aware networks; 2) what social information is amenable to being captured and utilized by these networks (assuming privacy preserving capabilities); 3) what mechanisms and system architectures are necessary to enable dynamic network adaptation to geo-social context changes; 4) how to leverage these mechanisms to design socially-aware protocols and services; 5) how to define and enforce individual and global privacy policies, in general and for accessing social context; and 6) which programming abstractions provide both flexibility and simplicity for rapid mobile social computing application development. <br\/><br\/>This research will lead to a self-organizing, self-adaptive, community-oriented, two-tier network infrastructure for mobile social computing. The mobile human-centric tier runs mobile applications and collects geo-social context information. The peer-to-peer system tier runs services in support of mobile applications and adapts to the geo-social context to enable energy-efficient, scalable, secure, and reliable applications. For large-scale evaluation, this project uses the NSF-sponsored SmartCampus testbed with hundreds smart phones distributed to students.<br\/><br\/>This research will expand the understanding of mobile social computing, an area of great practical relevance to the society at large. To spur the dissemination of results, the source code is made publicly available. Noteworthy educationally is that both institutions, University of South Florida and New Jersey Institute of Technology, are among the national leaders in the percentages of graduates from under-represented groups, and the researchers have specific plans to attract students from these groups to research opportunities in the project. Finally, the foundational results of the project are integrated in an inter-disciplinary studio course that creatively explores design ideas in mobile social computing.","title":"Collaborative Research - ANET: Mobius: A Multi-Tier \\\\Socially-Aware Network Infrastructure","awardID":"0831753","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["538322","383971"],"PO":["565090"]},"143012":{"abstract":"EMT: Biomimetic Self-Assembly of Functional Nanostructures for Computing and Communications<br\/><br\/> The properties of DNA, particularly its capacity for reliable and programmable molecular recognition, have led to the birth of the field of DNA-based nanotechnology. Recent experimental successes in DNA-guided self-assembly include the ordering of matter with unprecedented accuracy and parallelism, nano-scale organization of metal particles and proteins, along with fabrication of highly conductive metallic nanowires. The proposed project focuses on the use of complex, self-assembled DNA superstructures based on DNA ?tiles? as templates for the fabrication of programmable nanoscale electronic and plasmonic components and wiring. This research will impact nanoelectronics and nanophotonics research with potentially profound implications for the future of computation. This project fits very well with the stated goals of the EMT program especially in its mandate ?to enable radical innovations in... computing and communication systems through the support of projects that capitalize upon research opportunities at the intersection of computing and biological systems, nanoscale science and engineering?.","title":"EMT\/Nano: Biomimetic Self-Assembly of Functional Nanostructures for Computing and Communications","awardID":"0829749","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["523694","531216"],"PO":["565223"]},"143133":{"abstract":"The Investigators propose to develop and implement novel and<br\/>efficient Gaussian-beam-based computational methods for large-scale computational<br\/>electromagnetics motivated by industrial and military applications. The project addresses<br\/>several fundamental issues in scientific computing and applied mathematics, such as<br\/>multi-scale modeling, simulation, and inverse problems, which are essential in material<br\/>sciences and nanotechnology. Computational electromagnetics has become a fundamental,<br\/>vigorously growing technology in diverse science and engineering disciplines, ranging<br\/>from radar, sonar, seismic imaging, medical imaging, lithography processing, detection of<br\/>land mines, submarine detection, stealth technology, remote sensing, electronics to<br\/>microscopy and nanotechnology. The new methodology resulting from Eulerian Gaussian beam<br\/>methods has substantial impact on computational electromagnetics and applications,<br\/>particularly at high frequencies.<br\/><br\/>The research involves large-scale direct and inverse scattering problems for electromagnetic<br\/>wave propagation with high wave numbers. The Investigators will develop efficient and accurate<br\/>Eulerian Gaussian beam methods for Maxwell's equations in inhomogeneous media in the high<br\/>wave number regime. Furthermore, the Investigators will apply the new Gaussian methods to three<br\/>challenging and practically important problems for Maxwell's equations: inverse medium<br\/>problems, large cavity problems, and diffraction grating problems. New Eulerian<br\/>Gaussian-beam-based algorithms will be developed for the first time for these applications.<br\/>The Gaussian beam method will be capable of producing uniform asymptotic solutions beyond<br\/>caustics for wave propagation with high wave numbers. These new Eulerian Gaussian beam<br\/>methods should also provide an efficient tool for many other applications related to<br\/>electromagnetic waves in inhomogeneous media.<br\/>-------------------------------------------------------------------","title":"Gaussian Beam Methods for Large-Scale Computational Electromagnetics and Applications","awardID":"0830161","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["536664","519339"],"PO":["565157"]},"143254":{"abstract":"Collaborative Research: Minimum Sobolev Norm Methods<br\/><br\/>The aim of this research project is to design fast and accurate<br\/>numerical algorithms for the solution of large classes of mathematical<br\/>equations that arise in engineering and science. In particular, the<br\/>main concerns are the solution of integro-differential equations on<br\/>complex domains and of signal and image processing problems. The<br\/>approach is based on formulating the estimate of the solution of the<br\/>equation at a point as the value of the smoothest solution (on<br\/>average) at that point based on the given data. The resulting discrete<br\/>equations can be shown to have specially structured matrices, which<br\/>can be exploited to create fast solvers for these equations. The<br\/>resulting methods have two main computational advantages. First, they<br\/>can be designed to avoid gridding or triangulation of the complex<br\/>domain. Second, these methods exhibit local convergence; that is, the<br\/>rate at which the approximant converges to the solution at a point<br\/>depends only on the local smoothness of the solution. These advantages<br\/>enable the method to tackle equations with complicated singularity<br\/>structures with relative ease.<br\/><br\/>Let Hs denote a Sobolev Hilbert space whose elements have s > 1<br\/>fractional derivatives. Suppose an unknown function f in Hs satisfies<br\/>the equation L(F) = g, where L is a linear operator and g is a known<br\/>function. Let Ln denote n linear functionals on Hr. Let q denote a<br\/>linear functional on Hs. Then the best minmax estimate for q(f) can be<br\/>computed from the minimum Sobolev norm function p in Hs that satisfies<br\/>the constraints Ln(L(p)) = Ln(g). This p can be computed very rapidly<br\/>since the optimal p is given by a nice set of equations that has Fast<br\/>Multipole Method (FMM) structure when written in the proper<br\/>representation. Also, it is possible to work with Lp Sobolev spaces<br\/>with p = 1. In these cases the optimization problem is more<br\/>complicated and can be reduced to linear programming problems, for<br\/>which fast solvers are being developed that exploit the underlying FMM<br\/>structure of the constraint matrix. The theoretical work consists of<br\/>studying the convergence of the solution as n gets bigger, and also in<br\/>proving the FMM structure of the resulting discrete equations. The<br\/>algorithmic work consists of designing fast algorithms for<br\/>constructing the FMM representation and then designing fast algorithms<br\/>for the direct (non-iterative) solution of these equations. The<br\/>application work consists of applying these ideas to image<br\/>segmentation and multi-rate signal processing. Also, mesh free,<br\/>locally convergent schemes are being developed for the solution of<br\/>integral equations and elliptic partial differential equations on<br\/>complex domains in two dimensions.","title":"Collaborative Research: Minimum Sobolev Norm Methods","awardID":"0830604","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[381444],"PO":["565157"]},"143496":{"abstract":"Mobility is central to various applications, from the classical<br\/>problems of searching for a moving target and rescue mission in<br\/>military and disaster settings, to deploying mobile ad-hoc\/sensor<br\/>networks for surveillance and data communication over hostile<br\/>terrain and underwater. While the random mobility pattern of nodes in<br\/>these networks has been considered as the main source of uncertainty and<br\/>disruption of communication links, it can also facilitate reliable and<br\/>predictable performance, if properly controlled and actively exploited.<br\/>Most existing approaches have taken advantage of nodes' mobility only as<br\/>passive vehicles for data forwarding upon contact by chance, with an<br\/>over-simplifying assumption that every pair of nodes is making contacts<br\/>with each other according to a Poisson process, which is recently<br\/>contradicted by empirically observed non-Poisson nature with high degree<br\/>of heterogeneity in various mobile networks.<br\/><br\/>The long-term goal of this research is to develop a unified methodology<br\/>for efficient protocol design and control of nodes in heterogeneous<br\/>mobile ad-hoc networks and delay\/disruption-tolerant networks under<br\/>non-Poisson contacts. The goal further extends to the use of mobile<br\/>nodes with controllable mobility that can autonomously exploit the<br\/>changing diversity. The broader impact of this research lies in its<br\/>interdisciplinary nature in addressing the analysis and design of<br\/>algorithms running on mobility with heterogeneous non-Poisson contacts,<br\/>which is also central to other scientific areas such as epidemiology,<br\/>foraging ecology and biology, all driven by contacts among living<br\/>organisms. This tight relationship between mobile ad-hoc networks<br\/>and various phenomena in nature will serve as a vivid example for<br\/>undergraduate students at a freshmen level or even to general public.","title":"NEDG: Efficient Design and Control of Heterogeneous Mobile Networks: Beyond Poisson Regime","awardID":"0831825","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["517733"],"PO":["557315"]},"147500":{"abstract":"Abstract<br\/><br\/>IIS - 0849499 <br\/>SGER: Music Similarity Retrieval Using Power-Law Metrics<br\/>Manaris Bill Z.<br\/>College of Charleston <br\/><br\/>The project is developing a music search engine based on identifying aesthetic similarities. This engine is utilizing power-law metrics to extract statistical proportions of music-theoretic and other attributes of music pieces (e.g., Pitch, Duration, Pitch Distance, Duration Distance, Melodic Intervals, Harmonic Intervals, Melodic Bigrams, etc.). The engine searches for pieces that are aesthetically similar to the input piece using a mean squared error (MSE) approach. Preliminary testing has been done using the Classical Music Archives corpus (14,695 MIDI pieces), combined with 500+ MIDI pieces from other styles (e.g. Jazz, Rock, Country, etc.). A first year effort has demonstrated functionality for additional file formats, including MP3, the predominant format used in web-based music corpora. Assessment and validation experiments will be continued to compare to computational findings indicating aesthetic similarity of retrieved pieces. This research is potentially transformative to the internet music economy and functionality.","title":"SGER: Music Similarity Retrieval Using Power-Law Metrics","awardID":"0849499","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["556149","552414"],"PO":["433760"]},"143265":{"abstract":"Networked wireless communications over multiple hops is rapidly emerging as the main architecture of future wireles systems, including multihop extensions of cellular and WiFi networks, mesh networks, and sensor networks. Common among these types of networks is that they are not completely unstructured (or ad hoc) networks, but traffic is routed and accumulated towards a common destination. Due to this characteristic property, these networks will be referred to as Networks with Traffic Accumulation, or NETAs. Traffic accumulation creates hot sposts or bottlenecks around the common destination because of the increased traffic load and interference. Despite the severity of the hot spot problem, there is a lack of efficient methods to cope with it. <br\/><br\/>This research addresses the hot spot issue in NETAs by developing new distributed error correction strategies tailored to two important subclasses- line networks and tree networks. In line networks, the investigators study fundamental properties and the design of distributed channel coding protocals using serially concatenated and protograph-based constructions to strengthen the error correction capability near the destination without sacrificing badwidth efficiency. In tree networks, several source nodes may wish to employ a common relay node to broadcast their information to multiple destination nodes which may have access to side information from \"overheard\" source messages. The investigators explore a novel approach where each source uses a distinctlow rate code for transmission to the relay, whereas the decoded messages are re-encoded using a high rate \"nested code\". In addition, interlayer issues are considered, in particular the joint design of efficient channel access and routing schemes together with the porposed coding schemes.","title":"Collaborative Research: Distributed Error Correction Strategies in Wireless Networks","awardID":"0830651","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["508186","540624"],"PO":["564924"]},"144002":{"abstract":"Program performance is highly dependent on the amount of memory available to the<br\/>program. In traditional computing systems, the memory working set of an application<br\/>has a bounded size - providing more memory to an application improves performance<br\/>until its working set is met. Once the working set is met, additional memory yields little<br\/>or no benefit. However, in the presence of garbage collection (a technique for memory<br\/>management where space that is unlikely to be reused by an application is<br\/>automatically reclaimed), the relationship between program performance and memory<br\/>allocation is more complex. Data is managed at three levels: the compiler manages<br\/>data objects at the program level, the garbage collector manages the heap at the virtual<br\/>machine level, and the virtual memory manager manages virtual memory at the<br\/>operating system level. The middle layer plays a critical role. Increasing an application's<br\/>heap size can reduce the frequency of garbage collections and improve performance,<br\/>but too large a heap may trigger paging and degrade performance.","title":"Collaborative Research: CSR-PSCE, SM: Adaptive Memory Management in Shared Environments","awardID":"0834323","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383867],"PO":["535244"]},"145454":{"abstract":"Due to the critical resource constraints, it is important to provide an optimization of utilization of the deployed resource for reliable communication in wireless sensor networks (WSNs). One of the main challenges is developing a scalable and distributed method to identify the resource deficit and redundancy and to control the corresponding scheduling\/balancing activities within local area, i.e., a cost-effective localized solution for max-min problem.<br\/><br\/>This project focuses on the management of the deficit and the redundancy of network coverage and connectivity to solve the hole problem in WSNs. The research provides a comprehensive understanding of network topology in the local view of each sensor node as well as a local description of the network configuration for the resource of the coverage and connectivity. As a result, the global configuration can be inferred into each local description. Therefore, the global optimization can be achieved while keeping all the actions into a limited area. The investigator finds a new routing scheme to avoid the effect of resource deficit caused by holes, provides a hole-repair process with synchronization and conflict control, and extends all the results from 2-D networks to 3-D networks and from uniform disk communication range model to realistic model. This research leads to new directions in efficient resource management in the fields of energy conservation, secured communication establishment and management, data delivery in delay tolerant mobile networks, and node collaboration. It is likely to make WSNs more affordable and amenable to commercial, civilian, and military applications.","title":"SGER: TF:SING - A New Method for Resource Management in Wireless Sensor Networks","awardID":"0840891","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7939","name":"WIRELESS COMM & SIGNAL PROCESS"}}],"PIcoPI":["375576"],"PO":["432103"]},"143034":{"abstract":"EMT: Collaborative Research: Primate-inspired Heterogeneous Mobile and Static Sensor Networks <br\/><br\/>Although previous bio-inspired models have concentrated on invertebrates (such as ants), mammals such as primates with higher cognitive function are valuable for modeling the increasingly complex problems in engineering. Understanding primates? social and communication systems, and applying what is learned from them to engineering domains is likely to inspire solutions to a number of problems.<br\/>This research involves studying and modeling modes of group behavior and communication of coppery titi monkeys, rhesus macaques, and other primate models, and applying what the investigators learn to the distributed control of heterogeneous mobile and static sensor networks. The investigators will model the social and communication behavior of these primates, which will provide biological inspiration for solving problems in communication and networking. The phases of this research include: 1) identification, interpretation, and translation of primate behavioral models, 2) assessment of the effectiveness of small and large group formations based on primate grouping models in heterogeneous mobile and static sensor networks, 3) development of bio-inspired message-based communications, and 4) development of bio-inspired behavior-based communications. This research aims to achieve a deeper understanding of effectiveness of bio-inspired communications and networking by studying primates, and to establish interdisciplinary research and education in the fields of biological modeling, sensor networking, and robots control.","title":"EMT\/BSSE: Collaborative Research: Primate-based Heterogeneous Mobile and Static Sensor Networks","awardID":"0829828","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[380835,"406111"],"PO":["564898"]},"143155":{"abstract":"TF: Error Correction Algorithms for DNA Repair: Inference, Analysis, and Intervention<br\/><br\/>Bane Vasi\u00e6, David W. Galbraith, and Michael W. Marcellin<br\/>The University of Arizona, Tucson<br\/><br\/>Maintaining integrity of genetic material is vital to the survival of species, and is achieved through deoxyribonucleic acid (DNA) repair, a process in the cell in which DNA-damage is continually monitored and corrected. For example, ionizing radiation can induce single and double strand breaks, the most dangerous type of damage, which if uncorrected leads to cell death, while inaccurate repair can be mutagenic. This research establishes a framework for rigorous treatment of genetic error correction, or more specifically, for inferring the error correction coding system of the living cell and describing its functionality quantitatively and algorithmically. <br\/>This framework is based on probabilistic graphical models that are used in error correction theory to design codes enabling transmission of information in the presence of very high noise levels and ensuring fault-tolerance and reliable storage of information in systems built of faulty components, which precisely corresponds to the DNA-repair scenario. By combining experimental data with existing knowledge of gene-protein and protein-protein interactions, the investigators are creating global functional interaction networks of genes involved in DNA repair. This enables a study of the error correction algorithms and their dynamics, resulting in a formal logical and causal description of interaction among genes, proteins and inducible factors, or a genetic wiring diagram. Such a wiring diagram can be viewed as a digital logic circuit of a genetic decoder. The investigators study the decoder structure and behavior, and more particularly: (i) inferring the decoder from the existing knowledge and new experiments, (ii) predicting the dynamics of the error control system, and (iii) controlling the dynamics using external factors.","title":"TF08: Error Correction Algorithms for DNA Repair: Inference, Analysis, and Intervention","awardID":"0830245","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[381183,"408606","548044"],"PO":["564924"]},"143276":{"abstract":"The usage of wireless devices unavoidably induce user mobility in diverse settings over multiple space\/time scales, ranging from traditional cellular phone users and Wi-Fi users in moderate-to-dense networks, to nomadic users and mobile robots\/sensors in multi-hop wireless ad-hoc networks and delay\/disruption-tolerant networks over larger space\/time scales. As mobile communications go to large-scale and social networks, there is an acute and timely demand for exploring fundamental principles in mobility-induced link-level dynamics, which has not been studied systematically, but have tremendous impact on a wide range of research areas, such as theoretic limits of algorithms, and protocols in mobile networks, optimization techniques for performance analysis and estimation, network architecture and topology control, distributed sensor-actuator networks, and modeling of social networks.<br\/><br\/>The goal of this research is to develop a theoretical foundation for wireless mobile networks, through characterization of link-level dynamics by stochastic analysis approach. Specifically, the research focuses on (i) modeling, analysis, and statistical characterization of mobility-induced link dynamics, (ii) spatio-temporal dynamics in mobility modeling in multiple space\/time scales rather than being dependent on networking environments a priori, and (iii) scaling limits for link-level metrics under various network operating regimes. The theoretical foundation of mobility modeling, which integrates multidisciplinary characteristics, will be used as an experimental concentration area to initiate innovative curriculum design of network science. The investigators develop new courses on networking theory, starting from graduate-level and transit to under-graduate level, by demonstrating the charm and rigorousness of theoretic work in tangible, attractive wireless mobile applications to undergraduate students and K-12 students.","title":"TF-SING: A Theoretical Foundation of Spatio-Temporal Mobility Modeling and Induced Link-Level Dynamics","awardID":"0830680","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["534041","517733"],"PO":["564924"]},"143397":{"abstract":"While modern research in cryptography has transformed a significant portion of computer security from an art into a science, ultimately the security guarantees for all current cryptographic protocols (encryption, digital signatures, etc.) rely on conjectures, such as the intractability of factoring large integers is intractable or of finding collisions in certain \"hash functions\". The possibility that these conjectures are false (as was recently discovered for some popular hash functions) is a genuine threat to cybersecurity.<br\/><br\/>This project aims to reduce this threat by minimizing the assumptions needed for specific cryptographic primitives (such as commitment schemes, which can be viewed as \"digital safes\"), and attempting to base cryptography on the hardness of an entire class of problems. More generally, the project aims to strengthen the foundations of cryptography and cybersecurity using techniques from computational complexity theory.<br\/><br\/>Society benefits from this project because of the enabling role that cryptography plays in electronic commerce. Stronger foundations for cryptography help prevent electronic commerce from becoming vulnerable to unforeseen attack. Further impact of this project comes through the training of graduate and undergraduate students, both through their attendance in courses taught by the PI and through their direct involvement in the research.","title":"CT-ISG: The Assumptions for Cryptography","awardID":"0831289","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["525639"],"PO":["497499"]},"144013":{"abstract":"Many critical workloads today, such as web-hosted services, are limited not by raw CPU processing power but by interactions between the CPU cores, the memory system, I\/O devices such as disks and network interfaces, and the complex software (applications, middleware, operating systems, virtual machines) that ties all these components together. To improve the efficiency of these workloads and systems, designers and developers need tools to identify the bottlenecks, so that they can address them. However, existing performance analysis tools, such as software profilers, cannot account for hardware bottlenecks or for situations where software overheads are hidden due to overlap with other operations.<br\/><br\/>As computer systems become ever more complex networked aggregations of software and hardware from multiple vendors, the ability to isolate and address inefficiencies that reduce throughput and waste energy is daunting. The goal of the project is to address this problem by developing an analysis methodology and tool set that identifies true bottlenecks in complex systems spanning multiple software and hardware layers executing concurrently across multiple CPU cores and dedicated hardware devices. This research utilizes critical-path analysis to not only identify bottlenecks but also quantify their contribution and estimate the speedup obtainable if a particular set of bottlenecks is removed or reduced. Ultimately, this research will lead to qualitative performance improvements in software and hardware system designs as the methodology and tools produced aid designers and developers in focusing their efforts on removing the true bottlenecks.<br\/><br\/>The project involves both graduate and undergraduate students as researchers. The results will be fed into appropriate courses, including one on parallel architectures. The material and slides for this course will be made available over the web.","title":"CSR-PSCE,SM: Full System Critical Path Analysis","awardID":"0834435","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["535404",383899],"PO":["535244"]},"144134":{"abstract":"Although the terms \"signaling\", \"communication\", and \"network\" are deeply embedded in the parlance of biology, the more profound meanings of information and communication are overlooked often. Information can be quantified, its flow may be measured, and tight bounds exist for its representation and conveyance between transmitters and receivers in a variety of settings. Communications theory is about efficient communication when energy is at a premium, as is often the case in biology. Perhaps most fundamentally, information theory allows mechanism-blind bounds on decisions and information flow, i.e., the physics of a system allows determination of limits that any method of information description, delivery or processing must obey. Thus, a rigorous application of communications theory to biology seems both attractive and obvious as an organizing principle -- a way to tease order from the myriad engineering solutions that comprise biological systems. Similarly, the study of biological systems -- engineering solutions evolved over eons -- might yield new communication and computational theory.<br\/><br\/>A communications-theoretic approach to multicellular biological systems such as microbial ecosystems and human tissues has received scant (if any) attention, so this research will carefully explore this interdisciplinary intellectual gap. Topics to be considered include the input\/output capacity of multi-cellular biological systems, representation methods for structural information, and information flow during tissue development, maintenance, regeneration, and aging, especially in epithelia. Genotypically aberrant epithelial cells can selectively suppress malignant behavior when part of a phenotypically normal structure -- which implies that somehow the cells understand they are part of a normal structure. How such structural information is stored and communicated between cells could prove an important lens on the health and disease of such cellular communities.","title":"CDI Type I:A Communications Theory Approach to Morphogenesis and Architecture Maintenance","awardID":"0835592","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7750","name":"CDI TYPE I"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["466323"],"PO":["565272"]},"143045":{"abstract":"CCF-0829865<br\/>Code Representation and Performance of Graph-Based Decoding<br\/>PI: Paul H. Siegel, UC San Diego<br\/><br\/>Abstract: The discovery of channel codes that approach information-theoretic performance limits when paired with iterative graph-based decoding algorithms represents a major advance in coding theory and practice. Prime examples include turbo codes, low-density parity-check (LDPC) codes, and repeat-accumulate (RA) codes. These coding techniques and their variants have had a profound impact on data transmission applications, including deep space communications, digital video broadcasting, and mobile wireless telephony. They are also poised for use in other settings, such as high density digital data storage. What is common to these coding and decoding schemes is that their performance and implementation complexity depend not only on the code itself, but also on the choice of graphical representation of the code. This research studies the characteristics of code representations that influence the performance of iterative decoding algorithms, as well as methods for constructing the best representations. <br\/><br\/>The research has two major thrusts. The first is the examination of combinatorial and graphical properties of linear code representations that serve as figures of merit for iterative decoder performance on several types of channels having theoretical and practical significance. The understanding of these properties guides improved code design as well as the development of new graph-based decoding strategies. The second thrust is the analysis and enhancement of decoders based upon linear programming. The research studies the adaptive introduction of constraints to reduce decoder complexity and to improve performance. It also considers new linear programming algorithms that exploit sparse code representations. A dynamic interplay between these two research thrusts is achieved by studying the relationship between iterative and linear programming decoders, as well as their application to equalization and detection for channels with memory.","title":"Code Representation and Performance of Graph-Based Decoding","awardID":"0829865","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["522738"],"PO":["564924"]},"143287":{"abstract":"Abstract<br\/>Localization involves the estimation of the precise location of an object based on various forms of relative position information available of the object. Source and sensor localization is a fundamental capability broadly useful in a number of emerging applications. For example a network of sensors deployed to combat bioterrorism, must not only detect the presence of a potential threat, but also must pinpoint the source of the threat. Similarly, in pervasive computing, locating an errant mobile user permits the computer network to identify the most appropriate server with matching capabilities for the user. Likewise, in sensor networks individual sensors must know their own positions, so as to route packets, detect faults, and sense and record events. There is also an emerging multibillion dollar wireless localization industry. This research will address issues that hold the key to fast efficient localization.<br\/>The investigators will adopt a three pronged approach. First, various optimum estimates will be investigated under a variety of practical signal models. These include maximum likelihood and minimum variance estimates. Theoretical performance limits will be determined. Secondly, the investigators will generalize and analyze various algorithms that obtain these estimates efficiently with low complexity. Specifically, the investigators will study optimal localization involving minimization of non-convex cost functions that admit multiple local minima. To overcome the problem of multiple local minima, the investigators will develop a relaxation framework based on convex optimization to obtain fast near optimal solutions. Finally, the proper placement of wireless sensors and anchors impacts both the accuracy and the complexity with which localization is performed. Thus, the investigators will study optimum anchor placement to aid both these attributes. These investigations are critical to the understanding of the theoretical foundation of wireless source localization, and will fundamentally impact its broad applications.","title":"Collaborative Research: Robust Low Complexity Approaches to Source Localization and Sensor Placement in Wireless Networks","awardID":"0830706","effectiveDate":"2008-09-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["551148"],"PO":["564898"]},"145234":{"abstract":"e propose to hold a symposium on memristor and memristive systems in fall 2008 by inviting experts to discuss scientific principles, experimental findings, and potential exploration of related topics. The importance of this field initially pioneered by Leon O. Chua in 1971 and by Leon Chua and Sung-Mo Kang in 1976 was reaffirmed recently by unveiling of a two-terminal nanoscale device presented in Nature by Stanley Williams and his collaborators at HP Labs.<br\/>Potential development of more compact low power memory chips and realization of neuromorphic computing hardware among other possibilities has been suggested by the most recent experimental results. This workshop will potentially help accelerate significant progresses in this newly emerging field.","title":"A Symposium for Memristor and Memristive Systems","awardID":"0840136","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["429867"],"PO":["562984"]},"144024":{"abstract":"Revised Budget\/Statement<br\/><br\/><br\/>Abstract for our project (about 250 words) <br\/>Emerging 3D Geographically Distributed Tele-immersive Environments (3DTI) are becoming viable collaborative environments over the Internet to allow for joint physical activities such as tai-chi training and many other usages. These environments fuse multi-dimensional streams from multiple 3D cameras and other sensors across multiple sites to render a shared multi-view visual cyberspace at each site that immerses all participants. In the cyberspace, each participant manipulates herself (e.g., positions, size), and interacts with remote participants or graphical objects ?virtually?. <br\/>A fundamental challenge in these environments is the need for effective real-time interaction among multi-site geographically distributed 3DTI environments and the PI observes that maintaining a consistent, information-rich shared visual context is the key to effective real-time interaction. If the views of different participants become out of sync, their collaboration is severely disrupted. <br\/>In this project, the PI presents a novel view control management, called VIEWCALL. VIEWCALL presents (1) a holistic approach to view dissemination where the PI investigates forest construction and load balancing for view-centric stream dissemination among individual 3DTI environments; (2) a novel approach to visual context\/feedback distribution due to dynamics in view or resource demands, and (3) new distributed algorithms for capturing, updating, coordinating and synchronizing view-based visual context metadata. <br\/><br\/>The VIEWCALL system has an impact on the 3DTI environments where research community investigates new forms of dance choreography, students learning tai-chi, in-home care of ageing population, computer gaming, and wheelchair-basketball coaches training players with movement disabilities. VIEWCALL is regularly shown at the annual UIUC?s Engineering Open House as well as to ChicTech all-schoolgirl summer camp.","title":"CSR-DMSS, SM: View Control Management in Geographically Distributed Tele-Immersive Environments","awardID":"0834480","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7793","name":"DATA-INTENSIVE COMPUTING"}}],"PIcoPI":["561784"],"PO":["565255"]},"144145":{"abstract":"Wayfinding is an essential capability for any person who wishes to have an independent life-style. It requires successful execution of several tasks including navigation and object and place recognition, all of which necessitate accurate assessment of the surrounding environment. For a visually-impaired person these tasks may be exceedingly difficult to accomplish and there are risks associated with failure in any of these. Guide dogs and white canes are widely used for the purpose of navigation and environment sensing, respectively. The former, however, has costly and often prohibitive training requirements, while the latter can only provide cues about obstacles in one?s surroundings. Human performance on visual information dependent tasks can be improved by sensing which provides information and environmental cues, such as position, orientation, local geometry, object description, via the use of appropriate sensors and sensor fusion algorithms. Most work on wayfinding aids has focused on outdoor environments and has led to the development of speech-enabled GPS-based navigation systems that provide information describing streets, addresses and points of interest. In contrast, the limited technology that is available for indoor navigation requires significant modification to the building infrastructure, whose high cost has prevented its wide use. <br\/><br\/>This proposal adopts a multi-faceted approach for solving the indoor navigation problem for people with limited vision. It leverages expertise from robotics, computer vision, and blind spatial cognition with behavioral studies on interface design to guide the discovery of information requirements and optimal delivery methods for an indoor navigation system. Designing perception and navigation algorithms, implemented on miniature-size commercially-available hardware, while explicitly considering the spatial cognition capabilities of the visually impaired, will lead to the development of indoor navigation systems that will assist blind people in their wayfinding tasks while facilitating cognitive-map development.","title":"CDI-Type II: Collaborative Research: Cyber Enhancement of Spatial Cognition for the Visually Impaired","awardID":"0835637","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7751","name":"CDI TYPE II"}}],"PIcoPI":["553283"],"PO":["543539"]},"143056":{"abstract":"The creation of new compounds and materials that facilitate efficient electronic conduction is an important and on-going challenge for the development of nanoscale technologies. We study the development of a new family of one-dimensional materials whose core consists of a one-atom wide conducting wire that is surrounding by insulating protecting groups. These wires are designed simultaneously with their insulating coating such that the wires can be packed with maximum density. The insulating groups serve to isolate each wire for ease of synthesis, promote highly one-dimensional structures, and facilitate efficient conduction along but not between the wires.<br\/>The proposed materials build on established compounds from our group, proven methodologies, and are expanded with new ligands and redox reactions to achieve the desired electronic structures. Salts will be prepared in which each cation and anion contain a metal atom, M, surrounded by ligands based on L and X groups. Previous work established the use of metathesis reactions to synthesize double salts with d8 and d10 metal-containing cations and anions of the form [ML3X][MX2] and [ML2X2][MX2], where L is a neutral Lewis base donor and X is anionic group. The cations and anions stack in an infinite array forming a chain of metal atoms with infinite metal-metal contacts along the length of the chain. Extensive use has been made of Au and Pt structures to date which will be elaborated upon with Co, Ni, Cu, Rh, Ir, Pr, and Ag. Other ion motifs will be developed as well including [MX4]-and [MLX3]-, This synthetic scheme is extremely flexible and will therefore allow an essentially infinite array of different electronic occupations to be prepared.","title":"EMT\/NANO: Single Atom Wide Wires with Insulation: New Paradigm for Ballistic Transport","awardID":"0829890","effectiveDate":"2008-09-01","expirationDate":"2012-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":["505604"],"PO":["565157"]},"143298":{"abstract":"Collaborative Research: Minimum Sobolev Norm Methods<br\/><br\/>The aim of this research project is to design fast and accurate<br\/>numerical algorithms for the solution of large classes of mathematical<br\/>equations that arise in engineering and science. In particular, the<br\/>main concerns are the solution of integro-differential equations on<br\/>complex domains and of signal and image processing problems. The<br\/>approach is based on formulating the estimate of the solution of the<br\/>equation at a point as the value of the smoothest solution (on<br\/>average) at that point based on the given data. The resulting discrete<br\/>equations can be shown to have specially structured matrices, which<br\/>can be exploited to create fast solvers for these equations. The<br\/>resulting methods have two main computational advantages. First, they<br\/>can be designed to avoid gridding or triangulation of the complex<br\/>domain. Second, these methods exhibit local convergence; that is, the<br\/>rate at which the approximant converges to the solution at a point<br\/>depends only on the local smoothness of the solution. These advantages<br\/>enable the method to tackle equations with complicated singularity<br\/>structures with relative ease.<br\/><br\/>Let Hs denote a Sobolev Hilbert space whose elements have s > 1<br\/>fractional derivatives. Suppose an unknown function f in Hs satisfies<br\/>the equation L(F) = g, where L is a linear operator and g is a known<br\/>function. Let Ln denote n linear functionals on Hr. Let q denote a<br\/>linear functional on Hs. Then the best minmax estimate for q(f) can be<br\/>computed from the minimum Sobolev norm function p in Hs that satisfies<br\/>the constraints Ln(L(p)) = Ln(g). This p can be computed very rapidly<br\/>since the optimal p is given by a nice set of equations that has Fast<br\/>Multipole Method (FMM) structure when written in the proper<br\/>representation. Also, it is possible to work with Lp Sobolev spaces<br\/>with p = 1. In these cases the optimization problem is more<br\/>complicated and can be reduced to linear programming problems, for<br\/>which fast solvers are being developed that exploit the underlying FMM<br\/>structure of the constraint matrix. The theoretical work consists of<br\/>studying the convergence of the solution as n gets bigger, and also in<br\/>proving the FMM structure of the resulting discrete equations. The<br\/>algorithmic work consists of designing fast algorithms for<br\/>constructing the FMM representation and then designing fast algorithms<br\/>for the direct (non-iterative) solution of these equations. The<br\/>application work consists of applying these ideas to image<br\/>segmentation and multi-rate signal processing. Also, mesh free,<br\/>locally convergent schemes are being developed for the solution of<br\/>integral equations and elliptic partial differential equations on<br\/>complex domains in two dimensions.","title":"Collaborative Research: Minimum Sobolov Norm Methods","awardID":"0830764","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["550348"],"PO":["565157"]},"144035":{"abstract":"CSR # 0834537, ?DMSS: Towards Resource-Efficient Automated Video Surveillance Systems?, PI: Nabil J. Sarhan<br\/> <br\/>Abstract<br\/><br\/>The security problem has emerged as a major concern regionally, nationally, and globally. Therefore, the interest in video surveillance has grown dramatically. Automated video surveillance serves as an elegant and efficient approach for realtime detection of threats and monitoring their progress. Unfortunately, the design of an automated, scalable, and massively distributed surveillance system has been a significant research problem. Power consumption has also been a primary concern, especially in battery-powered video sources. This project addresses the scalability-cost and power consumption problems of large-scale surveillance systems by developing optimal bandwidth allocation schemes, which control the transfer rates of various video sources, considering the potential threat level, placement of video sources, location importance, site map, and rate-accuracy curves of vision algorithms. It also develops a model for determining the current overall warning level based on statistical aggregation of the potential threats detected over a certain period, their levels of sensitivity, the importance of the locations where they are detected, and their closeness to other important locations. Additionally, this project investigates other important aspects, including fault tolerance of the distributed processing architecture, adaptive streaming, feature extraction, and continuous querying. By addressing the scalability-cost and power consumption problems and dealing with several design aspects in a cohesive and integrated manner, this research provides significant contributions in the design of scalable and cost-effective automated video surveillance systems. Furthermore, it has significant broader impacts on homeland security, education, and participation of underrepresented groups.","title":"DMSS: Towards Resource-Efficient Automated Video Surveillance Systems","awardID":"0834537","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383956],"PO":["535244"]},"134487":{"abstract":"CAREER: Protecting Privacy in Untrusted Environments<br\/>----------------------------------------------------<br\/>PI: Vitaly Shmatikov (The University of Texas at Austin)<br\/><br\/>Information about individuals and organizations is increasingly collected<br\/>in massive databases, sent over public networks and shared across<br\/>organizational boundaries. This presents serious threats to privacy:<br\/>even if individual pieces of data are cryptographically protected,<br\/>sensitive information may still leak out due to mismatches between<br\/>privacy policies of different components.<br\/><br\/>This project aims to develop tools and techniques for protecting privacy<br\/>of sensitive data, focusing on three main research thrusts. The first<br\/>thrust is protection of public databases containing individual information<br\/>such as medical records, transactions, and preferences. This includes<br\/>design of provably secure methods for enforcing access policies directly<br\/>in published data, development of analysis tools for finding privacy<br\/>vulnerabilities, and evaluation on real-world data. The second thrust<br\/>is design and implementation of formal methods for checking privacy<br\/>policy compliance in order to ensure that data processing applications<br\/>do not violate the stated privacy policies of the enteprise. The third<br\/>thrust is development of new methods for analyzing privacy-preserving<br\/>communication networks.<br\/><br\/>Protecting data privacy is important not only for individuals, but<br\/>also for businesses and organizations that deal with individual data.<br\/>New technologies developed as part of this project will help detect<br\/>potential privacy violations and enforce privacy policies. They will<br\/>enable applications dealing with sensitive personal and organizational<br\/>data to be executed in open computing environments and support many<br\/>socially important tasks such as multi-institution medical trials that<br\/>do not violate patients' privacy and collaborative analysis of Internet<br\/>security threats.","title":"CAREER: Protecting Privacy in Untrusted Environments","awardID":"0746888","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["519592"],"PO":["543481"]},"143078":{"abstract":"This project exploits methods from statistical physics to provide fundamental advances in computing and communication systems. The intersection of computer science, information theory and statistical physics has seen a recent explosion of activity, resulting in new algorithms and new methods of analysis. Discrete computational challenges including constraint satisfaction, error correction and control of massive networks have benefited from techniques and insights offered by statistical physics. Physics, at the same time, has been significantly enriched by approaches from discrete computation, such as message-passing algorithms.<br\/><br\/>The investigators study two complementary approaches for addressing algorithmic challenges: 1) treating problem instances as members of a random ensemble that can be analyzed as a physical model, and 2) identifying specific classes of instances amenable to physical analysis. The first suggests a fundamental connection between algorithmic performance and an underlying physical phase structure, and has already led to significant new algorithms for unstructured random graphs or networks. The challenge is to generalize it to structured cases. The second uses techniques such as renormalization group and multiscale decomposition, and is proving to be a powerful new approach in probabilistic inference.","title":"EMT\/MISC: Collaborative Research: Harnessing Statistical Physics for Computing and Communication","awardID":"0829945","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[380947,"492134"],"PO":["565223"]},"144057":{"abstract":"Over 6.4 million automotive accidents occur in the US annually. Odds of someone being in an accident this year are 1 chance in 16. Any information that warns of problems along the road(s) ahead can therefore potentially save lives and reduce the frequency and\/or intensity of accidents. The vehicle of tomorrow is the programmable-networked vehicle. In our view, the networked vehicle of the future is one of the most complex Cyber Physical Systems (CPS) with active trajectory control, active navigation and on-line maintenance. V2V wireless networks are a special class of networked-CPS where the maximum relative speeds are in excess of 80m\/s, the node density can span over 9,000 vehicles\/mi^2 and, most importantly, the dynamics of the vehicle, the environment, driver reaction and interaction with other vehicles need to be considered in every communication and control decision. To meet these timeliness and coordinated communication requirements, we are developing a new set of networking capabilities that can lay the foundation for dynamic vehicular networks designed to make driving safer, more efficient and more enjoyable. This project is aimed at the design, analysis, implementation and evaluation of vehicular networks that will enable a wide range of applications including V2V and V2I communication for: (a) Bounded-latency broadcast protocols for active networked safety alerts (b) Protocols and algorithms for Real-Time collision avoidance (c) Native protocols for secure V2V and V2I communication. Real-time research in V2V networks will be the first step toward developing a Spatio-Temporal Real-Time theory and network protocols with wide-area time synchronization.","title":"Collaborative Research: CSR-EHCS(CPS), TM: AutoMatrix: Large-scale Test-bed and Real-Time Protocols for Vehicle-to-Vehicle Wireless Networks","awardID":"0834740","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["461012"],"PO":["561889"]},"144068":{"abstract":"As the transistor growth outpaced the design and verification effort in chip design, a large fraction of on-chip transistors are now allocated to storage structures, such as caches. The static power consumed by these storage structures worsen the critical problems of power and thermal issues faced by current chip designs. To reduce the static power, drowsy techniques are used, where inactive components of a storage structure can be placed in a low power state. Unfortunately, drowsy power states increase the susceptibility of transistors to transient errors. Motivated by these problems, this research explores the tradeoffs between static power, performance and reliability in chip multiprocessors. The fundamental contribution of this research is to develop a novel hybrid analytical\/simulation framework that allows designers to evaluate the impact of reducing static power on processor reliability and performance. Using this framework this research explores new cache management and cache protocols in chip-multiprocessors and the impact of these new schemes on reliability and performance of a computer system. The framework can also be extended to analyze the power, performance and reliability tradeoffs of other storage structures inside each core such as the reorder buffer, the branch prediction tables, and various instruction and scheduling queues.","title":"CSR-PSCE,SM: Trade-offs Between Static Power, Performance and Reliability in Future Chip Multiprocessors","awardID":"0834799","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["518643","518642"],"PO":["535244"]},"137776":{"abstract":"In 1972, Robin Milner created a proof-checking program at<br\/>Stanford University called LCF (Logic for Computable Functions). The proof-checking program LCF and subsequent systems have been under continual development by a dedicated group of researchers over the past 35 years. These programs have finally reached the level of maturity that they are capable of checking every logical inference of complex proofs such as the Four-color theorem by G. Gonthier, the Jordan curve theorem by the PI, and the Prime number theorem by J. Avigad.<br\/><br\/>The Kepler Conjecture asserts that the density of<br\/>a packing of congruent spheres in three dimensions is never greater than pi\/18^1\/2, or approximately 0.74048. This is the oldest problem in discrete geometry and is an important part of Hilbert's 18th problem. The problem remained unsolved for nearly 400 years until it was finally cracked in 1998 by S. Ferguson and the PI.<br\/><br\/>The purpose of the Flyspeck project is to produce a formal proof of the Kepler conjecture. The research of this proposal will complete the formal proof of the key parts of the Flyspeck Project. <br\/> <br\/>This proposal intends to follow the same general strategy that was pursued by G.<br\/>Gonthier in the formalization of the Four-Color theorem, that is, \"to turn almost<br\/>every mathematical concept into a data structure or a program.\" This proposal<br\/>provides detail about how the published text of the proof of the Kepler conjecture is to be converted to data structures or program. Specifically, many intricate proofs can be represented in terms of a collection of labeled rooted trees. Another part of the proposal gives details about how to automate the proofs of a collection of problems in geometry.<br\/>The Flyspeck project has become a high-profile project in<br\/>math and computer science. It has already been the subject of many invited presentations at international conferences in math, computer science, and philosophy. A number of graduate students (internationally) have become involved in the project. This broad participation will continue. The PI's Flyspeck proposal has been described in a large number of publications with wide circulation, including the Economist (2005), Science (2005), Nature (2003), and the New York Times.<br\/>This proposal has the potential to reshape the way mathematicians approach large-scale computer-assisted proofs. Formal verification methods<br\/>in general have the potential to unprecedented levels of reliability to long and complex mathematical proofs. This proposal explores novel methods to formalize a highly complex proof.","title":"The Formal Proof of the Kepler Conjecture","awardID":"0804189","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1265","name":"GEOMETRIC ANALYSIS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1268","name":"FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["479488"],"PO":["565063"]},"137666":{"abstract":"This project adresses the challenge of automatically extracting high-quality knowledge bases from text corpora. Previous work, led by Prof. Etzioni, developed KnowItAll (http:\/\/www.cs.washington.edu\/research\/knowitall), an unsupervised, domain-independent, scalable system that learns from the Web in an open-ended fashion. Another project, led by Prof. Domingos, has formalized and fully implemented a powerful framework called Markov Logic Networks (MLNs) (see http:\/\/www.cs.washington.edu\/ai\/srl.html) that enable inference and learning in large, first-order models. This project integrates KnowItAll and MLNs to build large-scale ontologies from text corpora: extracting relational tuples, using joint inference to merge and validate the tuples, mapping extracted phrases to a taxonomy, and using probabilistic inference rules to answer queries about the ontology.<br\/><br\/>Consider, for example, the query \"how many Nobel Laureates where born in Europe?\" In response, Google merely provides documents matching the keywords in the query. KnowItAll can only identify people who are explicitly identified as Nobel Laureates and Europeans. This project investigates a system that utilizes both information extraction and probabilistic reasoning to identify candidate answers, not explicitly stated in the text, and their likelihood of being correct. As a simple example, the system concludes that Einstein was born in Europe based on the sentence \"Einstein was born in Ulm, Germany\". The query \"what foods help prevent osteoporosis?\" is answered using a multi-step reasoning chain regarding the ingredients of the food and their ability to prevent the disease.<br\/><br\/>The broader impact of this research includes novel methods of building knowledge bases automatically. Such knowledge bases (after some manual tuning, perhaps) could be used to support a wide range of applications from question-answering systems, to knowledge-based systems for medical applications, to background knowledge in support of machine reading of text. The knowledge bases created by this project will be made freely available to the research community as a Web-site and also as a Web-based API via the project Web site (http:\/\/www.cs.washington.edu\/research\/knowitall\/ReadingTheWeb\/).","title":"III-Medium: Reading the Web: Utilizing Markov Logic in Open Information Extraction","awardID":"0803481","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[365925,365926,"450899",365928],"PO":["563751"]},"139613":{"abstract":"Children with Specific Language Impairment (SLI) experience a delay in acquisition of certain language skills, with no evidence of hearing impediments or other cognitive, behavioral, or neurological problems. To diagnose monolingual children with SLI, clinicians have at hand standardized tests, such as the Test for Early Grammatical Impairment, that provide \"cut-off\" thresholds defining the normal range for children of different ages. Diagnosing bilingual children with SLI is far more complicated, however, due to a lack of standardized tests, a lack of bilingual clinicians, and most importantly a lack of a deep understanding of bilingualism and its implications on language disorders. In addition, bilingual children often exhibit code-switching patterns that make the assessment task even more challenging. The PIs' goal in this project is to contribute to the early and accurate identification of English-Spanish bilingual children with SLI, by developing an automated method for discriminating syntactic patterns indicative of SLI. Recent approaches on differential diagnosis of bilingual children are focused either on assessing the phonological systems of both languages to identify children with speech disorders, or on the analysis of error patterns on specific morphemes, such as article gender and number agreement. In contrast, the PIs' approach is not to restrict the analysis to a specific syntactic structure, but rather to focus on adapting Machine Learning (ML) and Natural Language Processing (NLP) techniques so that they can learn the patterns that distinguish an otherwise typical language development. The PIs will pursue the objectives along two core topics: automatic part-of-speech (POS) tagging of bilingual discourse (in which they will investigate the use of ML approaches, in particular domain adaptation techniques, for combining existing linguistic resources on both languages), and statistical methods for discriminating patterns of language use indicative of SLI (in which syntactic information, generated by the tagger will be used to train statistical models). The intuitive motivation for this approach is that the language patterning of bilingual children with SLI will be different from those of typically developing children both at the syntactic level and at the interaction level of the two languages, and these differences will be captured by the statistical methods.<br\/><br\/>Broader Impacts: The clinical implications for this research are far-reaching, particularly regarding the issue of both over- and under-identification of bilingual children experiencing SLI. Because the criteria for this diagnosis involve identification of disordered patterns of language form, content, and use, children who engage in code-switching are at risk of being inappropriately labeled as SLI and placed in special education services. The ability to apply objective technology to the diagnostic process will serve as another sensitive evaluation instrument, eventually allowing for more accurate differentiation of children demonstrating language differences from those experiencing language disorders. For the NLP community, this research will advance the state-of-the-art by developing approaches that can solve problems where the task involves cross-linguistic features, children?s spontaneous speech, and small amounts of data. The NLP methods developed will be generalizable to other clinical tasks and bilingual populations.","title":"HCC-Small: Collaborative Research: Statistical Methods for Learning to Diagnose Specific Language Impairment in Bilingual Children","awardID":"0812134","effectiveDate":"2008-09-01","expirationDate":"2010-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["563325","562856"],"PO":["565227"]},"137677":{"abstract":"Video clips and corresponding narrations together provide much richer information than either in isolation, yet most current recognition systems process visual and textual information separately. The PIs focus on the task of learning how to recognize corresponding actions in videos and textual narrative accurately and robustly. In particular, they focus on semantic descriptions of human actions. This research will have broad impact on applications including video retrieval in digital libraries, human behavior modeling, and video surveillance.<br\/><br\/>The PIs' research will tightly couple methods in computer vision, natural-language processing, and machine learning through robust, automatically learned correspondences. With a collection of loosely aligned video-text annotation pairs (such as movies or TV shows with their associated screenplays), the task is to learn how to associate action descriptions in text with actions, objects and actors in videos. This correspondence is essential for semantic grounding of text using visual action appearance. The fundamental challenge is bridging the semantic gap of images and of text: images depict geometrical relationships and properties of image regions, while natural language encodes abstract semantic relationships in grammatical structures. Bridging this semantic gap in the context of action understanding is the focus of our research effort.<br\/><br\/>The eventual goal is to be able to recognize actions in videos and create text description for actions in videos. While this goal challenges both computer vision and natural language processing, it also opens up an exciting new and very fruitful collaboration between the two research areas where the task of recognition is achieved by simultaneous learning and inference in both domains.<br\/><br\/>Information on this project, including papers, results, database and open source codes, will be available at http:\/\/www.seas.upenn.edu\/~jshi\/#research","title":"RI-Medium: From Actors To Actions: Analysis And Alignment Of Images, Video And Text","awardID":"0803538","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[365960,"426074","557470"],"PO":["564316"]},"146037":{"abstract":"Ionic polymer metal composite (IPMC) materials represent a very different approach to building microgripper fingers, in contrast with the standard micromachining approach, where semiconductor fabrication tools are used to create fingers out of polysilicon and related materials. Although there has been substantial activity in modeling IPMCs, both as actuators and sensors, there is a significant gap in the knowledge of this fascinating material. There is no model that predicts the deformation and potential force that the material can produce when it is cut into an arbitrary shape. This is a significant gap in knowledge because it forces applications that use this material to simply guess a ?good? shape for the microgripper fingers. With a model, the design of finger shapes becomes an optimization problem, i.e., more scientific, less ad hoc. As a consequence, the modeling results feed into a robust intelligence concept of a microgripper finger that can perform micromanipulation while it senses its environment in an effective and intelligent fashion. This work will develop a model that computes the shape of an arbitrarily shaped IPMC microfinger given an excitation voltage placed at any location on the finger. This will eventually lead to microfinger designs that produce desired forces and deflections for specific applications.","title":"SGER: Modeling Ionic Polymer Metal Composite Actuators and Sensors for Microgripper Applications","awardID":"0843756","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["449805"],"PO":["564316"]},"139503":{"abstract":"This project involves the design and building of<br\/>a knowledge base (KB) system that<br\/>learns to more accurately integrate the many heterogeneous sources of<br\/>information that are relevant to a single scientist's research needs.<br\/>The system works by loosely integrating data of many sorts (including<br\/>unstructured text) into a single typed directed graph, and then<br\/>querying the graph using a query language that allows \"schema-free<br\/>similarity queries\". These queries specify a set of query terms<br\/>(e.g. keywords, entities in the KB, etc) and constraints on the<br\/>desired output (e.g. a target data type). The result of a query is a<br\/>ranked list of KB entities, ordered by similarity to the query terms.<br\/><br\/>After a query, a user can optionally label any subset of the ranked<br\/>list of suggested answers as ``relevant'' or ``non-relevant''. These<br\/>labels drive a learning phase, the goal of which is to produce a<br\/>better ranking. Types of learning currently being investigated<br\/>include EM-based parameter turning, learning to discriminatively<br\/>re-rank, and learning to restructure the graph (by adding or deleting<br\/>edges or vertexes). Queries collected in the laboratory of working<br\/>biologists are used to evaluate these learning methods.<br\/><br\/>The broadest impact of this project is on the problem of learning to<br\/>integrate heterogeneous data sources (including free text and<br\/>structured data). However, if successful, the KB system will have<br\/>broad impact in the biological research community; in particular, we<br\/>believe that adaptive personal KB systems of this sort will be a<br\/>valuable complement to existing biological KBs.<br\/><br\/>Further information on this project can be found at<br\/>http:\/\/www.cs.cmu.edu\/~wcohen\/querendipity\/","title":"III-CTX-Small: Adaptive Integration of Structured and Unstructured Data from Many Sources in a Biological Domain","awardID":"0811562","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["373896","533936"],"PO":["560586"]},"139525":{"abstract":"With the emergence of the multicore architecture comes the promise of integrating enormous computing power in a single chip, thereby enabling parallel computing in all types of platforms including handheld computers and desktop machines. Providing proper software support for applications is critical to harness the true power of this architecture. An inherent characteristic of multicores that presents a significant obstacle is runtime variation: reliability, energy\/thermal behavior and process variation will vary across identically designed components of a multicore, producing a negative impact on application power consumption and performance. Runtime variation has been identified as one of the key problems that could block further scaling of circuits if not properly addressed. <br\/><br\/>This research project is developing an advanced execution system, called a Robust Execution Environment (REEact), that dynamically mediates, controls and adapts an application's execution to the runtime resource landscape originating from runtime variations. It employs a combination of techniques in adapting both the hardware resources and the application software code to overcome the impact of runtime variations. At the hardware level, it adapts the resources, such as setting the speed\/voltage of a node on the multicore. At the software level, REEact dynamically optimizes code, taking into account performance and power consumption due to runtime variations. It elicits the help of the OS in determining what resources to use in running the application. REEact informs the OS about information it dynamically discovers about latency, power, and application behavior. REEact is built as multi-layer hierarchical runtime system that interacts with the parallel application, the OS, and the underlying multicore architecture to ensure that maximum performance is achieved.","title":"CPA-CPL-T: Collaborative Research: REEact: A Robust Execution Environment for Fragile Multicore Systems","awardID":"0811687","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["507752","542016"],"PO":["565272"]},"139536":{"abstract":"This project involves minimizing the power-performance efficiency<br\/>(PPE) loss in future highly uncertain, failure-prone, large-scale (>100 processor) Chip Multi-Processors (CMPs) in which the individual processors are deconfigured in various ways in order to remain operational. Under this scenario of dynamic heterogeneity, the achieved PPE will vary significantly depending on what applications run on which degraded cores. For some assignments of threads to cores, the degradation may be so severe as to render the system unusable. However, the large problem size precludes straightforward search techniques.<br\/><br\/>This problem is addressed through a judicious combination of design-time analysis and runtime measurements coupled with a hierarchical rules-based search algorithm. The algorithm operates at two levels, with the lowest level divided among subgroups of cores. Design time information regarding the characteristics of cores in different degraded states helps prune the search space, while runtime measurements provide feedback about the ``goodness'' of the solution. The result will be a workable solution for future large-scale, highly uncertain CMPs that permits maintaining as close to the peak PPE as possible. The broader impacts of this research involve integrated research and education, broadening the participation of under-represented groups, enhanced infrastructure for research, broad dissemination of results, and potential societal impact.","title":"CPA-CSA-T: Power-Performance Optimization Strategies for Highly Uncertain Multi-core Systems","awardID":"0811729","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["550886","550887"],"PO":["366560"]},"139426":{"abstract":"Large, distributed, user-contributed content sites like blogs, web forums, Slashdot and Wikipedia, have significantly changed the character of mass social interaction online. As individuals participate, their actions are visible to other participants. Examining the work of both preparing content and maintaining the community is key to understanding the nature of these systems. Social translucence is a conceptual framework for thinking about how social actions in an online community facilitate future social interaction. <br\/><br\/>Reputation systems, systems that represent the reputation of participants in an online community, have been well studied in the domain of e-commerce. However, theoretical principles or guidelines to designing and developing generalized reputation systems have not emerged. The proposed research will answer four critical questions about the relationship between reputation systems and the existing framework of social translucence: How can social translucence frame the development of reflective social systems? What methods unpack the components of \"reputation\" to reflect the full range of valued activities in an online community? What technical methods effectively collect, mine or elicit data related to specific dimensions of an individual's reputation? How can systems be built that enable individuals to understand their reputation as it is collected and interpreted by other members of the community? Until recently developing a reputation system to elaborate the framework of social translucence was prohibitively difficult. But the accessibility of large community datasets, like that of Wikipedia, has mitigated one major stumbling block, the availability of data.<br\/><br\/>Intellectual merit: The proposed research will extend social translucence as a framework for the design and development of social and collaborative systems. The ability of developers of distributed contributor communities to build reputation systems that can be appropriated by the community will be illustrated through the use of social translucence. Lastly, these results will develop a situated model of reputation in distributed contributor communities - one that sees a majority of online work activity as part of a community member's reputation.<br\/><br\/>Broader impacts: The primary contribution will be an improvement in the interaction, contribution and sense-making of the ever growing diversity of distributed contributors to online communities. As more people participate in online communities, the level of interaction and trust is limited by the amount of information that participants have about one another. Effectively supporting these communities is an important challenge of social computing. The proposed approach puts socially translucent tools in the hands of the community - those who are in the best position to create, reflect, debate and refine socially translucent representations of behavior.","title":"HCC: Informing Social Translucence with Open Composable Online Reputation Systems","awardID":"0811210","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["543781","508360","440155"],"PO":["565227"]},"139679":{"abstract":"Last Modified Date: 07\/23\/08 Last Modified By: Tatiana D. Korelsky <br\/><br\/>Abstract <br\/>In crisis response domains, emotional manifestations are very complex and extreme emotions are common. While speech technologies have shown significant progress over the years, recognizing and understanding emotional speech in noisy environments is still a big challenge. Understanding such language is daunting given fragmented and ungrammatical utterances in addition to errors <br\/>from automatic speech recognition (ASR). Furthermore, there is little <br\/>research on the analysis of the relationship between emotion detection and language understanding which have traditionally been viewed as parallel independent tasks. Even when the output of one of these tasks is used as an input feature to the other, typically, during training a \"true\" classification is used instead of the \"estimated\" classes (as would be the case if the system were to be used in a real-setting) resulting in a mismatch and degraded performance. This project attempts to overcome such a limitation of current approaches by focusing on analyzing the degree of the dependencies between emotion and intent and investigating joint classification methods via multitask learning for language understanding and emotion detection tasks. <br\/><br\/><br\/>The primary intellectual merit of the project is integrated research on <br\/>developing an end-to-end information processing system that has the potential to very significantly impact the crisis response process. For speech processing, communucation between individuals and emergency dispatch personnel <br\/>as well as communications during responders during response pose a big challenge since callers are typically very emotional and the language used reflects that. Processing such speech requires significant research, and this project can is a first step toward this genre.","title":"RI-Small: Collaborative Research: Dispatcher's Assistant for Emergency First Response","awardID":"0812610","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[371276],"PO":["565215"]},"142980":{"abstract":"This research involves the development of automated negotiation protocols <br\/>for autonomous agents in negotiation networks, where each agent can only <br\/>negotiate with few selected neighbors. That is, we develop agents that can <br\/>carry out any number of simultaneous negotiations with other selfish agents. <br\/>For example, they could be routers in the Internet negotiating over which <br\/>packets to route and which to drop, or purchasing agents in a supply chain <br\/>deciding what to buy and at what price, or web services deciding who to <br\/>provide services for and at what price, or sensors in a sensor network <br\/>deciding which measurements to make and who to forward the results. Our <br\/>research involves building the negotiation strategies for selfish rational <br\/>agents that individuals and companies will want to use, along with the <br\/>negotiation protocols that ensure the agents'<br\/>interactions result in efficient global resource allocations.<br\/><br\/><br\/>The problem of negotiation networks combines features of characteristic <br\/>form games and bargaining games from game theory, combinatorial auctions <br\/>and distributed mechanism design from Economics, network exchange theory <br\/>from Sociology, and distributed algorithm design from computer science. <br\/>Our research brings together results from these disparate fields into <br\/>one framework and provides algorithmic solutions for the dynamic, <br\/>distributed problem of resource allocation among self-interested parties, <br\/>in contrast with game theory and Economics' solutions which are typically <br\/>steady-state axiomatic solutions. Success in this research is potentially <br\/>transformative as it will provide the foundation for the engineering of <br\/>protocols and efficient algorithms for distributed resource allocation, <br\/>which is a pervasive problem in our ever-growing highly-interconnected <br\/>society.","title":"Negotiation Networks","awardID":"0829580","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7938","name":"SENSOR NETWORKS"}}],"PIcoPI":["525589"],"PO":["564924"]},"142991":{"abstract":"Abstract <br\/>This CPATH award funds a collaborative Transformative Implementation project between SUNY Stony Brook and Hofstra University to integrate entrepreneurship and leadership components into the undergraduate curricula at both institutions. The project includes development of modules about innovation, entrepreneurship, and global aspects in several required courses and development of an entrepreneurial computer science minor. In addition students work in global entrepreneurial teams with students at targeted universities in Germany, Romania, and Korea. The project includes community building activities with faculty in the greater New York area to promote adaptation and adoption of the curricula models and resources developed. <br\/><br\/>The intellectual merit of the project lies in the importance and currency of the topic and clear need for such changes in computing education to prepare the upcoming generation of computing professionals. The project has a strong collaborative team with entrepreneurial and educational experience and an enhanced evaluation component that should clearly demonstrate the impact of this innovative approach to the research and education community. The project also includes testing of methodologies for handling associated intellectual property issues that could be of value to many other researchers in the future. <br\/><br\/>The broader impacts of the project lie in the potential to prepare a diverse student and faculty population to pursue entrepreneurial activities in high-technology. The project includes dissemination to a broad community and opportunities for sharing of resources. There is potential for national models that can help to develop a technology-savvy workforce which is vital to the nation?s continued prosperity and security.","title":"Collaborative Research: CPATH TI: Project EXCE2L (Excellence in Computer Education with Entrepreneurship and Leadership Skills)","awardID":"0829641","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":[380708,"397813","397075","547154",380712],"PO":["564181"]},"143750":{"abstract":"Large scientific applications have complex communication needs, including intra-machine point-to-point and global communications, cross-machine checkpointing, and data outputs for online validation and remote data display. Coupled scientific models for multidisciplinary investigations further add to this complexity. This multi-purpose, rich, and dynamic nature of data communications in future exascale codes presents the first challenge addressed by this research. <br\/>Further, given the many-core nature of future computer chips and the likely presence of specialized hardware accelerator cores, application-level communications and I\/O face an increasingly complex set of on-chip, cross-node, and cross-machine interconnects. The complex nature of the physical communication infrastructures present in future exascale machines is the second challenge addressed by this research. In summary, the problem facing developers of future exascale applications for scientific discovery is how to effectively manage the complexity of their communication needs while protecting their most critical `core' communications from perturbation. <br\/><br\/>This project will develop higher level, explicit models for the data communications performed in future exascale codes. These models, called C-Models, will describe and implement the communications performed for I\/O for purposes of online analysis, storage, and visualization, and for data movements across coupled application codes, and will also capture the interaction of the data movements implied by all of the above with the internal data communications inherent to each single application. This abstraction and encapsulation of communication complexity is key to taming the complexity of future exascale applications. The C-Model infrastructure will also help protect the critical core communication component of these applications from perturbation, helping to maximize the performance of these applications on leadership-class machines.","title":"Collaborative Research: Actively Managing Data Movement with Models - Taming High Performance Data Communications in Exascale Machines","awardID":"0833039","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["539738","558205"],"PO":["565272"]},"143420":{"abstract":"The goal of this project is to develop the Application Aware Anonymity (A3) infrastructure, a flexible, secure, and low-overhead infrastructure for deploying anonymity-based services on the Internet. A3 allows applications to tailor their anonymity properties and performance characteristics according to their specific requirements. A3 employs a novel path selection scheme that allows hosts to discover routes with specific anonymity and network properties. Declarative networking is used as an extensible framework for users to specify their routes using a declarative interface. A3 utilizes network coordinate systems that map nodes to n-dimensional coordinates such that the distance between coordinates corresponds to network metrics such as latency and bandwidth between the nodes. To secure its network coordinate systems, A3 utilizes Veracity, a fully decentralized vote-based service that does not require a priori shared secrets or trusted nodes to verify network coordinates. The intellectual merit of this project is the integration of concepts<br\/>from networking, security, and distributed data management. The broader impact of this program is the development of an extensible routing infrastructure that enables the rapid development, deployment, and testing of anonymous source-routing protocols and applications. A3 will be disseminated as open-source, and deployed as a PlanetLab service with a declarative API to enable security and networking researchers to rapidly develop, deploy, and experiment with different anonymous routing strategies. Potential applications of this infrastructure include the anonymization of next-generation Internet services including voice-over-IP, video-on-demand, video teleconferencing, and live audio and video streaming.","title":"CT-S: Application-Aware Anonymity (A3) for the Masses","awardID":"0831376","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["402531","518109"],"PO":["543481"]},"141000":{"abstract":"NSF Proposals: 0820222\/0820170<br\/> Title: Recombinant Services -- Recasting the Web for Continuously Evolving Systems<br\/> PIs: Richard N. Taylor and Nenad Medvidovic<br\/> The broad features of many real-world systems belie the assumptions of classical systems and software engineering: there is no single overarching authority, change is managed by many independent organizations, the many systems serve many masters, and evolution is independent and uncoordinated. Ongoing improvements in the scale and reach of pervasive network infrastructure and the brisk development of diverse Web-accessible resources will play pivotal roles in future systems of systems. Apropos of this future, this project offers a radical rethinking of service abstraction that integrates modern network structures, computation as a fundamental unit of network exchange, and network-wide system awareness to deliver recombinant Internet-scale services for which service composition, reuse, autonomy, context-free interaction, and monitoring are first-order effects. This project pursues a set of architectural principles addressing the challenges, then developing infrastructure consistent with those principles, for the implementation, deployment, and operation of systems of systems. The engineering methods reifying recombinant services---CREST---will refashion the Web, from an architecture that is predominantly data-driven and reactive to one that is computation-driven and autonomic. Our results will be given to the open source community and external collaborating organizations, a technology transfer with the potential of transforming the Web.","title":"Collaborative Research: Recombinant Services -- Recasting the Web for Continuously Evolving Systems","awardID":"0820170","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":["551145"],"PO":["564388"]},"143783":{"abstract":"High-end distributed and distributed shared memory platforms with millions of processors will be deployed in the near future to solve the toughest technical problems. Their individual nodes will be heterogeneous multithreading, multicore systems, capable of executing many threads of control, but with relatively little memory per thread, low bandwidth to main memory and deep memory hierarchies. <br\/>A programming model that supports productive, portable, efficient parallel programming both within and across the nodes of these petascale systems is essential if their potential is to be realized. Since it is easier for application developers and tool vendors to extend existing software rather than adopt a new programming language, a programming model based upon a familiar paradigm is highly desirable. <br\/><br\/><br\/>OpenMP is a widely supported shared memory programming model that provides ease of maintenance. It is suitable for programming multicore nodes, but does not address the needs of distributed memory platforms. This research will significantly extend OpenMP so that it can be used to program all levels of a high-end petascale system. In order to accomplish this, the investigators will enhance its existing mechanisms for describing multiple levels of parallelism, provide additional features for specifying synchronization and for achieving high levels of locality, as well as develop a novel I\/O interface. Moreover, they will substantially improve the state of the art in OpenMP implementation technology, enabling high performance between nodes as well as within them. Results will be demonstrated via a state-of-the-art Fortran\/C\/C++ OpenMP compiler, a highly optimized communications library, and a range of large scale applications.","title":"COLLABORATIVE RESEARCH: Extreme OpenMP: A Programming Model for Productive High End Computing","awardID":"0833141","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":[383082],"PO":["565272"]},"143310":{"abstract":"Quantum information processing (QIP) uses quantum phenomena to perform information-processing tasks that are difficult or impossible with classical resources. These include communication protocols using quantum channels--for example, photons through optical fibers--to transmit quantum or classical information. In general, these quantum <br\/>channels are noisy, causing decoherence of the transmitted states. <br\/>This decoherence is countered by quantum error-correcting codes (QECCs), analogous to classical error correction. These codes are constructed from classical models, but with a certain restriction: the \"dual-containing\" constraint. The PI and collaborators have generalized the most common \"stabilizer\" codes to include both standard QECCs and entanglement-assisted QECCs (EAQECCs).<br\/><br\/>Entanglement is a property of quantum states that are correlated more strongly than classically possible: classical correlations cannot <br\/>boost the rate of information transmission, but entanglement can. <br\/>Entanglement is a resource for protocols such as teleportation and dense coding. In error correction, entanglement can either boost the rate of information transmission or increase the number of correctable errors. EAQECCs can be constructed from arbitrary classical linear codes, with good classical codes yielding good EAQECCs.<br\/><br\/>This project explores the properties of EAQECCs, their construction, and applications. The PI will study classes of codes such as quantum Turbo codes, LDPC codes, and convolutional codes, evaluate their performance, and determine the entanglement that they require. He will combine entanglement assistance with other coding ideas-- particularly operator codes--producing families of codes with a broad range of properties for different applications. The PI will also study hybrid codes for transmitting both classical and quantum information, and versions of EAQECCs for continuous-variable systems.","title":"Entanglement-assisted quantum error-correcting codes","awardID":"0830801","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[381587],"PO":["564924"]},"143794":{"abstract":"Most of the traditional HEC applications and current petascale applications are written using the Message Passing Interface (MPI) programming model. The MPI-1 standard provides communication semantics for two-sided operations. The MPI-2 standard added new one-sided communication semantics. However, most of the current candidate petascale applications continue to use the MPI-1 semantics.<br\/>These applications find the available MPI one-sided communication semantics and their implementations in existing MPI-2 libraries very restrictive to exploit performance, scalability and fault-tolerance. The investigators, involving computer scientists from Ohio State University (OSU) and computational scientists from Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), will study and analyze the current restrictions in the MPI one-sided communication semantics, their implementations and usages. Novel solutions will be proposed to alleviate these restrictions so that the next generation ultra-scale systems and applications can be scaled to hundreds of thousands of cores.<br\/><br\/>The investigators will specifically address the following challenges: 1) What are the limitations of using MPI one-sided operations in petascale applications? 2) What extensions are possible to the current MPI one-sided operations to alleviate such limitations? 3) How to design and implement these extensions in an MPI library for emerging ultra-scale HEC systems? 4) How to redesign petascale applications to take advantage of proposed one-sided extensions and their implementations? and 5) What kind of benefits (performance, scalability and fault tolerance) can be achieved by the proposed extensions for petascale applications on the next generation ultra-scale systems? The research will be driven by a set of petascale applications (ENZO, AWM-Olsen, PSDNS and MPCUGLES) from established NSF computational science researchers running large scale simulations on the TACC Ranger and other NSF HEC systems.","title":"Collaborative Research: Extending One-Sided Communication in MPI Programming Model for Next-Generation Ultra-Scale HEC","awardID":"0833169","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["561947"],"PO":["565272"]},"145620":{"abstract":"The proposed research has the potential to transform the nature of computer aided design (CAD) systems, specifically by building a system that allows sketching and 3D modeling to co-exist in a CAD system that abandons the assumption that a system must maintain a valid 3D model at all times. In exploring the middle ground between sketch and object, the proposed work addresses one of the holy grail problems in interactive graphics: how to move seamlessly between 2D and 3D. This understanding includes not only a computer graphics perspective (algorithms and representations) but also an architectural design perspective, where practitioners have a deep working knowledge of form creation but may lack a computational or mathematical background. To support sketching on 3D models, useful operations for creating design elements, incorporating source material and organizing them in 3D space need to be invented and specified by designers and those that are computationally feasible in a free flowing interactive environment are defined and implemented. The research method of iterative design and development allows this project to focus on the needs of creative designers in the context of computer scientists and their understanding of computational feasibility. Broader Impact: The project has a broader impact on the creative design process by offering a novel approach for creating and editing 3D form, and also impacts the communication of ideas where 2D representation has dominated.","title":"Exploratory Research in Creative Sketching for 3D Design","awardID":"0841534","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["541919","548698"],"PO":["424970"]},"143200":{"abstract":"There are many sequential decision problems which can be appropriately modeled as a repeated game, in which the decision-maker is competing with an adversary. For instance, in the problem of virus detection in a computer network, the aim is to label incoming packets as either clean or infected, while a hacker aims to design infected packets that escape detection. Similar problems arise in other areas of computer security (including spam filtering and detection of denial-of service attacks), in internet search (such as deciding if a highly-linked web page is genuinely authoritative and should have high page rank), and in financial applications (such as portfolio optimization). In these problems, the decision-maker aims to perform almost as well as the best element of some comparison class. Even for decision problems that are not inherently adversarial, it is often appealing to model them in this way, since the assumptions are sufficiently weak that effective learning algorithms for these adversarial settings are very widely applicable. Many of the key algorithmic approaches to online learning problems can be viewed as methods involving regularization, an idea that has its origins in the solution of ill-posed problems, such as statistical estimation problems. This project aims to exploit this regularization viewpoint in the analysis and design of methods for complex online learning problems. In particular, its aims are (1) To develop techniques for decision problems with limited feedback. (2) To develop techniques for decision problems with complex losses that cannot be simply decomposed into a sum across trials. (3) To develop efficient learning algorithms that can simultaneously compete effectively with a variety of rich comparison classes and a variety of constraints on the adversary. (4) To improve our understanding of the relationships between online decision problems (in adversarial settings) and statistical decision problems (in probabilistic settings). Successful research outcomes of this project are likely to increase our understanding of complex sequential decision problems and to provide design methodologies for effective learning algorithms for these problems, and hence have a significant potential for practical impact in many application areas, including computer security and computational finance.","title":"Regularization Methods for Online Learning","awardID":"0830410","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["485442"],"PO":["565251"]},"145862":{"abstract":"This goal of this research project is to investigate issues in the design and development of CS-BibCube, a multidimensional text data cube, constructed based on multidimensional categorical dimensions (e.g., author list, venue, and date) and unstructured text attributes (e.g., title, abstract, and contents), to facilitate multidimensional online analytical processing (OLAP) and mining of computer science literature. Data cube has become an essential engine in data warehouse industry and has been extended to handle relatively structured non-relational data, including spatio-temporal data, sequences, graphs, data streams, etc. However, it is still challenging to handle unstructured text data. This project is to explore and evaluate the possibilities and alternatives on the design, multidimensional modeling, implementation, performance improvement, and deployment of text-cubing and text-OLAP. The work will integrate multiple disciplinary approaches derived from data cube and OLAP, information retrieval, text mining, and machine learning, and further study is expected to be expanded to other multidimensional text databases with broad applications in business, industry, government agencies, scientific research, and education. <br\/><br\/>The research results are to be published in research forums on information retrieval, data mining, and database systems, and be integrated into the educational program at the University of Illinois at Urbana-Champaign. The progress of the project and the research results will be disseminated via the project Web site (http:\/\/www.cs.uiuc.edu\/~hanj\/projs\/csbibcube.htm).","title":"SGER: CS-BibCube: OLAPing and Mining of Computer Science Literature","awardID":"0842769","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["563535"],"PO":["563751"]},"143442":{"abstract":"As a result of recent wireless technology advances, mobile devices with significant computational abilities, gigabytes of storage capacities, and wireless communication capabilities have increasingly become popular. In addition, positioning techniques like GPS are incorporated into an increasing number of mobile devices. Emerging mobile applications allow users to issue location-dependent queries in a ubiquitous manner. It is believed that location privacy preservation represents important security and privacy problems in mobile computing environments. For instance, when a mobile user launches a series of queries to a location-based service (LBS) provider, that user?s trajectory can be tracked through the service logs. Although it is true that not all location-dependent queries are privacy-sensitive, it is of growing importance to offer users the choice of protecting their location privacy when it is necessary. The technical objectives of the project are to (1) develop space encryption and space decryption mechanisms, (2) design privacy-protected query processing algorithms to answer spatial queries based on encrypted search space, and (3) develop a sophisticated SPEAR system architecture for supporting real-world applications. This project also promotes education by exposing students to mathematical and technological underpinnings in the fields of security and data management. The success of this project will lead to another wave of increased usage for LBS and benefit the economy of our country. Research results will be disseminated through publications at conferences, journals, and the website at http:\/\/www.eng.auburn.edu\/~weishinn\/SPEAR.html","title":"CT-ISG: SPEAR: Space Encryption based Query Processing for Privacy-Aware Location-based Services","awardID":"0831502","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["564263","409962","409964",381968],"PO":["565327"]},"143211":{"abstract":"A scientific program is a mathematical one so there are two factors<br\/>contributing to its performance. One is the time required to perform<br\/>arithmetic. The other is the time needed to move data through the<br\/>memory hierarchy of the computer. In today's large applications, the<br\/>latter cost often dominates. The work funded by this grant focuses on<br\/>efficient computational methods for solving the problems in matrix algebra<br\/>that arise in a wide variety of science and engineering applications.<br\/>Codes for such problems are typically constructed as sequences of calls to<br\/>the routines known as the Basic Linear Algebra Subprograms (BLAS). Writing<br\/>programs in this way promotes readability and maintainability but can<br\/>be costly in terms of memory efficiency especially for matrices of<br\/>large order.<br\/><br\/>The grant will be used primarily to support a Ph.D. student who will<br\/>study ways to combine multiple BLAS routines into a single routine that<br\/>performs the functions of more than one BLAS. He will examine ways to<br\/>create composed BLAS via novel algorithms and performance programming<br\/>techniques, developing a general methodology for their creation in the<br\/>process. His work will ultimately form the basis for a tool that creates<br\/>composed BLAS automatically. Composed routines can significantly reduce<br\/>the amount of data read from main memory. Preliminary results include<br\/>speedups as large as 90\\%.","title":"Toward Software Tools for Memory-Efficient Matrix Algebra","awardID":"0830458","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7934","name":"PARAL\/DISTRIBUTED ALGORITHMS"}}],"PIcoPI":["518600"],"PO":["565157"]},"143453":{"abstract":"Large-scale botnets have become a blight on the Internet. Botnets engage in a variety of harmful activities, including initiating DDoS attacks, committing click fraud, propagating adware, and sending enormous volumes of spam. Though there is an increasing awareness of botnets, there are gaping holes in our understanding of botnets, both in terms of macroscopic properties as well as the ability to track and thwart specific attacks.<br\/><br\/>As part of this project, we develop a response to the botnet threat by building a monitoring system that gathers and distributes objective data on the problem. Our work offers three novel contributions. First, we solve many of the challenges involved in building a real-time botnet monitoring platform. For example, our system executes live botnet nodes, and as such, it must prevent these nodes from causing harm to other hosts on the Internet. Second, we implement several prototype defensive tools that take advantage of the real-time information provided by the platform. Third, our work exposes the rich texture of the botnet ecosystem by analyzing botnets from multiple perspectives and by correlating the attack vectors with observations of real bots executed in laboratory settings.<br\/><br\/>Our botnet monitoring platform thus advances our understanding of botnets and enables promising anti-botnet defense tactics. It thus serves as a crucial step in the development of a trustworthy network that can support a much wider diversity of uses than can be found on today's Internet.","title":"CT-M: A Real-Time Botnet Monitoring Infrastructure","awardID":"0831540","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["549890","532591"],"PO":["565136"]},"143343":{"abstract":"Project Abstract:<br\/><br\/>Without their realizing it, end-users have been turned into authors of access-control policies. Everywhere from Google to Facebook to Microsoft HealthVault and beyond, these policies are usually hidden behind simple user interfaces, but ultimately the users are responsible for setting and then taking responsibility for the consequences of these policies. Indeed, the apparent simplicity of the interfaces sometimes belie the significance of the outcomes.<br\/><br\/>Because applications are a black-box to end-users, it becomes difficult for users to predict the consequences of an action. Users see only their view of the world, but their access-control decisions affect the views of others. End-users need tools to determine the effect of their decisions, with special emphasis on the effect of policy changes. This project is developing user-friendly means to browse and investigate the details of these effects and changes. The underlying techniques are firmly grounded in theory, resulting in formal accuracy guarantees rather than approximate answers. The tools themselves summarize information in ways that account for the cognitive expectations and biases of users.<br\/><br\/>The broader impact of this project comes from the focus on end-users rather than programmers and other technical users. Some end-users are too eager to embrace new technologies without fully appreciating their consequences, while others are too tentative due to their concern about (sometimes imaginary) problems they might create. The tools from this project help the former become more cautious, and the latter more confident. The net result should be a more savvy, yet vastly more inclusive, Cyber society.","title":"CT-ISG: Power to the People: Tools for Explaining Access-Control Consequences","awardID":"0830929","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["485802","485803"],"PO":["497499"]},"143233":{"abstract":"Abstract:<br\/><br\/>The objective of this project is to develop novel digital forensic techniques and support it with experimental validation for later technology transfer to forensic investigators. The goal of digital forensics is to establish the origin, integrity, processing history, and meaning of evidence in digital form that includes digital images, video, or audio. This is an emerging and rapidly developing field typically approached using methods from signal processing, estimation, and detection combined with machine learning. The main thrust of this project is development of forensic methods that rely on systematic artifacts of imaging sensors due to sensor design, in-camera signal processing, and imperfections of the sensor manufacturing process itself. These artifacts form an equivalent of a digital sensor fingerprint. By detecting the fingerprint in a digital image, one can link a given image to the specific camera\/sensor that took it and establish thus the image origin and integrity or uncover the image processing history. Some of the research directions are highly relevant and complex problems that have not been addressed before, such as the possibility to detect attempts to forge the fingerprint. Other tasks involve using existing concepts in a novel manner or substantially improve upon existing technology. One of them is to group images according their source camera without having the cameras. The developed methods are subject to large scale experimental verification on a hundred thousand images obtained from more than 1000 cameras.","title":"Digital Forensic Methods Using Systematic Artifacts of Imaging","awardID":"0830528","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":[381397],"PO":["529429"]},"143475":{"abstract":"Users' desires to share high definition audio and video around the home are driving the need for ever-increasing wireless bandwidth. Wideband radios, whose bandwidth spans hundreds of MHz to many GHz, can provide the solution. Recent years have shown significant work on the design and implementation of wideband radio transceivers that function for a single link. However, this is not enough; for wideband technologies to become practical, we need a wideband link to coexist with narrowband technologies with which it shares the spectrum (e.g., 802.11 a\/b\/g and WiMax), as well as operate efficiently in the presence of other wideband links producing successful wideband networks.<br\/><br\/>This project presents FAWN, a Frequency Adaptive Wideband Network designed for wideband radios operating in the unlicensed spectrum. FAWN allows wideband transceivers to coexist with narrowband devices operating in the same frequencies, while forming a network of their own. <br\/>FAWN also exploits frequency diversity, introducing novel autorate algorithms and MAC protocols that are frequency-aware. We will implement FAWN in wideband radio hardware and evaluate our design in a testbed of wideband nodes and narrowband interferers.<br\/><br\/>The intellectual merit of this research involves designing and building the first system architecture for wideband networks that coexist safely with unknown narrowband devices. The work also has an important broader impact because it enables the rich-media wireless home which is of great interest to the wireless industry, and is closely integrated with an educational plan that involves both graduate and undergraduate students, with special emphasize on women and underrepresented groups.","title":"NeTS-NEDG: Adaptive Wideband Networks for the Multimedia Home","awardID":"0831660","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["560189"],"PO":["557315"]},"145301":{"abstract":"This project explores the impact of geographically distributed 3D tele-immersive environments on dancers' creativity and their perception of themselves and each other. More specifically, the project will study the impact of digital options, such as scale and multiply, on the dancers' creative expression and improvisation. A formal notation, called a creativity graph, will be derived from a Laban movement analysis of the dancers' movement. A dancer develops a creative dance, a new sequence of phrases, when he\/she generates a new association between two movement states (e.g. a new association between two Laban positions) based on some feedback from the immersive environment due to either invoking digital options or due to some unexpected performance of the system. This project will have a fundamental impact on our understanding of dancers' creativity within tele-immersive dancing environments and computing and will contribute a new concept of transformational movement graphs as a computational representation of dance creativity in distributed multi-site 3D tele-immersive spaces.","title":"SGER: Collaborative Research: Exploration of Distributed Creativity in Multi-Site 3D Tele-Immersive Spaces","awardID":"0840399","effectiveDate":"2008-09-15","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["563532"],"PO":["565227"]},"143002":{"abstract":"Behavior-based robotics was established around the idea that robots could be constructed by connecting elementary sensor and actuator modules, without the need to form any internal representation of the world in which they operate. When designed appropriately, such robots, both as individuals and as groups, exhibit complex and seemingly ?intelligent? behaviors, solving challenging tasks in real time, while reacting only to their local environment and obeying sets of local rules. In part, this approach to robotics was inspired by considering natural systems, such as self-organization and adaptability of social insects in their colonies. An analogous approach to initiate the development of the field of behavior-based molecular robotics will be performed over the next three years, leading to groups of molecules that would appear to an observer to show a variety of task-oriented behaviors or some form of purposeful and dynamic self-organization. <br\/><br\/>Individual behavior-based molecular robots are single molecules displaying multiple sensors-actuators. When exposed to artificial landscapes displaying substrates keyed to their sensors-actuators, the molecules start executing elementary steps, determined, in a stochastic sense, by their constantly changing local environments. On some landscapes, called prescriptive, individual molecular robots and their collectives will execute algorithms mimicking wound-up automata with sequence control mechanisms. In contrast, on non-deterministic landscapes, the robots and their collectives will demonstrate properties emerging from internal organization of individual sensors and actuators and through local interactions between molecules and their environments. Importantly, these new interpretations of molecular behaviors will allow radically different experiments from all previous approaches to molecular robotics, while keeping experimental designs realistic, leading to embodiment in the physical world.","title":"Collaborative Proposal: EMT\/MISC Behavior Based Molecular Robotics","awardID":"0829685","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["555498"],"PO":["565223"]},"143486":{"abstract":"Despite the clear success stories of wireless networks in the public, commercial, and governmental realms, recent work has shown that poor routing\/scheduling can significantly impair their performance. We note that the unacceptable performance loss observed is not a consequence of a lack or deficiency of scheduling\/routing algorithms in the literature; rather the cause is the information structure (channel state, queue state, topology, etc) that these algorithms assume. This proposal steers down a different path, initiating a conceptual shift toward the primary importance of information structure. We consider practical scenarios where only a small fraction of the network state can be explored. The goals of this proposal are two-fold: (a) characterizing the fundamental impact of partial\/delayed network state information (NSI) on network throughput and other performance metrics such as delay and reliability, and (b) developing high-performance and distributed algorithms that can operate optimally subject to partial information. <br\/>Broader Impact: We believe this work can contribute towards furthering basic network science required to design high-performance scheduling and routing with limited NSI. We also plan to bring some of the questions both as interesting undergraduate projects, as well as develop a new graduate-level course on the mathematics of the design of communication networks.","title":"Collaborative Research: NEDG: Network Scheduling and Routing under Partial Information Structure","awardID":"0831756","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["539541"],"PO":["557315"]},"143013":{"abstract":"EMT\/BSSE: A Controller for Autonomous Systems<br\/>Based on Principles of Vertebrate Neuromodulation<br\/>Robots and autonomous systems require some level of supervision and tuning of<br\/>parameters to fit a particular domain. However, biological organisms have the ability to<br\/>respond quickly and appropriately in an ever-changing world. Because this adaptability<br\/>is so critical for survival, all vertebrates have sub-cortical structures, which comprise the<br\/>neuromodulatory systems. These systems regulate fundamental behaviors and set the<br\/>organism?s internal and behavioral states. This research involves designing a controller<br\/>for autonomous systems that is modeled after the vertebrate neuromodulatory system.<br\/>This neurally inspired model enables robots to approach the behavioral complexity and<br\/>flexibility associated with higher order animals, and would be a major improvement in the<br\/>design of autonomous systems.<br\/>Neuromodulators in the nervous system signal environmental changes to the nervous<br\/>system that alter neuronal responses in such a way that the organism can respond<br\/>quickly and accurately to these changes. There are separate neuromodulators that<br\/>respond to threats, reward anticipation, novelty, and attentional effort. However, each of<br\/>these neuromodulatory systems have a similar effect, that is, to cause an organism to be<br\/>decisive when environmental conditions call for such actions, and allow an organism to<br\/>be more exploratory when there are no pressing events. A design strategy, based on<br\/>principles of the vertebrate neuromodulatory system, is used to control the behavior of<br\/>autonomous robot systems. This research shows that such a system responds<br\/>appropriately and adapts to environmental changes without human intervention.<br\/>Moreover, this research shows that a controller, which is based on neuromodulation, can<br\/>be extended to any system in which an agent is situated in a dynamic, unconstrained<br\/>environment.","title":"EMT\/BSSE: A Controller for Autonomous Systems Based on Principles of Vertebrate Neuromodulation","awardID":"0829752","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["541837"],"PO":["565223"]},"143255":{"abstract":"Collaborative Research:<br\/>New Directions in Graph-Based Code Design<br\/><br\/>Abstract<br\/><br\/>This collaborative research focuses on the physical layer of digital communication system design ? in particular on the analysis, design, and implementation of capacity-approaching low-density parity-check (LDPC) codes for practical communication environments. In the last ten years, the area of channel coding has undergone a revolutionary change with the growing popularity of graph-based codes and iterative decoding algorithms. These coding methods, which include both turbo codes and LDPC codes, approach the limits of channel coding performance promised by Shannon in his landmark 1948 paper. Currently, these codes are in the process of replacing conventional error control techniques in numerous digital communication and storage standards, including, among others, deep-space communication, next-generation wireless transmission, last-mile cable transmission, digital video broadcasting, and high-density digital magnetic recording.<br\/><br\/>The research addresses several issues related to graph-based codes. In particular, it focuses on the analysis, design, and implementation of LDPC convolutional codes, which have several advantages compared to LDPC block codes, but have not received much attention from the research community. Conventional convolutional codes, on the other hand, have had a transformative effect in numerous practical communication environments, and the same is likely to be true in the capacity-approaching world of LDPC codes. The project emphasizes bridging the gap between advanced theoretical research and realistic practical implementations. In particular, it is concerned with adapting LDPC convolutional code designs to various industry standards that require flexibility in both frame length and code rate and with developing VLSI implementations of hardware decoders that can be tested under real operating conditions.","title":"Collaborative Research: New Directions in Graph-Based Code Design","awardID":"0830608","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["547544"],"PO":["564924"]},"143376":{"abstract":"Abstract:<br\/><br\/>This research project studies the privacy and integrity properties of recently-proposed voting systems which aim to prove tally-correctness to voters and election observers. Privacy and verifiability are key characteristics of voting systems, and their importance cannot be overstated. The research focuses on two types of systems: (1) ones that use randomized partial audits of mixnet-like components and reveal information to computationally bounded adversaries, and (2) ones that do not use cryptography in the construction of ballots. The research approach builds on an information-theoretic model of voting to measure privacy loss and to study the fundamental limits of privacy and verifiability. The research has practical value for voting system designers and election officials. In addition, this project has a significant outreach component, including the development of course material for high school education in cryptography and voting systems.","title":"CT-ISG: The Privacy and Verifiability of Practical Voting Systems","awardID":"0831149","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["496577"],"PO":["565327"]},"143024":{"abstract":"Biological systems are the product of an evolutionary process of random tinkering and selection that resulted in unexpected and non-intuitive ?engineering? solutions to dynamically varying conditions. Thus, biological systems are robust, adaptive and evolvable information processing systems that operate asynchronously and in parallel on multiple scales. The examination and characterization of the design principles of biological circuits has the potential to revolutionize biology, medicine and the way computing and communication systems are built. This project is pioneering important advances at the interface between biology and computation by pursuing two complementary goals: (1) to develop a modular, parallel-ready simulator to replicate the multi-scalar architecture of complex biological systems; (2) to discover key design principles relevant to information processing systems in general by reproducing biological design in silico. <br\/><br\/>Information processing by cells encompasses multiple scales connecting molecular events to phenotypes. Current simulation techniques have limited multi-scale and modular capabilities, resulting in models that describe only a single feature of a given system and miss the relationships between architecture, function and behavior. This research effort addresses these limitations by representing biological systems as a hierarchy of functional executable modules. The design of the platform obeys four basic principles: 1) components are objects; 2) objects are governed by rules; 3) rules include some degree of stochasticity; and 4) objects and rules are organized in functional and spatial modules that compose a hierarchy. The development of the new platform is driven by the construction of simulations of key biological model systems with an unprecedented scope and precision, such as bacterial chemotaxis, epidermal growth factor receptor signaling, the acute inflammatory response, and parallel processing by bacterial colonies. The reproduction of these biological systems in silico is providing insights into their design principles, which in turn advances the future design and implementation of distributed technological systems.","title":"EMT\/BSSE: Hierarchical representation and simulation of modular cellular systems","awardID":"0829788","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[380806],"PO":["565223"]},"143145":{"abstract":"OPTIMAL TRIANGULATIONS FOR SCIENTIFIC COMPUTING<br\/><br\/>ABSTRACT FOR PROPOSAL#0830209<br\/><br\/>Recent progress in scientific computing motivates new triangulation<br\/>problems. As new numerical methods being developed, new geometric<br\/>constraint formulations that are key in the accuracy and convergence<br\/>analysis of these methods emerge. In this two-year project, practical<br\/>and theoretically sound algorithmic solutions for a number of<br\/>triangulation problems will be studied. More importantly, new software<br\/>based on these solutions will be developed and made available to the<br\/>public. In particular, first ever software for computing acute and<br\/>non-obtuse triangulation problems will be deployed. Such software are<br\/>sought for in scientific computing as a geometric tool to be coupled<br\/>with the widely used finite volume methods as well as in graphics<br\/>applications. Practical algorithmic solutions for other triangulation<br\/>problems such as minimum weight Steiner triangulations which is<br\/>expected to find use in networking applications, will also be studied.<br\/>In addition, parallelization of the aformentioned algorithmic<br\/>solutions will be studied within this project. The project will ensure<br\/>its broader impact through distribution and integration of new robust<br\/>software. Project personnel consists of one graduate student that will<br\/>be selected among underrepresented groups in engineering.<br\/>____________________________________________","title":"Optimal Triangulations for Scientific Computing","awardID":"0830209","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["391289"],"PO":["565157"]},"143387":{"abstract":"The ability to reason about different equational theories is very<br\/>important to the analysis of cryptographic protocols. An understanding of<br\/>how equational properties of a function used by a protocol can be<br\/>exploited by an intruder can be invaluable in finding flaws that might<br\/>otherwise be missed. Numerous examples exist in the literature and even<br\/>in fielded protocols. One very powerful tool for cryptographic protocol<br\/>analysis with equational theories is equational unification. Equational<br\/>Unification was the basis of the NRL Protocol Analyzer (Maude-NPA). Its<br\/>use of these theories allowed it to both reproduce existing flaws and find<br\/>new ones at a level of precision way beyond that available to other tools<br\/>at its time. This suggests that equational unification if properly<br\/>extended, can give support in a similar fashion to analysis of<br\/>cryptographic protocols that use functions that obey more expressive<br\/>equational theories. The aim of this project is to provide a<br\/>laboratory for equational unification that will develop the algorithms<br\/>and techniques that can be used to support the use of equational<br\/>unification in cryptographic protocol analysis. This effort will<br\/>consist of two parts: the development of unification algorithms for<br\/>theories of interest to cryptographic protocol analysis, and the development<br\/>of new techniques for employing unification in cryptographic protocol<br\/>analysis. The project will help in the design and implementation of next<br\/>generation tools for protocol analysis. Tools developed in the project<br\/>will be made available to other researchers working on formal protocol<br\/>analysis methods as well as to protocol designers for experimentation. The<br\/>educational component of the project will involve undergraduate and<br\/>graduate students at both institutions.","title":"Collaborative Research: CT-M: Unification Laboratory for Cryptographic Protocol Analysis","awardID":"0831209","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["402395"],"PO":["565264"]},"143277":{"abstract":"Complex networks, including communication, economic and social networks,<br\/>have long been studied due to their fundamental technological, <br\/>financial and societal significance. Over the last ten years, the dramatic<br\/>growth of the Internet, the WWW and their applications,<br\/>has intensified the study of complex networks more than ever.<br\/>This work isolates parameters, including semantics and incentives,<br\/>that differentiate the structure and function of distinct applications. <br\/>This work also develops models capturing features of<br\/>dynamic and evolving networks, such as online communities.<br\/>Otbaining accurate models for complex networks is of fundamental <br\/>significance in simulation, prediction and information retrieval.<br\/>All the above enhance performance and range of applications.<br\/> <br\/>The first generation of models for complex networks involved topologies<br\/>with skewed statistics and the small world phenomenon. However, in several<br\/>applications, these features are not adequate. This work develops <br\/>\"flexible models\" which generate graphs with a wide variety of further <br\/>characteristics. In particular, the work develops: <br\/>(a) Topologies with very low assortativity, such as the ones observed<br\/>in routing and biological networks. This is accomplished by an extension<br\/>of the classical Erdos-Gallai and Havel-Hakimi graph algorithms.<br\/>(b) Networks whose nodes and links capture semantics. In particular,<br\/>nodes are associated with vectors (one dimension for each relevant attribute),<br\/>and links are added with probabilities proportional to some vector <br\/>similarity function (such as the inner product). This is a vast generalization<br\/>of classical Erdos-Renyi random graphs.<br\/>(c) Networks formed by selfish agents, which evolve according to <br\/>incentive-based dynamics. The work also develops algorithms which exploit <br\/>the special features and structure of the network.","title":"Flexible Models For Complex Networks","awardID":"0830683","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[381502],"PO":["564924"]},"143398":{"abstract":"As records of individuals' activities become increasingly computerized and linked, privacy becomes an ever more challenging problem. It is especially challenging when legitimate security needs require the ability to link different transactions and even obtain details about the individuals involved. The focus of this project is on cryptographic technologies that achieve a compromise: transaction records should be anonymous until special circumstances (such as wrong-doing on the part of a particular individual, or an emergency that requires special measures) arise. One wants to specify these circumstances before a transaction takes place, so an individual can choose not to participate in a transaction that does not provide him or her with sufficient privacy guarantees.<br\/><br\/>The project considers several trade-offs between anonymity and accountability. Conditional anonymity\/conditional disclosure means that an individual user is anonymous until her activities violate a certain condition, at which point some piece of information about her becomes known. Revocable anonymity means that a user is anonymous to all but a special anonymity-revoking trustee that only becomes involved in case of emergency. Traceable anonymity means that, under special circumstances, it is possible to quickly trace all of a particular user's transactions; this can be a form of conditional anonymity where the circumstances are due to the user's misbehavior or revocable anonymity where a trustee decides when to run the trace algorithm. This project investigates techniques that allow one to develop and enforce anonymity contracts between individuals and organizations that spell out what can become known about the individual user and under what circumstances.","title":"CT-ISG: Crypto Algorithms for an Integrated Approach to Conditional, Revocable and Traceable Anonymity","awardID":"0831293","effectiveDate":"2008-09-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["448713"],"PO":["565264"]},"143046":{"abstract":"Biological networks show the complex interactions between bio-chemical entities that are often vital for the survival of organisms. The entities communicate with each other to collaborate and perform complex functions that they can not do individually. Numerous applications follow an interaction pattern that resembles biological networks. Wireless networks, sensor networks and homeland security are just a few examples to these applications. Employing biological networks to model the communication patterns in these applications is very promising as the biological networks are robust and flexible. The biological networks efficiently adapt to the alterations in genes or proteins to minimize the damage done to the network by finding alternative ways to keep the network stable whenever it is possible.<br\/><br\/>One of the critical problems in analysis of biological networks as well as many other applications with complex communication networks is finding similarities between them. To solve this problem, it is necessary to find an alignment of the interacting entities of the input pathways. An alignment of two networks is a one-to-one mapping between a subset of their nodes (i.e., entities). This research develops a generic framework that enables efficient alignment of two networks for gene interaction and metabolic networks. These two networks cover a broad spectrum of communication models ranging from Boolean to stoichiometric models. Unlike existing network alignment methods, this proposal considers the functional similarities of the interacting entities in addition to their structural and topological similarities. This research also develops new methods for indexing network databases. These index structures allow answering range and top-k queries efficiently over a network database.","title":"EMT\/BSSE: Biological networks as a communication model for entities with complex interactions","awardID":"0829867","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["538607"],"PO":["565223"]},"144025":{"abstract":"The objective of this project is to systematically design means of obtaining, tandardizing, and manipulating quantified Reliability, Availability and Serviceability (RAS) information from extreme-scale High Performance Computing (HPC) distributions, and to develop a novel, scalable framework for the real-time RAS monitoring and modeling of these systems via the research and creation of an optimal feedback control loop encompassing the entire computational environment. This work is necessitated by the continual and substantial increase in the size and scope of HPC systems, which is causing rapid inflation in the number of faults, errors, and other performance interruptions encountered by these machines.<br\/><br\/>As HPC systems move towards the petaflop era, a greater focus must be placed on the performance interruptions encountered by these machines, and the development of means by which they may continue uninterrupted computation. In this extreme-scale environment, efforts aimed towards maintaining high reliability and uptime are futile ? with their enormous processor and computational unit counts, these systems will inevitably encounter performance issues, and failure must be expected. This project aims to 1) research and develop advanced, standardized methodologies for gathering application- and system-level data and generating quantifiable RAS metrics, 2) provide a novel, scalable solution for improving accuracy in reliably predicting imminent node-wise and system failures in large-scale systems, and 3) devise defensive and proactive techniques for reducing the computational costs required to timely and accurately handle resilience issues and model system health. In summary, this work attempts to alleviate the time and cost limitations of contemporary, reactive fault tolerance schemes, and will advance the development of scalable, proactive, and intelligent resilience provision in large-scale computing deployments. In addition, the Resilience Consortium will be established to synergistically research and develop, share data and findings, and disseminate knowledge to the public.","title":"CSR-DMSS, PSCE: Collaborative Research: Scalable Resilience in Large-Scale Systems","awardID":"0834483","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383926,383927],"PO":["535244"]},"145356":{"abstract":"SGER: Collaborative Research: Contextual Machine Translation<br\/><br\/><br\/>Despite significant progress in machine translation, little work has been done to address large-scale translation of natural multi-party human interactions in real-world contexts. Multi-party interactions typically take place in a context where multimodal events such as hand gestures, head gestures, gaze, body movements, etc. are available and utilized by participants to interpret human language and establish common ground. This observation leads to the hypothesis that modeling multimodal environment and interaction discourse can improve automated language interpretation for the purpose of machine translation. Based on this hypothesis, this exploratory research investigates the role of multimodality in machine translation of multi-party conversations using a subset of the AMI meeting corpus. An appropriate level of multimodal representation is identified focusing on user gestures and presentation slides. Correct referents to referring expressions and correct senses to ambiguous words are annotated and used to evaluate whether and how multimodal context improves reference resolution and word sense disambiguation. The enhanced semantic processing utilizing multimodal information is incorporated in statistical machine translation for English-German. The objective is to conduct proof-of-concept experiments exploring the usefulness of multimodal and discourse information for statistical machine translation in real-world contexts. The results from this exploratory study will provide insights on algorithms and systems for automatic extraction of multimodal information, language interpretation using multimodal information, and incorporation of this information into statistical machine translation. The annotated data will be made available to the research community to facilitate the development of translation technology that produces more natural, human-like translations in real-world interactive situations.","title":"SGER: Collaborative Research: Contextual Machine Translation","awardID":"0840538","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["565127"],"PO":["565215"]},"144146":{"abstract":"Wayfinding is an essential capability for any person who wishes to have an independent life-style. It requires successful execution of several tasks including navigation and object and place recognition, all of which necessitate accurate assessment of the surrounding environment. For a visually-impaired person these tasks may be exceedingly difficult to accomplish and there are risks associated with failure in any of these. Guide dogs and white canes are widely used for the purpose of navigation and environment sensing, respectively. The former, however, has costly and often prohibitive training requirements, while the latter can only provide cues about obstacles in one?s surroundings. Human performance on visual information dependent tasks can be improved by sensing which provides information and environmental cues, such as position, orientation, local geometry, object description, via the use of appropriate sensors and sensor fusion algorithms. Most work on wayfinding aids has focused on outdoor environments and has led to the development of speech-enabled GPS-based navigation systems that provide information describing streets, addresses and points of interest. In contrast, the limited technology that is available for indoor navigation requires significant modification to the building infrastructure, whose high cost has prevented its wide use. <br\/><br\/>This proposal adopts a multi-faceted approach for solving the indoor navigation problem for people with limited vision. It leverages expertise from robotics, computer vision, and blind spatial cognition with behavioral studies on interface design to guide the discovery of information requirements and optimal delivery methods for an indoor navigation system. Designing perception and navigation algorithms, implemented on miniature-size commercially-available hardware, while explicitly considering the spatial cognition capabilities of the visually impaired, will lead to the development of indoor navigation systems that will assist blind people in their wayfinding tasks while facilitating cognitive-map development.","title":"CDI-Type II: Collaborative Research: Cyber Enhancement of Spatial Cognition for the Visually Impaired","awardID":"0835645","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7751","name":"CDI TYPE II"}}],"PIcoPI":["395034"],"PO":["543539"]},"143057":{"abstract":"This project investigates novel computational methods and interventions that might alleviate the suffering caused by complex diseases. Our disease model is the selective killing of cancer cells, but the algorithms developed might also have more general uses for the therapy of other complex diseases.<br\/>Emerging biological computing paradigms require control of highly non-linear complex networks that remain incompletely characterized. In particular, drug intervention can be seen as control of signaling in cellular networks and thus as information processing. Identification of control parameters presents an extreme challenge due to the combinatorial explosion of control possibilities in combination therapy and to incomplete knowledge of the systems biology of cells. In this project, we design algorithms that identify optimal control parameters in cellular networks based on a quantitative characterization of control landscapes, maximizing utilization of incomplete knowledge of the state and structure of intracellular networks. We apply our methods to the control of signal processing that leads to the life\/death decision of a cell. In many applications, this control has to be selective. For example, the response to a cytotoxic therapy targeted at cancer cells should ideally occur with minimal response in the normal cells. We define this desired response as selective cell death. The use of new technology for high-throughput measurements, which only recently has become available to academic researchers, is key to this research and essential for the characterization of control landscapes and implementation of the algorithms.","title":"The control landscape of selective cell death","awardID":"0829891","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["490669",380888,"532973","553514","561631"],"PO":["565223"]},"144036":{"abstract":"Abstract<br\/><br\/>From the western world to the third world, the use of handheld devices (cellphones, PDAs) has proliferated. The world of users is becoming both wireless and mobile. Web 2.0 has ushered in an age wherein the web is viewed as a provider of services and not just a repository of documents and\/or information. Despite this advance, the web remains just that, a single web with an inherent assumption that a powerful computing and communication infrastructure supports it. Couldn't mobile wireless devices in close proximity form a web of their own? This is the vision behind this project, the Web on Demand (WoD). WoD aims at bridging the gap between social networks and ad hoc networking. In other words, it aims to rethink the system software stack all the way from application to networking that would allow the creation and management of social networks without any assumption of infrastructure support. The core of the research is to develop software technologies for mobile devices that would allow the dynamic creation of thematic ad hoc overlay networks empowering (a) mobile people with similar interests (e.g., weather forecast), (b) friends and family (e.g., in a theme park), and (c) participants in mission critical applications (e.g., search and rescue), stay connected. WoD complements the World Wide Web (WWW) and leverages it when it is available, such as exploiting the ambient computing infrastructure to enhance user experience, and managing the dynamic creation of User Generated Content (UGC) by mobile users. The vision behind this project is to democratize access to services that are currently offered through WWW. In this sense, the results from this research can have far-reaching technological and societal consequences. Most importantly, the research will help breed a new class of computer scientists who are connected with societal causes in addition to advancing technology.","title":"CSR-DMSS, SM: Web on Demand - Bridging the Gap Between Social Networks and Ad Hoc Networking","awardID":"0834545","effectiveDate":"2008-09-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["564690","508256"],"PO":["535244"]},"143068":{"abstract":"Moore?s Law continues to drive higher levels of system integration. Shrinking transistor size and increasing circuit complexity make it very difficult to transmit signals fast and reliably. In other words, the performance of these systems has become increasingly limited by communications between their building blocks. Future high performance multi-core microprocessors demand a fundamental change in intra- and inter-chip interconnect technologies. Optical interconnect is widely accepted as the long-term solution and significant progress has been made in recent years. However, on-chip optical interconnect, which is the focus of previous research efforts, presents some significant challenges. Pure-optical switching and storage devices in silicon technologies remain far from practical, and hence an optical interconnect network requires significant overhead of repeated optical-to-electrical and then electrical-to-optical conversions. Simultaneously, efficient silicon electro-optic modulators remain challenging due to either large size or small bandwidth, not to mention that both approaches require significant and expensive changes in standard silicon technologies. Both limitations will result in unacceptable delay, circuit complexity, cost, and energy consumption.<br\/>This project will use free-space optics and supporting device, circuit, packaging, and architecture level techniques to create a CMOS-compatible, high-performance intra-chip interconnect technology. Integrated lasers, optical phase shifters. photodetectors and focusing microlens, will be implemented in GaAs or SiGe technologies, and 3-D integrated with CMOS circuits underneath, which will also include the transmitter and receiver electronics. This architecture allows point-to-point direct communication between any two nodes, bypassing the need for routing through intermediate nodes while managing packet collisions. This project will lead to a general technology and design framework applicable to a large variety of new applications in future high performance computing and other systems-on-chip.","title":"3D-Integrated Intra-Chip Free-Space Optical Interconnect for Future Multi-Core SoCs","awardID":"0829915","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["439617",380917,"553419","548271","517923"],"PO":["562984"]},"147314":{"abstract":"This grant funds an NSF-funded programming contest for students in the Database Management Systems (DBMS) area at the ACM SIGMOD 2009 conference. Student teams from degree-granting institutions will compete in a programming task selected by a committee of DBMS experts, including the principal <br\/>investigators. The topic chosen would be exciting to students, non-trivial to accomplish, and would have research potential. The goals of the contest are to:<br\/>a) Stimulate student interest in the DBMS field<br\/><br\/>b) Present a research challenge<br\/><br\/>c) Foster the creation of sharable open source code modules<br\/><br\/>d) Foster Computer Science education by creating a forum in which students can excel<br\/><br\/>e) Present a forum in which excellent students can get noticed, other than by writing papers.<br\/><br\/>The contest would be conducted during early 2009, with a bakeoff among <br\/>the finalists at the SIGMOD conference. A substantial prize <br\/>(approximately $5,000), donated by a DBMS company, will be awarded to <br\/>the winning team. In addition, a conference speaking slot will give <br\/>visibility to the winner. For more information visit the contest web <br\/>site, http:\/\/db.csail.mit.edu\/sigmod09contest\/.<br\/><br\/>The grant is used to support the creation of the necessary <br\/>infrastructure (test harness, test data, etc.) for the contest and for <br\/>travel grants for the contest finalists. If successful in year 1, the <br\/>principal investigators would make this an annual contest.","title":"2009 SIGMOD Programming Contest","awardID":"0848727","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["483585","525076"],"PO":["469867"]},"144047":{"abstract":"Cyber-physical systems will soon become ubiquitous. One of the major challenges that such systems pose is that the control algorithms that modulate the interaction with the physical world have traditionally assumed availability of unlimited computational resources. However, in cyber-physical systems, control tasks are executed on shared processors that can only provide time-varying and uncertain computational resources. Moreover, the real-time scheduling of control tasks provides new difficulties in this context given the sensitivity of control performance to jitter and latency caused by the time varying availability of computational resources. These are new problems that reside at the interface between control algorithms and real-time scheduling policies. This research provides a new joint control and task scheduling approach that maintaining a guaranteed control performance under time-varying processor availability while optimally utilizing the available computational resources. More specifically, this project aims at the design of novel anytime receding horizon control algorithms that provide progressively better performance as more computational time is provided and redistribute the available computational resources online based on the control performance and on the quality of service of other real-time tasks.<br\/>By addressing a fundamental bottleneck in the design of cyber-physical systems, this project directly impacts the society in profound ways. New classes of applications of receding horizon control in real-time environments, such as networked vision-based control and complex SCADA systems will become possible with economic and societal benefits. This project also focuses attention at the challenges at the intersection of control and real-time computation, and thus fosters collaboration between researchers in these two communities, also leading to new educational material.","title":"CSR-EHCS(EHS), SM: Collaborative Research: An Anytime Approach to Real-Time Embedded Control","awardID":"0834661","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["526930"],"PO":["561889"]},"143079":{"abstract":"Abstract<br\/>This research develops a completely new approach for automatically synthesizing and manually designing digital circuits that implement approximate-yet-acceptable versions of logic functions. This approach provides lower cost and higher speed versions of many widely used digital systems. The benefits of this research will grow with the adoption of each new nanotechnology.<br\/>All future nanoscale CMOS and emerging non-CMOS nanotechnologies will suffer from increasing levels of non-idealities ? especially high process variations, defect densities, and noise sensitivity. These non-idealities present a fundamental challenge, since, if left unchecked, they will erode most benefits provided by adoption of these nanotechnologies. This research involves development of approximate design and synthesis, a completely new paradigm for dealing with this fundamental challenge. This paradigm builds upon the recent demonstration that there is considerable flexibility in the behavioral specifications for wide classes of digital systems. Such flexibilities in specifications are captured in the form of acceptable deviations from the nominal behavioral specifications and the first logic synthesis and design approaches and tools are developed to exploit these deviations to reduce cost or enhance performance and yield. In contrast to all existing research directions, this paradigm mitigates the effects of non-idealities by simplifying the designed circuit. This approach is applicable to application-specific chips for many embedded systems and many types of widely used processors, e.g., arithmetic co-processors, processors for digital signal processing, and graphics processors. Collectively, the set of chips that benefit from this approach constitute a significant proportion of all chips sold in any given year. This research opens a completely new avenue for dramatically improving cost, performance, and yield for future nanotechnologies.<br\/>1","title":"EMT\/MISC: Theory and methods for design and synthesis of approximate logic circuits and systems: a paradigm for emerging technologies","awardID":"0829946","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["535238"],"PO":["565223"]},"145026":{"abstract":"NSF currently sponsors an intensive summer research workshop series in human language technology and robust intelligence, organized and hosted by Johns Hopkins University. These workshops have exhibited a relatively unique model of interactive peer review, dynamic expert team generation, research acceleration via a condensed and unusually close collaboration environment and intensive student mentorship. A Scientific Community On-Site Assessment<br\/>Workshop (OSAW) is being held on July 30-31, 2008, to review and and improve upon this model and investigate the potential for its extension to new scientific disciplines. During the OSAW, a team of expert research-community stakeholders observes current summer workshop processes, interviews participants, receives detailed briefs on prior peer-review, topic-selection and research team recruitment processes, and reaches findings and makes recommendations regarding these goals.<br\/><br\/>The OSAW workshop will contribute to the progress of science by: (1) improving the existing NSF-sponsored intensive summer workshop series that has already engaged over 300 international participants and lead to hundreds of peer-reviewed publications and major scientific innovations, (2) the insights and conclusions reached by the OSAW, especially with respect to the interactive peer-review and intensive mentorship processes employed by the summer workshop, have the potential to inform and provide ideas for other peer-review and mentorship processes, such as utilized by NSF and other organizations, (3) the recommendations of the OSAW will provide a roadmap for the extension of the intensive summer-workshop model to other fields, with potential for the accelerated scientific progress and unique collaboration framework already realized by this model in human-language-technology fields.","title":"Scientific Community On-Site Assessment Workshop for Robust Intelligence","awardID":"0839056","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["445169"],"PO":["565215"]},"144058":{"abstract":"CSR-DMSS, SM: Design and Evaluation of a Scalable Meta-Event<br\/>Dissemination System<br\/><br\/>Project Abstract:<br\/><br\/>Event dissemination systems offer a powerful middleware for<br\/>distributed applications such as electronic trading platforms, news<br\/>aggregators, and network intrusion detection systems. Such systems are<br\/>usually based on the publish-subscribe paradigm: published events are<br\/>delivered to clients who have a supplied a matching<br\/>subscription. While most current publish-subscribe systems support<br\/>subscriptions that evaluate to true\/false on a per-event basis,<br\/>emerging applications need support for subscriptions to higher level<br\/>\"meta-events\", which correspond to combinations of basic events. The<br\/>challenge in tracking such subscriptions is that there is no single<br\/>event, when processed in isolation, permits the evaluation of a<br\/>subscription to true or false.<br\/><br\/>This project designs, implements and evaluates an event dissemination<br\/>system that supports tracking of meta-events. The system converts<br\/>user-defined subscriptions into \"triggers\", and efficiently maintains<br\/>the state of the triggers using algorithms inspired by the literature<br\/>on data stream processing. The system scales with increasing numbers<br\/>of subscriptions through merging triggers by identifying commonalities<br\/>between them, and through indexing triggers for fast detection of<br\/>matching subscriptions for events. The system is evaluated using an<br\/>internet-scale testbed, such as Planetlab. Such a meta-event<br\/>dissemination system will provide a more expressive interface to the<br\/>designer of distributed and web applications, thus transforming the<br\/>way in which distributed applications are authored. This is expected<br\/>to impact the application of information technology in various fields<br\/>such as healthcare, finance, and security. The project enhances<br\/>undergraduate and graduate education through strengthening the<br\/>freshmen engineering course in Electrical and Computer Engineering and<br\/>through developing a strong algorithmic foundation for research<br\/>students in networking and distributed computing.","title":"CSR-DMSS, SM: Design and Evaluation of a Scalable Meta-Event Dissemination System","awardID":"0834743","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["521840","511486"],"PO":["565255"]},"147457":{"abstract":"Paulraj Abstract<br\/><br\/>Due to the broadcast nature of wireless transmissions, when a packet is forwarded by a node, many nodes other than the intended node overhear this transmission. The goal of this project is to exploit a new concept of capturing\/exploiting soft information at nodes in a multi-hop ad hoc network. Storing appropriate information from these overheard transmissions and its subsequent reuse can increase multi-hop throughput performance. This appears to be a new idea with significant potential to improve throughput and scalability of multi-hop networks. The nodes in such networks will need large storage capabilities to exploit this technique. However, with the dramatic reduction of memory costs, this is now economically feasible. <br\/><br\/>A variety of issues related to developing this new concept will be explored: (a) Scalability of the network throughput with increasing node density, with varying path loss exponent and in the presence of time selective fading. (b) Appropriate format or metrics for soft information captured from overheard packets (c) Memory requirements as a function of node density and other related parameters.","title":"SGER: Soft Information Capturing and Exploitation in Multi-Hop Ad hoc Networks","awardID":"0849347","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["535565"],"PO":["564898"]},"146489":{"abstract":"Structured hidden databases are widely prevalent on the Web. They provide restricted form-like search interfaces that allow users to execute search queries by specifying desired attribute values of the sought-after tuples, and the system responds by returning a few (e.g., top-k) tuples that satisfy the selection conditions, sorted by a suitable ranking function. Although search interfaces for hidden databases are designed with focused search queries in mind, for certain applications it may be advantageous to infer more aggregated views of the data from the returned results of search queries. Such aggregated information will facilitate learning data distributions or building mining models, which can then be used to power and optimize a multitude of emerging data analytical applications. <br\/><br\/>This research involves developing effective techniques for performing data analytics, especially sampling, over hidden structured databases via their public interfaces. The outcomes include efficient algorithms for sampling hidden databases with a heterogeneous mix of data types, achievability results for sampling different types of search interfaces, and a prototypical toolset which demonstrates the sampling of real-world hidden databases. The ability to pose high-level analytical queries over hidden databases is needed by knowledge workers in a wide variety of corporations, governments, and security agencies. Parts of this project will be integrated into teaching and carried out by students as part of advanced class projects, which will potentially attract motivated students to pursue doctoral degrees. The project Web site (http:\/\/dbxlab.uta.edu\/dataAnalytics.html) will be used for results dissemination.","title":"SGER: Data Analytics over Hidden Databases","awardID":"0845644","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["560637","464575"],"PO":["563751"]},"146137":{"abstract":"The purpose of this exploratory project is to test a proof-of-concept that reflects an emerging and potentially transformative mobile system to enhance recognition and way-finding function for users with severe visual impairments. The PI's Near and Far Environmental Awareness System (NaFEAS) will run on a mobile phone platform in concert with RFID tags and sensors which serve as inputs and outputs to a knowledge database that can be updated by the user. NaFEAS will recognize tagged objects and provide audio and other feedback to users within a designated envelope around the sagittal and coronal body planes. In addition, NaFEAS will facilitate way-finding using tagged objects in the near and far environments. The focus here is on the development of a low-fidelity prototype using participatory design; the theoretical basis is embodied cognition, which emphasizes the full use of senses, gestures, and space to understand objects and the environment. The PI will take special pains to minimize the cognitive biases occurring when sighted designers or researchers retrofit visually-dominant interaction patterns to develop accessible devices. The prototype will be evaluated by target group members using an experimental design with self-report, behavioral, and physiological variables. <br\/><br\/>Broader Impacts: Besides resulting in a prototype product that will be of great value to the target population, the findings from the experiments will advance our knowledge of how individuals with severe visual impairments successfully interact with next generation systems, including mobile, wearable, and ubiquitous systems. This knowledge will be generalizable to technologies beyond the test bed the PI will be using.","title":"Embodied Interaction Paradigm for Users with Severe Visual Impairments","awardID":"0844232","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["486496","505416"],"PO":["565227"]},"158237":{"abstract":"This is a project to develop new methods for scientifically studying and assessing human cognitive function. It will employ sophisticated statistical multimodal data analysis techniques that will fuse contextual, behavioral, and neural information simultaneously obtained from human beings in the process of completing complex batteries of cognitive tasks. The tasks will be presented in the form of customized computer games that are designed to exhibit the crucial aspects of established cognitive assessment tests and at the same time provide a motivating and engaging environment for the subject's interactions with the game and computer agents. The tasks will involve exploiting our existing capabilities of monitoring and controlling certain enjoyable and challenging computer games that involve various combinations of cognitive tasks ranging from working memory and attention to executive functions. Multimodal information fusion will be accomplished by utilizing Bayesian inference techniques and information theoretic data analysis and dimensionality reduction methods. <br\/><br\/>The work to be carried out under this grant aims to develop sophisticated pattern analysis techniques for the purpose of analyzing the fine-grain behaviors of elderly when they are engaged in complex cognitive tasks in the form of computer games. Expected significant scientific findings from the proposed research are two-fold: (1) improved statistical signal processing and pattern recognition algorithms for EEG processing, (2) an enhanced understanding of the interplay of multiple cognitive processes and their neural signatures in EEG during the execution of complex tasks. <br\/><br\/>The approach is innovative in terms of three aspects: (1) an advanced adaptive interaction protocol that modifies the task parameters to maintain maximal sensitivity to cognitive state changes will be employed, (2) novel information theoretic techniques will be developed and utilized for the extraction of maximally discriminative features from EEG measurements for cognitive state estimation and neural activity visualization, (3) the developed closed-loop system will be utilized to study the human-agent interaction in complex cognitive tasks resulting in mathematical models of micro-behavior in realistic evolving environments as opposed to traditional stationary repetitive experimental paradigms. <br\/><br\/>The successful completion of the work will open the way to further collaborative activities in brain interface design, closed-loop collaborative augmented cognition human-agent interfaces for improved performance, and early diagnosis of cognitive decline in elderly. An interdisciplinary research environment will engage the participating graduate students in a multidisciplinary educational setting and will help them develop skills to perform collaborative interdisciplinary research.","title":"HCC: Assessing Cognitive Function from Interactive Agent Behavior","awardID":"0934509","effectiveDate":"2008-09-01","expirationDate":"2011-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["506513"],"PO":["564456"]},"134279":{"abstract":"Embedded network systems are transforming the planetary compute fabric, changing the way we coordinate with peers, safeguard natural resources, and protect local communities. Ensuring the correctness and performance of these systems has immediate relevance to the health and welfare of the planet. The objective of this CAREER project is to develop the theoretical and applied foundations necessary to ensure these properties and to instill the requisite skills in the next generation of embedded network system engineers. The project relies on a pattern-centric approach motivated by the profound impact of design patterns on software reliability and programmer productivity in other domains and the observation that these benefits can be amplified through the development of formal foundations and supporting software tools. <br\/><br\/>There are four project components: First, the team is codifying expert knowledge in the form of new patterns for embedded network system design and implementation. Second, the team is developing a specification and reasoning formalism to capture pattern requirements precisely and to validate the correctness of pattern implementations. Third, the team is developing static analysis techniques and supporting software tools to automate the detection of pattern implementation errors. Finally, the team is applying these techniques and tools to reverse-engineering and code generation tasks, extending the benefits of model-based software engineering to this new domain. The dissemination and outreach program includes publication, undergraduate and graduate curriculum integration, undergraduate research involvement, high-school outreach and engagement, academic and dissemination to industry, and mini-workshops for evaluation and promulgation of findings, methods and tools.","title":"CAREER: Supporting Patterns for Embedded Network Systems","awardID":"0745846","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["490677"],"PO":["561889"]},"139614":{"abstract":"This research seeks to develop adaptive methods that will enable sensor-based systems to learn from their own sensory experience in order to improve their perceptual precision and dexterity. We will investigate the problem of sensor-based parameter estimation and state estimation on groups, with particular focus on the group of rigid body transformations. These group-structured problems naturally arise in three-dimensional sensing (e.g. range scanners such as laser scanners and multi-beam bathymetric sonars; Doppler sonar; and vision based state estimation) and dynamic state estimation (e.g. image-based state estimation; vision based control; and multi-degree-of-freedom vehicle navigation and control). Few identification techniques presently exist for the common problem in which the unknown parameter set or state possesses group structure. This is in contrast to the variety of well-known techniques (i.e. least-squares and adaptive) for parameter and state estimation for the case in which the unknown parameter or state appears linearly in the plant equations and the unknown is an element of a linear vector space. The anticipated impact will be robotic systems capable of adaptively learning to improve their navigation and sensing accuracy. We will apply these approaches to actual real-world problems arising in underwater vehicle sensing and control.","title":"RI-Small: Adaptive Parameter and State Estimation on Groups with Application to Sensor-Based Control","awardID":"0812138","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550512"],"PO":["543539"]},"137689":{"abstract":"The Web is enormous and in constant flux, causing much content to be lost over time. Historical collections of web content are thus of monumental value in preserving records of significant aspects of modern society. The Internet Archive offers access to hundreds of billions of historical web page snapshots. The scale of such archives, however, presents tremendous challenges to making this content fully searchable. This research effort investigates efficient and effective approaches to store, index, and retrieve web content from large-scale historical archives. In addition, the temporal content and structure of the archives are mined to exploit temporal characteristics that can improve search result ranking. Technological advances from this work are being tested on content from and in collaboration with the Internet Archive and integrated into its infrastructure, enabling new archival search capabilities for the public.<br\/><br\/>http:\/\/www.cse.lehigh.edu\/~brian\/nsf\/archives-08.html","title":"III-COR-Medium: Efficient and Effective Search Services Over Archival Webs","awardID":"0803605","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["521989",365997],"PO":["563751"]},"147248":{"abstract":"Flash memory storage devices are now considered to have tremendous potential<br\/>as a new storage medium that can replace magnetic disks. Flash memory, however, has its own limitations such as erase-before-update and exhibits poor performance for small-to-moderate sized writes requested in a random order. The goal of this project is to develop new flash-aware designs and optimization strategies that allow large-scale transactional database applications to run efficiently on computing platforms equipped with flash memory storage devices as a main storage medium. This project achieves its goal by (1) analyzing the performance implications of different I\/O workloads on flash memory, and the applicability of flash memory to different types of tablespaces in databases, (2) inventing new paradigms, designs, data structures and algorithms, hot spot separation, and write optimization techniques, so that enterprise database servers can run on the target computing platforms efficiently.<br\/>The findings from this project will not only overcome the limitations of flash memory but also exploit the advantages of flash memory for transactional database workloads exhibiting randomly scattered data access patterns.<br\/>The paradigms and processing strategies developed can be applied to other areas such as health-care and sensor-network based environment study that require data management on flash memory storage systems. The findings and deliverables of the project will be disseminated broadly in the form of software packages, research articles, technical reports, and data sets via the project web site(http:\/\/www.cs.arizona.edu\/projects\/fmdb).","title":"SGER: Flash Memory DBMS for Transactional Database Applications","awardID":"0848503","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[392322],"PO":["469867"]},"139515":{"abstract":"As general-purpose computing moves into the age of pervasive parallelism, programmability becomes the key hurdle limiting the effective use of available computing resources. Transactional memory promises to simplify parallel programming for application programmers. However, research in Transactional Memory is being seriously hampered by the lack of a reusable open source infrastructure. The project will develop the key pieces necessary to overcome this situation: A transactional memory library built out of highly decomposed pieces will provide reusable and replaceable parts suitable for investigating tradeoffs in software TM implementations. Standardized interfaces will allow libraries conforming to the interfaces to be used in a variety of environments. TM-aware run-time analysis tools, particularly profilers and debuggers, will provide the necessary tool support for TM implementors and application programmers to understand and improve the performance of software using transactions. Interesting benchmarks, in a variety of high-level languages, will move forward our understanding of TM performance characteristics.","title":"CPA-SEL-T: Collaborative Research: Unified Open Source Transactional Infrastructure","awardID":"0811631","effectiveDate":"2008-09-15","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["549800","558390"],"PO":["523800"]},"139636":{"abstract":"Probabilistic databases have many important applications in areas such as <br\/>crime fighting, data cleaning and integration, moving object tracking, and <br\/>science, and have recently moved into the limelight of data management <br\/>research. Probabilistic databases are expected to allow for new data <br\/>management applications to arise that currently require substantial <br\/>bootstrapping and development effort, if they are feasible at all. <br\/>Probabilistic databases allow for very flexible ways of representing and <br\/>managing incompleteness and are a possible foundation for a new breed of <br\/>more encompassing and powerful data integration systems. Probabilistic <br\/>databases could give rise to more effective, lazy integration systems, in <br\/>which the input is initially inserted into the database with a high degree <br\/>of uncertainty, which is subsequently reduced using various forms of <br\/>corpus data, evidence, and probabilistic mappings and rules.<br\/><br\/>The primary goal of this project is to perform foundational research and <br\/>develop a probabilistic database management system that can support such <br\/>applications. The first aim of this project is to work on efficient query <br\/>processing techniques, with a particular focus on the essential problems <br\/>of computing tuple confidence values and the closely related problem of <br\/>conditioning a probabilistic database. Both are known to be <br\/>computationally hard, and efficient techniques are expected to have to <br\/>make use of the state of the art in algorithms for constraint <br\/>satisfaction. In some applications, such as ad-hoc query processing for <br\/>decision support, exact query results are not necessary. Previous work has <br\/>pointed out efficient approximation techniques for simple queries, namely <br\/>for the ranked retrieval of results of conjunctive queries together with <br\/>their confidences; however, there is currently no work on approximating <br\/>queries that themselves make use of confidence values, for example in <br\/>selection operations. <br\/><br\/>Approximating expressive, compositional queries on <br\/>probabilistic databases is a second main focus of the planned project. <br\/>Thirdly, nearly all previous work on probabilistic databases has focussed <br\/>on designing representation systems and query languages and laying the <br\/>foundations for efficient query processing. For real data management <br\/>applications to use probabilistic database systems in the future, it must <br\/>also be possible to update such databases, and to run transactional <br\/>programs on them. While no work on this exists in the literature, <br\/>uncertainty in the data poses a number of fundamental challenges to be <br\/>addressed as the third main aim of the project.<br\/><br\/><br\/>The homepage of the MayBMS project which is the subject of this proposal <br\/>can be found at http:\/\/www.cs.cornell.edu\/database\/maybms\/ .","title":"III-COR-Small: Extending and Leveraging Probabilistic Databases for New Applications","awardID":"0812272","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["406765"],"PO":["469867"]},"139526":{"abstract":"With the emergence of the multicore architecture comes the promise of integrating enormous computing power in a single chip, thereby enabling parallel computing in all types of platforms including handheld computers and desktop machines. Providing proper software support for applications is critical to harness the true power of this architecture. An inherent characteristic of multicores that presents a significant obstacle is runtime variation: reliability, energy\/thermal behavior and process variation will vary across identically designed components of a multicore, producing a negative impact on application power consumption and performance. Runtime variation has been identified as one of the key problems that could block further scaling of circuits if not properly addressed. <br\/><br\/>This research project is developing an advanced execution system, called a Robust Execution Environment (REEact), that dynamically mediates, controls and adapts an application's execution to the runtime resource landscape originating from runtime variations. It employs a combination of techniques in adapting both the hardware resources and the application software code to overcome the impact of runtime variations. At the hardware level, it adapts the resources, such as setting the speed\/voltage of a node on the multicore. At the software level, REEact dynamically optimizes code, taking into account performance and power consumption due to runtime variations. It elicits the help of the OS in determining what resources to use in running the application. REEact informs the OS about information it dynamically discovers about latency, power, and application behavior. REEact is built as multi-layer hierarchical runtime system that interacts with the parallel application, the OS, and the underlying multicore architecture to ensure that maximum performance is achieved.","title":"CPA-CPL-T: Collaborative Research: REEact: A Robust Execution Environment for Fragile Multicore Systems","awardID":"0811689","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["385658","438734"],"PO":["565272"]},"139647":{"abstract":"The goal of this research is to develop techniques and systems that help people solve information problems that are complex, general, or ongoing and when information seeking takes place over multiple intervals or in collaboration with other people. The approach is to first study how people seek information and interpret results of searches as they use multiple systems over time and in collaboration with emphasis given to managing and optionally sharing result sets and items. Second, based on these initial investigations, systems are designed that support dynamic search and visualization and can serve both as a personal information manager and a group information manager. Third, these tools are evaluated in field and laboratory settings. The research is linked to educational theories of active learning and is embedded in university student and research team information needs over multiple months. Students play an active role in this project by participating in the design and evaluation of the information seeking systems.<br\/><br\/>The results of this research will provide guidance for designers of the next generation of systems that support a full range of complex information seeking needs. The project also contributes specific open source tools that people can easily adopt as plug-ins to popular web browsing software. Publications, software and other information items will be widely disseminated, including via the project Web site (http:\/\/www.ils.unc.edu\/infoseek\/). This work will thus have broad impact on Internet-based information activities in schools, homes, offices, and research laboratories.","title":"III-Small: Result Space Support for Personal and Group Information Seeking Over Time","awardID":"0812363","effectiveDate":"2008-09-01","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["531548"],"PO":["563751"]},"139537":{"abstract":"Most engineered artifacts, such as bridges and nuclear power plants, are tested by subjecting them to operating conditions and observing results.<br\/>Software is different. It manifests dynamic behavior when running on computers, and software quality (with respect to achieving specified<br\/>behavior) is normally tested that way. But software also can be considered purely symbolic -- a sequence of instructions -- and hence can be subjected to mathematical proof of correctness. Achieving such \"verified software\" has been identified as a \"grand challenge\" for computing research. The work of this project's interdisciplinary team of software engineers and logicians focuses on the thesis that practical, scalable, automated software verification is feasible, one component at a time, by combining careful language design with recent advances in automated theorem proving. The plan is to evaluate this thesis empirically by generating the logical verification conditions for a benchmark suite of software components like those used in computing courses and commercial software, and proving them automatically. The project's significance will derive from its proof of concept that the verified software grand challenge can be conquered, and from a better understanding of what the next generation of software engineers need to be taught to produce verified software.","title":"CPA-SEL: Collaborative Research - Continuing Progress Toward Verified Software","awardID":"0811737","effectiveDate":"2008-09-01","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["508443","508444"],"PO":["564388"]},"139548":{"abstract":"The increasing problems of power, heat dissipation, and design<br\/>complexity have caused a shift in processor technology to favor<br\/>multicore multiprocessors. Along with that shift, the sharing of<br\/>memory hierarchy becomes deeper, heterogeneous and more complex,<br\/>causing cache contention, increased conflicts, and also, synergy<br\/>sharing. Without understanding the implications of this change,<br\/>current multicore systems suffer from considerable performance<br\/>degradation, poor performance isolation and inferior fairness<br\/>guarantees. The urgency of these issues increases as the degree of<br\/>processor-level parallelism increments rapidly.<br\/><br\/>Prior studies, mostly in areas of architecture and operating systems,<br\/>rely on simple heuristics to estimate cache requirement of corunning<br\/>programs; the inaccuracy and overhead limits their scalability and<br\/>effectiveness. This work tackles these challenges uniquely from the<br\/>compiler aspect by constructing predictive behavior models for<br\/>corunning processes, developing cache-sharing-aware program<br\/>transformations and loop scheduling, and combining the program-level<br\/>knowledge of programming systems with the proactive resource<br\/>management by runtime systems. Specifically, this work proposes<br\/>inclusive reuse signatures to characterize inclusive locality---the<br\/>memory behavior of corunning programs on shared caches, and<br\/>inter-thread affinity models to capture data locality among parallel<br\/>threads. It tackles the challenges facing the measurement, prediction<br\/>and exploitation of inclusive locality. The analysis opens new<br\/>opportunities for shared-cache optimizations by both compilers and<br\/>runtime systems. The PI develops a series of program transformations,<br\/>such as inter-thread memory reorganization and cache-sharing aware<br\/>loop scheduling, to increase inter-thread spacial locality and<br\/>ameliorate conflicts, contention and false sharing. For runtime<br\/>systems, this work invents proactive cache management which partitions<br\/>caches or schedules processes according to predicted inclusive<br\/>locality proactively, overcoming the limitations of current reactive<br\/>schemes on scalability, accuracy and effectiveness.","title":"CPA-CPL: Exploring and Exploiting Heterogeneous Cache Sharing in Chip Multiprocessors Systems for Locality Optimization and Proactive Cache Management","awardID":"0811791","effectiveDate":"2008-09-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["551005"],"PO":["565272"]},"139669":{"abstract":"A plethora of digital data is being generated at unparalleled speed with an inordinate number of dimensions. Machine learning and data mining are approaches that can assist us in keeping pace with the rapidly advancing data gathering and storage techniques and help us mine nuggets or patterns from high-dimensional data. Semi-supervised learning can be interpreted as supervised learning that uses additional information from unlabeled data, or as unsupervised learning guided by constraints formed from labeled data. This research is addressing two key pressing issues with massive data: high dimensionality and a shortage of labeled data. In particular, this project is: investigating semi-supervised feature selection to remove irrelevant features; studying the combination of feature extraction and model selection to further reduce dimensionality; and developing a novel framework to integrate feature selection and feature extraction based on sparse learning. This study is an explicit attempt to connect and unify feature selection and extraction for hypothesis space reduction. The project is directly facilitating basic machine learning research and practical data mining and advances innovative research beyond feature selection and extraction. The work is engaging students in both teaching and research, and the algorithms, tools and databases will be made publically available for research purposes and for use as teaching resources.","title":"III-COR-Small: Beyond Feature Selection and Extraction - An Integrated Framework for High-Dimensional Data of Small Labeled Samples","awardID":"0812551","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["517806","456042"],"PO":["543481"]},"142970":{"abstract":"Behavior-based robotics was established around the idea that robots could be constructed by connecting elementary sensor and actuator modules, without the need to form any internal representation of the world in which they operate. When designed appropriately, such robots, both as individuals and as groups, exhibit complex and seemingly ?intelligent? behaviors, solving challenging tasks in real time, while reacting only to their local environment and obeying sets of local rules. In part, this approach to robotics was inspired by considering natural systems, such as self-organization and adaptability of social insects in their colonies. An analogous approach to initiate the development of the field of behavior-based molecular robotics will be performed over the next three years, leading to groups of molecules that would appear to an observer to show a variety of task-oriented behaviors or some form of purposeful and dynamic self-organization. <br\/><br\/>Individual behavior-based molecular robots are single molecules displaying multiple sensors-actuators. When exposed to artificial landscapes displaying substrates keyed to their sensors-actuators, the molecules start executing elementary steps, determined, in a stochastic sense, by their constantly changing local environments. On some landscapes, called prescriptive, individual molecular robots and their collectives will execute algorithms mimicking wound-up automata with sequence control mechanisms. In contrast, on non-deterministic landscapes, the robots and their collectives will demonstrate properties emerging from internal organization of individual sensors and actuators and through local interactions between molecules and their environments. Importantly, these new interpretations of molecular behaviors will allow radically different experiments from all previous approaches to molecular robotics, while keeping experimental designs realistic, leading to embodiment in the physical world.","title":"Collaborative Research: EMT\/MISC: Behavior-Based Molecular Robotics","awardID":"0829541","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[380636],"PO":["565223"]},"143993":{"abstract":"In formal verification, finite models of computer programs or digital circuits are checked against temporal logic properties such as safety (something bad never happens) and liveness (something good eventually happens). While formal verification received a lot of attention, the dual problem of formal synthesis, where the focus is to construct a provably correct system (e.g., safe by design) is still in its infancy. In addition, most systems, such as models of unmanned vehicles, are hybrid, combining continuous models of vehicle dynamics with finite state automata that model embedded controllers and communication protocols.<br\/>This project develops theoretical frameworks and computational tools for synthesis of provably-correct control and communication strategies for hybrid systems and distributed hybrid systems from specifications given in rich, human-like language. Central to the approach in this project is the notion of abstraction, which is used to construct finite descriptions of control systems. Such abstractions allow for the use of (adapted) temporal logics as specification languages, tools from formal verification and temporal logic games for analysis and control, and techniques inspired from synchronization in concurrency theory for synthesis of communication strategies.<br\/>The computational tools developed in this project are implemented as user-friendly software packages and tested in a miniature mobile robotics experimental platform. This research is closely integrated with a comprehensive educational and outreach plan, which includes undergraduate and graduate classes, involvement of undergraduate and high school students in research, and the participation of the PI as a judge and organizer in high school robotics competitions.","title":"CSR-EHCS(EHS), SM: A formal approach to control of hybrid systems with applications to mobile robotics","awardID":"0834260","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["496541"],"PO":["561889"]},"143762":{"abstract":"Writing a program to solve a large scale computational problem on a supercomputer is much more difficult than writing a program to solve the same problem on an ordinary computer. DARPA's High Productivity Computing Systems program has focused attention on a new family of parallel programming languages that reduce the effort required to develop supercomputer programs. One of the most widely used of these languages is Unified Parallel C (UPC). This research project develops a way to predict how long UPC programs will run so that programmers will know beforehand whether their programs will run efficiently. Predicting program run times reduces the need for the trial-and-error development of programs. This saves the time of the programmers and the supercomputer, both expensive commodities.<br\/><br\/>This project advances work on a performance model for UPC implementations that run on clusters. A model that describes the remote reuse distance for objects in the software cache is developed. This is a natural extension of local reuse distance for hardware cache. An analysis of remote reuse distance yields functions that predict cache behavior and its impact on performance. The effects of problem size, blocking factor, and the number of threads are included in the model. Common benchmarks, such as the NAS parallel benchmarks, are used to validate the model. The addition of a model of software cache behavior provides programmers with more accurate information about overall run times and suggests ways to improve software cache management for UPC and other languages in its family.","title":"A Performance Model for Partitioned Global Address Space Languages","awardID":"0833082","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["431308",382991,"530278"],"PO":["565272"]},"143531":{"abstract":"Ad hoc wireless networks are becoming important in several application domains. In these networks, nodes cooperate among themselves to achieve collective tasks, without requiring any pre-existing infrastructure. However, the cooperative nature and the possible hostile environments where the networks may be deployed make them vulnerable to a wide range of security attacks. Some of these attacks can be addressed through customized cryptographic primitives. However, the adversary can physically compromise the network nodes, and thus defeat the cryptographic measures. Further, the adversary may have much higher computational and communication capabilities than the legitimate nodes, and the malicious nodes can collude among themselves. This has created a difficult imbalance in securing ad hoc networks---defense is hard and resource-consuming while attack is often easy.<br\/><br\/>In this project, the PIs at Purdue University are developing a provably-assurable ad hoc network protocol suite to invert this imbalance. The system developed can give guaranteed security properties under a wide, and rigorously quantifiable, range of adversarial behaviors, including Byzantine behaviors. The project employs two main thrusts: (i) Security by diversification and randomization; and (ii) Security through accountability and reputation. The solution is applied to secure routing, secure data aggregation, and distributed leader selection.<br\/><br\/>Broader Impact: The project will significantly advance our understanding of the fundamental limits for security in wireless ad hoc networks. It will provide methods to design provably-secure wireless protocols for a large class of mission-critical applications. The results will be disseminated through public release of detailed design documents, software, and graduate course materials.","title":"NECO: Provably Assurable Ad Hoc Networks under Arbitrary Malicious Behaviors","awardID":"0831999","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["556632",382234],"PO":["557315"]},"145610":{"abstract":"Distributed actuation and sensing are important tools in the biological arsenal for achieving robust locomotion and manipulation. This proposal investigates a form of distributed actuation based on the \"water hammer\" effect. Intuitively, this is the transfer of momentum from the moving water to the valve as the valve suddenly closes. This transfer results in three separate effects. The momentum transfer to the valve imparts a push to the robot. The transfer to the hose has not been well studied for compliant hoses,<br\/>but appears to reduce friction of the tether, temporarily freeing it and propulsing the tether forward. Finally, a travelling pressure wave is established that has not been well studied, either. Preliminary results have shown that the combination of these effects allows a tethered robot to extend its operational sphere well beyond conditions where a conventional tether would fail. The high-risk, high-reward research in this exploratory proposal aims to achieve two goals: demonstrate the effectiveness of water hammer actuation for endoscopic applications and to develop a finite element model of the reverse pressure wave in compliant hoses. This model will lead the way toward harnessing the unexplained phenomena mentioned above, making the general application of these principles more achievable.","title":"SGER: Water Hammer Based Actuation for Application in Endoscopy","awardID":"0841483","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["543539"],"PO":["564316"]},"141254":{"abstract":"Proposal #: CNS 08-21121<br\/>PI(s): Leigh, Jason<br\/> Brown, Maxine D.; Johnson, Andre E.; Renambot, Luc<br\/>Institution: University of Illinois - Chicago<br\/> Chicago, IL 60612-7227<br\/>Title: MRI\/Dev.: Dev. of OmegaTable and OmegaDesk ? Instruments for Interactive Visual Data Exploration and Collaboration<br\/>Project Proposed:<br\/>This project, developing instruments for interactive visual data exploration and visualization, provides a powerful, easy-to-use information-rich cyberinfrastructure instrumentation in support of scientific discovery. Advanced visualization instruments serve as the eyepieces of a telescope or microscope, enabling researchers to view their data in cyberspace, and better manage the increased scale and complexity of accessing and analyzing the data. The OmegaTable and OmegaDesk are such eyepieces. The former supports multiple users sitting or standing around a table, and the latter is a single-user device that will ultimately replace the desk in one?s office. Both unify ultra-high resolution computer-enhanced collaboration workspaces and autostereoscopic virtual environments with multi-touch-sensitive surfaces so that users can intuitively point, write, touch, and manipulate the information displayed, and communicate and share this information with remote colleagues. These instruments act as digital assistants, anticipating and enabling those who work with them, benefiting global scientific collaboratories as well as providing a foundation for new computer science research. For the Electronic Visualization Lab (EVL) at UIC, these instruments represent the culmination of decades of experience and expertise developing immersive environments, from the room-size CAVE in the early 1990s, to the office-sized Immersa Desk in 1994, to GeoWall in 2000, and the more recent ultra-high resolution Lambda Vision tiled display wall and autosteoscopic Varrier tiled-display wall. Each new generation of display technology provides some advanced features ? higher resolution, unencumbered autostereoscopic viewing, multi-Gigabit network connectivity, and intuitive user interfaces - better coupling worldwide scientific virtual organizations, and better integrating scientific workplaces with advanced cyberinfrastructure. OmegaTable and OmegaDesk combine all this functionality in one set of instruments, enabling communities to view and share high-resolution 2D, 2.5D, and 3D stereoscopic imagery over distance and to manipulate the imagery with an intuitive touch interface. The most unique capability lies in their ability to display 2D and 3D stereoscopic imagery simultaneously, without users needing to wear 3D glasses of head-tracking equipment. The instruments open new opportunities in virtual reality, human-computer interaction, high-speed networking, scientific visualization, and Grid computing.<br\/><br\/>Broader Impacts: This project enables state-of-the-art equipment, opportunities, and supervision to enhance student education (providing scientific communities with highly integrated virtual-reality collaboration environments), to work with industry to commercialize new technologies that advance science and engineering, and to continue on-going partnerships with domain scientists world-wide. In addition to enhancing education, the instrument provides summer internships and enables jobs upon graduation. Society as a whole has much to gain by the possibilities to solve complex environmental, medical, and economic issues that these instruments offer.","title":"MRI: Development of OmegaTable and OmegaDesk - Instruments for Interactive Visual Data Exploration and Collaboration","awardID":"0821121","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["558518","558517","558519","495168"],"PO":["557609"]},"143443":{"abstract":"Location-based services are becoming an important part of our everyday lives. According to the Pew Internet & American Life Project survey, 47% of current cell-phone users prefer to have mapping features on their phones. Similar trends can be shown for other portable devices such as car navigation systems, PDA's and computer laptops. However, recent concerns over how such services can jeopardize a user's private information resulted in the newly coined term \"location privacy.\" Several breaches of subscriber's privacy by stalking their locations have been reported. This project is going beyond the conventional approaches proposed for location privacy by devising novel frameworks to eliminate the need for a trusted intermediary (acting as an anonymizer) in location-based services and that satisfy significantly more stringent privacy guarantees as compared to the anonymity-based approaches. The first approach is inspired by the work in the area of encryption and the second approach builds upon the framework of private information retrieval. Both of these research fields have been around for a long time enabling privacy protection but not yet fully exploited for location privacy. The broader impact of this study is to enable many users with privacy concerns to utilize customized location services in their mobile devices without compromising their location privacy. This in turn would increase the customer base for many industries and government agencies in the areas of geospatial information systems, online maps, car navigation systems, Location Based Services, and military and intelligence operations. Results will be disseminated through papers published in relevant conferences and journals; open-source code will be disseminated through the web: http:\/\/infolab.usc.edu\/projects\/LocationPrivacy\/.","title":"CT-ISG: Enabling Location Privacy; Moving beyond k-anonymity, cloaking and anonymizers","awardID":"0831505","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["550740"],"PO":["497499"]},"141023":{"abstract":"Proposal Number: 0820286<br\/><br\/>Title: Adaptation-Based Programming<br\/><br\/>PI: Alan Fern<br\/>Co-PIs: Martin Erwig and Thinh Nguyen<br\/><br\/>Developing software for complex, dynamic environments is a huge challenge using existing programming paradigms. Programmers must specify the exact behavior of a program at each point which, for complex problems, such as network control and designing intelligent game agents, is extremely difficult to do close to optimally. To better facilitate software development for such problems, the paradigm of adaptation-based programming (ABP) is investigated. In the ABP paradigm, programmers only exactly specify the parts of the program that they are confident about and leave other parts as adaptable. In addition, the programmer will specify an objective function to be optimized by the program. During program execution, the adaptable parts of the program will then be automatically optimized using state-of-the-art machine-learning techniques in order to maximize the objective function. The research focuses on three key directions: 1) developing theoretical foundations including formal programming-language semantics and learnability results, 2) speeding up the learning process via program transformations and programmer-specified adaptation advice, and 3) producing freely available ABP libraries for both the C++ and Haskell languages. Importantly all of the work is driven by a focus on applications of ABP to real problems from computer networking and intelligent agents for simulation and game environments.","title":"Adaptation-Based Programming","awardID":"0820286","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":["548134","399757","551065"],"PO":["564388"]},"143212":{"abstract":"Quickest detection is an important technique to detect the change of probability distribution in a random process being monitored. It is widely used in problems like financial decision making, environmental monitoring and industrial quality control. With the rapid development of networking techniques, there exist pressing demands to carry out quickest detection based on observations from many nodes and make decision at more than one node. Motivated by this demand, this research studies collaborative quickest detection in ad hoc networks, in which nodes exchange observation statistics and make local decisions about distribution change. In contrast to existing theory of decentralized quickest detection, the collaborative quickest detection does not need a data processing center, thus avoiding the round-trip time overhead and possible data congestion. Moreover, collaboration can enhance the agility and robustness of the detection of change. The research involves aspects of statistical signal processing (e.g. detection rule), information theory (e.g. source coding) and networking (e.g. scheduling or broadcast). An important application of collaborative quickest detection is spectrum sensing in cognitive radio systems. In such a system, secondary nodes need to monitor the activity of primary users and should quit the frequency band once primary users emerge. It is essentially a problem of quickest detection since the secondary nodes need to detect the change as quickly as possible. Therefore, this research also involves the design of collaborative quickest detection in cognitive radio networks. Inter-disciplinary essence of the research also lends itself to cross-disciplinary education. A one-semester graduate level course will be devised, which introduces quickest detection, cooperative communication and cognitive radio. This project also expects to attract traditionally underrepresented groups.","title":"Collaborative Research: Collaborative Quickest Detection in Ad hoc Networks with Application in Cognitive Radio","awardID":"0830462","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["544836"],"PO":["564924"]},"143454":{"abstract":"This project addresses the challenge of providing usable (plug-and-play, self-configuring, and autonomic) security solutions for Body Area Networks (BANs): networks of economically powered, wireless, wearable and\/or implanted health monitoring nodes (sensors and actuators), for collecting and communicating health information and, appropriately administering medicine or prosthetic actions. BANs have many diverse applications including sports health management, home-based health-care and post-operative care. Due to the sensitive nature of the data collected, securing BANs is important for privacy preservation and protecting the host from bodily harm. Traditional security methods require considerable setup efforts in terms of pre-deployment of cryptographic secrets (key distribution). In contrast, this project takes a cyber-physical approach and uses the concept of Physiological Value based Security (PVS), which employs physiological values (PVs) to generate cryptographic keys for securing BANs. Two important issues in this project are: 1) identification of appropriate PVs for generating high-quality keys; and 2) development of signal processing techniques to generate cryptographic keys from the physiological values. For the former issue, the use of common PVs (e.g. Electrocardiogram, Photoplethysmogram, etc.) would be investigated; for the latter, time and frequency domain feature generation techniques would be developed. This project will result in: a) benchmarks for selecting appropriate PVs, b) light-weight measurement and synchronization algorithms to extract PVs in real-time, c) a software and hardware (FPGA-based) prototype implementation of PVS in IMPACT lab?s AYUSHMAN pervasive health monitoring testbed. These results (to be made publicly available at http:\/\/impact.asu.edu) will be an important step toward providing dependable, affordable healthcare for all.","title":"CT-ISG: Physiological Value based Security for Body Area Networks","awardID":"0831544","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["535238"],"PO":["565327"]},"143223":{"abstract":"Although there has been a considerable amount of research done by using cross-layer optimization, fundamental understandings on the design limits of cross-layer optimization are still missing. The overarching goal of this project is to fill this knowledge gap. The objectives are to 1) qualitatively characterize multi-scale temporal-spatial behaviors of cross-layer optimization; 2) develop a novel complexity-scalable framework to support cross-layer optimization over heterogeneous network elements with different computation capabilities; and 3) evaluate the proposed framework by using extensive simulations as well as field test based on our existing testbed.<br\/><br\/>The intellectual merits and transformative significance of this project include: 1) to provide a theoretical framework to understand major design tradeoffs in cross-layer design and optimization and 2) to provide a common ground to quantitatively evaluate and compare different cross-layer design schemes. By developing a new theoretical framework to quantitatively analyze design tradeoffs in complex system optimization, the broader impacts of this project are: 1) to benefit wide applications in social networks, economics, physics, and biology, leading to a profound impact on all societal levels and 2) to expand the existing collaborations with industrial partners and enhance the ongoing effort on STEM education. Results, outcomes, software tools, benchmarks, and educational materials will be disseminated through a project?s web site as well as journal and conference publications. The research results of cross-layer optimizations will also be integrated into courses taught in the PI's university.","title":"TF:SING: Exploring the Fundamental Limits of Cross-Layer Optimization in the Next-Generation Internet","awardID":"0830493","effectiveDate":"2008-09-01","expirationDate":"2013-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["500372"],"PO":["564924"]},"143465":{"abstract":"Distributed data storage and access in wireless sensor networks (WSNs) recently has found increased popularity driven by many mission-critical applications. While WSN security has been extensively studied in recent years with focus on network communication security, little attention has been paid on the security and dependability of distributed data storage and access control. However, it is of paramount importance to ensure data security and dependability in the mission-critical applications, where data genuineness and availability can be about life or death. This project is aimed at solving this important challenge. On the one hand, the problem of how to store the sensor network data in a distributed manner while satisfying the requirements of both fault-tolerance and compromise-resilience is studied. The problem of dynamic data security and dependability after the initial data storage is tackled as along the time sensors may be compromised and\/or behave Byzantine failures. On the other hand, researches on distributed and fine-grained data access control are carried out to ensure sensor network data can only be accessed by authorized network users. The worst-case scenario is considered in which not only sensors may be compromised, but also network users may not be fully trusted. Novel symmetric key cryptography (SKC) and public key cryptography (PCK) based approaches are proposed, built on top of the in-depth security analysis of the state-of-the-art SKC solutions and efficiency enhancements over the most recent PKC primitives such as Ciphertext Policy- and Key Policy- Attribute Based Encryption, i.e., CP-ABE and KP-ABE.","title":"NeTS-NECO: Collaborative Research: New Approaches for Secure and Dependable Distributed Data Storage and Access Control in Mission-critical Wireless Sensor Networks","awardID":"0831628","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["564847"],"PO":["565303"]},"141166":{"abstract":"The End User Opt-In Working Group, which is one of five Working Groups dedicated to particular areas of technical design for a suite of experimental network infrastructure currently in its planning stages, holds three workshops each year that, on a continuing basis: 1) identify and resolve design issues, 2) identify areas of technical risk, 3) review documents prepared by the GENI Project Office, and 4) engage in outreach. This award gives the co-chairs funds for their participation in a one-year set of workshops. <br\/><br\/>By having an open, transparent, well-managed design process, the community will have ample opportunities to comment and contribute to the design of a suite of experimental network infrastructure. Engaging the community early in the design process will ensure that the infrastructure will have the functionality and capabilities required by many in the research community. Workshop topics include: detailed open design issues and tradeoffs, risk assessment and mitigation, common vocabulary and models, design and integration activities, use cases, and the evaluation of software and services, for example. <br\/><br\/>The chairs will be responsible for workshop preparation, chairing workshops, results dissemination, and progress reporting. These funds will enable the chairs to recruit a diverse set of attendees, conduct and attend meetings, and broadly disseminate the results.","title":"Collaborative Research: GENI Working Group Meetings - Opt-in","awardID":"0820795","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["563668"],"PO":["495796"]},"143113":{"abstract":"In this research, we study a set of geometric approaches that can be used to fundamentally extend our methodology of information theoretical analysis. The conventional approaches to study information transmissions are based on the key quantities like entropy and mutual information, which can be viewed as a distance between distributions. Such approaches are particularly useful in studying static point-to-point communication problems, where relatively few probability distributions are involved. Facing the challenge of understanding large dynamic wireless networks, as many distributions with high dimensionalities are often involved, the conventional approaches often become cumbersome in solving or even describing the problems. The geometric approach studied in this work is in a sense a ?calculus? on the space of distributions. By developing notions of inner products, projections, and coordinate systems in the space of distributions, we add a sense of ?direction? in information theoretic analysis. The new insights from this approach often lead to better understanding and extensions to the existing network information theory results.<br\/><br\/>The geometric approach is mainly used in two sets of problems. First, by visualizing the relation between multiple distributions, we develop the notion of divergence transition to describe the information exchange through general statistical coupling. This approach is particularly useful in studying mutli-terminal communication problems, in describing and controlling information contents at different terminals. Secondly, we used this approach to study error protections in dynamic communication channels. The conventional wisdom suggests that all information can be converted into bits and uniformly protected in transmissions, which is clearly inadequate for dynamic networked applications. In the study of new notions of heterogeneous information processing and unequal error protections, the geometric approach plays a crucial role.","title":"Information Theory with Directions: Geometric Structure and Coordinates on the Space of Probability Distributions","awardID":"0830100","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["530080"],"PO":["564924"]},"145423":{"abstract":"This project enhances the expressive ability of the fundamental medium of text with the development of tools that enhance the creativity of people while generating kinetic typography. Traditional forms of static typography can be augmented with time and motion, in order to manipulate the position, size, color, shape, and other properties of text displays over time. This new kinetic typography offers a number of potential advantages: the ability to convey emotional content and qualities of the speaker's voice, the potential for increased reading performance on very small displays, and the ability to communicate in new ways. These advantages have been largely unexploited because the technological tools needed to make kinetic typography easily accessible to both designers and the general public have only begun to be developed. This project explores the nature of tools that enhance human creativity in the context of generating kinetic text. The impact of this work will reach well beyond kinetic typography, with implications for understanding the design process, emotional communication, and creativity. This research will extend our basic knowledge of communication and information transfer, and give us a foundation for understanding and creating kinetic typography tools, with long-term implications for work in design schools and training environments.","title":"SGER: Enabling Creativity Using Kinetic Typography","awardID":"0840766","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["518043","526576"],"PO":["564456"]},"143003":{"abstract":"Quantum computers, if built, will have great impact on defense and<br\/>commerce in the area of secure communication. They will also allow<br\/>simulation of complex quantum systems, providing an important tool<br\/>for nanotechnology. The principal challenge in actually building<br\/>quantum computers is finding a way to tolerate imperfections. In<br\/>fault-tolerant quantum computing (FTQC), the faulty operations that<br\/>cause computational errors must be used to detect and correct those<br\/>errors. A radical approach to FTQC is to base the computational<br\/>scheme on topology. Computation is performed by braiding<br\/>topological structures; fault-tolerance arises naturally due to the<br\/>invariance of large-scale topology to small, local distortions<br\/>caused by random errors. Until recently, implementation of this<br\/>rather abstract idea has demanded exotic, unobserved physical<br\/>particles or unrealistically high-dimensional structures. However,<br\/>it was recently shown that topological FTQC may be achieved in a<br\/>two-dimensional lattice of quantum bits in a scheme relying on a<br\/>novel form of quantum entanglement called a cluster state.<br\/><br\/>This research investigates the conversion of topological<br\/>cluster-state-based FTQC into a realistic, scalable, solid-state<br\/>hardware architecture. The particular hardware studied relies<br\/>critically on computation via communication, enabling reasonably<br\/>sized quantum processors to be connected to form quantum<br\/>communication networks or large quantum multicomputers. The<br\/>investigators experimentally develop semiconductor-based quantum<br\/>memories in optical microcavities, optimized for quantum logic<br\/>mediated by light in integrated optical waveguides. The research<br\/>includes the development of experimental methods to fabricate and<br\/>characterize large arrays of these qubits, while detecting and<br\/>compensating for fabrication defects. These experimental efforts<br\/>are supported by theoretical development of scalable, topologically<br\/>fault-tolerant architectures designed around the observed hardware<br\/>limitations.","title":"Topologically Fault-Tolerant Distributed Quantum Computer Architecture","awardID":"0829694","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["417138"],"PO":["565157"]},"143124":{"abstract":"In 1953, Claude Shannon, the founder of information theory, pointed out that there is no theory via which information embodied in structure can be quantified; this situation remains in effect today. The need for such a theory has become pressing in recent years with the proliferation of structured data sets arising from diverse applications. We have yet to answer fundamental questions such as:<br\/>What are fundamental limits on storage and processing of structural information? What are fundamental bounds on extraction of information from large biological databases? Lack of understanding of such questions threatens to raise severe impediments to further advances in science and engineering of complex systems. The main goal of this work is to search for measures and algorithms to appraise the amount of organization and structure embodied in artifacts and natural objects.<br\/>We propose to make headway in information theory of data structures.<br\/><br\/>Data is increasingly available in various forms (e.g., sequences, expressions, interactions, structures) and in exponentially increasing amounts. Most of such data is multidimensional and context dependent; thus it necessitates novel theory and efficient algorithms to extract meaningful information from non-conventional data structures. In compressing such a data structure, one must take into account two types of information: the information conveyed by the structure itself, and then the information conveyed by the data labels implanted in the structure. The specific goals of this project are: (i) characterization of the total amount of information conveyed by a data structure (and how this decomposes into the two types of information mentioned above), and (ii) the design of efficient compression algorithms based upon the total amount of information conveyed in (i).","title":"Collaborative Research: Information Theory of Data Structures","awardID":"0830140","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["425343"],"PO":["564924"]},"143366":{"abstract":"Software is a common target of attacks on the current computing \/ communications infrastructure. Software continues to be vulnerable to attacks that exploit obscure or misunderstood language and program features. Detection of these software exploits (also called \"malware\") will therefore be needed for the forseeable future as one part of an effective defense. Virus checkers detect many known exploits, and are now widely used, but attackers have adapted by obfuscating and mutating their code to evade virus checkers.<br\/><br\/>Such techniques make precise identification of malware extremely difficult. This project will use key characteristics of attack code for identification purposes. Important features of this approach include: advanced disassembly techniques; translation of code into an intermediate form more amenable to analysis, and more resistant to obfuscation; static reconstruction of program control flow and data flow; and, extraction of properties of interest, followed by analysis of these properties. The properties of interest include the characteristic behaviors of encryption and compression, and the system calls executed by the code. Rather than relying on exact matching of these properties for malware identification, approximate matching will be used. Static analysis will be the focus, to avoid the performance penalties of dynamic execution monitoring. The application of data mining to identify important malware features, and construct high-level patterns or signatures in a completely automated way, will also be investigated. The method will additionally help identify malware relationships, with applications to forensics, recovery of attack strategies, and identification of new classes of attacks (including zero-day attacks).<br\/><br\/>The method will resist the introduction of noise, or targeted evasion by malware writers, and will provide much better protection against polymorphic and metamorphic exploit code, and new attack variations. A database of patterns \/ characteristics for known software exploits will be maintained and made public. Educational materials about malware detection will be developed and disseminated, and training of female researchers will continue to be a priority.","title":"CT-ISG: The Origin of the Code: Automated Identification of Common Characteristics in Malware","awardID":"0831081","effectiveDate":"2008-09-01","expirationDate":"2012-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["548079"],"PO":["565264"]},"143487":{"abstract":"To counter the inefficiencies of the current spectrum usage, regulatory bodies, all over the world, are exploring ways to deregulate the spectrum market by allowing flexible dynamic spectrum access (DSA) in a broad range of spatio-temporal scale. Recent advances in radio technology have given an impetus to this trend. For DSA to fulfill its promise of economic and societal impact, wireless services based on DSA must be commercially successful, and a tangible spectrum market must evolve that can be supported by technology. This research project will build a realistic DSA architecture for cellular networks supported by appropriate market mechanisms in an integrated fashion that is both technically and economically viable and efficient. This is a truly trans-disciplinary approach spanning the fields of wireless networking and systems, algorithmics, economics, simulation and modeling, which leads to a deeper understanding of the dynamics of the spectrum market by (i) realistic modeling of various market entities (i.e., buyers, sellers, and the market mechanisms), (ii) dynamic spectrum demands and bids based on innovative and realistic population dynamics models, and (iii) new and robust market clearing mechanisms with provable performance guarantees. The results will be validated using large-scale simulations, and experiments on a prototype test bed with reconfigurable radio hardware. In addition to fostering new topics in trans-disciplinary education, this project will offer insights into market driven spectrum sharing, provide useful tools for policymakers, and ultimately guide spectrum policy decisions in DSA technology. This will, in turn, open up new business opportunities in the use of wireless spectrum.","title":"Collaborative Research: NECO: A Market-Driven Approach to Dynamic Spectrum Sharing","awardID":"0831762","effectiveDate":"2008-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["486344"],"PO":["557315"]},"144345":{"abstract":"'News at Seven' is a fully automated approach to creating broadcast news developed at Northwestern University's Intelligent Information Laboratory (InfoLab). Starting with news stories in textual form (either selected manually, or found automatically using a programmable preference model), the system is able to edit the stories for broadcast presentation, find relevant background visual materials (e.g., videos and still images), and augment the primary text using sources such as blogs found on the Web. Together, these resources are then used to drive a set of animated characters who reside in a virtual 'news world' we have created for them.<br\/><br\/>This project is developing tools that will better enable content providers, who are not computing professionals, to prepare the necessary information aggregation and presentation capabilities for news delivery within niche populations with special information needs.","title":"SGER: A Proposal for Deploying News at Seven: Providing Research Visibility While Serving Underrepresented Populations","awardID":"0836564","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["410053"],"PO":["387198"]},"143256":{"abstract":"For over half a century various digital communication techniques have been developed to address reliable transmission over imperfect links. These developments include information theory, practical source and channel coding schemes, and various effective modulation techniques. Many different channel models are considered in the literature extensively, and devised solutions have been put to efficient practical use; impacting all aspects of the world we live in today. Random insertions and\/or deletions in addition to other, better understood, channel impairments (e.g. noise, intersymbol interference, etc.) are of utmost concern in many modern digital communication systems, including recording channels, transmissions through the Internet, character recognition systems, covert communications, and so on. The presence of insertions\/deletions is often the bottleneck for more efficient reliable communications and higher areal densities in recording. Despite many advances over the past fifty years, channels with deletions\/insertions are still far from being fully understood. To give a simple example, the capacity of a binary deletion channel is still unknown, and no practical coding\/decoding schemes with performance close to available theoretical bounds are available.<br\/><br\/><br\/><br\/>This project studies insertion\/deletion channels from both an information theoretic point of view, and a channel coding perspective. Specifically, it involves development of novel information theoretic results and practical <br\/>coding schemes for such channels, with extensions to channels with intersymbol interference, additive noise, and other impairments. Capacity approaching low density parity check (LDPC) and turbo coding schemes are considered. In addition, a new set of interesting problems involving insertions\/deletions over wireless fading channels, including multi-input multi-output (MIMO) communications, are formulated, and solutions are developed.","title":"Insertion\/Deletion Channels: Achievable Rates and Practical Coding Schemes","awardID":"0830611","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["486150"],"PO":["564924"]},"143498":{"abstract":"The Internet faces many challenges, including security, management and control. The research project explores a future Internet\/networking architecture based on a well-proven hierarchical architecture that is untested at global levels. The architecture includes a flexible addressing scheme and a compact four layer protocol stack. The radical change to the Internet architecture would be potentially transformative from a socioeconomic and business perspective. The scientific problems to be addressed are hence multidisciplinary. The technical and socioeconomic feasibility for the new network architecture, as well as the likely patterns of adoption for such a radical change are investigated.<br\/><br\/>Intellectual merit: This research asks how a fully hierarchical architecture, with adequate redundancy at different levels in this design, results in a robust structure that is also scalable. The solution distributes the \"switching decisions,\" the management of how traffic moves from one point to another, to all levels of a hierarchy of communications providers: it allows the traffic to be managed independently wherever the traffic is flowing. The replaces present highly interdependent routing and network management architecture. The research will show how this architecture is robust and stable with a potential for future growth. A key contribution of the architecture is a scalable and flexible hierarchical addressing scheme, where key network functions are relegated to different levels in the hierarchy, and this also provides better management and control. The research includes proof-of-concept by interfacing diverse wireless networks in the architecture.<br\/><br\/>Broader impact: A new network architecture resulting from the SWITCHNET research could create radical changes in the economics of communications networks and ultimately have a very large impact on the economy and society. The conceptual architecture is almost ideally scalable. It is very much simpler to manage than the present architecture. As an example of the broad impact of SWITCHNET manageability, by enabling fundamentally better provider control, the architecture would open the door to new methods of identifying and pushing back denial of service, an economically and nationally much needed capability. Furthermore, this research could provide the breakthrough architecture for attaining the all-optical goal and thus address that national challenge to create a more energy-efficient network. Approaches to achieving such a purely optical communications infrastructure have long been sought. <br\/><br\/>This research is a close collaboration between technical and business researchers; the architecture research is supported by research on its business and socioeconomic impacts and adoption patterns. Understanding the existing Internet industry and creating appropriate incentives for its stakeholders will increase the chances that the technological innovations in this research will be embraced and implemented.","title":"Collaborative Research: NeTS-FIND: SWITCHNET: A Switched Internetworking Architecture with Contracted Services","awardID":"0831830","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["531798","531799"],"PO":["565090"]},"143388":{"abstract":"The near ubiquity of Internet access has put a wealth of information and ever-increasing opportunities for social interaction at the fingertips of users. Driving this revolution is the modern web browser, which has evolved from a relatively simple client application designed to display static data into a complex networked operating system tasked with managing many facets of a users online experience. Support for dynamic content, multimedia data, and third-party plug-ins has greatly enriched users experiences at the cost of increasing the complexity of the browser itself. As a result, current web browsers are plagued with security vulnerabilities that provide hackers with easy access to end-user systems via browser-based attacks. Browser security efforts to date are essentially retrofits for existing web browsers and have enjoyed only limited success, as the design of modern web browsers is fundamentally flawed. To address the root of this problem, this research will develop an inherently more secure design methodology for any network-facing user application, which will be validated through the design and implementation of a new secure web browser called OP. The overall design approach combines separation and safety principles from the operating system community with validation and monitoring techniques developed by the formal methods community. By partitioning the browser into smaller subsystems and making all communication between subsystems simple and explicit, this research effort can leverage techniques from both of these communities to elicit formal guarantees about OP's correctness and ability to limit the effects of compromised subsystems.","title":"CT-ISG: An Architecture and Policies for Secure Network-facing Applications","awardID":"0831212","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["438346","548692"],"PO":["565264"]},"145456":{"abstract":"Collaborative Research: Complex-Valued Signal Processing and its Application to Analysis of Brain Imaging Data Complex-valued signals arise frequently in applications as diverse as communications, radar, and biomedicine, as most practical modulation formats are of complex type and applications such as radar and magnetic resonance imaging lead to data that are inherently complex valued. The complex domain not only provides a convenient representation for these signals but also a natural way to preserve the physical characteristics of the signals and the transformations they go though. The complex domain, however, also presents a number of challenges in the derivation and analysis of signal processing algorithms, and as a result, the vast majority of algorithms developed for the complex domain have taken shortcuts limiting their usefulness. This research establishes a framework for complex-valued signal processing such that the full potential of complex-valued signal processing can be realized. It allows for all computations to be carried out in the complex domain eliminating the need for many simplifying assumptions, such as the circularity of signal, both in the derivation and the analysis of the algorithms. It also allows for the use of fully complex functions rather than the more commonly utilized bounded but non-analytic functions. These functions provide attractive alternatives for performing independent component analysis (ICA) by efficiently generating higher-order statistical information. Using this framework, a new class of efficient algorithms are derived for performing ICA in the complex domain, in particular, for studying brain function using the medical imaging data in its native, complex form.","title":"Complex-Valued Signal Processing and its Application to Analysis of Brain Imaging Data","awardID":"0840895","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["486018"],"PO":["564898"]},"143036":{"abstract":"Large-scale databases from sequencing projects, microarray studies, gene-function studies, protein-protein interactions, comparative genomics, structural biology, and so on are growing at rapid rates. These data come in diverse forms including expression data, network\/interaction data, ontologies, text documents, and raw image\/spectrum data. An integrated mining approach, combining these vast public repositories, is a crucial component in answering important scientific questions. <br\/><br\/>This research is focusing on developing novel data mining techniques for integrated analysis and mining of complex gene\/protein expression and interaction datasets taken from multiple sources. In particular, this research aims at: 1) Mining patterns of gene expression and regulation via Boolean expression and coherent cluster mining. 2) Mining novel protein interactions and network modules\/motifs. 3) Integrated functional relationship mining over multiple genome-wide datasets. Given the proliferation of such complex, networked data in a variety of domains such as social networks, biological networks, semantic web, and so on, the methodology and algorithms being developed in this research will be widely applicable to other important areas.","title":"EMT\/BSSE: Discovery of Gene and Protein Expression Patterns and Networks","awardID":"0829835","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7750","name":"CDI TYPE I"}}],"PIcoPI":["541893"],"PO":["565223"]},"143278":{"abstract":"Cognitive radio is the key enabling technology for future generations of wireless systems that address critical challenges in spectrum efficiency, interference management, and coexistence of heterogeneous networks. This research aims to develop fundamental theories and practical algorithms for cognitive radio systems. It focuses on three key areas: (i) spectrum opportunity sensing and cognition; (ii) spectrum opportunity tracking and exploitation; and (iii) cognitive opportunistic networking. These three areas represent a logical progression that expands in scope and complexity.<br\/><br\/>Key innovations of this research include exploiting the heavy tail and self similar nature of primary traffic processes in the design of cognitive radio systems. The inevitability of sensing errors, which has mainly been studied in isolation at the physical layer, is explicitly taken into account in all aspects of this research, from the investigation of fundamental performance limits to the design of algorithms and protocols at all network layers. The overall objective of this research is to gain a rigorous understanding of the role of feedback and learning in cognitive radio systems, to characterize fundamental structures and performance limits of opportunistic spectrum access, and to develop efficient and robust approaches to harvest spectrum opportunities using networked cognitive radios. An integrative approach is being developed, integrating recent advances in traffic modeling, distributed statistical inference and consensus learning, and theories and techniques of dynamic optimization and stochastic control. This research promotes active learning and discovery and broadens participation of female students in engineering, from K-12 girls to undergraduate and graduate students.","title":"Feedback and Learning in Cognitive Radio Systems","awardID":"0830685","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["551133"],"PO":["564924"]},"143399":{"abstract":"Cyber threats have escalated rapidly over the past decade. \"Zero-day<br\/>attacks\" have become significant, delivered increasingly through<br\/>seemingly innocuous means such as web pages, images, and documents.<br\/>Malware is rampant, being installed surreptitiously on millions of<br\/>computers around the world using a combination of spam, phishing,<br\/>malicious shareware and freeware. <br\/><br\/>Today's defenses use techniques such as signature-based scanning and<br\/>file integrity monitoring to detect the presence of malware, and then<br\/>remove them. Unfortunately, clever adversaries can quickly develop<br\/>malware that conceals itself from these detection mechanisms, and<br\/>hence defeat such reactive defenses. In contrast, this project will<br\/>develop an approach that dramatically improves defenses against<br\/>malware, and put a computer owner back in control over the<br\/>attackers. This approach, based on synthesizing and enforcing<br\/>low-level information flow properties from generic high level<br\/>policies, will be used to identify components of a computer system<br\/>that are critical for its trustworthiness, and preserve their<br\/>integrity. In doing so, the approach will enable users to continue to<br\/>use popular operating systems, applications, and add-on software, while still<br\/>assuring system security.<br\/><br\/>Specifically, this project will develop techniques to protect (a) the OS<br\/>and critical applications from untrusted code or data, (b) critical<br\/>applications from modules and extensions (e.g., browser plug-ins and media<br\/>player codecs) that run within the same address space, and (c) the OS kernel<br\/>from damage due to untrusted kernel extensions such as device drivers.<br\/><br\/>In terms of broader impact, this project will train several graduate<br\/>students, the research will be integrated into the teaching activities of<br\/>the PIs, and finally, the solutions developed will be distributed as<br\/>open-source software and\/or tools.","title":"CT-T: Proactive Techniques for Preserving System Integrity: A Basis for Robust Defense Against Malware","awardID":"0831298","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["550275","477226","531979"],"PO":["565327"]},"138801":{"abstract":"The Internet has become the primary medium for accessing information and for conducting many types of online transactions, including shopping, paying bills, making travel plans, applying for college or employment, and participating in civic activities. The primary mode of interaction over the Internet is via graphical browsers designed for visual navigation. This seriously limits the access of people with impaired vision or blindness, a population that is large and growing ever larger. Existing assistive technology for non-visual Internet access typically forces users with visual impairments into an inefficient, sequential mode of information access. To do better, two kinds of models are needed. First, we need to build computational models to represent the structure of web pages and online transactions, and to present them effectively using non-visual modalities. Second, we need to better understand how users' mental models for online transactions are built and utilized; we then need to align the computational models with the users' mental models, so as to combine their strengths and significantly improve the efficiency of non-visual interactions. In previous work, the PI developed the HearSay non-visual web browser, which permits users to perform basic non-visual web browsing and search, contextual browsing, and online form-filling. However, HearSay does not take full advantage of the interaction context or the unique perceptual and processing strengths of people with visual impairments. In the current project, the PI seeks to combine basic computational and psychological research designed to produce accessibility technology embodying the synergy of computational modeling and users' mental models. In terms of computational research, the PI will: (i) automatically track the interaction context of user browsing actions; (ii) automatically build models for transactions that users perform online; and (iii) develop ways in which users can interact with transaction models through non-visual modalities efficiently and effectively. In terms of psychological research, user studies will be conducted to examine (i) how people build mental models for online transactions, and (ii) how they use modality-specific cues and their own short-term memory to utilize these mental models. The PI will incorporate the findings from these user studies into the computational models for online transaction processing, so as to align them with the users' mental models.<br\/><br\/>Broader Impacts: The ultimate goal of the PI's research is to empower people with visual impairments to lead completely independent lives with the help of the Internet. To this end, the PI has planned an extensive dissemination campaign involving workshops, collaborations with institutions that serve people who have visual impairments, and online dissemination of HearSay prototypes and HearSay component technologies. HearSay will also provide a means, in principle, for anyone who wishes to have non-visual Internet access (e.g., listening to Internet content while driving).","title":"HCC-Large: Using the Internet without using the Eyes: Models of Online Transactions for Non-Visual Interaction","awardID":"0808678","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["518366","518367","483394",369058],"PO":["565227"]},"144015":{"abstract":"This project aims to develop theory and algorithms for distributed sensing from high-dimensional, non-Euclidean data corrupted with noise and outliers. The development of such a distributed sensing framework faces several critical challenges. For instance, most distributed algorithms such as \"consensus\" proceed by locally averaging low-dimensional Euclidean data to obtain a global average. In most applications, however, data are high-dimensional, and plagued with noise and outliers. Moreover, the goal is not necessarily to average the measurements, but to reach a consensus on a model inferred from the measurements. Since the estimation of such models often involves optimization on manifolds, nearly all algorithms for solving these problems are centralized, and require resources not available in wireless sensor nodes. This project offers a significant paradigm shift in distributed sensing based on novel robust consensus algorithms on manifolds. The first goal is to develop distributed sensing algorithms based on geometric control, graph theory, and machine learning, for processing data related by parameters lying in Riemannian manifolds. The second goal is to develop distributed sensing algorithms based on robust statistics and machine learning, that are robust to noise, and outliers. The third goal is to apply these robust consensus algorithms on manifolds to several distributed localization problems in wireless sensor networks. <br\/><br\/>The development of robust distributed estimation techniques on manifolds can impact many application areas, such as surveillance, security, tele-immersion, space exploration, environmental monitoring, and assisted home living. Such applications require professionals trained at the intersection of hybrid, embedded, and networked systems, robotics, sensor networks, control theory, computer vision and machine learning. The multidisciplinary expertise of the team will foster training at the intersection of these disciplines.","title":"Collaborative Research: CSR-EHCS(EHS), TM: Distributed Sensing via Robust Consensus on Manifolds","awardID":"0834446","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["495278"],"PO":["561889"]},"136513":{"abstract":"Time integrators are crucial computational tools for studying nonlinear dynamical systems. Numerous time stepping methods have been developed over the years, many of which are now available in off-the-shelf solvers. However energy drifts and numerical dissipation problems present even in highly accurate algorithms still routinely plague engineering applications. Geometric time integrators have been recently proven greatly useful to elucidate and fix these issues in solid mechanics. Yet these contributions have not carried over to the Eulerian setting, where they could impact both the understanding and the reliability of time integrators for computational fluid dynamics. The goal of this research project is thus to develop novel, geometrically-based Eulerian time integrators for the class of problems whose dynamics is described by an action principle, possibly including dissipation and forcing---which encompasses the canonical Euler and Navier-Stokes equations, as well as many other models. Eulerian discretizations of the Hamilton-Pontryagin principle will be explored, and combined with mathematical and numerical tools such as Discrete Exterior Calculus, the semigroup of positive doubly-stochastic matrices, and implicit functions. Resulting integrators are expected, just like in the Lagrangian setting, to respect the structure of the physics, i.e., to introduce no artificial numerical loss of crucial physical quantities such as energy or circulation.<br\/><br\/>The proposed research activities aim at developing an infrastructure for predictive and high-order accurate simulations of fluid-mechanical systems that combine the power of modern applied geometry with modern computational mechanics. In particular, it promises the introduction of novel variational fluid simulation algorithms: this innovative computational approach relies on a multidisciplinary effort drawing upon techniques from geometric mechanics, discrete geometry, numerical analysis, and graphics, thus promising a broad theoretical and practical impact. The development of such variational integrators from a unified geometric standpoint represents a stepping stone for our long-term goal of solving complex physical phenomena such as a flowing dress, a swimming fish or splashing water, the simulation of which requires considerable improvement of the current state of the art to become commonplace. The research experience acquired during this project is to be disseminated to a wide range of audiences through publishing in mathematics, engineering and computer science journals, books, and conferences, as well as on our web sites, in summer schools, workshops, and other educational activities. Outreach efforts at our three institutions include the recruitment of students from underrepresented groups to help with this research project, leveraging existing efforts for enhancing the participation of women and minorities in scientific research.","title":"Collaborative Research: Geometric Time Integrators for Mechanical Dynamical Systems","awardID":"0757123","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7478","name":"DYNAMICAL SYSTEMS"}}],"PIcoPI":["541934"],"PO":["564988"]},"144026":{"abstract":"Serious new technical challenges are barriers to advances in microelectronics technology as technology scaling comes up against fundamental limits of material properties and lithography. Large process variations and other random performance and power constraining imperfections are now being observed as microelectronic devices are scaled down to atomic dimensions. This requires that module level performance in electronic designs be pessimistically guardbanded to ensure proper overall system level functionality, forcing systems to operate at performance levels far below the inherent capability of the underlying design fabric. In addition, embedded DSP systems must be designed to work under worst case operating conditions resulting from ill-conditioned input signals. Wider guardbands from increasing performance and input signal variability in future technologies can negate most of the performance benefits of scaling, stalling a key payoff from Moore?s Law for embedded systems. In such an environment, introduction of new scaled devices will be cost-effective only if the guardbands can be controlled down to acceptable margins, despite the presence of these uncertainties. This remains a major unsolved challenge, especially for embedded DSP processors that must be concurrently optimized for system level power, performance and reliability. To address these problems, this project is developing the concept of test, diagnosis and continuous signal monitoring enabled dynamic circuit-architecture-algorithm co-modulation (or co-tuning) for both static (procees) and dynamic (input signal) uncertainties. Under this new design paradigm, feedback driven reconfiguration control mechanisms involving circuitry and software (?tuning knobs?) are designed into the IC to support power-performance trade-off and reliability recovery post manufacture. The research pursues vertically integrated circuit-architecture-algorithm tuning methods that offer 10X benefits over optimizations performed at a single level of the design hierarchy. The diagnostic information generated is used to dynamically optimize (post-manufacture) individual module level behavior to optimize system level performance, power and reliability metrics via specially designed hardware and software control mechanisms. In this way, each instantiation of a design adapts to the maximum performance, power, and reliability levels it is capable of in the presence of process variations and adverse operating conditions. <br\/><br\/>Graduate students working on the project receive a unique kind of training in this multidisciplinary research, which together the fields of digital design and test, control systems, embedded digital signal processing architectures, and algorithms. The students participate in summer internship programs with industry. Through joint efforts at Georgia Tech, Auburn University, and Tuskegee University, this project also actively supports the goals of recruiting more U.S. citizens, women and minorities to graduate programs.","title":"EHCS: Dynamic Vertically Integrated Power-Performance-Reliability Modulation in Embedded Digital Signal Processors","awardID":"0834484","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550643"],"PO":["561889"]},"143058":{"abstract":"This project exploits methods from statistical physics to provide fundamental advances in computing and communication systems. The intersection of computer science, information theory and statistical physics has seen a recent explosion of activity, resulting in new algorithms and new methods of analysis. Discrete computational challenges including constraint satisfaction, error correction and control of massive networks have benefited from techniques and insights offered by statistical physics. Physics, at the same time, has been significantly enriched by approaches from discrete computation, such as message-passing algorithms. <br\/><br\/>The investigators study two complementary approaches for addressing algorithmic challenges: 1) treating problem instances as members of a random ensemble that can be analyzed as a physical model, and 2) identifying specific classes of instances amenable to physical analysis. The first suggests a fundamental connection between algorithmic performance and an underlying physical phase structure, and has already led to significant new algorithms for unstructured random graphs or networks. The challenge is to generalize it to structured cases. The second uses techniques such as renormalization group and multiscale decomposition, and is proving to be a powerful new approach in probabilistic inference.","title":"EMT\/MISC: Collaborative Research: Harnessing Statistical Physics for Computing and Communication","awardID":"0829893","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["532110"],"PO":["565223"]},"144389":{"abstract":"NSF and other Federal agencies will require further understanding of concepts, methods, and means for Exascale computing systems before formulation of future programs in enabling technologies, architectures, and methodologies. Such understanding will drive the unknowns to be determined, the challenges to be addressed, strategies to guide, and tasks to be performed on the way to Exascale system realization. The objective of realizing systems capable of sustained Exaflops computing by 2018 suggests that sponsored R&D programs dedicated to this goal be started no later than 2011 to provide sufficient time to complete the ensemble of necessary research, design, and implementation tasks. <br\/>The NSF Point Design Study for Exascale Computing is a collection of three tasks that will look at three different regimes of computing by experts in those areas. While each is stand alone, they will benefit strongly through cooperation and sharing of results, each being guided by the insights derived from the others. These are 1) applications and programming models, 2) system architecture and operating systems, and 3) hardware technology and component design. <br\/>This SGER will study system architecture and operating systems aspect of the Point Design Study for Exascale Computing.","title":"A System Architecture Point Design Study for Exascale Computing","awardID":"0836755","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["563330"],"PO":["565272"]},"143179":{"abstract":"ABSTRACT:<br\/><br\/>A mathematical model for a natural (e.g., the heart beat of a<br\/>human) or man-made process (e.g., the radio wave of a wireless<br\/>signal) is a mathematical expression or an algorithm that on<br\/>evaluation of system parameters (such as a point in time) yields<br\/>a model value (e.g., the amplitude of a wave). Models are<br\/>created by understanding the process, which suggests the form<br\/>of the expression, and by observing and measuring an actual<br\/>process. From those data points the best model is fitted by<br\/>a computation. Erich Kaltofen studies both how to fit to data<br\/>certain models, such as fractions of sparse polynomials, and<br\/>then how to certify that the computation has produced the best<br\/>possible model. The algorithms for creation of best fits and<br\/>subsequent certification of optimality can be compute-intensive<br\/>and require multi-processor computing environments.<br\/><br\/>Erich Kaltofen and his students and collaborators will design<br\/>algorithms for symbolic models such as sparse multivariate<br\/>rational functions and formulas with very large and even<br\/>parametric exponents. Our algorithms can work with both exact<br\/>and approximate data, the latter by hybrid symbolic\/numeric<br\/>techniques. Computation with floating point scalars requires<br\/>a new kind of probabilistic analysis when randomization is<br\/>applied, and we will make use of recent results on estimating<br\/>the spectra and condition numbers of random matrices. One<br\/>application of such randomization is the efficient solution<br\/>of highly under- and overdetermined dense linear systems.<br\/>A new alternative to error analysis is the exact validation via<br\/>symbolic computation of the global optimality of our approximate<br\/>solutions. Semidefinite programming and Newton refinement<br\/>are used to compute a numerical sum-of-squares representation,<br\/>which is converted to an exact rational identity for a nearby<br\/>rational lower bound. Since the exact certificates leave no<br\/>doubt, the numeric heuristics need not be fully analyzed.<br\/>We will search for rationalizations that can validate very<br\/>large sums-of-squares and hence apply to large inputs. We will<br\/>develop parallel and distribute computing tools for the arising<br\/>symbolic and hybrid symbolic-numeric computation tasks.","title":"Model Discovery and Verification With Symbolic, Hybrid Symbolic-Numeric and Parallel Computation","awardID":"0830347","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["485433"],"PO":["565251"]},"147425":{"abstract":"Advancing knowledge in the biological sciences involves<br\/>experimentally testing hypothesesand interpreting the <br\/>results based on prior scientific work. Researchers face <br\/>the challenge of collecting, evaluating and integrating <br\/>large amounts of different kinds of information about <br\/>organisms, cells, genes and proteins to generate a valid<br\/>hypothesis. And once a hypothesis is generated, the challenge <br\/>is to evaluate the hypothesis with respect to what <br\/>is already known. <br\/><br\/>Our proposed research will:<br\/><br\/>1. Test the scalability and extensibility of a novel <br\/> computer system that allows biologists to construct <br\/> and evaluate alternative hypotheses against a knowledge <br\/> base on the yeast Saccharomyces cerevisiae. <br\/><br\/>2. Test a knowledge archive that supports the archiving <br\/> and search of validated hypotheses.<br\/> <br\/>In our work, we define a hypothesis as a statement about <br\/>relationships between components of a biological system <br\/>that are intended to explain experimental observations. <br\/>A set of validated hypotheses can be used as building blocks <br\/>to construct larger, more complex models such as <br\/>pathways. We plan to develop and test a new paradigm: <br\/>that of hypothesis-driven querying of model organism knowledgebases. <br\/><br\/>The proposed work, called HyQue (for Hypothesis-based Querying <br\/>of pathway models), will take as input working hypotheses about<br\/>pathway models expressed in a knowledge-based formalism, <br\/>evaluate their consistency using existing data in a knowledgebase, <br\/>and provide as output contradictory evidence and suggestions for<br\/>improving hypotheses. HyQue will incorporate formal knowledge <br\/>representations based upon Semantic Web standards and <br\/>an ontology to represent biological objects and relationships.<br\/>HyQue will also contain a library of rules that determine <br\/>counts of support and contradiction for a given hypothesis. <br\/><br\/>We will prototype an archive of hypotheses that allows users <br\/>to compare their hypothesis with other hypotheses submitted by <br\/>their peers. We will explore the capability to:<br\/><br\/> (1) express working hypotheses about the yeast cell cycle; <br\/> (2) provide integration of data in the Saccharomyces Genome <br\/> Database to evaluate\/test pathway-specific hypotheses; <br\/> and<br\/> (3) archive these results. <br\/><br\/>As analytical tools and database resources proliferate, <br\/>biologists require facilities to integrate existing data <br\/>into knowledge that can create a shared understanding of<br\/>biological models. Our work will explore the expressivity <br\/>and scalability of unique and novel querying and contradiction based <br\/>reasoning methods that use rich formal knowledge specifications of<br\/>biological events in order to accomplish information integration. <br\/>Our proposed work will lead to a novel paradigm of querying <br\/>biological knowledge that can dynamically retrieve, integrate and<br\/>interpret information in terms of biologically relevant <br\/>relationships asserted as pathway models. Our work will <br\/>examine the value of Semantic Web technologies in building <br\/>a knowledgebase for such querying and reasoning and will <br\/>aid in standardizing models of biological knowledge and <br\/>add momentum to a range of ongoing ontology building efforts. <br\/><br\/>Further information on the project can be found at the <br\/>project web page: http:\/\/nigam.web.stanford.edu\/hyque","title":"III\/SGER: Hypothesis Based Query and Verification of Pathway Models","awardID":"0849207","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[392747],"PO":["560586"]},"144037":{"abstract":"Wireless sensor nets (WSNs) promise anytime\/anywhere information about the physical world with a spatio-temporal resolution that was unimaginable just a few years ago. However, development tools for WSNs are still conspicuously inadequate; they are notoriously unstable and difficult to debug, reprogram, or profile at run-time.<br\/><br\/>These problems motivate the need for virtual execution environments (VEEs) in the WSN domain. A VEE is a software system that sits above the hardware layer and modifies the execution behavior of a native binary executable at run time. VEEs are designed to perform operations on a program binary during execution, such as program shepherding, execution profiling, dynamic reprogramming, and debugging. <br\/><br\/>The overall goal of this research is to explore, design, implement, and evaluate VEEs for the WSN domain. This project considers three basic VEE architectures: injection-based, translation-based, and interpretation-based VEEs. A key aspect of this research is to revisit fundamental design decisions of conventional VEEs to make them operate with the limited resource profiles of sensor nodes. The results of this research will make it easier to quickly and robustly develop sensor network applications, thereby helping to transition this promising technology into the mainstream.","title":"CSR-EHCS(EHS), TM: Virtual Execution Environments for Wireless Sensor Networks","awardID":"0834555","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["438734","438733","543576"],"PO":["561889"]},"143069":{"abstract":"EMT jNANO: Computing with Protein-Based Associative Memory Processors<br\/><br\/>Abstract: The promise of new architectures and more cost-effective miniaturization has prompted interest in molecular computing. One such architecture is based on the development of associative memory processors using appropriate proteins. Prototypes of this architecture are currently in use for applications such as fingerprinting and image matching. The.commercial success and future development of protein-based associative memory processors (PBAMPs) hinges, to a large part, on expanding the realm of successful applications. No systematic study of the capabilities and limitations of these processors has been conducted especially in relation to the digital computers. In this project the investigators take up this study.<br\/>Present day computational algorithms are limited in part by the lack of optimal memory architectures. While serial memories have been highly optimized, memories which provide associative access are both expensive and provide only modest data storage capacity. Protein\u00acbased memory architectures provide for both large scale (three-dimensional) storage as well as associative processing. The bacteriorhodopsin protein with its unique light-activated photocycle, nanoscale size, and natural resistance to harsh environmental conditions, provides for protein-based memories that have a comparative advantage over magnetic and optical data storage devices. Protein based memory matrices are reusable and eco-friendly. In addition, the bacteriorhodopsin protein exhibits increased thermal, chemical and photochromic stability. Protein storage devices are capable of storing large amounts of data (1011_1013 bits) in a small volume of the memory medium. Bacteriorhodopsin-based storage media are portable, radiation-hardened, waterproof, and EMP-resistant. In this project advances will be made in optimizing protein based memories as well.","title":"EMT\/NANO: Computing with Protein Based Associative Memory Processors","awardID":"0829916","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["563608","485106",380924,"559638"],"PO":["565223"]},"148767":{"abstract":"Image data is of immense practical importance in medical informatics, and a subject of strong interest to researchers in industry and academia. While digital image databases are now prevalent in clinical and educational settings, and traditional means for interacting with and querying such collections can provide some level of useful functionality, there are few examples of systems that attempt to bridge the ?semantic gap.? The work proposed in this grant is a multi-institutional collaboration combining research in medical image processing, machine learning and pattern recognition, knowledge representation and querying, and evaluation by domain experts in the field, is intended to advance the state-of-the-art in this direction. The archive of 60,000 cervigram images assembled by the National Library of Medicine and National Cancer Institute is an ideal collection for this purpose. The NLM cervigram archive forms a narrow image domain that has a limited and predictable variability. In such cases, explicit representation of domain knowledge alleviates the semantic gap between the low-level sensory recordings of a scene (raw image data), and objects and processes implied from images (semantic interpretation). This research will follow an information hierarchy that proceeds from raw image data to low-level image features, recognition of objects and tissue types, knowledge-based reasoning about disease processes, and, finally, tools and visualizations to support diagnosis decisions by clinical and NLM\/NCI collaborators. The research team will employ an underlying paradigm known as Computer-Assisted Visual Interactive Recognition, or CAVIAR, which considers the domain expert an integral part of the equation and attempts to optimize the performance of the complete human-machine system. <br\/><br\/>Intellectual Merit <br\/><br\/>Image content understanding is still considered a vexing open problem at the same time databases are growing rapidly in size and complexity. It is anticipated that this work will have a positive impact in areas relating to medical image analysis, including information extraction, organization, representation, and querying, as well as in training. <br\/><br\/>Broader Impact <br\/><br\/>Through the focus on the NLM\/NCI cervigram archive, this research may help advance the role of cervicography as a more cost-effective procedure than pap smears and colposcopy in screening for cervical cancer. Results from this targeted-domain project could also illuminate gaps and help establish new priorities for research in broader domains such as multimedia content structuring, understanding, indexing, and retrieval.","title":"III-CXT-Small: Collaborative Research: Structuring, Reasoning, and Querying in a Very Large Medical Image Database","awardID":"0854606","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["517943"],"PO":["565136"]},"145258":{"abstract":"One of the Federal government's missions is to act as an unbiased mediator in the resolution of disputes. One avenue for achieving resolution is the court system. There are other avenues, though, such as the National Mediation Board (NMB) which is required to mediate labor disputes in the railway industry. <br\/><br\/>Building on a three-year collaborative technical project between NSF and NMB, this workshop will develop a broad technical and socio\/legal research agenda for on-line dispute resolution and labor negotiation\/mediation in Federal transporation agencies. In addition to a report to be widely distributed, the event will allow a forum for a broad selection of interested parties to begin forming a community of interest. Attendees will include lawyers, transportation experts, and computer scientists from academia, industry and Federal agencies that have mediation responsibilities.","title":"Workshop Proposal: A Research Agenda for Computing Technology and Dispute Resolution Focusing on the Transportation Sector","awardID":"0840248","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"I262","name":"Office of the National Coordin"}}],"PIcoPI":["440692","536735","536737",387450],"PO":["427499"]},"144048":{"abstract":"Current trends in computing architectures and programming support the rationales for the proposed project. One trend is the increasing complexity of software both in terms of the number of lines of source code, and number of layers (hypervisor, guest OS, libraries, virtual machines, applications, etc.). This increase in complexity could make software even more vulnerable to bugs and security attacks in the future. Another trend is the increasing support for thread-level parallelism in mainstream microprocessors through chip multi-processor (CMP) architectures. Four and eight processor cores per chip have either been realized or are in the future roadmap of major chipmakers. Currently, mostly applications with natural thread-level parallelism (server and scientific computing workloads) can benefit from them. Increasing reliability and security requirements support parallelism opportunities for common applications, and such parallelism may effectively be exploited through helper computing. <br\/><br\/>This project will address these issues by engaging in the following: exploring helper-computing's potentials for speeding up software reliability and security meta-functions with scenario-guided studies; a systematic, cross-layered study of efficient Meta-function Design, Extraction, Optimization, and Dynamic Adaptation to achieve low overhead execution in the main application; exploration of a prototype system which includes compiler, libraries, run-time system, OS support, and possibly hardware support. The exploration for such support will be holistic, i.e. by considering implementation layers that produce the best trade-offs in the feasibility of the solution in current systems, generality, and performance requirements. This prototype system will be made available to the public.<br\/><br\/>This team also proposes to create an advanced course that focuses on helper computing, and integrate its basic elements into existing hardware and software courses. Such courses would integrate programming language, compiler, OS, runtime system, and hardware in a synergistic way, helping to train future workforce with unique and integrated expertise in these areas.","title":"CSR-PSCE, SM: Exploring Helper Computing Parallelism in Multicore Architectures","awardID":"0834664","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["485790","550815"],"PO":["535244"]},"144059":{"abstract":"Ordinary PCs are widely employed for large scale scientific computing today. Released in 2004, BOINC middleware is managing over 500,000 volunteered PC nodes and providing computation power to around 30 scientific research projects including Rosetta@home, Climateprediction.net, and IBM World Community Grid. The main attraction of such \"virtual clusters\" is that computing is \"free\" as minimal additional hardware and personnel resources are needed. The potential for exploiting such idle cycles is immense since well under 1% of the world's 1 billion PCs are currently participating.<br\/><br\/>Currently idle PCs are exploited for sequential and independent parallel tasks, but not communicating parallel tasks, a common HPC paradigm. The central goal of this project is to achieve robust execution of communicating parallel applications on networked ordinary PCs. The challenge is that ordinary desktops are \"volatile\", i.e., their availability changes suddenly and frequently based on desktop owner's actions. Checkpointing of parallel applications, the state of the art in fault tolerant scientific computing, is not sufficient for high failure rate environments.<br\/><br\/>This project is developing the VolPEx framework (Parallel Execution on Volatile nodes) that employs managed redundancy as the core mechanism to achieve seamless forward application progress in the presence of routine failures. The canonical execution model consists of two or more concurrent replicas of each process with failed process replicas regenerated on-demand from healthy ones. The following communication APIs are provided for application development.<br\/><br\/>1. Virtual Dataspace: An abstract API for asynchronous anonymous Put\/Get communication among tasks. The BOINC programming model of independent tasks is being extended with the dataspace API to allow inter-task communication <br\/>2. Volpex MPI: A subset implementation of MPI with a communication layer customized for execution on volatile nodes. <br\/>The validation of this research will include execution of selected parallel applications on 100s of nodes across campus LANs and 1000s of nodes across the globe under Volpex\/BOINC.<br\/><br\/>The ability to transform ordinary PCs into a virtual cluster to run parallel codes will have wide ranging impact. Virtually all scientists will get access to HPC while the need for clusters and the cost of purchasing, operating, and maintaining clusters will diminish.","title":"CSR-PSCE, SM: Collaborative Research: VOLPEX: A Framework for Parallel Execution on Volatile Nodes","awardID":"0834750","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["486325","558507","434716"],"PO":["565255"]},"139604":{"abstract":"Recent advances in genome sequencing technologies have enabled the<br\/>sequencing of bacteria directly from the environment, providing a<br\/>broader outlook on the diversity of bacteria than ever before<br\/>possible. Recent studies of environmental samples have revealed<br\/>complex communities containing many previously unknown species, and<br\/>uncovered a large amount of genetic variation and diversity even among<br\/>closely related strains. Characterizing this genomic variation is<br\/>critical in studies of microbial ecology and evolution, yet currently<br\/>available computational tools, originally developed for the study of<br\/>single organisms, are ill-suited for this task.<br\/><br\/>This proposal aims to develop the theoretical and computational<br\/>infrastructure for the study of genomic variation within mixtures of<br\/>organisms. The proposed research relies on both theoretical and<br\/>empirical analyses of the structure of genome assembly graphs in order<br\/>to characterize graph signatures that are correlated with intra- and<br\/>inter- species polymorphisms. A particular focus is placed on<br\/>understanding and using the information provided by next generation<br\/>sequencing technologies as well as other high-throughput experimental<br\/>techniques. The proposed work provides critical analysis tools<br\/>to help biologists explore the genetic variation within the<br\/>environment.<br\/><br\/>Additional information about this project is available at<br\/>http:\/\/www.cbcb.umd.edu\/research\/Genomic_Variation.shtml.","title":"III-CXT-Small: Graphs to Diversity: extracting genomic variation from sequence graphs","awardID":"0812111","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"I162","name":"National Security Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"J226","name":"National Security Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"K577","name":"National Security Agency"}}],"PIcoPI":["486199","550675"],"PO":["565136"]},"139615":{"abstract":"This project pursues three concurrent activities: 1) analysis of common search spaces and previously proposed search strategies to broadly characterize the relevant regularities that can be exploited during search, 2) development of search algorithms that can efficiently learn, update, and exploit models of those parameters on-line during problem-solving, and 3) comprehensive evaluation of the new algorithms across many common benchmarks in terms of actual CPU time, taking into account their increased overhead. Algorithms will be developed for the two most common types of search problems: 1) shortest-path problems, such as task planning or robot navigation, where the depth of the search tree is not usefully bounded, and 2) bounded-depth problems, such as constraint satisfaction or combinatorial optimization problems, where the number of decision variables is known.<br\/><br\/>The rational search approach yields a form of hybrid metareasoning, in which the problem-solver reasons statistically about which combinatorial reasoning to do next. This combination promises significant gains in robustness and performance over the current paradigm in which search algorithms use the numerical information available to them only in simple ways, such as allowing it to directly dictate expansion order or using it only to prune. Rational search will provide a sound basis for moving beyond search strategies motivated by intuition to algorithms that adapt their behavior in unanticipated ways to suit precisely the problem at hand. By focusing<br\/>attention on optimal use of information, this project will help the field of heuristic search address the question of problem formulation: what problem-specific heuristic information is most useful to guide search, and how can it best be conveyed to the search algorithm? It will also strengthen the nascent links between machine learning and heuristic search, particularly around the issues of exploration vs exploitation and the value of information. Because they form the engines of most AI systems, improvements in heuristic search algorithms yield social benefits wherever such systems are used. Increasing the robustness and generality of search methods also makes industrial adoption of AI technology easier and faster,<br\/>widening its applicability.","title":"RI-Small: Combinatorial Search Algorithms as Rational Agents","awardID":"0812141","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["502353"],"PO":["491702"]},"139505":{"abstract":"Recently, the microprocessor industry has moved toward multicore microprocessor designs as a means of utilizing the increasing transistor counts in the face of physical and micro-architectural limitations. <br\/>Unfortunately, providing multiple cores does not directly translate into performance for most codes. To make use of multicore, many new languages have been proposed to ease the burden of writing parallel programs, yet the programming effort involved in creating correct and efficient parallel programs is still far more substantial than writing the equivalent single-threaded version. A more attractive approach is to rely on tools, both compilers and runtime optimizers, to automatically extract threads from sequential applications. Unfortunately, despite decades of research on automatic parallelization, most techniques have only been effective in the scientific and data-parallel domains. With recently gained insight, the investigators showed that the limits of prior thread-extraction approaches are not fundamental. By applying known and new compilation techniques in a systematic manner, the investigators found that SPEC CINT2000, among the most sequential of codes, has abundant scalable parallelism.<br\/><br\/>In this project, the team of investigators is taking the initial steps toward developing the techniques necessary to build an automatic thread extraction framework. These techniques include developing static transformations that extract parallelism and quantifying the opportunities for dynamic optimization. The system will ultimately consist of a series of static transformations and compiler-inserted hints combined with a run-time optimization component. This framework will address the multicore challenge by reliably extracting parallelism from a wide range of applications without burdening the programmer with what should remain to be low-level implementation details.","title":"CPA-CPL-T: Collaborative Research: Revisiting the Sequential Programming Model for Multicore Systems","awardID":"0811580","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["511660"],"PO":["565272"]},"139516":{"abstract":"Proposal ID: 0811632, <br\/>PI Name: Mohammad Tehranipoor, <br\/>PI Institution: University of Connecticut, <br\/>Title: CPA-DA: Dealing with Voltage Variations and Supply Noise During Performance Verification in Nanometer Technology Designs<br\/><br\/>ABSTRACT: <br\/>Power supply noise (PSN) will be a serious issue in nanometer technology designs especially during post-silicon performance verification and speed binning. Such effect must be efficiently taken into consideration, as it poses design, test and reliability challenges for the chip manufacturers\/foundries. This situation has grown more complicated with reducing supply voltage and the limitation of further reduction of threshold voltage leading to reduced noise margin and increased circuit sensitivity. <br\/><br\/>This project addresses the necessity of new comprehensive and efficient PSN analysis, power model, pattern generation, and design for testability (DFT) methods to alleviate the above issues during performance verification and speed binning in new technology generations. This project will also address the practical issues of power analysis and proposes new power model, statistical PSN analysis, and efficient measurement procedure for measuring performance degradation due to excessive voltage, process and environmental variations. The Intellectual Merit of the research is that it will significantly increase the quality and reliability of chips and reduce the manufacturing costs. The broader impact is (i) the contribution to debugging and diagnostic groups of EDA and semiconductor industry to assess and verify the performance of design prototypes, (ii) the education of undergraduate and graduate students, and (iii) the development of new courses, and (iv) the dissemination of data and methodologies to researchers in academia and industry.","title":"CPA-DA: Dealing with Voltage Variations and Supply Noise During Performance Verification in Nanometer Tehcnology Designs","awardID":"0811632","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["560771"],"PO":["562984"]},"139637":{"abstract":"This project will contribute to the development of new forms of mobile human-computer interaction that can be called \"microinteractions.\" These are short interactions that, in some cases, are performed almost without conscious thought, comparable to checking the time on a wristwatch by a mere glance. Such microinteractions are already major uses of mobile computing and communication technologies: being reminded of an appointment, displaying the caller ID of an incoming call, noting a phone number or URL, making an appointment, or receiving a short e-mail. Yet few devices support microinteractions well, leading to a lack of use of the functionality, and there exist few design guidelines for microinteraction interfaces. This research will increase the fundamental knowledge and the design principles required to achieve substantial advances in this emerging field.<br\/><br\/>The research will investigate barriers to microinteractions, create design guidelines for microinteraction interfaces, create a MAGIC (Mobile Action and Gesture Interface Control) toolkit that enables gesture-based microinteraction techniques, and produce and study prototypes that instantiate the research results. The project will concentrate on the wristwatch as a socially acceptable, fast-to-access interaction platform. Previous work suggests that access time - the time to retrieve a device and navigate through its interface - is a major contributor to \"balking,\" or deciding not to use an interface. The research will quantify the effects of access time on balking, create design guidelines on appropriate access times for typical microinteraction tasks, and give concrete suggestions for minimizing balking by quantifying access times for the wristwatch as compared to other typical device mounting locations such as the pocket or belt. Specifically, four types of microinteractions will be examined: glances, computer-initiated events, user-initiated events, and a new style of interaction, \"dual purpose speech,\" where key phrases in the user's conversation simultaneously trigger actions on the mobile device. <br\/><br\/>The project will create wristwatch-based gesture, touchscreen, and speech system prototypes that support each of the microinteraction types. This work will encourage new directions in mobile interface research. The extensive involvement of students in the project will allow them to develop a wide range of hardware and software prototyping skills.","title":"HCC-Small: Wristwatch Interfaces for Microinteractions","awardID":"0812281","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["517810"],"PO":["564456"]},"139648":{"abstract":"This project will develop new approaches for recovering the three-dimensional (3D) shape and pose of the human body in images and video sequences. The methods will use a detailed 3D body model learned from laser range scans of over 2000 people. The approach will model the shape variation across people as well as the non-rigid shape variation due to changes in pose. The project will develop and test methods for robustly recovering the body shape in surveillance video sequences, in scenes with strong lighting, from collections of snapshots and in unconstrained television\/film sequences. The recovered body model will be used to produce a variety of biometric measurements.<br\/><br\/>The majority of images and video sequences are of humans and recognizing people and their actions is a core problem in computer vision. The problem is challenging however because the human body is a complex, non-rigid, and articulated structure that can vary dramatically in pose, shape and appearance. Current methods focus on estimating human pose and typically ignore the problem of human shape estimation. This project will treat these problems together resulting in more robust solutions which will have a wide ranging impact in multiple disciplines. Human pose estimation is currently used in areas such as gait analysis, special effects, game development, human factors, and sports training to name a few. Robust video-based systems like the one developed here will extend the range of applications to home entertainment, elder care, autonomous vehicles and animal movement analysis. By extending previous methods to also estimate the three-dimensional shape of the human body in images and video sequences this project will enable additional applications in video forensics, surveillance, preventative medicine and special effects. More generally, methods like those developed here, that robustly recover the shape and pose of people in complex images and video streams, will be applicable to a wider range of problems in object recognition and tracking.<br\/><br\/>Project website: http:\/\/www.cs.brown.edu\/~black\/SCAPE.html","title":"RI-Small: Human Shape and Pose from Images","awardID":"0812364","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["498150"],"PO":["564316"]},"139659":{"abstract":"High-throughput experimental methods have revolutionized scientific <br\/>inquiry. In contrast to the hypothesis-driven scientific method, <br\/>data-driven science seeks to discover and explore hypotheses supported <br\/>by the huge volume of data generated in high-throughput experiments. <br\/>Such datasets are large and high-dimensional: they consist of a <br\/>multitude of samples and many measured attributes for each sample. A <br\/>typical hypothesis corresponds to a subspace of this dataset: a subset <br\/>of samples that share similar values on a subset of attributes.<br\/><br\/>The goal of this project is to develop a series of new data mining <br\/>methods that can effectively discover these subspaces, the embedded <br\/>patterns among the values, and the relationships between patterns. The <br\/>underlying problems are highly combinatorial and efficient algorithms <br\/>are required to enable users to mine and explore subspace patterns in <br\/>large and complex datasets. The proposed methods combine the advantages <br\/>of efficient matrix decomposition, effective sampling techniques, and <br\/>advanced graph algorithms. Solutions to these research problems will be <br\/>integrated into an interactive and visual interface to explore subspace <br\/>patterns mined from experimental data.<br\/><br\/>While the proposed methods are applicable across a wide range of <br\/>domains, the focus of project is the analysis of gene regulatory <br\/>networks and the analysis of protein structure, in collaboration <br\/>respectively with geneticists and pharmacologists.<br\/><br\/>Current progress and results are accessible and continuously updated at<br\/>http:\/\/compgen.unc.edu\/deps\/","title":"III-Core: Discovering and Exploring Patterns in Subspaces","awardID":"0812464","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["426282",371221,"234536"],"PO":["560586"]},"139318":{"abstract":"Abstract (Jovan Popovic, MIT)<br\/><br\/>Interactive computer-graphics simulations offer a proven method for learning complex relationships and procedures. Airplane simulators, for example, tutor pilots through sophisticated tasks until all procedures are performed safely and efficiently. Of particular importance to these applications are simulations that are consistent with the real world. In medicine, for example, physicians can master &#64257;ne motor skills within physiologically consistent simulations without endangering the lives of patients. However, the simulations in use today can only emulate reactive physical systems. Broader application of computer simulation in medicine, engineering, art, and entertainment requires simulation of active systems such as robotic devices, humans, and animals that act as well as react within physically simulated environments. A study of simulations for vehicle and workspace design, for example, has revealed a pervasive need for digital human models that can accomplish tasks within physically simulated environments. <br\/><br\/>Computer graphics today often simulates humans as lifeless mechanical structures (rag dolls) that do no more than crumple to the ground. Or it relies on motion-capture techniques (motion graphs) that generate physically inconsistent motions whenever simulations depart from a few motion-captured scenarios. In contrast, physically based simulation generates consistent motions, but the motions are often unnatural and difficult to create. This study applies the mathematics of modern control theory to establish a more rigorous foundation for control of such motions in interactive computer simulations. It simulates natural motions by stabilizing high-quality motion-capture data, transforming individual motion into a more general action that can be applied in many different circumstances. A single action may describe short tasks such as stepping, turning, walking, and jumping. More complex behaviors are assembled through composition, by concatenating one action after another.","title":"CPA-G&V: Physically Valid Simulation of Active Human Motion","awardID":"0810888","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["384455",370293],"PO":["532791"]},"139439":{"abstract":"PI: Zhou, Hai<br\/>Proposal No: 0811270<br\/>Title: CPA-DA: Efficient Sequential Synthesis and Optimization for High-Performance Circuits<br\/>Institution: Northwestern University<br\/><br\/>ABSTRACT<br\/>Processing speed and power consumption have become the most important design criteria in modern high-performance VLSI circuits. They are not only dependent on the combinational part but also on the memory elements (latches and flip-flops) and the clocking mechanism. In the project, we develop an efficient sequential synthesis and optimization framework based on incremental retiming and other network flow techniques. The framework is based on structural operations such as retiming, re-synthesis, and sweep, which are shown to cover almost all possible sequential transformations. A sequential optimization suite based on efficient algorithms for incremental retiming, constrained clock skew scheduling, and sequential floor-planning is developed and will be made available to the public.<br\/><br\/>The outcomes of combined research and education activities improve the performance and design productivity of nanometer VLSI circuits of all types, from high-speed microprocessors to the omnipresent systems-on-a-chips (SOCs) found in PDAs, cell phones, and more. The improvement on performance and design productivity translates to lower cost and better functionality to customers, thus giving all consumers, particularly socio-economically disadvantaged, access to affordable information technology. Furthermore, the high performance computers, enhanced by optimized performance and improved design productivity, will aid research in other important scientific areas such as genetics, bioinformatics, and medicine.","title":"CPA-DA: Efficient Sequential Synthesis and Optimization for High-Performance Circuits","awardID":"0811270","effectiveDate":"2008-09-01","expirationDate":"2013-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["485312"],"PO":["562984"]},"141442":{"abstract":"Proposal #: CNS 08-21839<br\/>PI(s): Han, Jianchao<br\/> Beheshti, Mohsen; Nahapetian, Ani<br\/>Institution: California State University ? Dominguez Hills<br\/> Carson, CA 90747-0001<br\/>Title: MRI\/Acq.: Acq. of a High Performance Cluster Grid for Research and Education on Computational Science<br\/>Project Proposed:<br\/>This project, acquiring cluster-grid instrumentation to fulfill the need for high-performance, contributes to computer science, biology, chemistry, bioinformatics, homeland security, sociology, psychology, and environmental sciences research and education. The high-performance cluster grid is expected to enable a quantum-leap in the computing capabilities accessible to the investigators. Diverse project directions include: <br\/>- Extending statistical techniques to situations with interval uncertainties<br\/>- Discovering gene interactions and gene-disease associations in online medical literature (using clustering, classification, and association mining technology), <br\/>- Automating computer network intrusion detection and prevention (integrating firewall and applying data mining technologies in computer network traffic data collected by SNORT), <br\/>- Developing an efficient and interactive online examination and assessment system(using knowledge-based expert system technology), and<br\/>- Designing and implementing healthcare and patient information management system.<br\/><br\/>Broader Impacts:<br\/> This project forms a foundation for new research capabilities at this minority-serving institution, impacting researchers and educators throughout the campus and beyond. It promotes the formation of research teams that may overcome traditional cultural and communication barriers enabling interdisciplinary approaches to technological and scientific challenges. The instrumentation provides students with the skills to cope with a rapidly changing workplace. Students will learn to communicate and interact with researchers of different disciplines, a valuable skill at this time.","title":"Acquisition of a High-Performance Cluster-Grid for Research and Education in Computational Sciences","awardID":"0821839","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":["479700",376392,376393],"PO":["557609"]},"143510":{"abstract":"NeTS-NECO: Robust, Delay-Tolerant Sketches for Aggregating<br\/>Sensor Data Streams<br\/><br\/>Emerging data-intensive mobile sensor networking applications promise<br\/>a close observation of the world around us at a relatively low data<br\/>acquisition cost. In these applications, it is a challenge to<br\/>aggregate observation streams from distributed sources, due to the<br\/>bandwidth and delay constraints of the network, and the energy costs<br\/>of data transmission. This project develops techniques for processing<br\/>voluminous sensor data streams within the resource constraints imposed<br\/>by a distributed infrastructure-less network.<br\/><br\/>Techniques are designed to summarize the observation streams into<br\/>small space \"sketches\". By disseminating sketches rather than the raw<br\/>data, it is possible to trade-off reduced communication cost for<br\/>approximate answers to user queries. The project investigates the<br\/>technical challenge of making the sketches robust and suitable for<br\/>transmission over unreliable links. The sketches are delay-tolerant,<br\/>i.e. they can tolerate long and variable network latencies. Various<br\/>classes of queries are supported including: basic aggregates of<br\/>numerical and categorical data, multi-dimensional temporal and<br\/>geographic aggregates, and time-decayed aggregates. The design of<br\/>sketches is informed and inspired by the recent rapid progress made in<br\/>the area of massive data stream processing. The project will result in<br\/>new algorithms and data structures for sketching sensor data with<br\/>provable guarantees on the quality of answers. The research is<br\/>expected to benefit data-intensive sensor networking applications that<br\/>are critical to our society, such as earthquake, pollution, and<br\/>traffic monitoring.","title":"NECO: Robust, Delay-Tolerant Sketches for Aggregating Sensor Data Streams","awardID":"0831903","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["511486"],"PO":["564993"]},"143631":{"abstract":"In the area of system reliability analysis, dynamic and dependent behavior such as functional dependence, cascading failures, and state dependence has been recognized as a significant contribution to problems in overall system reliability. However, with the incorporation of the dynamic and dependent behavior, resulting dynamic system reliability models cannot be efficiently and accurately solved by existing state space based methods such as Markov methods. This is due to two key limitations: the state-space explosion problem, and the inability to handle arbitrary failure distributions. This project explores efficient combinatorial models, formal methods, and transform methods to address these two limitations, providing a viable solution to the accurate reliability analysis of practical large-scale computer-based systems with complex dynamic behavior. The research has four major components: 1) reliability modeling and evaluation of dynamic and dependent behavior, 2) verification and performance analysis of the proposed reliability models and evaluation methods, 3) case studies\/industrial applications, and 4) GUI-based reliability software tool development. The new reliability models and evaluation methods developed through this project are fundamental contributions to the body of knowledge on the computer system reliability. This project has broader impact through its contributions to computer system reliability research, graduate and undergraduate education, and the reliable design of complex and dynamic industrial systems. A project website provides publicly available access to the publications generated from the project, software tools and tools-related materials.","title":"CSR-EHCS(EHS), SM: Efficient Dynamic Combinatorial Models (EDCM) for Reliability Analysis of Complex Dynamic Systems","awardID":"0832594","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["484154"],"PO":["561889"]},"143400":{"abstract":"Randomization has proved to be a vital part in building essentially any kind of secure cryptographic system: secret keys should be randomly generated and most cryptographic primitives, such as encryption, must be probabilistic. As a common abstraction, it is typically assumed that ideal randomness is available to all the participants of the system. In many situations, this assumption is highly unrealistic, and cryptographic systems have to be built based on *imperfect* sources of randomness.<br\/><br\/>Intellectual Merits of the Project: The research will address the question as to what extent cryptographic protocols can be adapted to work with imperfect sources of randomness. In particular, it will concentrate on the feasibility of cryptography imperfect sources of randomness and exposure-resilient cryptography. For the first topic, the PI will address the question if nearly ideal randomness is *necessary* for building various cryptographic primitives, such as encryption and authentication, or, perhaps, one can base such applications on more realistic imperfect sources. For the second topic, in many settings the adversary can gain partial knowledge of some secret information (e.g., a cryptographic key). The exact nature of this knowledge is often unknown, except that it is ``bounded''. Therefore, even if the secret was originally random, it looks like an imperfect random source to the adversary, given his partial knowledge about the secret. The PI will design novel protocols which are resilient to such leakage of partial information in the recently proposed Bounded Retrieval Model.<br\/><br\/>Broader Impacts of the Project: The availability of ideal randomness is a common assumption used not only in cryptography, but in many other areas of computer science, and engineering in general. Questioning the validity of this assumption and designing techniques to base a given system on weaker, more *realistic* assumptions will surely be useful in many other disciplines. Additionally, given the increasing spread of viruses, Internet worms and Trojan horses, the use of exposure-resilient protocols will result in more stable and secure application environments. Furthermore, because the research in this project will use tools from information theory and pseudorandomness to solve cryptographic questions, it will help find new connections among these fields.<br\/><br\/>The PI also regularly teaches courses on cryptography and network security, and will be able to incorporate the new results into such courses. In addition, the project has a significant graduate student training component.","title":"CT-ISG: On Imperfect Randomness and Exposure-Resilient Cryptography","awardID":"0831299","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["550227"],"PO":["565264"]},"144852":{"abstract":"Theorists in the social and historical studies of science and technology have long explored how technologies are constructed alongside assumptions about the organization of scientific labor and ideologies of the human and the machine. Space science projects are perhaps the most iconic examples of large-scale scientific endeavor in which to test these sociotechnical assumptions, involving collaborations across disciplines, across organizations, across nations, and even across interplanetary distance. Building on existing engagements, this project will explore these interconnections in detail through ethnography, interviews, and archival work on NASA's Mars Exploration Rover mission and comparative case studies. <br\/><br\/>In earlier studies of the NASA Mars Exploration Rover mission, team members consistently characterized their mission as uniquely successful and harmonious as compared to other NASA-funded unmanned missions which were seen as hierarchical and fragmented. Whereas earlier missions allegedly suffered antagonistic relationships between members of the spacecrafts' science and operations teams, the Rover mission was purposefully organized so as to espouse different principles. In this study, the investigators ask: Is the Rover project as unique as members believe? What factors distinguish this project, as a sociotechnical system, from others? What characterizes the role of cyberinfrastructure as an element of their work? Answers to these questions will offer a detailed, grounded case study of high-visibility scientific work conducted under extreme conditions of distribution and virtuality, in a complex organizational setting. By engaging in comparative work, the project will draw out the particular importance of, and relationship between, organizational culture and structure in these cases of \"interplanetary sociotechnical systems.\"","title":"VOSS: The Social Life of Spacecraft: The Organization of Interplanetary Sociotechnical Systems","awardID":"0838499","effectiveDate":"2008-09-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"5376","name":"INNOVATION & ORG SCIENCES(IOS)"}}],"PIcoPI":["464219"],"PO":["446196"]},"143411":{"abstract":"ABSTRACT:<br\/><br\/>Secure applications require trustworthy hardware for successful deployment. A trustworthy hardware device (e.g., a smart card) should maintain its security properties even against efforts at probing and reverse engineering; moreover, sensitive data stored on a trustworthy hardware device should be protected at all times. Side-channels attacks are used to learn the secrets stored by a device through monitoring the side effects of its computation. The well known power side-channel attack uses the effect that a cryptographic key has on the power waveform as the cipher runs. Another side-channel attack, the focus of this project, is the Differential Fault Attack (DFA). DFAs alter the computations performed by a trustworthy device and use the faulty results to uncover secrets. An approach to defeating DFAs is to continuously check dynamic assertions, made part of its cryptographic algorithms (including, e.g., symmetric block and stream ciphers, asymmetric ciphers, and message authentication), against DFA. The project advances secure hardware design methods and methodologies, develops course material on side-channel attacks and countermeasures, and promotes technology transfer through partnerships with industry. The project has a strong education component with opportunities for international research experience.","title":"CT-ISG: Collaborative Research: Fault Tolerance in Crypto Hardware via Dynamic Assertion Checking","awardID":"0831349","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["550604"],"PO":["497499"]},"143532":{"abstract":"CNS ? 0832000<br\/>Li, Jiang<br\/>Howard University<br\/>XPLR: Scheduling and Routing in Pigeon Networks<br\/><br\/><br\/>The project applies the concept of pigeon post with extension to computer networking. Such networks, called pigeon networks, are formed by autonomous mobile wireless devices emulating pigeons. Each of these devices, called messengers, is dedicated to one or more other devices (called home devices) that are the ultimate senders and\/or receivers of data. Like pigeons, messengers move back and forth between home devices, carrying and forwarding data, but they can also do much more when they communicate between themselves and cache data intelligently. The research of the project will focus on the algorithm design for scheduling the movement of messengers, their data pickup (from home devices) and the interactions between the two aspects. The concepts and results will be brought into related courses in the CS Department at Howard University to educate students (mostly minority) about the forefront of computer networking.<br\/><br\/>Intellectual Merit. Pigeon networks target communications under unconventional conditions such as disaster response and surveillance in remote wide areas where traditional computer networks cannot work. If successful, this model may lead to an innovative way of building computer network architectures that can provide people with communications even in the most difficult environments.<br\/><br\/>Since the network model carries critical distinctive properties that have rarely been studied, the project will study a number of interesting and yet challenging problems. Some of the problems have strong connections with industrial management and transportation. The project will hence benefit from previous research in that area. More importantly, it will contribute to the latter, which has broad social and industrial applications. The PI has been conducting research in computer networking and has been teaching the majority of the computer networking courses in his department. In particular, his research team has already begun the study and produced preliminary results. <br\/><br\/>Broader Impacts. The project will help foster computer networking research at Howard University as a historically black college and university (HBCU). At present, the Department of Systems and Computer Science (that the PI is affiliated with) only offers a Master degree, and the research in the department needs to be strengthened. Hosting active funded networking research programs in the department will help attract minority students into networking, and prepare them for more advanced graduate degree programs. The activities are also hoped to raise Howard's reputation in computer networking research. In the Internet era, that will not only attract students into this research area, but will eventually also help attract students into the computer science discipline overall, which has been experiencing significant enrollment downturn in recent years. All the results produced by the project will be submitted to academic conferences and journals, and be published online for public access. Since the results have natural applications in some other disciplines such as industrial management and transportation, articles will also be submitted to conferences and journals in related disciplines to further broaden the impacts of the project.","title":"XPLR: Scheduling and Routing in Pigeon Networks","awardID":"0832000","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["556651"],"PO":["565090"]},"143774":{"abstract":"The High End Computing (HEC) field is at a major crossroads due to the advent of many-core technology --- integration of tens and hundred processors (cores) on a single chip, heading to 1000 cores per chip in Exascale systems in the 2015-2020 timeframe. Unlike previous generations of hardware evolution, this shift in the hardware road-map will have a profound impact on future HEC software. First, the programmer will be faced with the scalability challenge of expressing and managing parallelism at the scale of millions to one billion threads in a single system. Second, the programmer will be faced with the locality challenge of optimizing data movement in a highly non-uniform system structure with order-of-magnitude gaps in bandwidth and latency between adjacent levels of the memory hierarchy. <br\/><br\/>The specific focus of this project is on developing programming models, compilers, and runtimes to address the above challenges of future HEC systems, guided by a specific many-core architecture to ensure practicality. The research will deliver results in the following areas: 1) new parallel programming constructs for many-core architectures such as asynchronous activities, places, and phasers that build on past experiences with the X10 language, but will be manifested in C\/C++ instead of Java in this research; 2) new parallel intermediate representations (PIR?s) and compiler optimizations for parallelism and locality; 3) a new thread virtual machine with two levels of parallelism, Synchronous Coarse-Grain Threads (SCTs) and Asynchronous Fine-Grain Threads (AFTs); and 4) evaluation of the programming model, compiler, and runtime on a Cyclops C64 many-core system that directly exemplifies the parallelism and locality challenges facing future HEC systems.","title":"Collaborative Research: Programming Models, Compiles, and Runtimes for High-end Computing on Manycore Processors","awardID":"0833122","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["475422"],"PO":["565272"]},"143422":{"abstract":"This project explores an architecture, mechanisms, and interfaces for helping users manage access control in the digital home.<br\/><br\/>The home is a challenging, yet critical, target for usable security.<br\/>It requires abstractions that are intuitive for laypeople, interfaces that allow users to manipulate those abstractions, and access-control and storage infrastructure that can support the abstractions. Without a holistic, usable approach to access-control management, adoption of new technology in the home will be slowed and there will be no effective data security once the transition inevitably occurs. Based on their early experiences with home storage, the PIs seek to adapt and integrate:<br\/><br\/>(1) the semantic query as an abstraction for describing a set of files to which a specific policy applies;<br\/><br\/>(2) the Expandable Grid as a visual interaction technique for creating, editing, and viewing security policies;<br\/><br\/>(3) logic-based access control as a rigorous foundation for specifying and implementing policies.<br\/><br\/>User studies, surveys, and test deployments are a core component of the project; they are used to discover needs of users in the digital home and users' ability to effectively apply approaches developed.<br\/><br\/>The project has several forms of impact. First, it develops tools and techniques that can significantly simplify the use of access control in the digital home. Second, it increases understanding of user behavior and access-control needs in the emerging home storage environment. Third, it enhances education at CMU and elsewhere, as new insights are embedded into storage systems, distributed systems, and computer security classes taught by the PIs.","title":"CT-M: Usable Security for Digital Home Storage","awardID":"0831407","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["563508","553878","486331"],"PO":["497499"]},"143543":{"abstract":"Throughout the life cycle of a civil structure, its serviceability must be assessed in a timely manner. Structural Health Monitoring (SHM) strategies measure structural response and aim to detect, locate, and assess damage produced by severe loading events and environmental deterioration. At the University of Houston, prototype piezoceramics-based smart aggregates (SA) have been designed for concrete structures. Inherited from the property of a PZT, the SA has the unique feature that it can function both as a sensor and an actuator. As an actuator, an SA excites elastic stress waves to propagate through the concrete structure, which can be detected by sensors. Distributed active sensing exhibit different sensing and energy characteristics compared to passive sensing and stand-alone active sensing systems. The goal of this project is the development of a scalable framework and efficient algorithms for cooperative active sensing in wireless SHM systems. The specific objectives are to i) investigate coverage models for active sensing under realistic domain-specific constraints; ii) develop a holistic energy-efficient solution to cooperative active sensing and processing iii) develop, implement and evaluate distributed active sensing solutions for SHMs using PZTs and off-the-shelf wireless components. This work will generate broader impact by closing up the gap between the practices of civil engineering and embedded sensor network community by joining expertise of researchers in both areas to enhance the safety of civil infrastructures of national interest. The interdisciplinary nature of this research lends naturally to a combined engineering and science curriculum development at both undergrad and graduate levels.","title":"NeTS-NECO: A Framework for Cooperative Active Sensing in Wireless Structure Health Monitoring","awardID":"0832089","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["490894","486325"],"PO":["565303"]},"143312":{"abstract":"This research investigates theories and algorithms for the geometric analysis of higher-order tensor fields and their applications to efficient surface remeshing. Remeshing, the process of producing a new mesh from an input mesh to improve its quality, has many applications that include shape modeling and manufacturing, medical imaging, solid and fluid simulation, and architectural design. Remeshing based on the intrinsic symmetries in the underlying surface can afford more faithful shape representation and greater user control. Such surface symmetries can be represented by higher-order tensors, whose analysis can benefit a wide range of applications beyond geometry remeshing, such as solid and fluid dynamics, electromagnetism, weather prediction, tsunami and hurricane modeling, aircraft design and testing, biometrics, arts and entertainment, motion analysis and synthesis, and education.<br\/><br\/>The project contains three research thrusts. First, the investigator explores the connections between higher-order tensor fields on a surface and regular or near-regular rotational symmetries. Second, the fundamental geometric and topological properties of higher-order tensor fields are studied. Finally, such knowledge is applied to obtaining efficient and highly controllable geometry remeshing algorithms. To explore the connection between tensors and symmetries, the investigator borrows results form existing work in tensor decomposition and extends them to near-regular and mixed symmetries. Work from vector and tensor field analysis is extended to higher-order tensor fields, and concepts such as differential geometry and dynamics systems are applied to higher-order tensor analysis. As a result of the research, the tensor analysis and remeshing algorithms are integrated into a system. In addition, the implementations of these algorithms, especially those supporting higher-order tensor analysis, are compiled into libraries to facilitate integration into host applications that requires higher-order tensor analysis. Both the system and its supporting libraries will be disseminated to the public. With respect to its impacts on education, the remeshing system and tensor analysis are integrated into the curriculum for undergraduate students majoring in science and engineering.","title":"Geometric and Topological Analysis of Higher-Order Tensor Fields on Surfaces","awardID":"0830808","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":["497248"],"PO":["565157"]},"141013":{"abstract":"NSF Proposals: 0820222\/0820170<br\/><br\/>Title: Recombinant Services -- Recasting the Web for Continuously Evolving Systems<br\/><br\/>PIs: Richard N. Taylor and Nenad Medvidovic<br\/><br\/>The broad features of many real-world systems belie the assumptions of classical systems and software engineering: there is no single overarching authority, change is managed by many independent organizations, the many systems serve many masters, and evolution is independent and uncoordinated. Ongoing improvements in the scale and reach of pervasive network infrastructure and the brisk development of diverse Web-accessible resources will play pivotal roles in future systems of systems. Apropos of this future, this project offers a radical rethinking of service abstraction that integrates modern network structures, computation as a fundamental unit of network exchange, and network-wide system awareness to deliver recombinant Internet-scale services for which service composition, reuse, autonomy, context-free interaction, and monitoring are first-order effects. This project pursues a set of architectural principles addressing the challenges, then developing infrastructure consistent with those principles, for the implementation, deployment, and operation of systems of systems. The engineering methods reifying recombinant services---CREST---will refashion the Web, from an architecture that is predominantly data-driven and reactive to one that is computation-driven and autonomic. Our results will be given to the open source community and external collaborating organizations, a technology transfer with the potential of transforming the Web.","title":"Collaborative Research: Recombinant Services -- Recasting the Web for Continuously Evolving Systems","awardID":"0820222","effectiveDate":"2008-09-15","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["533779"],"PO":["564388"]},"143202":{"abstract":"Changepoint problems deal with detecting anomalies or more generally changes in patterns. In the sequential setting, as long as the behavior of observations is consistent with the ``normal state,\" one is content to let the process continue. If the state changes, then one is interested in detecting that a change is in effect, usually as quickly as possible. Any detection policy may give rise to false alarms and attempting to avoid false alarms too strenuously will lead to a long delay between the time of occurrence of the change and its detection. The gist of the changepoint problem is to produce a detection policy that minimizes the average detection delay subject to a bound on the false alarm rate. <br\/><br\/>While the quickest changepoint detection problem has been studied for over fifty years, there has been remarkably little prior work on theoretical extensions to general stochastic models that go beyond independent and identically distributed observations in the pre- and post-change modes, and to the distributed sensor setting. The goal of this project is to investigate the properties of known changepoint detection procedures and to develop novel procedures for change detection and classification under general system models that are relevant in practical applications, as well as to provide an analytical framework to predict their performance. The usefulness of the theoretical advances will be demonstrated through two key application areas: (a) the rapid detection of intrusions and disruptions in computer networks, and (b) the efficient monitoring of critical infrastructures. In both cases, the distributions of the noisy observations change, and this change occurs at an a priori unknown point in time. Also, in both cases, the detection should be performed in a timely manner, while keeping the false alarm rate at an acceptable level. Our results will be validated using simulations as well as real data (to the extent possible).","title":"Collaborative Research: Optimal Changepoint Detection and Identification Algorithms with Applications","awardID":"0830419","effectiveDate":"2008-09-01","expirationDate":"2013-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["519308"],"PO":["564898"]},"143444":{"abstract":"NSF Proposal 0831508: CT-ISG High-Speed Network Defense with Massive and Diverse Vulnerability Signatures<br\/><br\/>PI: Yan Chen, Northwestern University<br\/><br\/>Given the ever-increasing sophisticated Internet attacks, network-based Intrusion Detection\/Prevention Systems (IDS\/IPS) are of critical importance. Such systems mainly have two important metrics: accuracy and throuput. Accuracy is of particular importance, especially for IPSes which are inline devices that throttle connections when they are identified as malicious via signature-matching. The latest works assume that regular expressions (RE) are the right choice for signature formatting. However, there are polymorphic and metamorphic variations that can evade the RE-based detection. The fundamental problem of RE signatures is that in many cases it cannot capture the vulnerability conditions.<br\/><br\/>In this project, we design a next-generation semantic based network IDS\/IPS system (called NetShield) which contains thousands of vulnerability signatures with rich diversity, including protocol, file and web semantic signatures. While offering much better accuracy, NetShield provides high throughput comparable to that of the state-of-the-art regular expression based IDS. We design algorithms for 1) efficient protocol parsing and 2) massive protocol semantic signature matching. Furthermore, we extend the parsing and matching solutions to Web and file semantic signatures.<br\/><br\/>This project has the potential for significant broad impact. The research component will produce fundamental knowledge that will advance the state-of-the-art in the network IDS\/IPS systems. Our wide collaboration with industry researchers will facilitate such technology transfer. In addition, we plan to disseminate our work through timely releases of software\/hardware, traces, and benchmarks to the open source community for broader usage. This research agenda is complemented by a strong and tightly integrated educational and outreach component.","title":"CT-ISG: High-Speed Network Defense with Massive and Diverse Vulnerability Signatures","awardID":"0831508","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["477110"],"PO":["565327"]},"143213":{"abstract":"In the setting of secure computation, a set of mutually distrusting parties wish to compute some function of their inputs while preserving security properties such as privacy, correctness, etc. One important property is fairness, which roughly means that if anyone learns the result of the computation, then everyone does.<br\/><br\/>It has been known since the mid-1980s that complete fairness is impossible, in general, without a strict majority of honest parties. For over 20 years, the accepted folklore has been that nothing non-trivial can be computed with complete fairness without an honest majority. Recent work of the PI and others shows that this folklore is false, and demonstrates that the question of fairness in secure computation is wide open.<br\/><br\/>This project seeks to expand the initial feasibility results (or prove corresponding impossibility results) and to develop a better understanding of fairness in cryptographic protocols more generally. This includes work on partial fairness, as well as exploring rationality as a means of obtaining fairness. The educational component of this project will focus on graduate student education and outreach efforts to the broader research community.","title":"Understanding Fairness in Secure Two-Party and Multi-Party Computation","awardID":"0830464","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["519626"],"PO":["565157"]},"145402":{"abstract":"Nanotechnology may be a foundation for the economy of the future. A potential way for new materials and devices to be constructed at the nanoscale is to use DNA molecules to assemble them. DNA, the primary genetic component of our cells, has a convenient, template-matching property that allows two DNA molecules to bind, and thus, assemble whatever components with which they are associated. Moreover, this template-matching reaction with DNA can be programmed to build complicated structures from the bottom-up. The 15th International Meeting on DNA Computing is a forum that brings together researchers from Computer Science, Mathematics, Molecular Biology, Chemistry, and Physics to discuss the molecular scale manipulation of matter through computational paradigms using DNA and other biological material. Thus, it is helping to establish the scientific foundations of a potentially important technology for the future. <br\/><br\/>The population of Arkansas has one of the lowest percentages of people with undergraduate or graduate degrees in the United States. In order to compete in the future economy, Arkansas students, particularly, undergraduates, need to participate in research and study of emerging technologies, like DNA Computing and Bionanotechnology. In addition, work in these emerging technologies is highly interdisciplinary, which is not the norm for traditional undergraduate education. A workshop at the 15th International Meeting on DNA Computing will bring together top researchers in this emerging technology, and teachers and students from Arkansas undergraduate institutions to discuss and foster the involvement of undergraduates in this field. Topics will include undergraduate research, curriculum development, and opportunities for both faculty and students to participate in funded research.","title":"A Workshop on Undergraduate Education in Emerging Technologies at The 15th International Meeting on DNA Computing","awardID":"0840708","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["548478"],"PO":["565223"]},"143345":{"abstract":"NSF NeTS-NECO Proposal 0830939: Protocol Design for Distributed Delaunay Triangulation and Its Applications<br\/><br\/>Award Abstract<br\/><br\/>Delaunay triangulation (DT) of a set of points in Euclidean space has many applications in different fields of science and engineering. A distributed DT can be used to improve performance for a variety of networks (e.g., P2P, overlay, sensor, and wireless networks) as well as simulation-type applications, including distributed virtual reality systems and multiplayer online games. The discovery of a necessary and sufficient condition for a correct distributed DT is the basis of protocol design in this project for a dynamically changing system of nodes (in d-dimensional space, d > 1) to construct and maintain a distributed DT. In addition to correctness and efficiency, accuracy is used as a performance metric because it is impossible to maintain a correct distributed DT continually (correctness of a distributed DT is broken as soon as a join, leave or failure occurs). A suite of protocols is designed that recovers quickly from incorrect system states such that a system that has been under ?churn? converges to 100% accuracy quickly after churning stops. Expected results include (i) the design of an improved protocol suite that is an order of magnitude more efficient than existing protocols and still satisfies requirements of correctness and accuracy, and (ii) protocols to construct a distributed Euclidean minimum spanning tree for systems under churn. Network application protocols are also designed for greedy routing, finding the closest node, broadcast, multicast, geocast, and clustering. The performance of these protocols for network nodes with inaccurate (or virtual) coordinates is being investigated.<br\/><br\/><br\/>Investigator: Simon S. Lam<br\/><br\/>List of Active Grant Awards<br\/><br\/>1. NSF NeTS-NECO Proposal 0830939: Protocol Design for Distributed Delaunay Triangulation and Its Applications, 9\/1\/08 to 8\/31\/11<br\/> Support for Simon Lam from this grant: 1 summer month (2009, 2010, 2011)<br\/><br\/>2. NSF CNS-0434515: NeTS-NR: A Novel Technique to Detect Shared Congestion and Applications, 10\/01\/04 to 9\/30\/08.<br\/> Support for Simon Lam from this grant: none (grant expires on 9\/30\/08)<br\/><br\/>3. NSF CNS-0627020: Collaborative Research: NeTS-NBD: Traffic Engineering in an Uncertain World, 9\/1\/06 to 8\/31\/09.<br\/> Support for Simon Lam from this grant: 1 summer month (2009)","title":"NECO: Protocol Design for Distributed Delaunay Triangulation and Its Applications","awardID":"0830939","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["516340"],"PO":["564993"]},"146623":{"abstract":"Going beyond in-person conference and symposium events this workshop develops and evaluates new types of Creative-IT community events that incorporate IT and IT artifacts of the Creative-IT community and leverage these in distributed online events. The events will focus on Creative IT community and knowledge development through participant engagement, generative activities, and reflective critique. Initially the distributed will be developed by curating (i.e., selecting, facilitating, and hosting the events) three events. The initial events will focus on three themes of this community, including: (1) Creativity Support Tools, (2) Multifaceted Creative Experiences with IT, (3) Creativity-IT Theories and Methodologies: An Evolving Canon. This project effectively defines a workshop that is distributed in time and space. The 2 major goals are to bring the CreativeIT community together using the technologies that are claimed to enable creativity, and to provide an opportunity to develop and evaluate the application of these technologies with a group of researchers that tend to be both creative and early adopters. The long term impact of the proposed work is to create, a series of interactive creative experiences that would become a self-motivated process of self-definition and growth that is owned by the Creative-IT community as a whole, or a rotating facilitating group of members.","title":"Workshop Proposal: Creativity and IT as Integral Elements of Growing Creative-IT Communities","awardID":"0846145","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["552861"],"PO":["424970"]},"143477":{"abstract":"NEDG: MIMO Links in Wireless Edge Networks: Cross-Layer Protocol Design<br\/><br\/>Abstract:<br\/><br\/><br\/>Wireless networking has gained significant acceptance in recent years. In particular, wireless local area networks (LANs) and mesh networks are increasingly being deployed in the unlicensed bands of the spectrum. While the density of wireless devices is likely to increase, the spectrum available for the devices remains limited. Therefore, techniques to improve performance of wireless networks are of great interest.<br\/><br\/>Intellectual Merit: This project will explore the use of multi-antenna technology in wireless networks. A particular focus will be on the design and evaluation of cross-layer protocols for multi-input multi-output (MIMO) channel access. This project will address the fundamental challenges of managing interference and intelligent use of MIMO technology in wireless edge networks. The topics to be covered include design of cross-layer protocols for MIMO-based wireless networks, development of accurate simulation models for MIMO channels, and experimental evaluation of a selected subset of protocols. Through these research activities, the project will contribute to enhancing the performance of wireless networks.<br\/><br\/>Broader Impacts: This project has the potential for broad and long-range impact to both the basic theory and practical methods and protocols for wireless networks that exploit MIMO technology. The blend of theoretical and experimental investigations in this project provides a unique and invaluable opportunity for training of future networking researchers and engineers. Selected results from this project will also be incorporated into graduate and undergraduate courses on wireless communications and networking.","title":"NEDG: MIMO Links in Wireless Edge Networks: Cross-Layer Protocol Design","awardID":"0831670","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["553537","223414"],"PO":["557315"]},"143246":{"abstract":"Abstract: Block ciphers are indispensable components of the communication infrastructure, yet few unifying principles for their design and analysis exist. As a result, it is not possible to easily characterize good and bad ciphers, and the design of good ciphers is difficult. While notable general treatments of the problem do exist and have contributed valuable insights, it has been difficult to combine the individual treatments of specific attacks and specific cipher designs into an overarching theory, and much remains to be done.<br\/><br\/>This project studies a large class of attacks known as statistical cryptanalysis, which exploits probabilistic relationships among the plaintext, key and ciphertext to determine the key. The research extends existing communication channel models of statistical cryptanalysis---where low capacity channels carry encoded symbols of the key to the adversary---which have been used very successfully to design attacks on stream ciphers, but are not as widely used for block cipher cryptanalysis. A key insight exploited by the project is as follows: existing cryptanalytic models do not provide a means of studying the combination of related-key attacks and statistical attacks. The PI observed that related secrets form codes over information leakage channels, improving adversary communication efficiency. Using this approach, the project obtains general results on attack efficiency---experimentally verified as far as possible---and related cipher design criteria. Through an established r elationship with local high school Chantilly Academy, where the PI teaches cryptography modules, the project contributes to K-12 education, and inspires high school students to study mathematics, engineering and computer science.","title":"Statistical cryptanalysis of block ciphers as channel communication","awardID":"0830576","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["496577"],"PO":["564924"]},"143367":{"abstract":"Information and Communications Technology (ICT) infrastructure failures and cyber attacks are realities that can have catastrophic societal effects. Information Assurance (IA) can be defined as the operations undertaken to protect and defend ICT systems by ensuring their dependability and security. There is a critical need for systematic IA methods that enable ICT systems to adapt and survive any type of disruption or attack. A major hurdle in the development of IA techniques is the lack of models and metrics which enable one to determine the effectiveness of IA mechanisms. This exploratory project seeds a collaborative effort between three PIs at different institutions: Duke University, University of Missouri Kansas-City, and the University of Pittsburgh focused on the development of metrics and models that will allow one to quantitatively study the technical aspects of information assurance (IA) for the network component of the ICT infrastructure. The basis of the approach is to unify attack trees, attack graphs, privilege graphs and fault trees into a common scalable framework with a well defined set of metrics and application scenarios. Extensions of the basic model that include state information, stochastic properties and rewards via Markov chains and stochastic Petri nets, enabling a wider variety of attack and fault scenarios are being studied. The impact of the models and metrics developed is that they provide the techniques and tools necessary to determine the effectiveness of IA mechanisms and allow one to detect bottlenecks and to evaluate the tradeoffs between levels of information assurance, performance and cost.","title":"Collaborative Research: CT-ER : MiMANSaS: Metrics, Models and Analysis of Network Security and Survivability","awardID":"0831090","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["541801"],"PO":["543481"]},"144698":{"abstract":"The 18th Fall Workshop in Computational Geometry (FWCG) is a regional workshop, economical for researchers in eight states to attend, including those without travel budgets, such as faculty at teaching colleges, grad students, and members of underrepresented groups.<br\/>Computational geometry has rapidly emerged as a field of intense research in computer science and many related disciplines. Since many of the computations associated with both real and simulated physical systems are geometric in nature, research in this field has been fueled by applications in computational molecular biology, computer graphics, geometric modeling, sensor networks, engineering design, manufacturing, robotics, machine vision, data mining, and statistics. Therefore advances in computational geometry improve the economic competitiveness of the United States.<br\/><br\/>The workshop is being held at Rensselaer Polytechnic Institute, Troy NY, Oct 31 - Nov 1, 2008. It will enhance the careers of junior researchers by being large enough for interesting people to be present, but small enough for them to be accessible. Half of the NSF support is for subsidizing grad students and underrepresented groups. An example of computational geometry's importance is the Voronoi diagram, a fundamental data structure. It facilitates predicting protein docking in computational biology, which leads to better drug design.<br\/>Computational geometry is also of importance to various mission agencies. The National Science Foundation's support has a unique role, enabling the more fundamental and basic research that their applied research and advanced development relies on.","title":"Fall Workshop on Computational Geometry 2008","awardID":"0838081","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[385994,"522222"],"PO":["565157"]},"143136":{"abstract":"Changepoint problems deal with detecting anomalies or more generally changes in patterns. In the sequential setting, as long as the behavior of observations is consistent with the ``normal state,\" one is content to let the process continue. If the state changes, then one is interested in detecting that a change is in effect, usually as quickly as possible. Any detection policy may give rise to false alarms and attempting to avoid false alarms too strenuously will lead to a long delay between the time of occurrence of the change and its detection. The gist of the changepoint problem is to produce a detection policy that minimizes the average detection delay subject to a bound on the false alarm rate. <br\/><br\/>While the quickest changepoint detection problem has been studied for over fifty years, there has been remarkably little prior work on theoretical extensions to general stochastic models that go beyond independent and identically distributed observations in the pre- and post-change modes, and to the distributed sensor setting. The goal of this project is to investigate the properties of known changepoint detection procedures and to develop novel procedures for change detection and classification under general system models that are relevant in practical applications, as well as to provide an analytical framework to predict their performance. The usefulness of the theoretical advances will be demonstrated through two key application areas: (a) the rapid detection of intrusions and disruptions in computer networks, and (b) the efficient monitoring of critical infrastructures. In both cases, the distributions of the noisy observations change, and this change occurs at an a priori unknown point in time. Also, in both cases, the detection should be performed in a timely manner, while keeping the false alarm rate at an acceptable level. Our results will be validated using simulations as well as real data (to the extent possible).","title":"Collaborative Research: Optimal Changepoint Detection and Identification Algorithms with Applications","awardID":"0830169","effectiveDate":"2008-09-01","expirationDate":"2013-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["223414"],"PO":["564898"]},"143257":{"abstract":"Abstract<br\/>This research examines theoretical, algorithmic, and computational issues that arise in signal processing problems where there is a need to compute sparse solutions. There are numerous signal-processing applications where sparsity constraint on the solution vector naturally arises. Brain imaging techniques such as MEG and EEG, sparse communication channels with large delay spread, high-resolution spectral analysis, direction of arrival estimation and compressed sensing are a few examples. The generalization and extension of the sparse Bayesian learning (SBL) techniques considered in this research will broaden the application domain and provide a very powerful complement to the existing maximum a posteriori (MAP) methods commonly used and in some cases even surpass them.<br\/>The investigators study extensions and generalizations of the sparse source recovery problem to greatly broaden the application domain. A key consideration in the work is developing a rigorous framework to deal with dependency in the sparsity framework. Motivated by applications with sparse but local structure, the research considers intra-vector dependency in the single measurement case, as well as intra-vector dependency as required in the multiple measurement contexts, among others. The research also includes the development of connections between multi-user communication theory and the sparse signal recovery problem to shed light on the stability with which sparse signal recovery is possible and to develop an understanding of the limits of suboptimal source recovery methods. To deal with non-stationary environments, the research develops on-line adaptive algorithms that exploit the inherent sparse structure of the application. The research also includes evaluation of the resulting algorithms in several important application domains.<br\/><br\/>Level of Effort Statement<br\/>At the recommended level of support, the PI and co-PI will make every attempt to meet the original scope and level of effort of the project.","title":"Theory and Algorithms for Exploiting Sparsity in Signal Processing Applications","awardID":"0830612","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["496128","499664"],"PO":["564898"]},"143499":{"abstract":"Providing each node in a multihop wireless network with one or <br\/>more multi-channel radios offers a promising avenue for improving <br\/>the networking performance. Recently, the impact of multiple channels <br\/>and radios on network throughput has been extensively studied. But <br\/>little is known about their impact on communication latency, which <br\/>is an important performance metric for a broad class of time-critical <br\/>applications such as emergency response and disaster recovery. While <br\/>it is of no doubt that multiple channels and radios hold potentials <br\/>of reducing the communication latency, what really matters is how <br\/>much benefit can be realized by specific algorithms. This research <br\/>conducts comprehensive algorithmic studies of minimizing the <br\/>communication latency by utilizing multiple channels and radios. <br\/>The algorithms developed in this research can be applied directly <br\/>to support real-time applications in multihop wireless networks. <br\/>The theoretical analysis of the performance against the number of <br\/>channels and\/or the number of radios can also provide guidance to <br\/>industries on cost-effective design of multihop wireless networks.<br\/><br\/>This research focuses on the communication schedules for four <br\/>primitive communication tasks which are involved in almost all<br\/>applications: broadcasting, aggregation, gathering, and beaconing. <br\/>The problem of seeking a shortest communication schedule for each <br\/>of these four tasks is NP-hard. The investigators develop <br\/>constant-approximation algorithms for them and analyze the impact <br\/>of the number of channels and the number of radios on the latencies <br\/>of the constructed communication schedules. This research is of <br\/>interest to a number of research communities, and may serve as a <br\/>basis for interesting future projects on multi-channel multi-radio <br\/>multihop wireless networks. Both student training and curriculum <br\/>development are integrated into this research.","title":"NeTS-NECO: Minimizing Communication Latency with Multiple Channels And Radios in Multihop Wireless Networks","awardID":"0831831","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["518604"],"PO":["557315"]},"135403":{"abstract":"Traditional 9-1-1 systems, which date back to 1970s, support only voice, while non-emergency communications now feature other media. Adding additional media for 9-1-1 presents opportunities and challenges. Text messages, images captured by cell phones, video clips, and automatic crash notification messages can dramatically enhance the 9-1-1 services by expediting emergency responses and reducing crash clearance times. The rapid increase of residential, nomadic and mobile VoIP usage requires the development of VoIP-based next generation 9-1-1 systems and services that will replace the current circuit-switched 9-1-1 systems. Beyond limitations in media and mobility support, existing systems are inefficient and cannot easily accommodate new functionality. This project is a collaboration among University of North Texas, Columbia University and Texas A&M University, where UNT will be the lead institution, to develop a testbed that will enable research on understanding and analysis of next generation 9-1-1 services. The testbed make possible research and development in reliability, security, function-appropriate privacy and other areas that already difficult in large scale VoIP, but which become daunting when the VoIP system is critical infrastructure. <br\/><br\/>The broader impacts of this project are many. A testbed for Internet-based 9-1-1 research is particularly important as both state and federal governments are in the process of planning next-generation emergency communication platforms, unfortunately often without adequate vendor-neutral testing and evaluation. Users of the testbed will investigate issues related to locating<br\/>9-1-1 callers, securing Public Safety Answering Points, ensuring continuous availability of 9-1-1 services during large-scale emergencies, predicting emergencies, providing citizen alerts (\"reverse 9-1-1\"), and improving inter-agency coordination. The PIs expect to translate results from research on this infrastructure to engineering guidelines and disseminate results across government organizations, standards bodies such as IETF and National Emergency Number Association (NENA) and 9-1-1 centers. Moreover, the findings from the experiments in this project will be useful for the residents across USA.","title":"Collaborative Research: CRI: IAD: A Testbed for Research and Development of Next Generation 9-1-1 Services","awardID":"0751118","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["239096",359702,359703],"PO":["565327"]},"134556":{"abstract":"Access control is a multi-faceted area that has been advanced by a wide range of computer science research communities including programming languages, human-computer interaction, computer<br\/>architecture, and operating systems. In general, this body of work has either sought to improve the expressiveness of access control logic or introduce novel mechanisms for enforcing policies. Each<br\/>approach relies on a human operator or programmer to manually specify access control policies which are then enforced by a trusted reference monitor. Unfortunately, policy specification is often an error-prone process and can lead to damaging breaches of confidentiality due to<br\/>access control misconfiguration. This work takes a three-phased approach to mitigating the effects of access control misconfiguration: 1) develop heuristics and models of proper access control enforcement, 2) design and implement system monitoring mechanisms capable of automatically identifying suspicious sharing patterns, and 3) evaluate the effectiveness of these heuristics and implementations through user studies and honeypots. These activities target both ubiquitous<br\/>Internet systems such as the web and email as well as emerging mobile systems such as mobile social networks and mobile banking.","title":"CAREER: Tolerating Access Control Misconfiguration","awardID":"0747283","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["451799"],"PO":["543481"]},"143026":{"abstract":"Researchers are developing computational elements and systems based on nucleic acids for use in biodetection applications. The elements are based on oligonucleotides (short single-strands of DNA) and deoxyribozymes (oligonucleotides that enzymatically act on other oligonucleotides). These elements are organized into information-processing systems as gates, cascades, amplifiers, and larger application-specific circuits.<br\/><br\/>Biodetection applications of the elements and systems being developed include gene expression profiling (in basic molecular biology research), human genetic screening (in public health), pathogen detection (in containment of infectious diseases in the field), and civil defense (in rapid detection of bioterrorism). The direct display devices the researchers are developing provide a read-out that is easy to interpret without specialist training and allows immediate practical decisions to be made; for instance, in the field, health care providers can select individual patient treatment, and public health professionals can select options for containment of infectious disease outbreaks.<br\/><br\/>Researchers are solving four practical problems. To reduce the expense of laboratory implementation procedures, they are studying oligonucleotide interaction models and devising libraries for modular circuit construction. To make the information-processing components interface to the relevant biodetection applications, they are building robust and selective biosensor modules. To make the visual display of biodetection results easy to use, they are experimenting with different display designs, with encoding schemes based on colors and patterns, including alphanumerics. Finally, to make the deployment of devices practical, they are establishing clear and easy to follow procedures for integrating with specific biological applications.<br\/><br\/>As a prototype, researchers are developing a biodetector for the class of mosquito-borne flaviviruses. The prototype allows identification of at least 11 different flavivirus species, among them the Yellow Fever virus and the West Nile virus, using equipment that is readily deployable in the field.","title":"Collaborative Research: EMT\/MISC: Making Molecular Computation Practical for Biodetection Applications","awardID":"0829793","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7946","name":"BIO COMPUTING"}}],"PIcoPI":["459219",380811],"PO":["565223"]},"143268":{"abstract":"Linear programming (LP) and polyhedral formulations have earned an unparalleled reputation in the last 60 years as a unifying framework for both practically and theoretically attacking discrete optimization problems, ranging from resource allocation and scheduling to routing and VLSI design. From a theoretical perspective they are the basis of most known approaches to constructing generic approximation algorithms for NP-hard problems. Despite LP's distinguished status, the PI feels that the full impact of polyhedral techniques on computer science has yet to be realized. The PI aims to address this gap by (1) developing frameworks for designing approximation algorithms that better leverage the structure that polyhedral formulations provide, and, as an ambitious long term goal, (2) developing a complexity theory based on polyhedral formulation size.<br\/><br\/>In particular, the PI proposes an iterated packing technique for rounding packing problems that enjoys many of the benefits that the elegant and powerful iterated rounding method offers for covering problems: iterated packing is conceptually simple, and large fractional components facilitate rounding. One may view LP as a bona fide model of computation, with formulation size as a measure of complexity. This perspective has the potential of inducing different types of separations than traditional complexity measures; for example, although matching is a fundamental problem in P, there is no polynomial-sized polyhedral formulation known for it. The PI also seeks to unearth connections between traditional and polyhedral complexity. For any discrete optimization problem, the PI suggests a canonical decision problem and conjectures that the optimization problem is solvable in polynomial time if and only if the associated decision problem is in NC^1.<br\/><br\/>A broader goal of this inherently interdisciplinary research is to forge deeper connections between the operations research and theoretical computer science communities: LP is a foundational component of the former, and the PI seeks to elevate the status of LP beyond that of just a tool in the latter. The PI will attempt to engage collaborators from both communities, and the research conducted will be disseminated in venues visible to both communities.","title":"Polyhedral Frameworks for Approximation Algorithms","awardID":"0830661","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[381482],"PO":["550329"]},"144005":{"abstract":"This project develops a system that improves parallel efficiency on large numbers of processors - up to tens or hundreds of thousands - without running a program at scale. This system is called MPI-PPA: MPI Performance Prediction and Advisement. MPI-PPA takes as input a scientific computing application along with the input variables, including the desired number of processors, p. With executions on fewer than p processors only - so that these executions will occur quickly - MPI-PPA will produce a list of program phases that are predicted to achieve poor scalability, allowing the programmer to quickly address and possibly re-implement these phases - as well as a prediction for the entire program run.<br\/><br\/>MPI-PPA makes these predictions using statistical regression to develop a prediction function that can be used with any number of processors. MPI-PPA will not require significant program comprehension, an important aspect when considering that computational scientists are typically experts in their scientific domain and not in computer science. The approach of MPI-PPA involves heavy reliance on statistical techniques, so the work in this project will be interdisciplinary between computer science (the PI) and statistics (the co-PI). MPI-PPA will be validated by using benchmark suites such as NAS and ASCI codes, along with large-scale applications - such as Paradis and Raptor - that are of interest to national labs.<br\/><br\/>The broader impact of this work is multifold. First, MPI-PPA will be beneficial for computational scientists as well as cluster administrators. Among the benefits will be a simple and fast performance tuning system, an increase in overall cluster efficiency, and a reduction in response times for individual applications. The technology developed in this project will be transferred, in the form of performance tuning and prediction software, and made available to the public through cooperation with Lawrence Livermore National Laboratory. Second, more interdisciplinary interaction between statistics and computer science will be fostered through the supervised statistical consulting center at the University of Georgia. Third, efforts will continue recruiting students from strong historically black colleges and universities in the area, such as Morehouse University.","title":"CSR-PSCE, SM: MPI-PPA: Improving Efficiency of Large-Scale Clusters Through Statistical Performance Prediction","awardID":"0834356","effectiveDate":"2008-09-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383873,"517411"],"PO":["551712"]},"143037":{"abstract":"Biological systems are the product of an evolutionary process of random tinkering and selection that resulted in unexpected and non-intuitive ?engineering? solutions to dynamically varying conditions. Thus, biological systems are robust, adaptive and evolvable information processing systems that operate asynchronously and in parallel on multiple scales. The examination and characterization of the design principles of biological circuits has the potential to revolutionize biology, medicine and the way computing and communication systems are built. This project is pioneering important advances at the interface between biology and computation by pursuing two complementary goals: (1) to develop a modular, parallel-ready simulator to replicate the multi-scalar architecture of complex biological systems; (2) to discover key design principles relevant to information processing systems in general by reproducing biological design in silico. <br\/><br\/>Information processing by cells encompasses multiple scales connecting molecular events to phenotypes. Current simulation techniques have limited multi-scale and modular capabilities, resulting in models that describe only a single feature of a given system and miss the relationships between architecture, function and behavior. This research effort addresses these limitations by representing biological systems as a hierarchy of functional executable modules. The design of the platform obeys four basic principles: 1) components are objects; 2) objects are governed by rules; 3) rules include some degree of stochasticity; and 4) objects and rules are organized in functional and spatial modules that compose a hierarchy. The development of the new platform is driven by the construction of simulations of key biological model systems with an unprecedented scope and precision, such as bacterial chemotaxis, epidermal growth factor receptor signaling, the acute inflammatory response, and parallel processing by bacterial colonies. The reproduction of these biological systems in silico is providing insights into their design principles, which in turn advances the future design and implementation of distributed technological systems.","title":"Collaborative EMT\/BSSE: Hierarchical representation and simulation of modular cellular systems","awardID":"0829836","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[380842],"PO":["565223"]},"135425":{"abstract":"Traditional 9-1-1 systems, which date back to 1970s, support only voice, while non-emergency communications now feature other media. Adding additional media for 9-1-1 presents opportunities and challenges. Text messages, images captured by cell phones, video clips, and automatic crash notification messages can dramatically enhance the 9-1-1 services by expediting emergency responses and reducing crash clearance times. The rapid increase of residential, nomadic and mobile VoIP usage requires the development of VoIP-based next generation 9-1-1 systems and services that will replace the current circuit-switched 9-1-1 systems. Beyond limitations in media and mobility support, existing systems are inefficient and cannot easily accommodate new functionality. This project is a collaboration among University of North Texas, Columbia University and Texas A&M University, where UNT will be the lead institution, to develop a testbed that will enable research on understanding and analysis of next generation 9-1-1 services. The testbed make possible research and development in reliability, security, function-appropriate privacy and other areas that already difficult in large scale VoIP, but which become daunting when the VoIP system is critical infrastructure. <br\/><br\/>The broader impacts of this project are many. A testbed for Internet-based 9-1-1 research is particularly important as both state and federal governments are in the process of planning next-generation emergency communication platforms, unfortunately often without adequate vendor-neutral testing and evaluation. Users of the testbed will investigate issues related to locating<br\/>9-1-1 callers, securing Public Safety Answering Points, ensuring continuous availability of 9-1-1 services during large-scale emergencies, predicting emergencies, providing citizen alerts (\"reverse 9-1-1\"), and improving inter-agency coordination. The PIs expect to translate results from research on this infrastructure to engineering guidelines and disseminate results across government organizations, standards bodies such as IETF and National Emergency Number Association (NENA) and 9-1-1 centers. Moreover, the findings from the experiments in this project will be useful for the residents across USA.","title":"Collaborative Research: CRI: IAD: A Testbed for Research and Development of Next Generation 9-1-1 Services","awardID":"0751205","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["13545","528479","376280"],"PO":["565327"]},"144016":{"abstract":"Multi-core processors require increasingly sophisticated operating systems and middleware in order to ensure both security and performance isolation. Operating systems need to be aware of the on-chip resource requirements of individual threads and processes. Shared resources such as parts of the memory hierarchy and the off-chip bandwidth imply that interactions among concurrently executing processes might affect their performance and perceived priority. Resource-aware policies are imperative for improved performance, fairness, and scalability. <br\/><br\/>Some of the challenges faced in making the policies resource-aware include: identifying application resource requirements in a transparent manner, understanding the interactions among conflicting resource requirements, and enforcing the resource constraints in a manner that scales as the number of cores is increased. This research addresses these challenges by developing new system-level support for resource management in multi-core processors. The key idea is to use low-overhead hardware-provided statistics via counters as a mechanism to learn about resource requirements and conflicts without application involvement. Hardware counter statistics can also serve as a signature of program execution, benefiting tasks such as workload classification and phase change identification. At the level of the operating system, the project utilizes the online processor execution statistics to improve the efficiency and fairness of CPU scheduling and memory management, and manage the hardware-provided statistics as a first-class resource so that multiple applications can take advantage of the information. <br\/><br\/>By working with an existing open-source OS (Linux) and virtual machine monitor (Xen), the experimental work directly impacts today's multi-core users. Strong on-going collaborations with industry partners, in particular, IBM and Ask.com, permit this team to transfer technological advancements to mainstream processors and commercial applications. As one possible outcome, the hardware design of processor counters can be augmented to cooperate with the software system support. With a better understanding of software needs (for both statistics utilization and management), more informed tradeoffs can be made at the hardware level. The education component of this project will include direct research participation of both graduate and undergraduate students as well as curriculum enhancement for related systems courses. As a result, students will acquire valuable multidisciplinary (software\/hardware) experience and training required to understand and advance increasingly complex future computer systems.","title":"CSR-PSCE, SM: Operating System-Level Resource Management in the Multi-Core Era","awardID":"0834451","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["556692","550397"],"PO":["535244"]},"143048":{"abstract":"Quantum information processing (QIP) uses quantum phenomena such as superposition, interference, and entanglement to perform information-processing tasks that are difficult or impossible with classical resources. This research project studies two important tools in QIP, and their relationship to each other: quantum walks and weak measurements.<br\/><br\/><br\/>Quantum walks are analogous to classical random walks. A \"walker\" at one of the vertices of a graph repeatedly moves randomly along the edges to neighboring vertices. In the quantum version the walker simultaneously moves in a superposition along every possible edge, producing novel interference effects. These effects are strongly influenced by the global symmetry of the graph, opening up the possibility of new quantum algorithms. However, in quantum mechanics measuring the location of the walker disturbs the state of the system and destroys the interference effects. To avoid this problem we use weak measurements that give less information but also disturb the state less. Choosing the optimal measurement strength maximizes the likelihood of finding the system in the desired state while minimizing the expected hitting time.<br\/><br\/><br\/>The researchers address several important problems involving quantum walks and weak measurements. They study the effect of symmetry on hitting time for continuous-time quantum walks, the existence of infinite hitting-time walks, and how these allow a new class of quantum algorithms. They are developing path-integral techniques for quantum walks on graphs. They also study how to decompose strong generalized measurements into a sequence of weak (or continuous) measurements. It is an open problem, given a family of possible weak measurements, to determine which generalized measurements can be produced.","title":"Quantum Walks and Weak Measurements","awardID":"0829870","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["381587"],"PO":["565157"]},"143169":{"abstract":"This research studies new methods for constructing<br\/>economical plans to deploy wireless sensor nodes of<br\/>multiple types and form a reliable wireless sensor<br\/>network to monitor, throughout a given time period,<br\/>a large set of targets spread across a geographical<br\/>space. It initiates and carries out a thorough<br\/>analytical investigation on deploying multiple types<br\/>of sensors for constructing a wireless sensor network<br\/>that satisfies, simultaneously, the min-cost, the<br\/>lifetime, the fault-tolerance, and the connectivity<br\/>requirements. It integrates concepts and methods in<br\/>linear programming, convex programming, reliability<br\/>theory, graph theory, and approximation algorithms<br\/>to model these requirements and find efficient solutions.<br\/>Results of this research provide a substantial impact on<br\/>large-scale, terrestrial and underwater, wireless sensor<br\/>network applications and communication protocol designs.<br\/><br\/><br\/>More specifically, this research seeks analytical models<br\/>to provide, with minimum monetary cost on sensor nodes,<br\/>reliable sensor placements and time scheduling to hinge<br\/>on spatial-temporal information and power consumption.<br\/>Despite failures and limited power supply of sensor<br\/>nodes, the network must guarantee that each spatial<br\/>target be watched by at least one active sensor at any<br\/>given moment during a pre-determined period of time<br\/>required by the underlying application. This implies<br\/>sensor placements must satisfy the spatial-temporal<br\/>coverage, the fault-tolerance, and the min-cost<br\/>requirements for single-hop networks; as well as the<br\/>additional connectivity requirement for multi-hop<br\/>networks, which ensures that each sensor node is<br\/>connected to a base station through a communication path.","title":"TF-SING: Collaborative Research: Reliable Spatial-Temporal Coverage with Minimum Cost in Wireless Sensor Network Deployments","awardID":"0830314","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7938","name":"SENSOR NETWORKS"}}],"PIcoPI":["554394"],"PO":["564924"]},"144027":{"abstract":"The purpose of this project is to explore novel embedded processor architectures for electronic textiles (e-textiles). E-textiles are intelligent fabrics where sensors and computation are intrinsic to the cloth, with applications in medicine, entertainment, and sports. E-textiles occupy an extremely constrained point in the embedded system design space, having a large number of networked sensors and processors distributed throughout the fabric and tight requirements on performance, energy consumption, and reliability.<br\/><br\/>Research at the Virginia Tech E-textiles Lab has shown that a Model-Driven Engineering (MDE) approach allows designers to effectively manage the complexity of the e-textile design space, which includes issues such as fiber selection, the weave pattern, the physical topology of the electrical\/communication network in the fabric, the number\/type\/location of sensor\/actuator nodes, the number and type of computational nodes, the system software organization, and the application algorithms.<br\/><br\/>However, an MDE approach does not yield an implementation that is easily portable while being power- and performance-efficient, due to a fundamental mismatch between the MDE design specification and the target microprocessors. The approach in this project is to develop an event-driven computer architecture family that presents an abstraction that is well-matched to the MDE design specification.<br\/><br\/>The potential benefits of this project include using the architecture family to improve the reliability, design cost, and energy efficiency of embedded systems besides e-textiles. The broader impacts of the project include e-textile applications in medical monitoring, wearable computing, and pervasive computing. Educational benefits include providing undergraduate and graduate students with multidisciplinary research experience.","title":"CSR-EHCS(EHS), SM: Investigating a Novel Embedded Processor Architecture for Electonic Textiles in Wearable and Pervasive Computing","awardID":"0834490","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["485861","553340"],"PO":["561889"]},"144148":{"abstract":"Large networked computer systems are some of the most complex and sophisticated engineered systems ever constructed. Because of this complexity, our existing computing environment is unreliable, insecure, and difficult to use. One intriguing aspect of this situation is that most problems people encounter in practice are not new --- they have previously been seen and solved by others. Developing techniques that enable the automated analysis of this rich knowledge source is therefore a key step in advancing our understanding of this important class of engineered artifacts. The research effort focuses on developing techniques that automatically process existing repositories of human interactions regarding previously encountered problems and solutions to isolate potential solutions to the problem. It will then use multiple virtual machines to evaluate each of the potential solutions, then use this evaluation to automatically select an candidate.<br\/><br\/>The expected benefits of this approach are three fold. First, developing methods that jointly analyze documents in natural language and formally structured computer information can lead to new insights and a deeper understanding of these important phenomena. Second, this research will yield a new class of techniques for reasoning about computer systems in the presence of uncertainty by leveraging collective knowledge of a user community. Finally, by automatically finding solutions to system problems, these methods hold out the promise of substantially reducing the cost and risks associated with using computers to perform many of the key activities in our society.","title":"CDI-Type II: Exploiting Collective Human Knowledge to Understand and Evolve Complex Networked Systems","awardID":"0835652","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7751","name":"CDI TYPE II"}}],"PIcoPI":["497068","401949","560189"],"PO":["565215"]},"144038":{"abstract":"Program performance is highly dependent on the amount of memory available to the program. In traditional computing systems, the memory working set of an application has a bounded size - providing more memory to an application improves performance until its working set is met. Once the working set is met, additional memory yields little or no benefit. However, in the presence of garbage collection (a technique for memory management where space that is unlikely to be reused by an application is automatically reclaimed), the relationship between program performance and memory allocation is more complex. Data is managed at three levels: the compiler manages data objects at the program level, the garbage collector manages the heap at the virtual machine level, and the virtual memory manager manages virtual memory at the operating system level. The middle layer plays a critical role. Increasing an application's heap size can reduce the frequency of garbage collections and improve performance, but too large a heap may trigger paging and degrade performance.<br\/><br\/>Software developers take advantage of garbage collection (GC) for the many benefits it provides by using either garbage-collecting languages, such as Java and C#, or conventional languages (e.g., C and C++) augmented with conservative garbage collectors. While a conventional program uses exactly as much memory as it needs, the memory use of a garbage-collected program can be adjusted by changing the size of the heap used by the garbage collector. This difference can allow an advanced execution system to control applications' memory demands in response to the changing amount of available memory in a shared environment. This concept is increasingly important for today's multicore, multiprocessor machines.<br\/><br\/>Building on previous work, this project develops the technology required to model the memory demand of garbage-collected programs and enable adaptive management in existing virtual machines and operating systems. Specifically, the project extends the PIs' work on whole-program locality and phase models and adaptive memory management, combining program analysis, garbage collection control, and on-line system monitoring.<br\/><br\/>This work develops program-level adaptive memory management (PAMM) for garbage-collected programs running concurrently with other garbage-collected programs and with conventional applications. The goal is to adjust all applications' demands to fully use available memory and avoid contention from periods of over demand.","title":"Collaborative Research: CSR-PSCE, SM: Adaptive Memory Management in Shared Environments","awardID":"0834566","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550486"],"PO":["535244"]},"144159":{"abstract":"Wayfinding is an essential capability for any person who wishes to have an independent life-style. It requires successful execution of several tasks including navigation and object and place recognition, all of which necessitate accurate assessment of the surrounding environment. For a visually-impaired person these tasks may be exceedingly difficult to accomplish and there are risks associated with failure in any of these. Guide dogs and white canes are widely used for the purpose of navigation and environment sensing, respectively. The former, however, has costly and often prohibitive training requirements, while the latter can only provide cues about obstacles in one?s surroundings. Human performance on visual information dependent tasks can be improved by sensing which provides information and environmental cues, such as position, orientation, local geometry, object description, via the use of appropriate sensors and sensor fusion algorithms. Most work on wayfinding aids has focused on outdoor environments and has led to the development of speech-enabled GPS-based navigation systems that provide information describing streets, addresses and points of interest. In contrast, the limited technology that is available for indoor navigation requires significant modification to the building infrastructure, whose high cost has prevented its wide use. <br\/><br\/>This proposal adopts a multi-faceted approach for solving the indoor navigation problem for people with limited vision. It leverages expertise from robotics, computer vision, and blind spatial cognition with behavioral studies on interface design to guide the discovery of information requirements and optimal delivery methods for an indoor navigation system. Designing perception and navigation algorithms, implemented on miniature-size commercially-available hardware, while explicitly considering the spatial cognition capabilities of the visually impaired, will lead to the development of indoor navigation systems that will assist blind people in their wayfinding tasks while facilitating cognitive-map development.","title":"CDI-Type II: Collaborative Research: Cyber Enhancement of Spatial Cognition for the Visually Impaired","awardID":"0835714","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7751","name":"CDI TYPE II"}}],"PIcoPI":["549664"],"PO":["543539"]},"147437":{"abstract":"Genocide. HIV\/AIDS. Famine. Deforestation. Habitat destruction. Species extinction. Forced exodus. These problems share some commonalities. In one way or another, they entail widespread losses to human beings, to other sentient beings, or to the natural world; moreover, recovery from those losses is not likely within the time frame of a single human lifespan (if ever). It is also the case that information and the processes around information may have much to contribute to the solutions of these problems. How then might this class of problems be addressed through information system design? What unique opportunities exist for information systems and what roles might they play? In a field known for cutting edge innovation, where devices over five years old are regarded as legacy, how does one begin to consider processes and solutions that span periods that likely extend beyond a single human lifespan? <br\/><br\/>This proposed technical and social research initiative takes up the multi-lifespan design challenge by working with significant real world problem spaces to begin outlining possible opportunities and roles for information systems to help construct longer-term solutions. This inaugural project area is the information heritage of the United Nation's International Criminal Tribunal for Rwanda (ICTR). The goals are twofold: (1) to contribute meaningful information system designs to the specific real world problems under study; and (2) to generate more general knowledge about the design of multi-lifespan information systems. This SGER will fund one research team and film crew to travel to Tanzania to conduct Futures Workshops and roughly 20 contextual interviews with ICTR personnel. This is time critical. As of January 1, 2009 the ICTR is scheduled to initiate a radical downsizing. The vast amount of knowledge in defining and implementing an international justice system will quickly dissolve as individuals scatter globally to find new career opportunities.<br\/><br\/>Broader impact. This is truly a multi-lifespan information system design effort. The project will address unique technical and social challenges through its efforts to create an information system that serves the needs of an international audience who access the system through a multiplicity of information tools. An adaptable infrastructure that preserves and manages the ICTR's information heritage for the next one hundred plus years and addresses the difficult challenges of privacy, access, and security will be highly relevant to other international tribunals.","title":"SGER: Multi-Lifespan Information System Research Initiative - The Information for an International Justice System","awardID":"0849270","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["542095"],"PO":["565227"]},"144049":{"abstract":"This research is developing a novel class of Data Stream Management Systems (DSMSs) called Reconfigurable Logic Assisted DSMS (RLADSMS). The project is developing a prototype called SYMBIOTE to demonstrate real-time processing of high bandwidth multimedia data streams in a computer vision based traffic monitoring system. <br\/><br\/>Using hardware-assisted designs and decomposition strategies to achieve its stream storage model, this project introduces new query operators such as the split-choose and merge operators, and a macro-operator called split-merge. It is building a library of such DSMS operators, realizable in reconfigurable logic hardware, that are reusable, parameterizable, and exhibit reduced run-time and reduced spatial complexity Example hardware-accelerated operators include Filter, Map, Union, Bsort, Aggregate and Join. The project also investigates hardware implementations of strategies such as approximate query answering for sliding windows; landmark windows; and sketches, random sampling, and wavelets. <br\/><br\/>This project extends the Borealis query language of ?boxes and arrows? with the additional intent to address deployment characteristics in SYMBIOTE. The objective is to perform static query optimization using a heuristic best-effort static query planning and diagram placement algorithm that together can minimize the dynamic configuration effort of RLADSMS. This optimization strategy uses a novel cost model for RLADSMS based on the spatial complexity of hardware DSMS operators, run-time complexity of hardware\/software operators, and many SYMBIOTE sensor node properties. Sensor node properties can include camera characteristics, frame-rate, number of look-up tables (LUTs)\/logic cells, and embedded block RAMs. The optimization can also consider communication properties such as interconnect latencies and bandwidth. The research outcomes of this project are expected to be widely applicable to multimedia sensor network based DSMSs, such as border patrol and building security applications, and possibly to the financial data processing domain. Moreover, this research spurs the academic exploration of novel ideas and creation of new courses in the area of hardware\/software codesign of DSMSs.","title":"CSR-EHCS(EHS), SM: Development of SYMBIOTE, A Reconfigurable Logic Assisted Data Stream Management System for Multimedia Sensor Networks","awardID":"0834682","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383994,"393140",383996],"PO":["561889"]},"137647":{"abstract":"Quantum information science (QIS) is devoted to understanding how the fundamental laws of quantum physics might be harnessed to dramatically improve the acquisition, transmission, and processing of information.<br\/>The potential scientific and technological payoff from QIS is vast, but many deep and fundamental problems must be solved before that potential can be fulfilled. These advances will require contributions from scientists trained in a variety of subjects who are bold enough to disregard the traditional boundaries between disciplines and to pool their efforts in building an important new field.<br\/>With this need in mind, the Institute for Quantum Information (IQI) was founded at Caltech in September 2000, supported by a five-year $5M Information Technology Research (ITR) award from NSF. Since September 2005, the IQI has been supported by a three-year award sponsored jointly by the Physics at the Information Frontier program (PIF) in MPS\/PHY, the Emerging Models and Technologies for Computation Cluster (EMT) in CISE\/CCF, and the Office of Multidisciplinary Activities (OMA). The IQI, led by a multidisciplinary team of five Caltech professors, is devoted to building the theoretical foundations of quantum information science across a broad front encompassing quantum algorithms, quantum cryptography, quantum information theory, fault-tolerant quantum information processiThe research accomplishments of the IQI have clear intellectual merit. Since September 2000, IQI participants have produced 282 publications covering all aspects of theoretical QIS; of these 86 were generated since the onset of our current award in September 2005.<br\/>Some noteworthy achievements during the past two years are: (1) Progress in quantum algorithms, such as the discovery of an exponential speedup for finding hidden nonlinear structures. (2) Insights into quantum communication, such as a proof of the quantum channel capacity theorem based on decoupling of the environment. (3) Studies of quantum entanglement, such as a characterization of the monogamy of nonlocal correlations. (4) Tools for fault-tolerant quantum computation, such as a scheme for protecting against highly biased noise. (5) Proposals for quantum hardware, such as a protected qubit based on a superconducting current mirror. (6) Connections between quantum information theory and quantum many-body theory, such as new proposals for experiments that probe the non-abelian statistics of quantum Hall quasiparticles.<br\/>The broader impact of the IQI also has many facets. With the end of scalability of conventional siliconbased information technology on the horizon, it is vitally important to explore aggressively new paradigms for information technology. IQI contributions are broadening the nation's technical base, ensuring US leadership in the future development of quantum science and technology. The IQI has attracted and trained top postdoctoral scholars, 16 of whom have moved on to faculty positions (or the equivalent) elsewhere, thus significantly strengthening the world effort in QIS. The IQI has also trained many Ph.D. students who are still working in the field, and we have sponsored a variety of undergraduate research projects.<br\/>A particularly important aspect of the IQI?s broader impact is a vibrant visitor program.<br\/><br\/>Since 2000, we have sponsored 120 visits by senior and postdoctoral scholars, and 70 visits by graduate students from other institutions. The visitor program fuels intellectual excitement,facilitates collaborations and exchanges of scientific ideas, and performs a highly valued service for the international QIS community.<br\/>The IQI can best ensure its continued success by nurturing its distinctive qualities: a focus on interdisciplinary research, an emphasis on fostering the career development of world-class postdoctoral talent, and devotion to an active visitor program. At the same time, in response to new scientific opportunities, the mission of the IQI will evolve in important ways over the next several years. Two increasingly prominent themes will be the exciting interface of QIS with condensed matter physics, and the daunting challenge<br\/>of closing the considerable gap between the theory and experimental practice of QIS and physical implementations of quantum computing. Basic advances in all of these areas are needed to bring revolutionary quantum technologies closer to realization.","title":"Institute for Quantum Information","awardID":"0803371","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"7281","name":"QUATM INFO & REVOLUTIONARY COM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1266","name":"APPLIED MATHEMATICS"}}],"PIcoPI":["490112","511760","550549",365875,365876],"PO":["564326"]},"137658":{"abstract":"The genomes of individuals alive today are derived from some common ancestor(s) by the actions of mutations and meiotic recombinations. Recombination mixes two homologous chromosomes in an individual to produce a third recombinant, mosaic chromosome, consisting of alternating segments of the two homologs. A recombinant chromosome is then passed on to a child of the individual. Therefore, the derivation of genomes in a current population, from some ancestral genome(s), cannot be represented by a tree, but rather must be represented by a directed acyclic graph, called a Phylogenetic Network or Genealogical Network, or Ancestral Recombination Graph (ARG). Explicitly knowing the historically correct genealogical network that derived extant genomes, or knowing critical features of the network, would greatly facilitate the solution of fundamental problems in biology, and has important practical applications, for example in association mapping in populations, a technique to find genes affecting diseases and important economic traits. However, since we cannot directly examine the past, we must computationally deduce a genealogical network, or features of it, from genomes that we can examine in populations today. The development of algorithms for such computation requires significant interaction of ideas from biology, computer science, graph-theory, mathematics, and algorithm and software engineering. This interdisciplinary project, conducted by computer scientists in collaboration with a population geneticist, is focused on developing efficient algorithms to infer and exploit complex genealogical histories under a variety of biological models of the evolution of genomic sequences and of genetic traits, using different types of existing and emerging biological data. These algorithms will be implemented in software that can be used to study fundamental biological questions, and applied to practical problems such as association mapping of complex traits. The central thesis of the project is that explicit genealogical networks can be efficiently computed, and that these networks capture enough of the true history (even if the networks don?t capture all of it) to allow researchers to more effectively answer fundamental biological questions, and more effectively solve practical biological problems. The project also addresses fundamentally new algorithmic problems and biological applications, driven by new kinds of population variation data that are becoming available, new areas of biology where population data is becoming available, new biological models that have been recently proposed for the evolution of sequences and genetic traits in populations, new understanding of different genomic variations that affect important traits, and biological controversies and questions about the nature (and even the existence) of recombination, and about its role in other biological phenomena. This work will contribute to algorithmic computer science and also to several areas of biology, particularly population genetics. The algorithms and software that will be developed will allow biologists to deduce complex genealogical histories, to better understand the role of recombination, and to address both fundamental biological problems and applied practical problems. The software will be disseminated on the web, along with slides and videos of lectures on the algorithms underlying the software. The project will allow the joint mentoring of graduate students and post-doctoral researchers by advisers from both computer science and biology. The participation of researchers from both computer science and biology makes the research more visible to their respective communities, encouraging other interdisciplinary <br\/>research.","title":"III-CXT-Medium: Collaborative Research: Inference of Complex Genealogical Histories in Populations: Algorithms and Applications","awardID":"0803440","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["485645"],"PO":["565136"]},"139605":{"abstract":"In traditional social choice, it is assumed that each agent explicitly ranks all of the alternatives. In Artificial Intelligence applications this is generally impractical: for example, there are exponentially many joint plans<br\/>or allocations of tasks\/resources. Nevertheless, even the computational social choice community has so<br\/>far focused primarily on the explicit-ranking model. While this was a necessary phase to establish a solid<br\/>foundation for this line of research, it is now time to move on and consider the combinatorial domains with<br\/>exponentially many alternatives that motivated computational social choice in the first place. This is what<br\/>the proposed research will do.<br\/>The PI proposes the following 5-part research plan. First, he plans to study how agents? votes should<br\/>be represented, that is, what language the agents should use to express their preferences in a combinatorial<br\/>domain. Once the language has been determined, he plans to study what rule should be used to make a<br\/>decision based on the votes. Such a rule would be useless without a good algorithm for executing it, that<br\/>is, for solving the winner determination problem. Even with a good language, it may be overwhelming<br\/>for an agent to report its complete preferences, so the PI plans to study how to elicit the (relevant parts of)<br\/>agents? preferences by asking the agents simple queries. Finally, he plans to address the problem of agents<br\/>voting insincerely to manipulate the decision, in part by investigating whether such manipulation can be<br\/>made computationally infeasible.<br\/>Broader Impacts<br\/>The proposed research will allow agents to coordinate when solving a complex problem, even if they have<br\/>been created by different designers with different objectives. For example, robots in search-and-rescue or<br\/>other exploration settings can vote over how they will divide the exploration. This allows a much greater<br\/>diversity of agents to participate in such a task, undoubtedly leading to better results. Also, some of the<br\/>research is likely to be applicable to human decision making.<br\/>Europe is starting to take the lead in computational social choice; if funded, this proposal will ensure that<br\/>the U.S. retains expertise in and continues to shape this burgeoning research area. Of course, research is not<br\/>a zero-sum game, and the PI plans to collaborate closely with the other researchers in the area. In fact, this<br\/>proposal corresponds to the PI?s part of a 12-investigator proposal that was just recommended for funding<br\/>by the European Science Foundation (ESF). This (NSF) proposal would also support the PI?s collaboration<br\/>with Jeff Rosenschein (Hebrew University); for this collaboration, Jeff and the PI already received a small<br\/>US-Israel Binational Science Foundation grant that serves to support the Israeli side as well as travel.<br\/>The proposal also includes plans to develop a new graduate course on computational social choice,<br\/>mentor graduate and undergraduate students, build connections to economics and political science, and<br\/>attract more women to computer science (as well as participate in other outreach activities).","title":"(RI+hcc)-Small: Computational Social Choice: Aggregating Preferences in Combinatorial Domains","awardID":"0812113","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["556682"],"PO":["562760"]},"139616":{"abstract":"Online Question-Answering Sites (Q&A sites, for short) are a new and rapidly growing piece of the community-created content landscape. From Knowledge iN in South Korea to Microsoft's Live QnA in the U.S., to Yahoo! Answers in 26 countries world-wide, users are flocking to sites where they can post questions and get answers. Yahoo! Answers alone attracts about 18 million unique visitors monthly and has accumulated over 400 million answers since its launch in 2005. Not only do these sites meet individual needs for information, the content they generate is an important source for online search and knowledge discovery. However, a casual browse through any one site reveals significant noise in the signal. Too many questions receive sarcastic or even insulting answers. Too often, it seems, users re-ask a question rather than finding value in the answers already posed.<br\/><br\/>Because the dramatic growth of Q&A sites is so recent, there has been little opportunity to empirically investigate them as a new information resource and as a new type of social space. This project examines five Q&A sites using a variety of observational and experimental methods to gain an understanding of how users interact in and with these sites while also developing tools to help users better meet their goals. Specifically, this project will: 1.) identify structures or properties in questions posed on Q&A sites that affect response characteristics such as quantity, quality, and timeliness; and explore the use of templates, critics, and bots to help question-askers obtain better responses; 2.) identify structures or properties in response threads that suggest high risk of failure; and explore the use of bots to intervene and \"rescue\" derailed Q&A threads; 3.) understand the lifecycle of Q&A site participants, including their social interaction on the sites; and develop tools to help support user integration into online Q&A communities.<br\/><br\/>Given their importance as an information resource and social phenomenon, understanding online Q&A sites has intellectual merit in its own right, in addition: this work advances an important line of research on online and computer-mediated communications that has helped rhetoric and communications experts contribute to the design of more effective online communication tools. It will also be the first work to study online Q&A from both a social and an information resource perspective, giving new insight into the nature of online voluntary knowledge creation.<br\/><br\/>Broader Impacts: Online question-answering sites have become an important source of information and advice for individuals and businesses, as well as an important source of content for web search engines. By understanding these sites and developing tools to support their continued successful operation, we will help Q&A community designers understand approaches that can both promote beneficial social experiences for users and the construction of valuable community-contributed repositories of knowledge.","title":"HCC-Small: Understanding and Supporting Online Question-Answering Sites","awardID":"0812148","effectiveDate":"2008-09-01","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["550380","550379"],"PO":["565342"]},"139627":{"abstract":"Object detection cascades are one of the most significant recent developments in computer vision. By enabling real-time object detection, they are a potentially disruptive technology, which is already making a commercial impact in industries as diverse as digital photography, automotive, surveillance, personal identification, and traffic safety, among others. However, this disruptive potential is currently stifled by the substantial complexity of training detector cascades. In practice, this complexity limits the application of the cascaded architecture to a small set of domains (most notably face detection) which have been heavily researched by the academic community and for which detectors are publicly available. This project aims to eliminate the complexity hurdle, by laying the theoretical and algorithmic foundations for the fully-automated, low-complexity, design of optimal detection cascades, which guarantee high detection-rate while minimizing false-positive rate and detection complexity. In particular, the project addresses major current roadblocks in architecture design, detector design, and training complexity, through novel contributions in cost sensitive boosting, weak learners, and optimal cascade design algorithms. All contributions will be evaluated in the context of an effort to deploy real-time animal detectors in some of the most popular wild-life attractions of San Diego. This also provides an exciting and unusual opportunity for the involvement of undergraduates in research.<br\/><br\/>More information on the project can be found at http:\/\/www.svcl.ucsd.edu","title":"RI-Small: Optimal Automated Design of Cascaded Object Detectors","awardID":"0812235","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["513306"],"PO":["564316"]},"147129":{"abstract":"This SGER proposal concerns the accumulation and representation of skills and control knowledge by robots that interact with unstructured environments. There has been comparatively little work on representations that capture re-useable knowledge in robotics---an issue that lies at the heart of many future applications. Thus, this SGER represents a potentially transformative technology and addresses significant gaps in the state-of-the-art for which the payoff, despite the risk, is extremely high. We aim our 1 year study on learning techniques that accumulate knowledge related to grasping and manipulation. We shall extend pilot studies and build prototypes for self-motivated learning techniques and generative models for manipulation and multi-body contact relationships. The approach relies on learning to discover and exploit structure over the course of several staged learning episodes; from sensory and motor knowledge concerning the robot itself, to controllable relationships between the robot and external bodies, to multi-body contacts involved in tasks like stacking and insertion. The project has three principal technological goals: to advance the state-of-the-art of robotic manipulation and knowledge representation; to extend machine learning methods toward intrinsically motivated, cumulative, and hierarchical learning; and to advance computational accounts of the longitudinal processes of sensorimotor and cognitive development in humans and machines.","title":"SGER: Well Modeled Modular Robots for Complex Dynamic Motion","awardID":"0848118","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["553516"],"PO":["403839"]},"139528":{"abstract":"Hardware reliability is becoming an increasing concern in the late CMOS era. Components in shipped chips will fail for many reasons, requiring mechanisms to detect, diagnose, recover from, and repair\/reconfigure around these failed components so that the system can provide reliable operation. The pervasiveness of the problem across a broad market demands low-cost and general reliability solutions that can be deployed in general-purpose, commodity systems running applications with varying reliability requirements. Traditional reliability solutions involving excessive redundancy are too expensive, as are piecemeal solutions that address individual failure modes. This work proposes a full system solution that aims to provide a common framework for error detection, diagnosis, recovery, and repair\/reconfiguration for a variety of hardware failure modes, with a customizable reliability vs. overhead tradeoff.<br\/><br\/>Two key high-level observations motivate the approach. First, the hardware reliability solutions need handle only the device faults that propagate through higher levels of the system and become observable to software. Second, in spite of the reliability threat, fault-free operation remains the common case and must be optimized, possibly at the cost of increased overhead once a fault is detected. The proposed system therefore detects faults by watching for anomalous software behavior (or symptoms of faults), using novel zero to low-cost hardware and software monitors. After a fault is detected, it invokes an innovative, but potentially expensive, procedure for diagnosing the fault source to enable reconfiguration\/repair (in the case of hard faults). For recovery, it relies on a checkpoint\/replay mechanism, including pure hardware and hybrid software assisted recovery depending on detection latency. Coordinating all of the above is a thin firmware layer that provides flexibility and customizability. A major component of the work is a much needed formulation and validation of microarchitecture level fault models, required to drive high-level reliability solutions.","title":"CPA-CSA-T: Low Cost and Comprehensive Hardware Reliability","awardID":"0811693","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["551063","542046","551097"],"PO":["366560"]},"139649":{"abstract":"Advances in remote sensing techniques have made available large datasets of topographic measurements pertaining to terrestrial and planetary land surfaces. However, the scientific utilization of these datasets is hampered by a lack of tools for effective automated analysis. This project seeks to develop a system for fast, objective and transparent conversion of topographic data into knowledge about land surfaces. The project has two complementary goals: 1) to develop a tool that autonomously produces geomorphic maps mimicking traditional, manually derived maps in their appearance and content, and 2) to develop a tool that classifies entire topographic scenes into characteristic landscape categories. The mapping tool is based on the object-oriented supervised classification principle. A number of novel solutions, including semi-supervised learning, meta-learning, and a wrapping technique coupling classification and segmentation, are proposed to address challenges posed by the specificity of topographic data. The scene classification tool is based on information-theoretic metrics and incorporates novel solutions to problems posed by the raster character of topographic datasets. <br\/><br\/>Intellectual Merit <br\/><br\/>The project employs a novel fusion of machine learning and computer vision techniques to open new possibilities. In the process of constructing the mapping and classifying tools, novel machine learning methodologies will be developed and tested. The products of this research will enable a qualitatively new type of analysis of land surface topography: the large scale statistical comparison of spatial distribution of landforms. <br\/><br\/>Broad Impact <br\/><br\/>Successful mapping and classifying tools will have impact beyond the analysis of natural landscapes; they can be also be applied to the study of surface metrology (the numerical characterization of industrial surfaces). The nature of this project will attract interest and collaboration with specialists from diverse disciplines, such as computer science, remote sensing, geomorphology and hydrology. Such links will broaden the base of expertise for each discipline, as well as enrich participants from contributing domains.","title":"III-CXT-Small: Collaborative Research: Automatic Geomorphic Mapping and Analysis of Land Surfaces Using Pattern Recognition","awardID":"0812372","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[371194],"PO":["563751"]},"150595":{"abstract":"Modern global society is increasingly reliant on computing and information professionals to serve key roles in creating, managing, and maintaining the global computing and information infrastructure that is critical to research, education, commerce, and quality of life. As such, computing and information professionals need to learn not only technical skills but also how to resolve ethical issues such as in\/outsourcing, intellectual property, and information privacy. Working in the new global economy requires that computing and information professionals are able to consider the range of cultural values and ethical perspectives as represented by leading thinkers such as Aristotle, Bentham, Buddha, Confucius, Gilligan, and Kant. To broaden and deepen the ethical perspectives of computing and information professionals, it is essential to develop and teach courses in computing and information ethics as part of professional graduate programs in computer science and the interdisciplinary information field. Further, to enable current and future computing and information professionals to appreciate and understand the relevance of ethics in their work, it is necessary to find educationally motivating ways to engage graduate students in professional computing and information programs to consider key ethical issues. In this project the PIs hope to accomplish this goal through development and evaluation of an educational simulation for computing and information ethics, which serves as the cornerstone for an innovative course focusing on the role of values and ethics in computing and information within a global society. Building on their prior research, the PIs will explore three main research questions: How do graduate students in a computing and information ethics course use educational simulation software to gain understandings of and hands-on experiences with important computing and information ethics issues such as intellectual property in a global society? Does geographical co-location have an impact on use of the simulation in the context of an internationally-oriented computing and information ethics graduate course with a diverse range of students? Do students in a graduate-level computing and information ethics course benefit most from interaction with peers, or with the software agents developed through this project?<br\/><br\/>Broader Impacts: Project outcomes will include a free and open-source simulation for computing and information ethics, which allows students both to participate in and learn from cases, and to develop and implement their own cases that they can then share with peers. The PIs will develop a novel and highly participatory course built around the simulation, which will be implemented in professional computing and information master's programs across the country and the world, and which will better prepare computing and information professionals to deal with ethical issues throughout their careers.","title":"Collaborative Research: Educational Simulation for Computing and Information Ethics","awardID":"0903985","effectiveDate":"2008-09-01","expirationDate":"2011-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[401493],"PO":["565227"]},"143500":{"abstract":"The increasing integration of novel network applications, in particular, peer-to-peer (P2P) applications, into the Internet content distribution infrastructure is posing significant new challenges to achieving efficient and fair utilization of Internet network resources. In particular, by largely network oblivious, these network applications can cause substantial problems on network efficiency, Internet economics, and application performance.<br\/><br\/>The objective of this project is to develop a novel, light-weight framework called P4P that provides standard, modular interfaces to allow Internet network providers and network applications to jointly optimize their respective performance.<br\/><br\/>The P4P interfaces are designed through rigorous mathematical derivations to achieve extensibility, scalability and efficiency in large-scale, heterogeneous networks. The interfaces consider not only network provider privacy but also end-user privacy. The interfaces take into consideration not only traditional performance metrics but also increasingly important economic and business policies.<br\/><br\/>This project advances the state of the art in facilitating the interactions across different Internet network layers and entities, and presents a transformative research that enables others to effectively integrate network applications into network management.<br\/><br\/>The interfaces designed in the project will be specified in an open, standard language. The reference implementations will be made available broadly to facilitate integration into the Internet infrastructure. A P4P research group will be formed to maximize open research and disseminate results widely among network operators and application developers.","title":"NECO: P4P: Provider Portal for (P2P) Network Applications","awardID":"0831834","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["555996",382122],"PO":["565303"]},"143995":{"abstract":"As safety-critical systems become increasingly more complex, statutory certification organizations are increasingly mandating that formal techniques be used to prove that such systems meet their specifications. Techniques from real-time scheduling theory are commonly applied during this proof process to demonstrate compliance with temporal specifications. This project focuses on the emerging field of meta-real-time scheduling theory. Meta-scheduling theory attempts to understand those properties that cause certain scheduling-theoretic techniques to be more successful than others in designing and implementing real-time systems, and seeks general principles that are common to such successful techniques. The objective is to identify several such principles and establish that scheduling techniques complying with these principles are more likely to yield error-free real-time systems. We will seek methods of deriving scheduling techniques that are compliant with these principles. Using these techniques, the project seeks to provide theoretical foundations for the analysis of timing constraints in such systems, and obtain new methodologies for obtaining system designs that are provably correct by construction (thereby concurrently obtaining both correct designs and their formal proofs of correctness). Broader impacts include joint research with industry colleagues, building on strong expressions of interest from system designers in the topics and possible outcomes of this research. All tools and development platforms implemented as part of this project are being made public, and can be used by other institutions for research and teaching purposes.","title":"CSR-EHCS (EHS), SM: Formal Foundations of Real-time Systems Analysis: Principles and Potential Pitfalls","awardID":"0834270","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["31436","518412"],"PO":["561889"]},"145821":{"abstract":"Significant progress has been made in exploiting the practical utility of antineutrino detection for nonproliferation purposes, in particular, reactor monitoring, and for geophysics purposes, by mapping the earth's core, crust, and mantle. Despite these remarkable successes, these inter-related communities have had only modest overlap. As a result, the fields of applied and basic antineutrino physics lack an overarching strategy that would allow efficient exploitation of the resources and strengths provided by each stakeholder. This workshop aims to develop the basis for such a strategic alignment of interests between the fundamental and applied antineutrino physics communities. <br\/><br\/>Workshop reporters will summarize the state of the art in current antineutrino detection; technologies and applications, and participants will work intensively to create an environment for collaborative research across the community. Further, participants will examine the practicality of establishing a National Center for Neutrino Study (NCNS) similar in nature to the Space Telescope Science Institute or the Advanced Photon Source. User community members will be invited to participate in the workshop and will be asked to describe their vision of utilizing neutrino detection devices nationally with the intent to integrate their use globally. These visions will be informed by a clear understanding of the prospects and limitations intrinsic in antineutrino physics provided by the US neutrino science community attendees. The importance of this workshop will be to unite disparate groups of multi-disciplinary scientists with shared goals in antineutrino detection research and applications. A goal of the workshop is to develop stronger bonds of communications between these fields. This step is necessary in order to fully understand the various the components of an antineutrino spectrum and identify correctly the sources of the signal.","title":"Second Workshop on Neutrino Detection for Nuclear Monitoring","awardID":"0842586","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"H188","name":"DEFENSE INTELLIGENCE AGENCY"}}],"PIcoPI":["551625"],"PO":["565136"]},"143302":{"abstract":"Symbolic Software for Conservation Laws of Multi-Dimensional <br\/>Continuous and Discrete Nonlinear Equations<br\/><br\/>Willy Hereman and Michael Colagrosso<br\/><br\/>Abstract<br\/><br\/>The aim of the project is to create mathematical methods, algorithms, <br\/>and symbolic software for the computation of conservation laws of nonlinear <br\/>models from the applied sciences and engineering.<br\/>Specifically, the research concerns nonlinear partial differential equations <br\/>in multi-dimensions, nonlinear differential-difference equations and <br\/>fully-discretized lattices.<br\/>Using differential-geometric tools and techniques from the calculus of <br\/>variations, the algorithms are straightforward to implement in computer <br\/>algebra languages and the software will be easy to use by non-experts. <br\/>Potential users are researchers concerned with conserved quantities and<br\/>integrability of nonlinear equations that arise in soliton theory, <br\/>dynamical systems, control theory, and mathematical physics. <br\/>In particular, the software will gain insight in reaction-diffusion<br\/>models, population and molecular dynamics, nonlinear networks, and <br\/>chemical reactions.<br\/>In addition to its use in research, the software will serve as an educational<br\/>tool for courses in nonlinear wave phenomena in fluid dynamics, <br\/>plasma physics, electrical circuits, quantum chemistry, bio-genetics, <br\/>and nonlinear optics.<br\/><br\/>An essential part of the project is the development of homotopy operator<br\/>methods to handle the needed integration and summation by parts on jet <br\/>spaces.<br\/>In view of their versatile applicability, fast algorithms and stand-alone <br\/>packages will be developed for continuous and discrete homotopy operators.<br\/>Adhering to a calculus-based approach, the generalization of the algorithms <br\/>to integro-differential equations and delay differential equations will be <br\/>pursued. <br\/>New mathematical techniques are expected to come from these explorations in <br\/>uncharted terrain.<br\/><br\/>The research fosters collaboration between mathematicians and computer<br\/>scientists, creating novel mathematics and quality software, as well as<br\/>producing students with good software engineering skills. <br\/>The software development process, including design, development, and testing,<br\/>combines the strengths of undergraduates, graduate students, and faculty. <br\/>The comprehensive software packages are envisioned to have a wide impact <br\/>with pure and applied mathematicians, physicists, engineers, and others <br\/>interested in the analysis of nonlinear equations, enhancing the <br\/>productivity of researchers in several domains.","title":"Symbolic Software for Conservation Laws of Multi-Dimensional Continuous and Discrete Nonlinear Equations","awardID":"0830783","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["408491",381567],"PO":["565251"]},"143544":{"abstract":"Historical static spectrum allocation has led to an artificial spectrum <br\/>scarcity, leaving no usable spectrum for future wireless networks. <br\/>Dynamic spectrum access is the ideal solution to break such scarcity and <br\/>make spectrum available to new wireless networks. However, without <br\/>providing proper reliability guarantees, dynamic spectrum access is <br\/>unacceptable to many networks and services. Thus, instead of focusing <br\/>solely on improving spectrum utilization, dynamic spectrum access should <br\/>provide reliable spectrum usage that meets network's individual needs. <br\/>This research develops SAFIRE, a robust architecture for dynamic spectrum <br\/>access that provides reliable and efficient spectrum usage to large wireless <br\/>networks. This research holds great practical values for wireless network <br\/>designers and service providers who rely on available and reliable spectrum <br\/>access to deploy and advance their networks.<br\/><br\/><br\/>Recognizing the fundamental tradeoffs between spectrum utilization and <br\/>reliability, this research focuses on efficient algorithms to meet <br\/>individual networks? reliability requirements while improving spectrum <br\/>utilization. The investigators will develop statistical mechanisms to <br\/>proactively regulate spectrum demands based on their reliability requirements, <br\/>and apply distributed coordination and cross-layer design to quickly adapt <br\/>spectrum allocations and application patterns to network and spectrum dynamics. <br\/>To support large-scale dynamic networks, this research focuses on <br\/>computational-efficient mechanisms with minimum management overhead. <br\/>These advancements will help attract large commercial interests into <br\/>dynamic spectrum systems, facilitating their wide adoption. Finally, <br\/>the investigators will use this research as a basis to develop <br\/>interdisciplinary computer science training across both software and <br\/>hardware layers.","title":"NeTS NEDG: Dynamic Spectrum Access for Availability and Reliability","awardID":"0832090","effectiveDate":"2008-09-01","expirationDate":"2013-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["551126"],"PO":["557315"]},"145733":{"abstract":"Spatial and temporal data are critical components in many applications. <br\/>This is especially true in analytical applications ranging from scientific <br\/>discovery to national security and criminal investigation. This exploratory <br\/>research develops of new methods for modeling and querying spatial, temporal <br\/>and thematic (STT) data. The methods differ significantly from traditional<br\/>approaches for STT data management; they follow a paradigm that goes beyond <br\/>querying for resources to querying about the relationships between resources.<br\/>Three STT data management advances this will lead to are: <br\/>(1) new query operators that exploit the graph-centric nature <br\/>of Semantic Web data models, (2) new indexing and query processing <br\/>techniques for STT data that are specialized for Semantic Web data models <br\/>and (3) an extension of the SPARQL RDF query language to support STT queries.<br\/><br\/>A second aspect of this project is to compare the STT-RDF <br\/>approach described above with an alternative approach based on OWL-DL <br\/>and qualitative spatial and temporal reasoning. This exploratory <br\/>study evaluates whether the STT-RDF analytics approach provides a <br\/>more efficient and expressive query language than the <br\/>OWL-DL approach with space-time ontology. Specifically, <br\/>(1) for the queries that can be encoded in both the formalisms, <br\/>is the former implementation more efficient? and (2) are there <br\/>queries that can be formulated in the STT-RDF analytics formalism<br\/>that cannot be expressed as OWL-DL queries? <br\/><br\/>Throughout this project, special attention is given to<br\/>repeatability of experimental results. All code will be open source, <br\/>and all benchmarks, datasets and ontologies will be available <br\/>through the project web site at<br\/>http:\/\/knoesis.wright.edu\/research\/semweb\/projects\/stt\/","title":"III-SGER: Spatio-Temporal-Thematic Queries of Semantic Web Data: a Study of Expressivity and Efficiency","awardID":"0842129","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["560054",388834],"PO":["560586"]},"143313":{"abstract":"Project Summary: Interference Management and the Capacity of Wireless Networks<br\/><br\/>Interference is believed to be the principal barrier to achieving higher data rates in wireless networks. The recent emergence of the idea of interference alignment has shown that the capacity of wireless networks can be significantly higher than prior estimates. The key idea is to design signals cleverly to occupy half the bandwidth in such a manner that they cast overlapping shadows at each receiver where they constitute interference, while they remain distinct at the receivers where they are desired. The surprising conclusion is that \"everyone gets half the cake\", i.e., regardless of the number of users competing for the same wireless spectrum, potentially every user is able to access half the bandwidth with no interference from other users. This research integrates the interference alignment perspective into the existing array of wireless network capacity results and uses the collective insights, tools and techniques to develop optimal interference management algorithms. The integrated view of interference management resulting from this research is a significant contribution to both the theory and practice of wireless network design. It contributes analytical insights that are pivotal in not only estimating the performance limits of wireless networks but also in finding efficient ways to approach these limits. Understanding the capabilities of wireless networks is essential for the industry, the academia, the government agencies and the society in general to have realistic expectations from the wireless networks of the future.<br\/><br\/>The investigators adopt a layered approach with the following three thrusts - (1) Capacity characterizations for elemental scenarios, (2) Generalized degrees of freedom characterizations for different classes of networks, and (3) Distributed interference management algorithms for large networks. At the microscopic level, the research explores the capacity of a parameterized continuum of elemental networks that capture the transitional regime between scenarios where interference alignment is capacity optimal and the classical strong interference, very strong interference and noisy interference cases. On a coarse but larger scale, capacity approximations are obtained in the form of generalized degrees of freedom characterizations for wireless networks, including the impact of relays, feedback and cooperation. The third stage takes a macroscopic view of wireless networks and combines the insights obtained from the capacity and degrees of freedom perspectives into distributed interference management algorithms. The stability of network management algorithms and their robustness to channel uncertainty is also investigated.","title":"Interference Management and the Capacity of Wireless Networks","awardID":"0830809","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["550258"],"PO":["564924"]},"143434":{"abstract":"Considering the popularity and wide adoption of social network systems and the competitive edge these systems provide, there has been a rapid growth in use of these systems to access, store, and exchange personal attribute information in distributed and\/or federated environments and this trend is expected to continue. Efficient, secure, and user-centric techniques are important for the successful deployment of such systems. Our goal in this project is to develop a comprehensive and compelling framework SNGuard (Social Network Guard) that satisfies diverse privacy properties, access control issues, identity management requirements, and usage patterns. The vision of dynamic social networks is a complex and highly sophisticated one that requiring ongoing research and analysis to continue concurrent with the changing role and face of digital information creation and usage including personal information and contents in social networks. The principal intellectual products resulting from this project will be the development of novel frameworks to facilitate user-centered privacy management, content management and risk-aware access control, thereby making SNGuard solutions more trustworthy, more reliable, and less vulnerable. This research effort will have broad societal impact by providing a key mechanism to enable new business and community models for the sharing of personal attributes including identity information to safely, easily, and quickly establish social networking environments in cyberspace. In addition to these potential benefits, other anticipated, broad-based benefits to be facilitated by this research include significant influence to K-12 education, international collaboration, and industrial and government partners.","title":"CT-M: Collaborative Research: Securing Dynamic Online Social Networks","awardID":"0831452","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["483863"],"PO":["497499"]},"143555":{"abstract":"Brewer 0832153<br\/>B: Project Summary<br\/>Collaborative Research: NECO:<br\/>Designing Intermittency-Aware Networked Systems<br\/><br\/>Both the Internet and phone system exhibit deep assumptions about continuous connectivity that are simply not true in much of the world, including rural areas and developing regions in particular. Intermittent network connectivity is a fact of life in most developing countries due to a variety of factors including high usage costs, frequent power outages, network failures and the use of delay tolerant networks. Traditional networked applications are not designed to work in intermittent environments. Although work on delay-tolerant networking (DTN) has created viable low-level networking protocols that support intermittency, it remains difficult to write applications that tolerate disconnections well.<br\/><br\/>This project proposes Intermittent Aware Network Architecture (IANA), a new network platform that can enable a spectrum of intermittent-aware applications. This work will develop several novel applications that show the power of intermittent networking in developing regions and that test and validate the IANA platform. The proposed applications include intermittent versions of web search, collaborative wiki software, automatic teller machine (ATM) support, and voice-messaging cellular phones. These are challenging applications that cover a range of scalability, data sharing, and security issues. Pilot versions of all the four applications will be deployed in India or Africa to understand how well the platform enables real applications in challenging environments.<br\/><br\/>An overarching goal in the design of IANA is to determine the underlying principles in building a generic network architecture that can be both adopted across different types of intermittent networks and be used as a common platform across several intermittent applications. An important related challenge is to determine an appropriate and generic Application Programming Interface (API) for intermittent applications. In pursuit of this larger goal, this work will 1) classify the kinds of intermittent networks, which facilitates appropriate responses to disconnection; 2) develop an intermittent-aware overlay network above DTN that enables application-specific in network optimizations, which in turn enables both better performance and better data distribution and sharing; and 3) develop a range of new APIs that move beyond low-level connection APIs (sockets) to support intermittent sessions, data collection and distribution, data replication and consistency, and mobility.<br\/><br\/>Intellectual Merit: This work will result in a substantial increase in the ability to create applications that work well in intermittent environments. The key contributions include an overlay network to help manage replication, mobility, aggregation and sharing, new higher-level APIs validated by real applications, and an evaluation of the effectiveness of the new platform via large-scale deployments in developing regions.<br\/><br\/>Broader Impact: The target applications have direct value in bridging the digital divide. The collaboration software empowers rural content creation and sharing and facilitates content distribution for education, health care, and emergency response. ATM software can help address the economic inefficiencies due to limited cash flow and limited access to banking services. The voice-messaging phone can extend the impact of the cellular revolution to rural areas and to low-literacy users. The PIs have a history of impactful applications in developing regions and expect similar impact from the proposed deployments and from the free, open-source release of the apps. Field work and new curriculum will also promote development-aware graduate students with strong international exposure. The applications and deployments facilitate inspiring internships, which will encourage diversity and multi-disciplinary research.","title":"Collaborative Research: NECO: Designing Intermittency-Aware Networked Systems","awardID":"0832153","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["550677"],"PO":["565090"]},"143214":{"abstract":"Proposal: CCF- 0830467<br\/>Institution: Ohio State University<br\/>PI: Dey, Tamal<br\/>Title: Inferring Topology and Geometry for Dynamic Shapes<br\/><br\/>ABSTRACT<br\/>Many applications in science and engineering deal with three dimensional shapes that move, deform, and\/or evolve with time. These applications need computational methods to simulate such shapes in motion for visualization, inspection, prototyping, and further developments. We propose to focus on the problem of inferring topology and geometry of a dynamic shape from an appropriate representation. We argue that the user can be given a choice of maintaining a data structure of appropriate complexity depending on the goal of the simulation. A lighter data structure can be used if the goal is to capture only topology whereas a more complex data structure can be chosen for capturing both geometry and topology. This view point generates a plethora of mathematical and algorithmic questions that we propose to investigate in this project. <br\/><br\/>Topology and geometry inference of shapes in motion with theoretical guarantee is a difficult but important problem. A key challenge is to keep the update costs for the maintained data structures low. Recent developments in topological analysis of different types of complexes in the context of surface reconstruction and data analysis have opened up the possibility of representing a shape at different levels of complexity depending on the need. An efficient use of these representations in a kinetic setting is","title":"Inferring Topology and Geometry for Dynamic Shapes","awardID":"0830467","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["549998"],"PO":["565157"]},"145524":{"abstract":"Proposal Number: 0841126<br\/>PI: Jon Eisenberg<br\/>Institution: National Academy of Sciences<br\/>Lead<br\/><br\/><br\/><br\/>Title: Usability, Security, and Privacy of Information Systems: A Workshop<br\/><br\/><br\/>The National Academy of Sciences will organize and hold a workshop to involve the community in topics of increasing priority to NSF\/CISE: namely usability and privacy. Usability is concerned with providing tools to help users of all kinds (e.g., home users, enterprise administrators, network administrators, forensics analysts) manage the security and privacy of their systems. The privacy topic is very broad, as it deals with a range of challenges and issues, e.g., what is privacy, what mechanisms will help achieve it, how can privacy be measured, are their impossibility results relating to the attainment of privacy?","title":"Usability, Security, and Privacy of Information Systems: A Workshop","awardID":"0841126","effectiveDate":"2008-09-01","expirationDate":"2010-11-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["560890"],"PO":["529429"]},"143225":{"abstract":"This research involves the development of improved methods for<br\/>automatic processing of text documents. The research focuses<br\/>in particular on two specific text-handling problems: lossless<br\/>data compression and classification. Lossless data compression<br\/>is useful for reducing storage and transmission requirements<br\/>for large text documents. It is also an important step in<br\/>sending text securely via cryptography. Classification arises<br\/>frequently in web search problems and also in forensics, where<br\/>the goal is to determine the authorship of an anonymous document.<br\/><br\/>The approach is to draw new insights into these problems by<br\/>using a novel asymptotic regime in information theory, which<br\/>more accurately models real text sources than classical<br\/>models do. Specfically, the investigators consider a regime in which <br\/>the size of the data set and the source alphabet are comparably large. <br\/>It can be shown that most classical information theory techniques<br\/>fail in this \"rare-events\" regime. Nonetheless, new techniques can <br\/>be developed that are tailored to this regime that yield new algorithms <br\/>and insights.","title":"Collaborative Research: Information Theory for the Rare Events Regime","awardID":"0830496","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["518371"],"PO":["564924"]},"143467":{"abstract":"To counter the inefficiencies of the current spectrum usage, regulatory bodies, all over the world, are exploring ways to deregulate the spectrum market by allowing flexible dynamic spectrum access (DSA) in a broad range of spatio-temporal scale. Recent advances in radio technology have given an impetus to this trend. For DSA to fulfill its promise of economic and societal impact, wireless services based on DSA must be commercially successful, and a tangible spectrum market must evolve that can be supported by technology. This research project will build a realistic DSA architecture for cellular networks supported by appropriate market mechanisms in an integrated fashion that is both technically and economically viable and efficient. This is a truly trans-disciplinary approach spanning the fields of wireless networking and systems, algorithmics, economics, simulation and modeling, which leads to a deeper understanding of the dynamics of the spectrum market by (i) realistic modeling of various market entities (i.e., buyers, sellers, and the market mechanisms), (ii) dynamic spectrum demands and bids based on innovative and realistic population dynamics models, and (iii) new and robust market clearing mechanisms with provable performance guarantees. The results will be validated using large-scale simulations, and experiments on a prototype test bed with reconfigurable radio hardware. In addition to fostering new topics in trans-disciplinary education, this project will offer insights into market driven spectrum sharing, provide useful tools for policymakers, and ultimately guide spectrum policy decisions in DSA technology. This will, in turn, open up new business opportunities in the use of wireless spectrum.","title":"Collaborative Research: NECO: A Market-Driven Approach to Dynamic Spectrum Sharing","awardID":"0831633","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["459010","516909","516910"],"PO":["557315"]},"143236":{"abstract":"Classical image processing has mostly disregarded semantic image representations, in favor of more mathematically tractable representations based on low?]level signal properties (frequency decompositions, mean squared error, etc.). This is unlike biological solutions to image processing<br\/>problems, which rely extensively on understanding of scene content. For example, regions of faces are usually processed more carefully than the bushes in the background. The inability to tune image processing to the semantic relevance of image content frequently leads to the sub?]optimal allocation of<br\/>resources, such as bandwidth, error protection, or viewing time, to image areas that are perceptually irrelevant. One of the main obstacles to the deployment of semantic image processing systems has been the difficulty of training content?]understanding systems with large scale vocabularies. This is, in great part, due to the requirement for large amounts of training data and intensive human supervision associated with the classical methods for vocabulary learning. This research aims to establish a foundation for semantic image processing systems that can learn large scale vocabularies from<br\/>informally annotated data and no additional human supervision. It builds on recent advances in semantic image labeling, which have made it possible to learn vocabularies from noisy training data, such as that massively (and inexpensively) available on the web. The research studies both theoretical<br\/>issues in vocabulary learning, and the design of image processing algorithms that tune their behavior according to the content of the images being processed. Semantic image processing could lead to transformative advances in areas such as image compression, enhancement, encryption, de?]noising, or<br\/>segmentation, among others, which are of interest for applications as diverse as medical imaging, image search and retrieval, or security and surveillance.","title":"Large-vocabulary Semantic Image Processing: Theory and Algorithms","awardID":"0830535","effectiveDate":"2008-09-01","expirationDate":"2015-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"I198","name":"National Security Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"J168","name":"National Security Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"L565","name":"National Security Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"L570","name":"National Security Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"L585","name":"National Security Agency"}}],"PIcoPI":["513306"],"PO":["564898"]},"143478":{"abstract":"Providing each node in a multihop wireless network with one or <br\/>more multi-channel radios offers a promising avenue for improving <br\/>the networking performance. Recently, the impact of multiple channels <br\/>and radios on network throughput has been extensively studied. But <br\/>little is known about their impact on communication latency, which <br\/>is an important performance metric for a broad class of time-critical <br\/>applications such as emergency response and disaster recovery. While <br\/>it is of no doubt that multiple channels and radios hold potentials <br\/>of reducing the communication latency, what really matters is how <br\/>much benefit can be realized by specific algorithms. This research <br\/>conducts comprehensive algorithmic studies of minimizing the <br\/>communication latency by utilizing multiple channels and radios. <br\/>The algorithms developed in this research can be applied directly <br\/>to support real-time applications in multihop wireless networks. <br\/>The theoretical analysis of the performance against the number of <br\/>channels and\/or the number of radios can also provide guidance to <br\/>industries on cost-effective design of multihop wireless networks.<br\/><br\/>This research focuses on the communication schedules for four <br\/>primitive communication tasks which are involved in almost all<br\/>applications: broadcasting, aggregation, gathering, and beaconing. <br\/>The problem of seeking a shortest communication schedule for each <br\/>of these four tasks is NP-hard. The investigators develop <br\/>constant-approximation algorithms for them and analyze the impact <br\/>of the number of channels and the number of radios on the latencies <br\/>of the constructed communication schedules. This research is of <br\/>interest to a number of research communities, and may serve as a <br\/>basis for interesting future projects on multi-channel multi-radio <br\/>multihop wireless networks. Both student training and curriculum <br\/>development are integrated into this research.","title":"NEDG: A Universal Approach to Channel-Adaptive Resource Allocation and Scheduling for Wireless OFDM Networks","awardID":"0831671","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["540553"],"PO":["557315"]},"143247":{"abstract":"Solvent interactions play a critical role in determining the structure and<br\/>function of biomolecular systems. However, accurate modeling is a challenging<br\/>task due to long-range correlations and vast numbers of solvent atoms and ions.<br\/>For very large systems, one method for reducing this expense is the application<br\/>of an implicit solvent model which replaces the explicit atoms and ions with a<br\/>dielectric continuum. One of the more accurate implicit solvent models is<br\/>described by a generalized Poisson equation or Poisson-Boltzmann equation with<br\/>point charge source terms. Although the generalized Poisson approach is<br\/>considered in very many cases to be sufficiently accurate, most current methods<br\/>for approximating its solution have been deemed prohibitively expensive for<br\/>very large systems and for applications requiring rapid repetitive evaluation.<br\/><br\/>This project centers on the creation and analysis of algorithms of unsurpassed<br\/>effectiveness for approximating the solution to the generalized Poisson<br\/>equation (GPE) and its extensions. The approach involves solving for an<br\/>\"effective charge distribution\" given by the Laplacian of the solution<br\/>to the GPE. The corresponding electrostatic potential can be recovered using<br\/>a fast N-body solver. Broader impact of this project will be realized through<br\/>the incorporation of novel algorithms in simulation software. Better methods<br\/>and software in the hands of scientists will enable biomolecular simulation of<br\/>much larger systems with improved accuracy which will result in contributions<br\/>to society. In addition, this project provides an opportunity for a graduate<br\/>student to engage in research that spans computer science, physical science,<br\/>and mathematics.","title":"Collaborative Research: Laplacian-Centered Poisson Solvers and Multilevel Summation Algorithms","awardID":"0830578","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[381425,381426],"PO":["562984"]},"143368":{"abstract":"Abstract:<br\/><br\/>This research project focuses on the development of cryptographic mathematical models and constructions that address realistic security requirements at the implementation level. This is a fundamental problem as cryptographic security formalisms are often criticized for lack of relevance given the wide range of attacks available at the implementation level. Indeed, traditional cryptographic attacks are restricted in the way private data can be accessed; hence, the security of systems relying on such constructs is contingent on external non-cryptographic means for enforcing the necessary tamper resilience. Unfortunately, this physical tamper resistance is either too expensive or unreliable. The research extends models of cryptographic attacks to include various forms of private data tampering and access and brings the theory of cryptographic constructions closer to security concerns in practice. In particular, the tamper proofing of a wide set of cryptographic primitives is considered in an extended adversarial setting, such as digital signatures, public key encryption, secure function evaluation, as well as arbitrary cryptographic functions. This research thus explores the boundaries of what is achievable algorithmically and practically through cryptographic means.","title":"CT-ISG Collaborative Research: Tamper Proofing Cryptographic Operations","awardID":"0831094","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["485875"],"PO":["565327"]},"143258":{"abstract":"Error-correcting codes are a critical part of almost all digital communications and data storage systems, including cell phones, the Internet, space missions, and compact disks. Error control coding, also known as channel coding, protects data against transmission errors to ensure adequate transmission quality. The emergence of graph-based codes (such as turbo codes and low-density parity-check (LDPC) codes) with iterative decoding techniques has revolutionized channel coding. It is now clear that LDPC codes will be an essential part of future communication systems. Despite their astonishingly good performance, LDPC codes suffer from important problems. This research provides improvements in understanding and practical implementation of LDPC codes using new approaches introduced by the PI.<br\/><br\/><br\/><br\/><br\/><br\/>The areas of research considered are 1. Parametric Iterative Decoding: A novel decoding method for LDPC codes, called parametric iterative decoding, is investigated. The new decoding method provides better trade-offs among reliability, bandwidth usage, and cost than the standard iterative decoding. 2. Capacity-Achieving LDPC Codes: This research studies a new approach to the analytical design of capacity-achieving LDPC codes based on zero-rate codes and puncturing. Extensions of the ideas to construct universal rate-adaptive LDPC codes are also investigated. 3. Error Floor of LDPC Codes: This project develops a rigorous framework to answer the following questions: Given an accepted level of the error floor, what is the best performance we can obtain? How can we achieve this performance?","title":"Fundamental Trade-Offs and Parametric Decoding for LDPC Codes","awardID":"0830614","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["390131"],"PO":["564924"]},"143379":{"abstract":"Energy infrastructure is a critical underpinning of modern society that any compromise or sabotage of its secure and reliable operation will have a prominent impact on people's daily lives and the national economy. Past failures such as the massive northeastern power blackout in August 2003 and the recent Florida blackout in February 2008 have revealed serious defects in both system-level management and device-level designs. This project is aimed at a hardware-in-the-loop reconfigurable system with embedded intelligence and resilient coordination schemes to tackle the vulnerabilities of the power grid. The research in this project will investigate a number of critical issues related to this system, including the design of a hybrid system architecture, the development of advanced power electronic devices with embedded intelligence for reconfiguration of the power grid, the development of control- theoretic algorithms to guarantee real-time adaptation to system changes caused by various attacks, and the threats to existing state estimation algorithms and their defenses.<br\/><br\/>The proposed research can help secure the grid and prevent potential outages from happening. Instead of limiting the system to manage existing devices, the developed system is adaptive to the future power grid in years to come. The power grid is a typical example of complex networks of highly interacting subsystems. Solving these fundamental problems to create a resilient power grid has a direct and immediate impact on this and other critical infrastructures.","title":"CT-M: Collaborative Research: A Resilient Real-Time System for a Secure and Reconfigurable Power Grid","awardID":"0831165","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":[381789],"PO":["497499"]},"145205":{"abstract":"Creative problem solving lacks computational models that go beyond expected analogies or learning transfer. Many of the computational models that do exist exploit the semantics of the domain in which they are demonstrated. This project considers a sub-symbolic approach to a computational model of creative problem solving. The development of creative problem solving in sub-symbolic systems requires innovative research in knowledge representation, meta-learning and knowledge transfer mechanisms and will result in a well-grounded (sub-symbolic) computational explanation for several aspects of creativity: analogy, re-representation and insight. To explore and demonstrate a sub-symbolic approach to creativity, the project implements a system for learning the mappings inherent in the dimensions of a data manifold, and will implement a landmark-based meta-learning system for knowledge transfer mechanisms and knowledge representation. The project will analyze and empirically validate the system on a suite of problems. Broader Impacts: The proposed research will benefit society by making systems more capable of creative, human-like problem solving; and will integrate education with research by involving undergraduate and graduate students in research activities and by providing teaching and mentoring opportunities for the graduate students.","title":"Learning to Develop Insight: A Sub-symbolic Approach to Learning Transfer","awardID":"0840004","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["493953"],"PO":["491702"]},"143027":{"abstract":"Intellectual Merit: A major challenge in Nanoscience is the design of synthetic molecular devices that run autonomously (that is, without externally mediated changes per work-cycle) and are programmable (modi&#64257;able behavior without full redesign). DNA-based synthetic molecular devices have advantages of simplicity of design and engineering, due to predictable secondary structure and well-established biochemistry. <br\/>Tasks: The main Task is optimized design and experimental demonstration of molecular devices composed of DNA strands and DNAzyme. with unique advantages of being autonomous and programmable, requiring no protein enzymes. The experiments will be executed in carefully stages of increasing complexity. First to be demonstrated is a limited ability computational device, termed a DNAzyme calculator. Next to be demonstrated is a &#64257;nite state automata device, DNAzyme FSA, which executes &#64257;nite state transitions using DNAzymes. Further demonstrations will be extensions to probabilistic automata and non-deterministic automata. The keystone demonstration is a DNAzyme doctor device giving transduction of nucleic acid expression: programmed to respond to under-expression or over-expression of certain nucleic acid strands that may indicate a disease, and providing a response release of appropriate strands that can act as a drug remedy. Secondary Tasks are computer software for optimized design\/simulation using realistic stochastic models and correlated to laboratory tests. <br\/>Broader Impact: The development of autonomous and programmable molecular devices has a vast number of critical applications in the assembly and control of molecular scale devices. Although this work will be limited to the test tube to demonstrate fundamental capabilities, the impact of a DNAzyme doctor operating in vivo could be enormous in medicine. Important educational impact of the proposed research includes the highly interdisciplinary training of graduate students and postdocs.","title":"EMT\/NANO: Autonomous Programmable DNA Devices Using DNAzymes","awardID":"0829797","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["550822"],"PO":["565223"]},"143148":{"abstract":"High Order Kinetic-Based Quadrature Moment Method for Gas Particle Flows<br\/> <br\/>Abstract<br\/><br\/>This research involves developing and demonstrating a physics-based predictive tool for gas particle flows. Physical phenomena involving gas particle flows are ubiquitous in engineering. Spray combustion, pollutant transport, helicopter brownout are a few such examples. The understanding and prediction of gas particle flows are critical in engineering design to enhance performance (e.g. improved combustion efficiency) or mitigate harmful effects (e.g. helicopter brownout). The primary objective of this project is to develop a tool, which can be applied to a wide range of gas particle flows, and is fast and accurate enough to impact the design process. Predictive computational models developed in this work will have direct applications in the design of automotive and aircraft engines, chemical processing plants, rocket engines, diesel and spark ignition engines, and industrial furnaces. <br\/><br\/>Several methods are popular in gas particle multiphase flows, for example, the Lagrangian method, the volume of fluid method and the level set method. Although they all achieved success in some applications, they also possess some fundamental weaknesses. In this project, the investigators apply the recently developed direct quadrature method of moments to treat the kinetic equation in an Eulerian framework. Since it is not necessary to follow and resolve each individual particle, as in the Lagrangian method, the DQMOM method is several orders faster. In addition, an unstructured grid based high-order spectral difference method is employed to discretise the governing equations to achieve higher resolution, and better accuracy and efficiency for complex configurations than the current state-of-the-art second-order methods. The developed tool is validated with several benchmark test problems.","title":"Numeric Computing: A High-Order Kinetic-Based Quadrature Moment Method for Gas-Particle Flows","awardID":"0830214","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["556512","556512","511485","511485"],"PO":["565157"]},"143269":{"abstract":"Networked wireless communications over multiple hops is rapidly emerging as the main architecture of future wireles systems, including multihop extensions of cellular and WiFi networks, mesh networks, and sensor networks. Common among these types of networks is that they are not completely unstructured (or ad hoc) networks, but traffic is routed and accumulated towards a common destination. Due to this characteristic property, these networks will be referred to as Networks with Traffic Accumulation, or NETAs. Traffic accumulation creates hot sposts or bottlenecks around the common destination because of the increased traffic load and interference. Despite the severity of the hot spot problem, there is a lack of efficient methods to cope with it. <br\/><br\/>This research addresses the hot spot issue in NETAs by developing new distributed error correction strategies tailored to two important subclasses- line networks and tree networks. In line networks, the investigators study fundamental properties and the design of distributed channel coding protocals using serially concatenated and protograph-based constructions to strengthen the error correction capability near the destination without sacrificing badwidth efficiency. In tree networks, several source nodes may wish to employ a common relay node to broadcast their information to multiple destination nodes which may have access to side information from \"overheard\" source messages. The investigators explore a novel approach where each source uses a distinctlow rate code for transmission to the relay, whereas the decoded messages are re-encoded using a high rate \"nested code\". In addition, interlayer issues are considered, in particular the joint design of efficient channel access and routing schemes together with the porposed coding schemes.","title":"Collaborative Research: Distributed Error Correction Strategies in Wireless Networks","awardID":"0830666","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["551001"],"PO":["564924"]},"144006":{"abstract":"Project Abstract<br\/>Proposal Number: 0834357<br\/>PI: Anand Tripathi<br\/>Critical infrastructure services over the Internet are subject to constantly increasing threats posed by attackers. The goal of this project is to investigate a system architecture for building highly available Internet services using the mobile agent paradigm. In this research, mobile agent based techniques are being investigated for replication of critical components in different domains, dynamic regeneration of service components in case of attacks and failures, and distributed mechanisms to actively monitor the components of a service in different domains. Mobile agent-based mechanisms are utilized for dynamically replicating, regenerating, or migrating a service from one domain to another to ensure its continued availability, cognizant of network-wide and service-level operating conditions. Mobile agent based techniques are also used for detection and notification of critical conditions -- such as failures, intrusions, and overload conditions -- across different network domains for building resilient services over the Internet. A secure overlay network is used for isolating service components from clients, and for providing location-transparent access to migratory services. The overlay network also serves as a registry for storing service access policies and different communication models. Towards the broader impact, this projects aims to contribute to the research infrastructure in the form of an experimental system to serve as a testbed for building and experimenting with secure and resilient services.","title":"CSR-DMSS, SM: System Architecture for Resilient Internet Services","awardID":"0834357","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["74434"],"PO":["493916"]},"143049":{"abstract":"EMT\/QIS: GaAs hole spins as qubits: Eliminating hyperfine interaction-induced<br\/>decoherence<br\/>Quantum Computing is an emerging area of science and engineering. Its broad goal<br\/>is to harness the superposition of quantum states for computing purposes. The elementary<br\/>building block of a quantum computer is the quantum bit (qubit). Quantum bits have been<br\/>demonstrated in a wide variety of systems ranging from trapped ions, to electron spins in<br\/>semiconductors, to superconductors. In order to be useful, a qubit must be properly<br\/>initialized, measured, and coupled to other qubits. The quality of a qubit is characterized<br\/>by a lifetime (T1) and a coherence time (T2). These time scales vary by several orders of<br\/>magnitude from one system to another (e.g. trapped ion versus superconducting qubits).<br\/>It is therefore customary to quote a quality factor, Q, which is the ratio of the coherence<br\/>time to the typical gate operation time.<br\/>We will build qubits based on hole spins in GaAs nanometer-size quantum dots.<br\/>Hole spins are promising candidates since the hole wavefunctions have p-like orbitals. As<br\/>a result, hyperfine interactions with the host crystal nuclei are expected to be negligible,<br\/>leading to spin coherence times approaching the hole spin lifetime of 300 microseconds.<br\/>Taking into account the typical gate operation time of ~150 ps, a hole spin qubit, if<br\/>realized, could have a quality factor of nearly 2 million, well beyond the threshold for<br\/>fault tolerance. The primary goal of this project is to accurately measure, and determine<br\/>what limits, the quantum coherence times of hole spins in GaAs quantum dots.","title":"EMT\/QIS: GaAs hole spins as qubits: Eliminating hyperfine interaction-induced decoherence","awardID":"0829872","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["390985",380870,"490408","543750"],"PO":["565157"]},"144028":{"abstract":"CSR-DMSS, SM: DoC - Distributed Opportunistic Computing (0834493)<br\/><br\/>The objective of this project is to carry out preliminary, fundamental research work in the area of opportunistic computing. When pairs of devices come within each others? communication range, opportunistically, short-lived links (or opportunistic links) are created. Opportunistic computing exploits the opportunistic links created by pair-wise contacts, to share information content, resources and services, leading to a wide variety of applications. Groups of computing nodes and their associated pair-wise contacts in an opportunistic network give rise to a distributed opportunistic computing system. Essentially, opportunistic computing can be described as distributed computing with the caveats of intermittent connectivity and delay tolerance.<br\/><br\/>The novelty of the proposed work lies in the exploitation of opportunistic communication contacts to provide collaborative computing services to applications and users. The unique contributions of this project include the development of an adaptive protocol for opportunistic communication in support of computing, a middleware architecture for masking the disruptive nature of the underlying network from applications and users, and a mechanism for delay tolerant, remote execution of tasks. <br\/><br\/>The research will lead to tangible contributions to the improvement in human quality of life and make significant impact on Internet based applications, computer science and engineering. The project outcomes have relevance to applications in crisis management situations, entertainment, transportation, education and military. In general, the outcomes of this project have broad applicability in such areas as entertainment, health care, transportation and automobiles, military and education. Research findings will be incorporated into graduate and undergraduate courses.","title":"CSR-DMSS, SM: DoC - Distributed Opportunistic Computing","awardID":"0834493","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["486502","492475"],"PO":["535244"]},"144039":{"abstract":"Abstract for Dr Alyssa Apsel<br\/>An integrated low power radio transceiver combined with a backend processor in a single networkable node enables applications that are not possible with traditional sensor node platforms. If such a node can be mass produced in a CMOS process without a costly external crystal such that the network formed by a set of homogenous nodes is robust to network changes or any node failing, then reliable ad-hoc networks may be formed from sets of these nodes with group intelligence akin to a distributed machine. This project focuses on the development and exploitation of a distributed computing machine formed out of a network of integrated single chip microsystems that combine both communication and computation functions with a total power budget that is under 50\u00ecW. Low power performance, critical to long lifetimes is achieved through duty cycling of the transceiver and establishment of a global clock via natural phenomenon of pulse coupled oscillators (PCOs) as well as the use of simple event driven computation. Although each node has limited computing capability, the power of such a network will arise from aggregate capabilities to perform distributed computing tasks. This project explores three issues; the construction of the network from simple nodes, communication within the network, and the computing potential for such a constrained platform. The final goal of this work is to develop a complete network with computing models and primitives capable of exploiting the network?s unique features.","title":"CSR-DMSS,TM: Distributed Computing With an Ad-Hoc Network","awardID":"0834582","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["475375","550699",383969],"PO":["565255"]},"137626":{"abstract":"Spurred by financial scandals and privacy concerns, governments worldwide have moved to ensure confidence in digital records by regulating their retention and deletion. The goal of this project is to develop and explore a database management system (DBMS) architecture that supports a spectrum of approaches to regulatory compliance, thereby extending the level of protection afforded by conventional file-based compliance storage servers to the vast amounts of structured data residing in databases. The key challenge of this work is to provide compliance assurances for the DBMS, even against insiders with super-user powers, while balancing the need for trustworthiness against the conflicting requirements for scalable performance guarantees and low cost. The resulting architecture provides tunable tradeoffs between security and performance, through a spectrum of techniques ranging from tamper detection to tamper prevention for data, indexes, logs, and metadata; tunable vulnerability windows; tunable granularities of protection; careful use of magnetic disk as a cache and of secure coprocessors on the DBMS platform and compliance storage server platform; and judicious retargeting of an on-disk encryption unit.<br\/><br\/>This work enables compliance laws to be applied to business, government, and personal data now stored in databases, increasing societal confidence in such data. A new web course on compliance data management will raise the computer science community's awareness of compliance issues and will help train a new generation of professionals cognizant of these challenges and solutions. The software prototypes and technical papers describing them will be disseminated through the project's web site http:\/\/web.crypto.cs.sunysb.edu\/cdb\/","title":"III-COR Medium: Collaborative Research: Achieving Compliant Databases","awardID":"0803280","effectiveDate":"2008-09-01","expirationDate":"2014-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["438346"],"PO":["565136"]},"138847":{"abstract":"Proposal Number: 0808907<br\/>PI: Ronald L Rivest<br\/>Institution: Massachusetts Institute of Technology<br\/>Title: SGER: Cryptographic Techniques for Trustworthy Computation in Faulty <br\/> and Non-Confining Execution Environments<br\/><br\/>The objective of this exploratory research is to find methods for conducting secure computation within insecure, imperfect, and possibly malicious environments. To realize this goal, two complementary approaches are being pursued. The first one, inspired by the technique of ?proof carrying code,? is a ?Proof-Carrying Data? framework that can address many of the difficulties with current approaches. In this framework, the system designer prescribe the desired properties of the computation?s output, usually expressing a security or privacy property. Proofs of these properties are attached to the data flowing through the system, and are mutually verified by the system?s components. <br\/>The second research focus is the investigation of means by which concrete realizations of computation can guarantee security properties despite inevitable risks. The novelty in the method is that the assumptions on which the risks are based are enumeratable, so they can be checked to be sure they that are, ideally, minimal, realistic, and verifiable.","title":"SGER: Cryptographic Techniques for Trustworthy Computation in Faulty and Non-Confining Execution Environments","awardID":"0808907","effectiveDate":"2008-09-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":[369204],"PO":["529429"]},"147229":{"abstract":"This Small Grant for Exploratory Research (SGER) award is to explore foundational research to study logical, timing, and probabilistic properties of a discrete state model, which is quite challenging because it is a strictly symbolic approach that combines algorithms from diverse areas, ranging from logical verification to numerical analysis and statistics. The work could lead to radically new approaches for modeling and analyzing today?s increasingly complex computer-based systems, with broader impacts from efficient techniques for achieving higher software reliability.","title":"SGER: Symbolic Computation of Bounds on Timing and Probabilistic Properties of Computing Systems","awardID":"0848463","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["451554"],"PO":["564388"]},"139617":{"abstract":"A fundamental challenge in artificial intelligence is to achieve intelligent coordination of a group of decision makers in spite of uncertainty and limited information. Decision theory offers a normative framework for optimizing decisions under uncertainty, but due to computational barriers, developing decision-theoretic reasoning algorithms for multi-agent systems is a serious challenge. This project will advance foundational contributions to the understanding of decision-theoretic planning in stochastic multi-agent domains as well as the development of efficient new algorithms that provide exponential savings in memory requirements and computation time. Moving beyond toy problems is a hard computational challenge that has been broadly recognized by the multiagent systems community. Research under this project will transform the ability of researcher and practitioners to apply decision-theoretic planing to a new range of domains.","title":"RI-Small: Decision-Theoretic Planning for Multi-Agent Systems","awardID":"0812149","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["485994"],"PO":["564316"]},"139628":{"abstract":"The goal of this research is to allow a (live) conductor to create a personal musical performance by controlling a computer-driven (virtual) orchestra through gesture captured on video. The immediate focus is to provide an educational tool for conducting students, or others with some serious musical training who are capable of communicating musical intent clearly through the traditional language of gesture used by conductors. However, the PI expects that less-schooled or novice users will also be able to learn from and find enjoyment with the outcome of this research, which will culminate in a computer system that runs on generic computer hardware, and will be made freely available. The system will take video of a conductor as input, reducing this input into a two-dimensional conducting trace that describes the movement of the tip of the conductor's baton over time. The system will perform real-time estimation of the conductor's precise \"state\" within the composition, using an approach that fuses hidden Markov model methodology with a Kalman filter model for musical timing. Using this on-line estimate, the system will predict the location of future musical events, thus addressing the inevitable issue of detection latency. Concurrently, the system will synthesize real-time audio to follow the conducted performance, using a previously recorded performance whose timing is continually warped using phase-vocoding. The initial focus of this work will be on musical timing rather than dynamics, articulation, etc., as this is the aspect of conducting that is most clearly communicated through motion and usually also that which affords the most expressive potential and sense of \"ownership\" of the performance. Educated musicians find surprising agreement when evaluating the accuracy with which a musician or ensemble follows a knowledgeable conductor, suggesting that the conductor's \"signal\" must be relatively unambiguous. Making mathematical sense of the relationship between this signal and its meaning constitutes a challenging dimension of this research.<br\/><br\/>Broader Impacts: This work will have lasting impact on conducting pedagogy, by providing a tireless and responsive laboratory for musical experimentation. The research will also make contributions to instrumental and voice pedagogy, by allowing a musician to focus on the interpretive aspects of a piece without simultaneously addressing the technical challenges. The problem of planning the orchestra's musical evolution with uncertain and continually evolving knowledge of the conductor's actions is deeply challenging; thus, this work has implications for the general domain of planning under uncertainty. Perhaps most importantly for society at large, a successful conducting system would bring the pleasure of music-making to a broad and international collection of users who might otherwise have little or no experience creating music.","title":"RI-Small: Real-Time Planning of a Conductable Orchestra","awardID":"0812244","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["535948"],"PO":["565227"]},"139639":{"abstract":"Automatic paraphrasing is considered vital to applications as diverse as machine translation (MT), question answering, summarization, and dialogue systems. Paraphrasing has also been shown recently to hold promise for automatic methods of evaluating MT, when the paraphrases are of sufficiently high quality. <br\/><br\/>This project investigates novel methods for acquiring and generating such high<br\/>quality paraphrases in order to automatically approximate the human translation<br\/>error rate (HTER) metric for MT evaluation, where human annotators post-edit MT<br\/>outputs into acceptable paraphrases of the reference translations. The project<br\/>emphasizes the use of a linguistically informed, grammar-based parser and<br\/>realizer for acquiring and generating paraphrases using disjunctive logical<br\/>forms (DLFs), in sharp contrast to most recent work that relies entirely on<br\/>shallow methods. Specifically, the project investigates methods of (1)<br\/>engineering a broad coverage English grammar from the CCGbank, with semantic<br\/>roles integrated from Propbank; (2) scaling up OpenCCG for efficient parsing and realization with this grammar, adapting supertagging and parse ranking methods for generation; (3) adapting and extending previous methods of acquiring paraphrases to work on DLFs; (4) generating high quality n-best paraphrases of one or more reference sentences; and (5) experimentally evaluating whether the automatically generated paraphrases can be used with current MT metrics to yield improved correlations with human judgments of translation quality. <br\/><br\/>By providing a way to automatically approximate the HTER metric, the project will help drive future MT research. Additionally, by dramatically extending the realization capacity of OpenCCG, the project promises to benefit a wide range of NLP tasks where the breadth of target texts is of crucial importance.","title":"RI-Small: Learning to Generate High Quality Paraphrases with a Broad Coverage Lexicalized Grammar","awardID":"0812297","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["553805"],"PO":["565215"]},"139529":{"abstract":"A source of weakness in parallel programming has been the lack of compositionality; independently-written parallel programs, e.g., libraries and packages, can't be combined to produce large parallel programs with predictable performance or functionality. One may have the best parallel implementations of FFT and Linear Equation Solvers and yet a large parallel program that calls both of these procedures in parallel may not run well. The problem is much worse for applications that are inherently parallel but produced by combining existing sequential implementations. Consider your cell phone: it may drop an incoming call while you are surfing the web, or not stop playing the music when you answer a call. The problem is compositionally -- two independently written programs don't interact properly in a parallel setting.<br\/><br\/>A scalable model for software composition is space-multiplexing, where a set of resources (e.g., processors and memory) are devoted exclusively to executing a given program fragment or module. This research plans to define and use Bluesoft -- a derivative of the Bluespec SystemVerilog (BSV) -- as its source language. Bluespec?s semantics are well understood and there is substantial experience in using it for hardware design in both academia and industry. Bluesoft adds to Bluespec a ?sequential connective? for combining atomic actions, a crucial feature needed for software design. The Bluesoft compiler will combine the strategies used for compiling Bluespec and StreamIt, a stream-based language developed at MIT for tiled architectures. The Bluesoft compiler will borrow the static elaboration phase of the BSV compiler to generate a network of modules and use the sophisticated program analysis and code generation techniques used in the StreamIt compiler for the backend.","title":"CPA-CPL: A hardware-design inspired methodology for parallel programming","awardID":"0811696","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":[370891,"382922"],"PO":["565272"]},"142973":{"abstract":"Behavior-based robotics was established around the idea that robots could be constructed by connecting elementary sensor and actuator modules, without the need to form any internal representation of the world in which they operate. When designed appropriately, such robots, both as individuals and as groups, exhibit complex and seemingly ?intelligent? behaviors, solving challenging tasks in real time, while reacting only to their local environment and obeying sets of local rules. In part, this approach to robotics was inspired by considering natural systems, such as self-organization and adaptability of social insects in their colonies. An analogous approach to initiate the development of the field of behavior-based molecular robotics will be performed over the next three years, leading to groups of molecules that would appear to an observer to show a variety of task-oriented behaviors or some form of purposeful and dynamic self-organization. <br\/><br\/>Individual behavior-based molecular robots are single molecules displaying multiple sensors-actuators. When exposed to artificial landscapes displaying substrates keyed to their sensors-actuators, the molecules start executing elementary steps, determined, in a stochastic sense, by their constantly changing local environments. On some landscapes, called prescriptive, individual molecular robots and their collectives will execute algorithms mimicking wound-up automata with sequence control mechanisms. In contrast, on non-deterministic landscapes, the robots and their collectives will demonstrate properties emerging from internal organization of individual sensors and actuators and through local interactions between molecules and their environments. Importantly, these new interpretations of molecular behaviors will allow radically different experiments from all previous approaches to molecular robotics, while keeping experimental designs realistic, leading to embodiment in the physical world.","title":"Collaborative Proposal: EMT\/MISC Behavior Based Molecular Robotics","awardID":"0829552","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[380642],"PO":["565223"]},"143710":{"abstract":"The goal of this research is to contribute to our knowledge of the relationship of values (i.e., moral motivation and moral commitment) to ethical sensitivity and ultimately to moral functioning, an area of the theory of moral pedagogy on which little research has been done, especially within ethics education in science and engineering. To these ends, the PI will first inventory the personal and professional values of a diverse group of graduate students. He will then assess the impact of such values on ethical sensitivity to issues of responsible conduct of research (RCR), using a tool created as part of this study to measure ethical sensitivity. The research will examine role concept and moral motivation as predictors of ethical sensitivity within a sample of multi-national science and engineering graduate students. The PI will seek correlations between these students' personal and professional values, and their ability to discern multifaceted aspects of situations involving issues in RCR such as intellectual property, plagiarism, etc. Using value surveys, the PI will gauge students' moral motivation for, commitment to, and emotional engagement with issues involving RCR. Ultimately, he hopes to discover which value-sets, both personal and professional, are indicative of greater or lesser levels of ethical sensitivity to RCR. <br\/> <br\/>Broader Impacts: This research addresses one of the major challenges facing graduate student ethics education. This is because, relative to undergraduates, the graduate science and engineering student population represents a far more diverse cultural and national demographic, and brings with it a greater diversity of personal and professional values. Project outcomes will help other educators and researchers, by providing them with the information they need to modify their own teaching methods appropriately and to spur research into creative and innovative approaches to educating graduate science and engineering students in ethics. This will be accomplished through presentations, journal publications, and dissemination through Michigan Tech?s Center for Educational Technology, Research, and Assessment (CETRA). The impact of this work will extend not just to universities but also to society at large, since today's students will be tomorrow's faculty members and researchers in private industry.","title":"Responsible Conduct of Research in Science and Engineering Education: Moral Motivation and Ethical Sensitivity in Multi-National Graduate Students","awardID":"0832922","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7179","name":"GRAD TEACHING FELLOWS IN K-12"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7787","name":"EESE"}}],"PIcoPI":[382828,"455183",382830,382831,382832,"536945",382834],"PO":["493916"]},"142984":{"abstract":"The approach to improving computing education is to connect computer science teachers at different levels in order to develop a community focused on common goals and activities that will revitalize both undergraduate and high school computing education. The disciplinary commons of computing educators (DCCE) takes the path of transforming computing education by transforming computing educators. This project will focus on introductory computer science teacher in Georgia. Any change or reform in education must involve the teachers; this project starts with the teachers. The strategy is to gather introductory computer science teachers from different universities along with high school computer science teachers, especially advanced placement teachers. These teachers naturally have common interests in approaches and common topics to discuss. High school teachers are interested in what happens at the post-secondary level and how best to prepare their students. University teachers want to enhance the transition of students from high school and to encourage more students to learn computing. The discussions are encouraged by developing engaged researchers and community leaders. Different approaches will be supported, with ideas seeded into the community. The ongoing research will enhance the teaching done, which has been shown to contribute to student learning. The project will create a model for teachers in other areas of computing and the rest of the country.","title":"CPATH CB: Improving Computing Education by Developing Regional Communities of Computing Educators","awardID":"0829601","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["521278"],"PO":["565136"]},"142995":{"abstract":"This CPATH award funds a collaborative Transformative Implementation project between SUNY Stony Brook and Hofstra University to integrate entrepreneurship and leadership components into the undergraduate curricula at both institutions. The project includes development of modules about innovation, entrepreneurship, and global aspects in several required courses and development of an entrepreneurial computer science minor. In addition students work in global entrepreneurial teams with students at targeted universities in Germany, Romania, and Korea. The project includes community building activities with faculty in the greater New York area to promote adaptation and adoption of the curricula models and resources developed. <br\/><br\/>The intellectual merit of the project lies in the importance and currency of the topic and clear need for such changes in computing education to prepare the upcoming generation of computing professionals. The project has a strong collaborative team with entrepreneurial and educational experience and an enhanced evaluation component that should clearly demonstrate the impact of this innovative approach to the research and education community. The project also includes testing of methodologies for handling associated intellectual property issues that could be of value to many other researchers in the future. <br\/><br\/>The broader impacts of the project lie in the potential to prepare a diverse student and faculty population to pursue entrepreneurial activities in high-technology. The project includes dissemination to a broad community and opportunities for sharing of resources. There is potential for national models that can help to develop a technology-savvy workforce which is vital to the nation?s continued prosperity and security.","title":"Collaborative Research: CPATH TI: Project EXCE2L (Excellence in Computer Education with Entrepreneurship and Leadership Skills)","awardID":"0829656","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["410650","410653","410651","410652","397939"],"PO":["564181"]},"143985":{"abstract":"Real-time embedded systems (RTES) found in many real-world applications demand that timing requirements for computations and communications are satisfied. In addition, energy consumption is becoming a major consideration for such systems, because of the proliferation of mobile, wireless, and embedded systems with limited energy resources. Energy consumption significantly impacts the cost, performance, and life time of such systems. This research addresses the lack of system-wide energy conservation approaches by developing integrated scheduling techniques to reduce the overall energy consumption of wireless real-time systems while meeting applications' timeliness requirements. Integration in this context refers to reducing the energy consumption of a system as a whole by maximally exploiting the Dynamic Voltage Scaling (DVS) capabilities of CPU cores and the Dynamic Power Management (DPM) capabilities of other resources and devices. Important results of this work are novel and highly applicable algorithmic techniques for energy-efficient RTES. The broader impacts of this project can be found in both education and research, in addition to its enormous social impacts due to the pervasiveness of real-time systems and environmental concerns. The results of the experimental studies are being made available online for other researchers to pursue their own research and education interests.<br\/>The effort also helps greatly to alleviate the problem of many theoretical system-level power reduction techniques not being adopted due to the lack of sound studies based on actual hardware platforms.","title":"CSR-EHCS(EHS), SM: Collaborative Research: Integrated Energy-Aware Resource Scheduling for Wireless Real-Time Systems","awardID":"0834230","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383825],"PO":["561889"]},"143512":{"abstract":"Abstract for proposal #0831874 <br\/>Energy constraint has persisted as a fundamental problem in sensor networks, especially when the networks are required to operate for a long time. Although many solutions have been proposed to address the problem, their limitations are salient: resource-conservation schemes can slow down energy consumption but cannot compensate energy depletion; current environmental resource harvesting schemes have low, unstable efficiency due to uncontrollable environmental conditions and technological limitations; incremental deployment may cause environment pollution and may be too costly. This project leverages the emerging wireless energy charging technology to address the problem from a brand new perspective. Specifically, a hierarchical architecture is first proposed to include an energy provision station as the stable source of energy, a few autonomous mobile energy chargers, and a large number of sensor nodes. On top of the architecture, schemes are developed to solve a unique three-tier, long-term, multiple-time and on-line scheduling problem present in resource replenishment. The architecture and schemes are evaluated through prototyping and extensive experiments. The project will potentially transform the current research on energy management in sensor networks, improve the sensor network performance from both practical and theoretical aspects, and thus promote the wider application of sensor networks in structural health monitoring, smart factories, garden\/orchid monitoring, road\/traffic monitoring and so on. It will also train a diverse cadre of young scientists, students and professionals for network research and applications.","title":"Collaborative Research: NeTS-NECO: Energy Replenishment for Wireless Sensor Networks","awardID":"0831906","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["492076"],"PO":["557315"]},"143996":{"abstract":"Multi-core systems have become mainstream in everything from server to embedded computing markets. As hardware and software designers adopt transactional memory as a cost-effective multi-core programming paradigm, evaluating TM\/multi-core design, using a wide spectrum of transactional applications, is crucial to quantify the trade-offs among different design criteria. One roadblock that the TM\/multi-core research and design community faces is the lack of representative transactional memory benchmarks. This project aims to develop innovative techniques, methods, and new tools for accurate, effective, and automatic transactional memory workload synthesis to address these challenges. This may be significantly beneficial to the multi-core architecture research and design community. The synthetic TM benchmarks preserve the behavior of original multithreaded workloads while the parameterized transaction synthesizer can produce transactional code with widely varied behavior that can effectively stress the TM design in different dimensions. Moreover, the parameterized synthetic workloads will have significantly reduced runtime, which allows architects and designers to quickly explore the large TM\/multi-core co-design space within which numerous design tradeoffs need to be evaluated. The successful development of innovative transactional memory workload synthesis techniques, methods and tools will have significant impact on the design, evaluation and optimization of future multi-core based systems. Specifically, it will: (1) significantly improve multi-core processor and system design and research productivity; and (2) largely ensure that the design is really beneficial to multi-core oriented applications. This project's integrated education and outreach plan will help to educate a broad spectrum of citizens and students. This includes expanding computer architecture curriculum with advanced modeling and evaluation methods, recruiting minorities and undergraduate students into research, and collaborating with the computing industry to integrate the developed technologies into real-world multi-core processor design flow.","title":"CSR-PSCE, SM: Automatic Multithreaded and Transactional Memory Workload Synthesis for Efficient Multi-core Design Space Evaluation","awardID":"0834288","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550720"],"PO":["535244"]},"143402":{"abstract":"ABSTRACT:<br\/><br\/>Secure applications require trustworthy hardware for successful deployment. A trustworthy hardware device (e.g., a smart card) should maintain its security properties even against efforts at probing and reverse engineering; moreover, sensitive data stored on a trustworthy hardware device should be protected at all times. Side-channels attacks are used to learn the secrets stored by a device through monitoring the side effects of its computation. The well known power side-channel attack uses the effect that a cryptographic key has on the power waveform as the cipher runs. Another side-channel attack, the focus of this project, is the Differential Fault Attack (DFA). DFAs alter the computations performed by a trustworthy device and use the faulty results to uncover secrets. An approach to defeating DFAs is to continuously check dynamic assertions, made part of its cryptographic algorithms (including, e.g., symmetric block and stream ciphers, asymmetric ciphers, and message authentication), against DFA. The project advances secure hardware design methods and methodologies, develops course material on side-channel attacks and countermeasures, and promotes technology transfer through partnerships with industry. The project has a strong education component with opportunities for international research experience.","title":"CT-ISG: Collaborative Research: Fault Tolerance in Crypto Hardware via Dynamic Assertion Checking","awardID":"0831301","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":[381849],"PO":["497499"]},"143523":{"abstract":"The Internet faces many challenges, including security, management and control. The research project explores a future Internet\/networking architecture based on a well-proven hierarchical architecture that is untested at global levels. The architecture includes a flexible addressing scheme and a compact four layer protocol stack. The radical change to the Internet architecture would be potentially transformative from a socioeconomic and business perspective. The scientific problems to be addressed are hence multidisciplinary. The technical and socioeconomic feasibility for the new network architecture, as well as the likely patterns of adoption for such a radical change are investigated.<br\/> <br\/>Intellectual merit: This research asks how a fully hierarchical architecture, with adequate redundancy at different levels in this design, results in a robust structure that is also scalable. The solution distributes the \"switching decisions,\" the management of how traffic moves from one point to another, to all levels of a hierarchy of communications providers: it allows the traffic to be managed independently wherever the traffic is flowing. The replaces present highly interdependent routing and network management architecture. The research will show how this architecture is robust and stable with a potential for future growth. A key contribution of the architecture is a scalable and flexible hierarchical addressing scheme, where key network functions are relegated to different levels in the hierarchy, and this also provides better management and control. The research includes proof-of-concept by interfacing diverse wireless networks in the architecture.<br\/><br\/>Broader impact: A new network architecture resulting from the SWITCHNET research could create radical changes in the economics of communications networks and ultimately have a very large impact on the economy and society. The conceptual architecture is almost ideally scalable. It is very much simpler to manage than the present architecture. As an example of the broad impact of SWITCHNET manageability, by enabling fundamentally better provider control, the architecture would open the door to new methods of identifying and pushing back denial of service, an economically and nationally much needed capability. Furthermore, this research could provide the breakthrough architecture for attaining the all-optical goal and thus address that national challenge to create a more energy-efficient network. Approaches to achieving such a purely optical communications infrastructure have long been sought.","title":"Collaborative Research: NeTS-FIND: SWITCHNET: A Switched Internetworking Architecture with Contracted Services","awardID":"0831957","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["551119"],"PO":["565090"]},"143765":{"abstract":"Scaling scientific problems to 10,000,000 processors for the next generation HEC systems is today severely challenged by conventional practices of programming models, languages, and their supporting compilation systems. To achieve this goal one must expose greater degree of parallelism and improve parallel computing efficiency than is otherwise feasible with conventional methods such as MPI. <br\/>The goal of this collaborative research project is to dramatically enhance the scalability of challenging physics problems, through the application of an innovative programming model. The strategy is to replace static message-passing course grained processes using global barrier synchronization in a distributed memory space with a model using dynamic message-driven multiple threads using lightweight synchronization objects in a partitioned global address space. Parallelism is to be extracted directly from the large irregular sparse and time varying data structures. Ephemeral user-threads will permit many simultaneous tasks over the data structures, exposing the intrinsic near-fine grain parallelism. System-wide latency will be hidden by overlapping computation with communication through the advanced communication strategy of asynchronous message-driven processing. Consequently this will enable a class of physics problems that cannot currently be done using conventional methods.","title":"Collaborative Research: A Study and implementation of Semantic Constructs for Highly Scalable Leading-edge Scientific Computing","awardID":"0833090","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["545178"],"PO":["565272"]},"143534":{"abstract":"The Internet faces many challenges, including security, management and control. The research project explores a future Internet\/networking architecture based on a well-proven hierarchical architecture that is untested at global levels. The architecture includes a flexible addressing scheme and a compact four layer protocol stack. The radical change to the Internet architecture would be potentially transformative from a socioeconomic and business perspective. The scientific problems to be addressed are hence multidisciplinary. The technical and socioeconomic feasibility for the new network architecture, as well as the likely patterns of adoption for such a radical change are investigated.<br\/><br\/>Intellectual merit: This research asks how a fully hierarchical architecture, with adequate redundancy at different levels in this design, results in a robust structure that is also scalable. The solution distributes the \"switching decisions,\" the management of how traffic moves from one point to another, to all levels of a hierarchy of communications providers: it allows the traffic to be managed independently wherever the traffic is flowing. The replaces present highly interdependent routing and network management architecture. The research will show how this architecture is robust and stable with a potential for future growth. A key contribution of the architecture is a scalable and flexible hierarchical addressing scheme, where key network functions are relegated to different levels in the hierarchy, and this also provides better management and control. The research includes proof-of-concept by interfacing diverse wireless networks in the architecture.<br\/><br\/>Broader impact: A new network architecture resulting from the SWITCHNET research could create radical changes in the economics of communications networks and ultimately have a very large impact on the economy and society. The conceptual architecture is almost ideally scalable. It is very much simpler to manage than the present architecture. As an example of the broad impact of SWITCHNET manageability, by enabling fundamentally better provider control, the architecture would open the door to new methods of identifying and pushing back denial of service, an economically and nationally much needed capability. Furthermore, this research could provide the breakthrough architecture for attaining the all-optical goal and thus address that national challenge to create a more energy-efficient network. Approaches to achieving such a purely optical communications infrastructure have long been sought. <br\/><br\/>This research is a close collaboration between technical and business researchers; the architecture research is supported by research on its business and socioeconomic impacts and adoption patterns. Understanding the existing Internet industry and creating appropriate incentives for its stakeholders will increase the chances that the technological innovations in this research will be embraced and implemented.","title":"Collaborative Research: NeTS-FIND: SWITCHNET: A Switched Internetworking Architecture with Contracted Services","awardID":"0832008","effectiveDate":"2008-09-01","expirationDate":"2012-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[382244,382245],"PO":["565090"]},"143776":{"abstract":"Peta-scale systems have tens of thousands to millions of cores. To exploit the performance for applications such as climate prediction, environmental modeling, astrophysics, biology, life-sciences, etc, the problems of programming these systems must be solved. Programming language enhancements, compiler techniques and runtime support must be developed to enable computing and knowledge discovery at this unprecedented scale. The challenges faced by a user in programming these machines include performance, power, productivity, and portability, which are inter-related in a complex way. <br\/><br\/> This project entails the design and development of programming language, programming model, and compilation optimizations for I\/O and storage performance and power optimizations. The project is investigating ?What minimal set of changes or enhancements to programming models, programming languages, and what optimizations to compilers and runtime systems are needed to enable better I\/O, file and storage systems performance while optimizing power and improving productivity?? Some specific questions include: What language enhancements can be used by to specifically improve the I\/O and storage performance? Should interfaces be developed that can be used across languages for I\/O? What compiler optimizations are needed? Can the compiler identify transform codes that can inform the I\/O runtime and storage systems on phases to power-down disks to save power at certain times as required? The project tasks include: the design and development of programming-language enhancement; the design of a compilation framework and performance- and power-oriented I\/O optimizations using novel compiler analyses; and the design of a novel hint-handling mechanism within the I\/O stack.","title":"Collaborative Research: Advanced Compiler Optimizations and Programming Language Enhancements for Petascale I\/O and Storage","awardID":"0833126","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["542016"],"PO":["565272"]},"143303":{"abstract":"Computational Complexity excels at posing fundamental questions with far-reaching consequences regarding the nature of computation, but so far it has been not nearly as successful at answering them. To propel the field forward, broadly useful tools and techniques must be cultivated, and new ones invented.<br\/>This project aims to significantly broaden the reach of error-correcting codes as a powerful tool to attack central problems in Complexity (and one fundamental problem in Algorithms). Error-correcting codes lie at the core of some of the deepest results in Complexity; recent developments in the area reveal possible routes to a number of further breakthroughs.<br\/><br\/>The PI will pursue research organized in the following three<br\/>thrusts: (1) developing a generalization of Parvaresh-Vardy codes possessing a crucial feature -- local decodability -- often exploited in Complexity applications, with applications to derandomization and surrounding problems; (2) devising a real analog of error-correcting codes possessing an approximate version of the defining feature of error-correcting codes -- that two codewords that differ in one coordinate must differ in most coordinates -- with a concrete application to proving circuit lower bounds in the complexity class MA; and (3) utilizing error-correcting codes to bridge the gap between \"approximate\" and exact algorithms for matrix multiplication, with the intention of obtaining an optimal algorithm for matrix multiplication using a group-theoretic approach developed by the PI and coauthors.<br\/><br\/>The overall goal is to develop new tools and techniques around error-correcting codes, while attacking well-motivated and significant problems in different application domains. Resolving fundamental problems in Complexity and Algorithms in turn enhances our understanding and mastery of efficient computation.","title":"New Applications of Error-Correcting Codes in Complexity and Algorithms","awardID":"0830787","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["485618"],"PO":["565157"]},"143424":{"abstract":"National Science Foundation<br\/>NSF Cyber Trust Program<br\/><br\/>Proposal Number: 0831409<br\/>Principal Investigator: David Kotz<br\/>Institution: Dartmouth College<br\/>Proposal Title: CT-ISG: Dartmouth Trace Sanitization Framework<br\/> <br\/>Project Summary<br\/><br\/>Computer-network research advances more quickly when researchers are able to analyze the activity of live computer networks. Because it is difficult to collect traffic traces from production computer networks, it is critical for the network-research community to share traces. Researchers who capture network traces and wish to share them, of course, must properly \"sanitize\" the trace to remove sensitive information. Sanitization always involves a challenging trade-off between sanitization effectiveness (providing anonymity for network users and secrecy for network operational information) and research usefulness (since only the information retained can be used by the researcher). This project aims to increase network-trace sharing by making it safer and easier to sanitize network traces. To this end, the project will develop and release NetSANI (Network Trace Sanitization and ANonymization Infrastructure), a flexible and extensible suite of software tools for sanitizing network traces, based on user-specified sanitization goals and user-specified research goals. The tools will be verified on extensive traces collected at Dartmouth College, and evaluated by providing early releases to external collaborators who will test the tools on their traces. The NetSANI project will have broad academic and practical impact: (a) better tools will enable and encourage more network-trace sharing, which helps the research community do better research, (b) better access to network traces will help companies develop better network products, and (c) better anonymization methods will protect network users' privacy. The outreach efforts will help spur the research community into defining norms and best practices for trace sanitization. Finally, the project will involve graduate and undergraduate students in research and incorporate research results in courses.","title":"CT-ISG: Dartmouth Trace Sanitization Framework","awardID":"0831409","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["553543"],"PO":["543481"]},"143787":{"abstract":"Next-generation high-end machines will include interconnected computer nodes, each having heterogeneous accelerators and multi-core CPUs with complex memory hierarchy. They demand a programming model with a unified abstraction for programming dramatically different on-chip and off-chip parallel processing capabilities. None of the existing models is suitable for this need. The most fundamental challenge here is natural expression of parallelism in applications and efficient mapping of such parallelism to the hardware, including data distribution, locality, communication, synchronization, and load balancing. <br\/><br\/>This collaborative research between Syracuse University and Sandia Labs aims at developing an efficient programming model for high performance computing (HPC) applications using multi-core and heterogeneous processors. The specific goal of this study is to develop a high-level parallel programming abstraction with new high-level language constrctions, data types, and runtime library. Hardware features such as cores, memory hierarchy, processor heterogeneity, and interconnection will be embedded in the semantics of the language constructs and data types. The programming abstraction will guide the design and expression of parallel algorithms in the high-level language, mapped automatically onto the hardware for efficient execution. Users will be free from the low-level hardware details. The approaches include: memory virtualization,communication virtualization and processors virtualization.","title":"Collaborative Research: An Efficient Programming Model for HPC Applications on Next-Generation High-end Parallel Machines","awardID":"0833152","effectiveDate":"2008-09-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["485290"],"PO":["565272"]},"143435":{"abstract":"Proposal Title: Collaborative Research: CT-M: Unification Laboratory for<br\/>Cryptographic Protocol Analysis<br\/>Institution: SUNY at Albany<br\/>Abstract Date: 08\/13\/08<br\/>The ability to reason about different equational theories is very<br\/>important to the analysis of cryptographic protocols. An understanding of<br\/>how equational properties of a function used by a protocol can be<br\/>exploited by an intruder can be invaluable in finding flaws that might<br\/>otherwise be missed. Numerous examples exist in the literature and even<br\/>in fielded protocols. One very powerful tool for cryptographic protocol<br\/>analysis with equational theories is equational unification. Equational<br\/>Unification was the basis of the NRL Protocol Analyzer (Maude-NPA). Its<br\/>use of these theories allowed it to both reproduce existing flaws and find<br\/>new ones at a level of precision way beyond that available to other tools<br\/>at its time. This suggests that equational unification if properly<br\/>extended, can give support in a similar fashion to analysis of<br\/>cryptographic protocols that use functions that obey more expressive<br\/>equational theories. The aim of this project is to provide a<br\/>laboratory for equational unification that will develop the algorithms<br\/>and techniques that can be used to support the use of equational<br\/>unification in cryptographic protocol analysis. This effort will<br\/>consist of two parts: the development of unification algorithms for<br\/>theories of interest to cryptographic protocol analysis, and the development<br\/>of new techniques for employing unification in cryptographic protocol<br\/>analysis. The project will help in the design and implementation of next<br\/>generation tools for protocol analysis. Tools developed in the project<br\/>will be made available to other researchers working on formal protocol<br\/>analysis methods as well as to protocol designers for experimentation. The<br\/>educational component of the project will involve undergraduate and<br\/>graduate students at both institutions.","title":"Collaborative Research: CT-M: Unification Laboratory for Cryptographic Protocol Analysis","awardID":"0831462","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["531861"],"PO":["529429"]},"141015":{"abstract":"Proposal Number: 0820061\/0820034\/0820230<br\/><br\/>TITLE: Design and Run-time Techniques for Physically Coupled Software<br\/><br\/>PIs: Ramesh Govindan (USC), Rajesh Gupta (UCSD), Mani Srivastava (UCLA), and Paulo Tabuada (UCLA)<br\/><br\/>ABSTRACT:<br\/><br\/>Many real-world systems are deeply embedded in the physical world and their operational behavior is determined in large part by a tight coupling between the system components and the physical environment. This project seeks to establish the scientific principles governing software for such physically-coupled systems by focusing on four challenges in the context of distributed sensing and control applications: 1) Support for physical context in the form of programming structures that enable application software to explicitly capture the state of the physical world as an observable in an embedded computation; 2) Formal methods for composing software modules that indirectly interact with each other through the physical world, and a run-time safety supervisor that provably enforces correctness of composition; 3) Programming structures to enable design and verification of applications with resource provisioning that is driven by and adapts to physical-world dynamics; 4) System software support for sharing physically-coupled sensor and actuator resources in distributed settings. In addition, educational techniques targeting the teaching of topics in physically-coupled computational systems are being explored by creating shared educational content in the form of self-contained reusable modules.","title":"Collaborative Research: Design and Run-time Techniques for Physically Coupled Software","awardID":"0820230","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":["553707"],"PO":["564388"]},"143204":{"abstract":"A major potential in large wireless networks is cooperation: A transmission from a single node is overheard not only by the intended receiver, but by all other nearby nodes; by analogy, any receiver not only captures the signal from the intended transmitter, but from all other nearby transmitters. The pessimist's perspective on these facts has shaped communication network designs of the past decades: Clever algorithms and protocols have been devised to avoid interference. Recent work, however, has revealed many scenarios under which interference turns out to be beneficial, provided it is suitably shaped. This is often referred to as physical-layer cooperation. Most of the cooperation schemes that have been proposed to date harvest the statistical dependence of the underlying signals. By contrast, in this project, novel codes are developed that permit to exploit the algebraic structure of the interference, enabling efficient and reliable computation of functions of the involved messages. Such codes will be referred to as computation codes. They are of independent interest in applications that explicitly call for computation, such as sensor networks.<br\/><br\/>More generally, the computation coding perspective is used to develop a new framework for larger networks: Inside the network, judiciously chosen functions of the messages (rather than the messages themselves) are being passed around. As soon as a receiver has sufficiently many functions, it can infer the underlying message (i.e., the bits). This is reminiscent of so-called network coding, with the important difference that in the new framework, the question of which functions of the messages should be passed around is decided according to the actual interference characteristics, which can lead to significant gains.","title":"Computation Codes - A New Tool for Multi-user Communication","awardID":"0830428","effectiveDate":"2008-09-01","expirationDate":"2012-11-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["426349"],"PO":["564924"]},"143215":{"abstract":"Abstract for ?TF08: Fundamental Bounds on Decentralized Adaptive Detection in Hidden Markov Models? (PI: Yajun Mei, NSF proposal #0830472)<br\/><br\/><br\/>In modern information era, the advance of sensor, computing and communication technologies offers promising opportunities for the decision makers and organizations to make effective decisions quickly in many areas of real-world applications of sensor network systems. However, without timely updating or adaptation to reflect the changing environments, even the best decision-making methods are irrevocably vulnerable in applications such as threats detection in bioterrorism and hacking. The challenges become more difficult due to the complex spatio-temporal correlations among sensors and the constraints on communications, energy and computing. <br\/><br\/>This research is concerned with the development of a general and systematic foundation and methodologies for decentralized adaptive detection when sensor observations are from hidden Markov models, with the focus on deriving the fundamental information limitation on the ability to reliably detect the changes. The investigator studies decentralized adaptive detection in hidden Markov models for two scenarios of sensor network systems. The first is the system where sensors do not have access to their past observations, in which the research topics include (i) optimal stationary quantizers; (ii) lower bounds on adaptive detection; (iii) robust detection via tandem quantizers; and (iv)adaptive detection with censored sensor observations. The second is the system where sensors have access their past observations, in which the research investigates: (i) universal information bounds; (ii) computing-friendly, effective schemes; (iii) schemes with controlled detection delay; and (iv) blockwise transmission.","title":"Fundamental Bounds on Decentralized Adaptive Detection in Hidden Markov Models","awardID":"0830472","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["481051"],"PO":["564898"]},"143457":{"abstract":"Cross-layer design problems in wireless networks are usually very complex, <br\/>since they require simultaneously optimizing a large amount of algorithms <br\/>and parameters. Most existing solutions for cross-layer optimization rely <br\/>on heuristic procedures to solve this problem. However, to obtain an optimal <br\/>utility for the wireless user, cross-layer optimization should be formulated <br\/>rigorously as a sequential decision problem that takes into account the <br\/>capability of the various layers to autonomously make forecasts about their <br\/>experienced dynamics, and perform foresighted adaptation, while adhering to <br\/>the existing layered network architecture. To address this challenge, the <br\/>investigators study a new, systematic framework for cross-layer optimization <br\/>that allows each layer to make autonomous and foresighted decisions on the <br\/>selected transmission strategies (e.g. protocol parameters and algorithms), <br\/>while cooperatively maximizing the utility of the wireless user by optimally <br\/>determining what necessary information should be exchanged among layers.<br\/><br\/>This research involves two main thrusts: (a) Develop a novel cross-layer <br\/>optimization framework with message exchange among layers in which each layer <br\/>optimizes its own protocol parameters and algorithms based on its own <br\/>experienced dynamics and the information exchanged with other layers, in <br\/>order to cooperatively maximize the wireless user?s utility in a foresighted <br\/>manner while adhering to the layered network architecture; (b) Design layered <br\/>on-line learning algorithms for the cross-layer optimization to allow each layer <br\/>to interactively learn the experienced dynamics and other necessary information <br\/>from other layers, such that the cross-layer strategies can be optimized <br\/>cooperatively. This research leads to a fundamentally new way for designing <br\/>wireless networks, systems and applications, where devices evolve and become <br\/>smarter, by learning from their interactions with the environment over time.","title":"NEDG: A New Systematic Framework for Cross-layer Optimization","awardID":"0831549","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["558132"],"PO":["557315"]},"144788":{"abstract":"The National Science Foundation (NSF) and other major funding agencies have emphasized the need for more distributed centers of research in order to solve problems of science and engineering that are too big for single investigators. Entry into new work settings is already challenging enough when newcomers are physically co-located with coworkers, but joining such a virtual research organization may prove even more difficult. This multi-method project studies the socialization processes newcomers face when joining existing virtual research organizations (e.g., collaboratories). Specifically, this study focuses on Post-Doctoral Fellows as newcomers joining either distributed or co-located research teams. Survey, comparative case study, and agent-based modeling methods will be employed to better understand how Post-Doctoral Fellows transition into their new positions and how such transitions can be improved. The objectives of the project are (1) to better understand under what general conditions Post-Doctoral Fellows are successful and (2) to identify social and technical barriers for Post-Doctoral Fellows entering virtual organizations. Understanding the practices that lead to Post-Doctoral Fellow success under co-located as compared to distributed conditions will improve virtual organizations by providing benefits to both the researcher and the research program. The long-term goal of this study is to identify, formalize, and gain insights into the smart practices of virtual organizations planning for and engaging newcomers. The results will be of value to all domains of science and engineering interested in virtual organizations, particularly those with fluid team membership.","title":"Collaborative Research: VOSS: Joining a Virtual Organization: A Multi-Method Study of Newcomers to Established Collaborations","awardID":"0838295","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["397790"],"PO":["446196"]},"147824":{"abstract":"Chip multiprocessors (i.e., multicores) are rapidly emerging as the dominant platform for computing. This computing environment features parallel computing with shared-memory and a hierarchy of caches. This research will develop parallel and cache-efficient algorithms for many important problems that can be used effectively on multicores. The outcome of this research will be a set of provably correct methods for the efficient use of multicores, which is of critical importance given the imminent reality of multicores on every desktop. These results will have broad applicability to all fields that rely on compute-intensive tasks.","title":"Design and Analysis of Parallel Cache-efficient Algorithms","awardID":"0850775","effectiveDate":"2008-09-15","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["550960"],"PO":["562944"]},"135845":{"abstract":"This project supports U.S. participation in an international Network to work toward achieving interoperability among language resources, for which parallel European support has been provided by the European Commission eContentplus Programme. The Network involves members of the language processing community and related areas who are working to build consensus regarding the sharing of data and technologies for language resources and applications, interoperability of existing data and tools, and the promotion of standards for resource building and annotation. The participation of colleagues in Asia is also being sought. <br\/><br\/>The resources and technologies to be addressed include annotated corpora (texts, audio), lexicons, ontologies, automatic speech recognizers, lemmatizers, taggers for all levels of linguistic phenomena, named entity recognizers, information extractors, as well as systems for search, access, and annotation. The creation and use of these resources span the field of linguistics and several related but relatively isolated disciplines within computer science and engineering, including natural language text and speech processing, information retrieval, machine translation, and the semantic web. The goal is to turn existing, fragmented technologies and resources developed within these groups in relative isolation into accessible, stable, and interoperable resources that can be readily reused across several areas. This project is being funded through the INTEROP solicitation in the Office of Cyberinfrastructure, with funding also from the Directorates of Social, Behavioral & Economic Sciences and Computer & Information Science & Engineering, and the Office of International Science & Engineering.","title":"INTEROP: Sustainable Interoperability for Language Technology","awardID":"0753069","effectiveDate":"2008-09-01","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"1311","name":"LINGUISTICS"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7701","name":"DATA INTEROPERABILITY NETWORKS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["562587","501442"],"PO":["565215"]},"145767":{"abstract":"NSF-CISE-CCF-EMT Workshop on Emerging Models Technologies for Computations in Nanoelectronics<br\/>Project Summary<br\/>Nanotechnology is an exciting field with many potential applications. Its impact is already being felt in materials, engineering, electronics, medicine, and other disciplines. Current research in nanotechnology requires strong multi-disciplinary knowledge and emerging models technologies for computations in the field of nanoelectronics with promising. Nanoelectronics become the main research domain that successes in today?s microelectronics. It brings an unprecedented revolution in not only communications and computing, but also in other scientific and engineering fields such as nano-scale measurements in industrials, bioscience, medical and environmental sciences.<br\/>A NSF workshop, funded by a NSF grant from the Emerging Models and Technologies (EMT) for Computation (EMT) Program was organized by Drs. Kathleen Meehan and Yong Xu, Virginia Tech. on October 15-16, 2007 in Arlington, Virginia. The primary objective of the workshop was to allow the current principal investigators and leading researchers in the field of nanoelectronic systems to review the impacts of the EMT Program for the nanoelectronics program element on various fronts: research, technology transfer culminating into better competitiveness of the USA and establishing new start-up companies, undergraduate and graduate education, promotion of cross-disciplinary teaching and research activities, training of engineers, and societal benefits, namely engendering enthusiasm amongst high school students and society in general. The workshop focused several challenge topics: molecular electronics, nanoarchitecurre, and CAD tools for uncertainty and defects in quantum electronics. The attendees also recommended NSF to create student exchange programs for information and experience sharing, to develop short course training, interdisciplinary conference\/workshops with tutorials at different levels, and to create high-level lectures on the state-of-the-art research, and as well as interdisciplinary grant proposals for small but collaborative investigations.<br\/>Based on the recommendations, the PI proposes to organize the need-based external NSF funded workshop to enhance the existing EMT Program in nanoelectronic systems research. The proposed workshop aims to providing an opportunity to share and develop education components. In addition to promote a series of short courses and lectures in nanoelectronics, the workshop will have a training session for undergraduate students, high school teachers of science and selective K12 students.<br\/>The objectives of this education workshop is a great opportunity fro faculty to share their experience and course curriculum which are related to emerging model technologies for nanoelectronics and to discuss how the current research topics can be integrated into existing graduate credit courses, and to develop short and knowledge-based courses for undergraduate students, and how to create off school program to encourage K12 students and their teachers to participate in and involve to nanotechnology learning and exploration.<br\/>The workshop will provide (1) basic courses about fundamental nanoelectronics in nanotechnology for K12, undergraduate and graduate students, (2) provide online, open-access repository and course curriculum developments for public; thus to promote college education in this interdisciplinary field, (3) Develop short courses for college students and PIs to learn multiple subjects and materials that are related to nanoelectronics, (4) provide advanced tutorials for workshop participations and organize a tutorial sessions, and (5) solicit scientific papers; and organize workshop technical presentation, poster sessions; and process workshop paper proceeding publication and selected paper recommended for journals.<br\/>The intellectual merits of this proposal lie in the shortage of existing internal workshop and education program mechanism the multidisciplinary domain in nanoelectronics. The proposed workshop attempts to organize a national-recognized workshop in this domain and make it available not only to research institutes but also to public. There are three major scientific merits in this workshop. The first is to coordinate a workshop for education at all levels, and the second is to remain the state-of-the-art of technical tutorials and lectures in nanoelectronics, especially through EMT Program to provide the linkage between nano-computation promoted by NSF-CISE-CCF and other NSF programs. The third one is to promote nanoelectronics device inventions and its applications.<br\/>The broader impact of this proposal is considered that the area of nanoelectronics has great potentials for future applications. Nanotechnology has emerged as a significant and exciting field, primarily with respect to the basic sciences, and will undoubtedly have a broad impact on nano-electronic device","title":"NSF-CISE-CCF-EMT Workshop on Emerging Models Technologies for Computations in Nanoelectronics","awardID":"0842324","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["554242"],"PO":["565272"]},"143347":{"abstract":"Project Abstract:<br\/><br\/>Without their realizing it, end-users have been turned into authors of access-control policies. Everywhere from Google to Facebook to Microsoft HealthVault and beyond, these policies are usually hidden behind simple user interfaces, but ultimately the users are responsible for setting and then taking responsibility for the consequences of these policies. Indeed, the apparent simplicity of the interfaces sometimes belie the significance of the outcomes.<br\/><br\/>Because applications are a black-box to end-users, it becomes difficult for users to predict the consequences of an action. Users see only their view of the world, but their access-control decisions affect the views of others. End-users need tools to determine the effect of their decisions, with special emphasis on the effect of policy changes. This project is developing user-friendly means to browse and investigate the details of these effects and changes. The underlying techniques are firmly grounded in theory, resulting in formal accuracy guarantees rather than approximate answers. The tools themselves summarize information in ways that account for the cognitive expectations and biases of users.<br\/><br\/>The broader impact of this project comes from the focus on end-users rather than programmers and other technical users. Some end-users are too eager to embrace new technologies without fully appreciating their consequences, while others are too tentative due to their concern about (sometimes imaginary) problems they might create. The tools from this project help the former become more cautious, and the latter more confident. The net result should be a more savvy, yet vastly more inclusive, Cyber society.","title":"CT-ISG: Power to the People: Tools for Explaining Access-Control Consequences","awardID":"0830945","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["519555"],"PO":["497499"]},"143468":{"abstract":"This project builds a holistic model of reliability and performance for large-scale networking system (LSNS) and develops model-based self-improvement (MBSI) technology for high reliability, high performance, and smooth network communication in LSNS. This project addresses a number of problems, including the precise formulation of a holistic model of the LSNS reliability and performance, the analytical evaluation of the reliability and performance, the scheme to reduce the jams of network traffics, and further uses the self-improvement technology to automatically glean data, build models, evaluate designs, and optimize tasks. The methods, like graph theory, Bayesian approach, maximum entropy principle, universal generating function, and Monte Carlo simulation, are mainly adopted in modeling and evaluating the LSNS, while autonomic computing technologies are implemented for the self-improvement functions. As a result, this research further advances the theory, algorithm and technology in reliability, and fills the gap of reliability modeling and self-improvement in LSNS. <br\/><br\/>Expected results: (1) Novel formulations will be designed to effectively model a LSNS with all essential components; (2) An innovative HSA (Hybrid Stochastic Algorithm) will be developed to evaluate the task reliability and performance for LSNS; (3) New optimization schemes for LSNS will be designed; (4) A novel MBSI technology will be developed for self-improving the reliability, performance, and network communication; (5) Research outcomes will be applied in a variety of LSNSs, such as NASA's outer-space exploration, tele-medicines, grid computing, etc; (6) A set of software tools will be developed. (7) Research results will be disseminated through journal\/conference publications and PIs' websites.","title":"(NECO) Collaborative Research: Reliability Modeling for Large-Scale Networking System (LSNS), and Self-Improvement in LSNS","awardID":"0831634","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["37090"],"PO":["565090"]},"146625":{"abstract":"This project includes the development and evaluation of pilot studies of creativity in an IT research organization with three primary objectives: (1) combine IT and social science methodologies; (2) extend our understanding of individual and team creative processes; (3) develop and evaluate new types of interventions and interfaces that incorporate affect, context, and social interaction to leverage opportunities to promote creativity. The intellectual merit lies in the development of hybrid methodologies, combining social science and computer science approaches to multiple behavioral and affective elements for creativity in an IT research organization. The research will improve our understanding of organizational creativity and advance the development a new generation of integrated Creativity Support Tools that are applicable within teams and organizations. Broader Impacts: Creativity is required for advances in all areas of human endeavor. The methods and frameworks developed are broadly applicable throughout academic and industrial IT research organizations, and they are also adaptable to a broad spectrum of organizations that increasingly need guidance and support to foster creative practices and innovation. The methods and tools will also apply to people performing a wide range of personal endeavors as they engage in their own creative pursuits. The findings and systems will advance academic and industry collaborations and will be broadly disseminated throughout multiple research communities, IT organizations, and on-line communities.","title":"SGER: Creativity in IT Research Organizations","awardID":"0846148","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[390875,"552861"],"PO":["424970"]},"145415":{"abstract":"MIND NanoArchitecture Workshop: Exploring Design with post-CMOS Devices<br\/><br\/><br\/>The purpose of this workshop is to explore the implications of proposed post-CMOS devices at the circuit, architecture, and system levels. Recognized experts will present their vision, and strong session chairs will guide the discussion. Anticipated outcomes of this workshop include benchmarks and figures of merit for evaluating candidate devices, and identification of alternative circuits, architectures, and\/or compute systems in which specific, promising new structures can extend or supersede CMOS design-space?s superiority<br\/>As CMOS technology is approaching fundamental limits, much effort is concentrated on finding Post-CMOS switching devices. These novel devices necessitate the identification of appropriate circuit architectures, which might be different from what is being used for conventional CMOS devices. This workshop will explore architectures and designs which will support Post-CMOS, potentially hybrid, technology applications. As such, the outcomes of this workshop will lay the intellectual groundwork for transformative advances that are needed for the design of Post-CMOS computer systems. <br\/>This workshop will have broad impact by bringing together leading experts from the traditionally-distinct device, circuit, architectures, and systems communities. This work will increase the interactions between NSF and SRC-NRI Centers, and help coordinate the work of the whole NSF \/ NRI community. We anticipate that the outcomes of this workshop will direct future developments in this field, including architectural findings and benchmarks and figures of merit for evaluating candidate devices. On a grander scale, these outcomes will help identify strategic directions for the continuation of the microelectronics industry beyond the end of the CMOS technology roadmap.","title":"MIND NanoArchitecture Workshop: Exploring Design with post-CMOS Devices","awardID":"0840741","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["560914"],"PO":["565223"]},"143237":{"abstract":"Machine Learning Theory is concerned with designing algorithms with provable guarantees for learning from data, and understanding what types of guarantees are inherently achievable. This turns out to have a number of important connections to key problems in Algorithmic Game Theory, Database Privacy, and Clustering. This project intends to further develop and expand these connections in order to address fundamental questions in all three areas.<br\/><br\/>A major thrust of Algorithmic Game Theory has been quantifying the inefficiency self-interested behavior via the notion of \"Price of Anarchy\". However, the assumption made that users behave according to a Nash equilibrium can be overly optimistic, especially in large or information-limited settings (even for a centralized controller these equilibria can be computationally hard to find). Instead, simple guarantees achieved by online learning algorithms can directly model a minimal concept of self-interested behavior, and make sense even in large, information-poor arenas. This work will develop an alternative approach to analyzing systems of self-interested entities based on this idea.<br\/><br\/>In the area of Database Privacy, it is clear that protecting privacy of those involved in a database while still providing useful statistics is a challenging task. Unfortunately, while positive results have been achieved for interactive query-answering mechanisms, most results for actually releasing a database satisfying the stringent condition known as differential privacy have been negative. Learning Theory, however, suggests a possible approach: rather than attempting to preserve all possible statistics, one can instead define interesting classes of statistics (much like concept classes in PAC learning) and ask: when can one approximately preserve all statistics in this class while still satisfying privacy? We have been able to produce computationally inefficient mechanisms that apply to a broad collection of such classes, and one main thrust of this work is to develop computationally tractable procedures.<br\/><br\/>Finally, while many different algorithms have been developed for data clustering, the theory of what information about data is sufficient to be able to cluster it accurately remains not well understood. Theoretical models either make strong statistical assumptions (such as mixtures of Gaussians) or else aim to optimize graph-based objectives that may not be directly related to error rate. In this work we aim to create an analog of the PAC learning model for clustering that is able to directly address this issue. In recent work we have made initial progress in this direction and this project aims to more fully develop this approach.","title":"A Learning Theory Approach to Algorithmic Game Theory, Database Privacy, and Clustering","awardID":"0830540","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["554195"],"PO":["565251"]},"154248":{"abstract":"CSR-AES: Intelligent Optimization of Parallel and Distributed Applications<br\/><br\/>ABSTRACT<br\/>This project derives a systematic solution for performance optimization and adaptive application mapping to obtain scalable performance on parallel and distributed systems consisting of tens of thousands of processing nodes. With expert domain scientists in molecular dynamics (MD) simulation, we expect to achieve performance levels on MD codes even better than what has been derived manually after years of development and many ports to a variety of architectures.<br\/>The application components are viewed as dynamically adaptive algorithms for which there exist a set of variants and parameters that can be searched to develop an optimized implementation. A workflow is an instance of the application where nodes represent application components and dependences between the nodes represent execution ordering constraints. By encoding an application in this way, we capture a large set of possible application mappings with a very compact representation. The system layers explore the large space of possible implementations to derive the most appropriate solution. Because the space of mappings is prohibitively large, the system captures and utilizes domain knowledge from the domain scientists and designers of the compiler, run-time and performance models to prune most of the possible implementations. Knowledge representation and machine learning utilize this domain knowledge and past experience to navigate the search space efficiently.<br\/>This multidisciplinary approach impacts the state-of-the-art in the sub-fields of compilers, run-time systems, machine learning, knowledge representation, and accelerates advances in MD simulation with far more productive software development and porting. More broadly, this research enables systematic performance optimization in other sciences.","title":"CSR---AES: Collaborative Research: Intelligent Optimization of Parallel and Distributed Applications (WP2)","awardID":"0917775","effectiveDate":"2008-09-01","expirationDate":"2010-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[410444],"PO":["493916"]},"134547":{"abstract":"Many mobile embedded systems are designed to be small and compact to favor portability. As the user demand expands for more powerful, versatile, and integrated solutions, the designers endeavor to pack more and more devices into small embedded form factors, thanks to technology advancement. In parallel with this trend, the microprocessor technology has evolved into an era of multicore processor in both 2D and 3D space. As can be foreseen, the marriage of future embedded systems and future microprocessors raises the specter of dramatically increased power density that renders thermal management a key challenge for embedded processors<br\/><br\/>This CAREER project addresses this challenge and seeks innovative solutions to tackle thermal problems for embedded chip multiprocessors (CMPs). The objective of this research is to develop proactive thermal management techniques that prevent the temperature from increasing above the threshold and yet avoid performance throttling. This is in contrast to traditional techniques, which only react to thermal violations, and do so by enforcing performance throttling to cool down the processor. The proposed techniques leverage the natural discrepancies in thermal behavior among different applications, and schedule them among multiple cores to keep the chip temperature within a given budget. The mission of such scheduling is to minimize thermal violations across all cores on-chip, improve performance, and diminish overheating-induced problems such as reduced reliability, low circuit speed, and high leakage power. <br\/><br\/>The education component of this project seeks to train students from different fields and with different backgrounds in a tightly integrated framework. The goal is to establish broad familiarity and preparedness for future professions in the field of embedded systems, micoarchitectures, operating systems and software analysis.","title":"CAREER-EHS: Thermal-Aware Task Scheduling for Embedded Planar and 3D Chip Multiprocessors","awardID":"0747242","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["508314"],"PO":["561889"]},"143259":{"abstract":"The demand for wireless communication is only growing, as wireless becomes the dominant means of Internet access. To meet this demand, wireless systems may employ new concepts such as multiple antennas, multiple user processing, transmitter coordination, and interference alignment to improve system capacity. Unfortunately, implementing these communication techniques requires substantial feedback from the receiver to the transmitter and more complex signal processing to reap the promised capacity gains. Fortunately, there is special manifold structure in these signal processing problems that is not yet exploited. <br\/>Manifolds are generalizations of surfaces to higher dimensions and can be used to capture structure in signals. Special manifolds like the Stiefel and Grassmann manifolds are of particular interest in wireless communication systems that use multiple antennas. This research involves developing a suite of signal processing techniques for analyzing, filtering, predicting, and optimizing signals with curved manifold structure.<br\/><br\/>This project applies new manifold signal processing tools to three emerging problems in wireless communication: multiple user multiple antenna communication, interference aligned transmission, and coordinated base station transmission. Each problem requires progressively more sophisticated manifold signal processing techniques to enable their eventual application in commercial and military wireless communication systems. The most immediate impact of this research will be to improve the quality and capacity of wireless communication links thus impacting their design, implementation and deployment. The long range impact will be tools and analytical techniques that influence other disciplines including control theory, optimization, image and video processing, data mining and manifold learning. Broader impacts of this research program will occur in education through the training of undergraduate and graduate students and in industry through rapid dissemination of research results through electronic preprints.","title":"Signal Processing on Special Manifolds with Applications to Wireless Communication","awardID":"0830615","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["550451"],"PO":["564898"]},"134558":{"abstract":"The ubiquity of collections of personal and sensitive data (census surveys, online social networks, and public health data, to name a few) has created a host of new problems stemming from conflicts between data access and privacy. An important challenge for these collections is to discover and release global characteristics of the database without compromising the privacy of the individuals whose data they contain. The problem has been studied extensively in such diverse fields as statistics, databases and data mining. However, the approaches proposed in the literature, until very recently, had either no formal privacy guarantees or ensured security only against limited types of attacks. This project seeks to lay a firm conceptual foundation for the field of privacy in statistical databases, taking into account realistic, sophisticated adversarial attacks and bringing together ideas from several different sub-disciplines of statistics and computer science.<br\/><br\/>The research is centered around three themes: (1) formulating realistic models and definitions of privacy that provide resistance against strong, even active, attacks; (2) understanding the types of information that can, and cannot, be revealed while retaining privacy according to the definitions discussed above; (3) investigating techniques which \"break\" anonymization protocols, in order to inform protocol design in the same way that cryptanalysis informs modern cryptography. The research is closely tied to questions of resilience and robustness in machine learning and statistics. To ensure the broader impact of the research, this project includes a program of educational and outreach activities including new course development and workshop organization.","title":"CAREER: Rigorous Foundations for Data Privacy","awardID":"0747294","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["501279"],"PO":["565157"]},"143028":{"abstract":"A central goal of DNA nanotechnology is to develop methods for assembling complex, aperiodic structures for nanofabrication tasks. The critical challenge addressed in this work is robust biomolecular system design to avoid errors in complex nanoscale pattern formation via controlled directional assembly. Algorithmic DNA self-assembly makes use of DNA nanostructures (tiles), which assemble together via hybridization, theoretically forming DNA lattices with complex patterns, but are limited by significant assembly mismatch errors that prevent further growth. The project?s innovative approach is assembly error avoidance (rather than crystal error correction) using self-activating and reactivating DNA protocols driven by the use of DNA polymerase enzyme. A novel protection\/deprotection strategy (using DNA polymerase displacement) enforces the direction of tiling assembly growth to avoid growth errors. Initially, a tile is in an inactive state, with output pads protected from binding with other tiles, preventing lattice growth in (unwanted) reverse direction. After other tiles bind to this tile?s input pads, it enters an active state where its output pads are exposed, allowing further growth. Tasks include various experimental demonstrations of activatable tiles and computer simulation software tools for design and kinetic probabilistic simulation of the tile assembly process and protocols. The controlled directional assembly of tiling assemblies eliminates a major roadblock in the development of applications of patterned DNA lattices, providing a methodology for vastly increasing the complexity of synthetic molecular patterned nanostructures. Additional novel applications to be demonstrated include assemblies for molecular sensing, concentration (via activation of assembling tiles only when a speci&#64257;c target molecule docks at a particular site on the tile), and catalyzation. The work spans many &#64257;elds including chemistry, biochemistry, physics, and computer science, with applications in bioengineering, biomedical engineering and nano-engineering. It provides students exciting and challenging interdisciplinary training opportunities unique to the degree of its span of multiple disciplines, impacting the critical national need in training in multiple disciplines.","title":"EMT\/NANO: Polymerase-Based Self-Activating and Reactivating DNA Systems","awardID":"0829798","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["550822","531216"],"PO":["565223"]},"137704":{"abstract":"The design and use of information systems to support the collaborative activity of collocated teams in dynamic, high-risk scenarios remains a challenge. This project will develop novel methods to more efficiently capture and communicate this activity in environments that currently rely on human observation, verbal communication, and collective memory. More efficient teamwork capture processes will enable both larger-scale collection (which supports retrospective analysis that is critical for improved training and technology design) and contemporaneous collection (which provides real-time feedback to workers to assist in error detection). <br\/><br\/>To achieve these goals, domain-specific knowledge and probabilistic reasoning will be used to identify patterns of work and communication. The representative domain of trauma resuscitation is ideal for this work since the roles and tasks of players are well-defined and the flow of work follows a general schema regardless of the patient?s injuries. Because of the complexity of this environment, manual tracking of all activities using video recordings requires repeated review and is very time-consuming even for experienced observers. A computer system will be developed that uses video analysis to determine the location of each player, motion analysis to track their movements, and speech recognition targeted at a limited lexicon to identify their communication. Using these inputs, a probabilistic reasoning model will be constructed that correlates data from the environment with a domain-specific model of teamwork. The tagged recording of the resuscitation event will be available in real time during the event as well as post-event for analysis.<br\/><br\/>The scientific importance of this work is in the need to tag these video observations. Many forms of videos are of repetitive behaviors, whether in surveillance applications, work situations, or other uses. In all such cases, applying a grammar to the video, and matching actions and sounds to that grammar, has the possibility of greatly simplifying work analysis, which is the critical phase in the development computer support for complex, high-risk human activities.<br\/><br\/>The proposed approach will develop novel algorithms and methods for: (i) person and resource tracking in crowded collaborative environments; (ii) recognition of human activity based on fusion of unreliable data from multimodal sensors and a model of the process being recorded; and (iii) reasoning about human activities at different time scales based on heterogeneous technologies (Hidden Markov Models, Bayesian Nets, and Petri Nets) that mutually interact for activity and event detection. Moreover, the methods will be developed and evaluated in a clinical environment that currently uses limited information technology.<br\/><br\/>Broader Impacts. This work will also provide the foundation for implementing decision aids in environments such as trauma resuscitation and related medical domains that lack effective methods for instrumented tracking of teamwork. Trauma care is a significant health care crisis and any improvements in resuscitation processes will save lives.","title":"HCC-Medium: Collaborative Research: Multimodal Capture of Teamwork in Collocated Collaboration","awardID":"0803732","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["409115","518508"],"PO":["564456"]},"144007":{"abstract":"Accelerating single programs on multicore processors remains an outstanding challenge in computer systems design. Unfortunately, existing parallel systems achieve little speedup on programs other than regular dense-matrix codes. And, most of the world's programs are in this category, broadly termed non-regular code. Of course some non-regular codes have little parallelism beyond instruction level parallelism (ILP); hence no speedup is possible on multicores. However in other non-regular code, parallelism is present but is not exploitable. Reasons include high synchronization costs, non-loop parallelism, non-array data structures, recursively expressed parallelism and parallelism that is too fine-grained to be exploitable. <br\/><br\/>Previous work by the PIs presented the PRAM-based XMT parallel architecture which has demonstrated good speedups on non-regular codes: 23X on breadth-first search in graphs and 9X for finding spanning tree in graphs, using 64 processors vs. the best-in-class serial processor.<br\/><br\/>This project is developing new compiler technologies for XMT to achieve scalable performance in the face of architecture decisions made for scalability. It is studying better compiler techniques to achieve scalable performance for UMA architectures such as XMT. These include better task schedulers using global queues rather than work stealing; improved pre-fetching tailored for XMT's unique memory hierarchy; and using scalable non-cache-coherent Scratch-Pad Memory local to each XMT processor to reduce the need to go to expensive remote memory.<br\/><br\/>The broader impacts of this project are (i) the development of compiler technologies necessary to reduce the research risk of XMT to the point where industry is willing to commercialize the technology; (ii) the delivery of scalable speedups for erstwhile hard-to-parallelize applications; (iii) the demonstration of technologies for robust performance across large classes of serial, regular parallel, and non-regular parallel programs; (iv) demonstrating a serious contender for a future universal desktop architecture; and (v) educational and outreach initiatives to popularize XMT and improve the skills of the future workforce.","title":"CSR-PSCE,SM: Compiler-Directed System Optimization of a Highly-Parallel Fine-Grained Chip Multiprocessor","awardID":"0834373","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["508237","540124"],"PO":["535244"]},"143039":{"abstract":"PETASCALE SIMULATIONS OF DNA DYNAMICS AND SELF-ASSEMBLY<br\/>Priya Vashishta?PI, Rajiv K. Kalia, Aiichiro Nakano (University of Southern California)<br\/>Ananth Grama (Purdue University)<br\/><br\/>DNA translocation through solid-state nanopores and nanofluidic channels underlie ?lab-on-a-chip? technology and solid-state nanopore ?microscopy? for molecular structure and high-speed sequencing. Highly efficient methods for directed self-assembly of DNA offer unprecedented opportunities for the synthesis of novel genes, chromosome mapping, biosensors, molecular machines, nanoelectronics and nanomechanical systems, and formulations of mesoscopic structural motifs as building blocks of emerging periodic and aperiodic nanostructures consisting of DNAs.<br\/>This project involves the study of DNA self-assembly and translocation through nanometer-scale pores in silica and silicon nitride membranes using a predictive hierarchical petascale simulation framework consisting of: (1) Highly accurate quantum mechanical (QM) simulations to describe chemical processes in DNA translocation and concatenation; (2) multibillion-atom molecular dynamics (MD) simulations for structural properties and dynamical processes of DNAs in confined fluidic environments, with interatomic interactions validated by QM calculations and key experiments; (3) hybrid MD and adaptive lattice Boltzmann (LB) simulations in which MD is embedded in translocation\/concatenation regions, and LB in the rest of the fluid; (4) accelerated dynamics approaches to reach macroscopic time scales for direct comparison with experimental data; (5) metascalable, self-tuning, multicore parallel simulation algorithms; and (6) automated model transitioning to embed higher fidelity simulations inside coarser simulations on demand with controlled error propagation. A metascalable (or ?design once, scale on new architectures?) parallel application-development framework is also being developed for first-principles simulations of directed DNA self-assembly.","title":"Collaborative Research: EMT\/BSSE:Petascale Simulations of DNA Dynamics and Self-Assembly","awardID":"0829844","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["540776"],"PO":["565223"]},"147769":{"abstract":"This award supports a National Workshop on Cyber-Physical Systems (CPS) Research Directions for<br\/>Transportation: Aviation, Automobiles and Rail. The workshop is to be held in Seattle, Washington, November 18-20, 2008, with sponsorship from the NITRD High Confidence Software and Systems interagency Coordinating Group. The purpose of the meeting is to identify common needs and research challenges that must be addressed to support the healthy future of constituent transportation sectors (e.g., aviation, automotive, railway) and for the collective transportation sector as a whole. All sectors share the characteristic that future vehicles will interact with other vehicles and with a dramatically changing infrastructure in a way that is fundamentally cyber-enabled. Cyber-physical systems are systems in which networked, real-time, embedded control is pervasive at all levels of integration, and there is a continuum of autonomy and authority shared by humans and systems. When viewed as cyber physical systems, future vehicle and transportation system designs can lead to improvements in safety, security, energy-efficiency, control, and maintenance of such systems. This national workshop will help to facilitate the identification of common as well as distinct research needs of the sectors and the research challenges that will be identified by viewing these large distributed systems, with heterogeneous constituent vehicular systems, as dynamic cyber-physical systems.","title":"National Workshop on Cyber-Physical Systems Research Directions for Transportation: Aviation, Automobiles and Rail","awardID":"0850549","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"H184","name":"National Security Agency"}}],"PIcoPI":[393827],"PO":["561889"]},"144018":{"abstract":"Abstract<br\/><br\/>Dr. Tao Xie<br\/><br\/><br\/>Highly reliable, high performance and energy-efficient storage systems are essential for mobile data-intensive applications such as remote surgery and mobile data center. Existing mobile storage systems generally consist of an array of independent small form factor hard disks connected to a host by a storage interface in a mobile computing environment. Although hard disks are cost-effective and can provide huge capacity and high-throughput, they have some intrinsic limitations such as long access latencies, high annual disk replacement rates, fragile physical characteristics, and energy-inefficiency. Compared with hard disk drives, flash disks are much more robust and energy-efficient, and can offer much faster access times. A major concern on current flash disk is its relatively higher price. This project develops a hybrid disk array system, which integrates small capacity flash disks with high capacity hard disk drives to form a robust and energy-efficient storage system for mobile data-intensive applications. In particular, an array of new data management techniques including energy-efficient data placement, self-adaptive and reliability-aware data redistribution, and self-triggered data replication for data-intensive mobile applications built on the hybrid disk array framework will be developed. In addition, this project implements a simulation toolkit, which will be designed specifically to study a variety of data management techniques on top of the hybrid disk array architecture. This project will also promote teaching, learning, and training by exposing students to technological and scientific underpinnings in the field of energy-efficient storage systems. To enhance education outreach to local underrepresented groups of undergraduate students, this project organizes a summer workshop on energy-efficient computing at San Diego State University.","title":"CSR-DMSS, SM: Energy-Efficient and Reliability-Aware Data Management in Mobile Storage Systems","awardID":"0834466","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["562727"],"PO":["535244"]},"137616":{"abstract":"?Perceptual salience? is a term used by psychologists of vision to describe the power of an object to draw viewer attention; for example, it has been demonstrated that eye movements target salient objects sooner than less-salient objects, and that salient objects are detected more quickly than less-salient objects. The first sub-goal of this research is to develop automatic measurements of perceptual salience for auditory events, defined here to be a center-surround contrast in terms of amplitude, spectrum, or temporal features such as zero-crossing rate and periodicity. The second sub-goal of this research is to test salience measurements in an audio event detection paradigm, using the 2007 University of Illinois CLEAR evaluation system (Classification and Labeling of Events, Activities and Relationships). The third sub-goal of this research is to compare audio event transcriptions generated by human labelers viewing an audiovisual record of a meeting vs. transcriptions generated by labelers who listen to the audio without watching any accompanying video; the experimental hypothesis states that auditory salience predicts audio-only labels better than it predicts audiovisual labels. This research is designed as a collaboration between experts in computer vision and audio signal processing. If successful, the proposed methods will help to add an audio channel to the video security monitoring systems currently installed in many hospitals, nursing homes, government buildings and industrial sites.","title":"RI Medium: Audio Diarization - Towards Comprehensive Description of Audio Events","awardID":"0803219","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550184","426305",365799],"PO":["565215"]},"147208":{"abstract":"This work pursues the development of a new algorithmic framework which<br\/>allows for the first time efficient computation of higher-order<br\/>interactions in biomolecules. Algorithms are created to demonstrate<br\/>two important applications on much larger scales than were previously<br\/>tractable, each representing a new door to a larger class of further<br\/>possibilities: Axilrod-Teller (3-body) simulation, and Hartree-Fock<br\/>(4-index) quantum-level simulation. The multidisciplinary project<br\/>brings together experts in computer science, protein folding, and<br\/>quantum chemistry.<br\/><br\/>Biomolecular simulations usually break down complex chemical systems<br\/>into a balls-and-springs mechanical model augmented by torsional<br\/>terms, pair-wise point-charge electrostatic terms, and simple<br\/>pair-wise dispersion (van der Waals) interactions. However such models<br\/>often fail to capture important, complex non-additive interactions<br\/>found in real systems. Though the criticality of multi-body<br\/>potentials for more accurate and realistic molecular modeling has been<br\/>argued by various authors, their evaluation in systems beyond tiny<br\/>sizes has not been previously possible due to the unavailability of an<br\/>efficient way to realize the computation, which is cubic or higher.<br\/><br\/>The work augments a framework for computational problems called<br\/>Generalized N-Body Problems, which contains any such higher-order<br\/>physical potential. The framework was originally developed to<br\/>accelerate common bottleneck statistical computations based on<br\/>distances, utilizing multiple kd-trees and other space-partitioning<br\/>data structures to bring down computation times both asymptotically<br\/>and practically by multiple orders of magnitude. This work extends<br\/>the framework with higher-order hierarchical series approximation<br\/>techniques, demonstrating how to do a fast multipole-type method for<br\/>higher-order interactions for the first time, effectively creating a<br\/>Generalized Fast Multipole Method.<br\/><br\/>The algorithms are validated in biochemical systems chosen to clearly<br\/>illustrate many-body interactions: hydrogen bonds and three-body<br\/>dispersion interactions. Parameters for potential functions are<br\/>obtained using customized machine learning methods on dual data sets<br\/>generated by the co-PI's labs: high-quality quantum mechanical<br\/>benchmark data and experimental protein structures.<br\/><br\/>The goal is to demonstrate working many-body codes able to explore the<br\/>effect of modeling higher-order interactions on a larger scale and<br\/>more systematically than ever attempted previously. The intellectual<br\/>merit of the work is the elucidation of the first multi-tree multipole<br\/>method capable of accurately and scalably performing these fundamental<br\/>types of higher-order physics computations. The potential broader<br\/>impact is the ability to perform more accurate next-generation<br\/>molecular modeling, with implications for fundamental biology and drug<br\/>design. <br\/><br\/>For further information see the project web page at <br\/>http:\/\/www.cc.gatech.edu\/~agray\/gfmm.html","title":"III-SGER: Algorithms for Next-Generation Protein Modeling: Beyond Pair-wise Interactions","awardID":"0848389","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["541028","518447","544194"],"PO":["560586"]},"139508":{"abstract":"CCF-0811592<br\/><br\/>CPA-SEL: Practical Typestate Verification with Assume-Guarantee Reasoning<br\/><br\/>Jonathan Aldrich<br\/><br\/>One of the main difficulties in modern software development is using software libraries and frameworks correctly. This project is developing new tools to verify temporal usage properties of libraries and frameworks, capturing the permitted ordering of calls to an object and the state of that object when the calls are made.<br\/><br\/>Key verification challenges addressed by this project include inheritance and subtyping, recursive callbacks from a library into the client and back, and multiple aliased pointers to a library object. The project is applying assume-guarantee reasoning to these challenges: allowing multiple clients to access an object cooperatively, with an agreement about how that object should be used so that each client can safely make assumptions about other clients? behavior, and in turn each client guarantees that it will not violate other clients? assumptions.<br\/><br\/>The project is developing the underlying theory behind the approach, but is also building practical tools and evaluating them on real-world applications and libraries through scientific case studies. If successful, the project will increase the productivity of software engineers when using libraries, reduce the number of defects in software, and help students to learn about the theory and practice of lightweight software verification tools.","title":"CPA-SEL: Practical Typestate Verification with Assume-Guarantee Reasoning","awardID":"0811592","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["498106"],"PO":["564388"]},"139629":{"abstract":"Label-free quantification of analytes is gaining recognition as a very good strategy for biomarker discovery. However, such quantification is typically not addressed adequately in the instrument specific software packages. Biological variability and disease heterogeneity in human populations further complicate the MS-based biomarker discovery. The specific research aims of this project are the following: (1) Investigate data preprocessing algorithms for label-free quantification of peptides in serum using liquid chromatography?mass spectrometry (LC-MS) technologies. The initial focus will be on investigating and developing normalization, alignment, and peak detection methods to preprocess LC-MS spectra generated by two LC-MS instruments. (2) Investigate feature selection algorithms that can isolate subgroup-specific biomarkers; this process will account for biological variability and disease heterogeneity among human populations. <br\/> <br\/>Spike-in study will be conducted to obtain replicate LC-MS spectra with known peptide differences. The spectra from this study will be used to optimize the proposed data preprocessing and feature selection algorithms, and compare them with other existing solutions. The optimized analytical tools will be applied to select the most useful peaks for detecting hepatocellular carcinoma (HCC) at a treatable stage. Blood samples collected from patients with cirrhosis, HCC cases, and healthy controls in Egypt, United States, and Thailand will be used. The peptides that correspond to the selected peaks will be identified by sequencing the peaks using MS\/MS instruments and complementary methods. The ability of these candidate peptide markers to detect HCC will be validated using appropriate isotope dilution mass spectrometric assays. <br\/> <br\/>Novel aspects of the proposed biomarker discovery algorithms include the capability to (1) normalize and align spectra to allow the discovery of candidate peptide biomarkers whose performances are not influenced by non-disease related artifacts in the data, (2) detect peaks that are consistent with the ones manually selected by MS experts, (3) select biomarkers through an approach that accounts for biological variability and disease heterogeneity among subjects, and (4) provide a framework for a complete analytical pipeline beginning from sample preparation to validation of candidate peptide biomarkers. <br\/> <br\/>The project will establish new interdisciplinary research collaboration in the areas of bioinformatics, mass spectrometry, and tumor biology, in which faculty and student members of the project will be cross-trained to transcend traditional disciplinary boundaries. It will provide research-enriched learning experiences to students through two educational activities at Georgetown University Medical Center. <br\/> <br\/>Further information on the project may be found at the project web site: http:\/\/microarray.georgetown.edu\/ressomlab","title":"III-CXT-Small: Analysis of Mass Spectrometry Data for Biomarker Discovery","awardID":"0812246","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[371147,371148],"PO":["565136"]},"140971":{"abstract":"Proposal Number: 0820061\/0820034\/0820230<br\/><br\/>TITLE: Design and Run-time Techniques for Physically Coupled Software<br\/><br\/>PIs: Ramesh Govindan (USC), Rajesh Gupta (UCSD), Mani Srivastava (UCLA), and Paulo Tabuada (UCLA)<br\/><br\/>ABSTRACT:<br\/><br\/>Many real-world systems are deeply embedded in the physical world and their operational behavior is determined in large part by a tight coupling between the system components and the physical environment. This project seeks to establish the scientific principles governing software for such physically-coupled systems by focusing on four challenges in the context of distributed sensing and control applications: 1) Support for physical context in the form of programming structures that enable application software to explicitly capture the state of the physical world as an observable in an embedded computation; 2) Formal methods for composing software modules that indirectly interact with each other through the physical world, and a run-time safety supervisor that provably enforces correctness of composition; 3) Programming structures to enable design and verification of applications with resource provisioning that is driven by and adapts to physical-world dynamics; 4) System software support for sharing physically-coupled sensor and actuator resources in distributed settings. In addition, educational techniques targeting the teaching of topics in physically-coupled computational systems are being explored by creating shared educational content in the form of self-contained reusable modules.","title":"Collaborative Research: Design and Run-time Techniques for Physically Coupled Software","awardID":"0820034","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":["457653"],"PO":["564388"]},"139519":{"abstract":"The PLASMA project lays the ground work for a new generation of dense linear algebra libraries that achieve the fastest possible time to an accurate solution on multicore systems by efficiently using all the processors that such systems make available. To work within this design space and leverage the power of million way parallelism, PLASMA researchers are combining new, highly parallelizable algorithms, a programming and execution model that can exploit massive task parallelism, and a flexible memory management facility that helps optimize data locality across a range of different computing platforms. PLASMA's design space is also conditioned by the fact that, in order to support the broadest possible range of computational science problems, the PLASMA library framework must be able to scale both up and down, running at all levels of the platform development chain.<br\/><br\/>The PLASMA project focuses on the following two objectives:<br\/>- Explore new, highly parallelizable algorithms: PLASMA researchers have shown that a wide range of algorithms (e.g. Cholesky, LU and QR factorizations) can be expressed as algorithms by tiles, greatly improving parallel performance of these operations on multicore processors. They are currently extending this concept to a broad range of linear algebra algorithms.<br\/> - Develop an algorithm description abstraction for expressing parallelism: Building on the well established concepts from dataflow architectures, PLASMA researchers are developing high-level abstractions for algorithm description that make task dependencies explicit and therefore facilitate scheduling on multicore systems. Since all the information about parallelism in the algorithm is contained in its dependency graph, which is a Direct Acyclic Graph (DAG), the PLASMA approach replaces a programming language algorithm definition with a graph-based algorithm definition.","title":"CPA-ACR-T: PLASMA: Parallel Linear Algebra Software for Multiprocessor Architectures","awardID":"0811642","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["558550","486084"],"PO":["565272"]},"142940":{"abstract":"Quantum computation lies at the intersection of computer science and physics and is a promising emerging technology. Quantum algorithms take advantage of quantum effects not available to conventional computers. <br\/>There are striking examples where a quantum algorithm, running on a quantum computer, would outperform any classical computer attempting the same task. These include finding the prime factors of a number, searching an unordered list and determining who wins a game. The discovery of new algorithms is key to the growth of this field. The investigators hope to develop new algorithms as well as study the performance of certain known algorithms whose run time is not yet established. The hope is to continue to develop a fruitful synergy between techniques used in physics and conventional algorithm design. <br\/>The research here would bridge the fields of computer science and physics looking for new algorithms as well as deepening our understanding of the physics which underlies the quantum speedups. The development of a large scale functioning quantum computer would change the world of computation and this research attempts to aid the effort to develop such a machine.<br\/><br\/> The topics to be investigated include quantum walk algorithms, algorithms for evaluating the Jones polynomial and other related quantities, as well as cryptographic protocols for unforgeable identification. The adiabatic algorithm will be studied using a Matrix Product State ansatz as well as by large scale numerical simulation and the fault tolerance of this algorithm will also be investigated.","title":"EMT\/QIS: Physics Based Approaches to Quantum Algorithms","awardID":"0829421","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["518163","518164"],"PO":["565157"]},"141972":{"abstract":"After recent high profile failures, the spotlight has settled on the nation's aging infrastructure. The prevalent means of damage evaluation is a qualitative visual inspection which can identify some external but no internal damage. The goal of structural health monitoring is the determination of damage state, or \"health,\" at any given time. Any damage increases local stresses as well as overall motion such that a structure is less likely to resist another abnormal event. Experimental studies can contribute to understanding progressive collapse resulting from this damage accumulation. The goal is to limit the total damage caused by all abnormal events as well as aging throughout the structure's lifetime, allowing more effective condition-based maintenance and increasing public safety. Longer term goals include in-field testing and required reinforcements. Future expansion areas include hazard resistance, non-destructive testing, and design revisions.<br\/><br\/>The overall objective is to synthesize research and educational activities through an experimental structural health program. The three project objectives are initiation of research on structural health management, creation of unique laboratory experiences for a diverse population, and further development of the PI as a role model for underrepresented groups. Through a laboratory-based research and educational plan, a cross-disciplinary vision exists for a practical research facility that doubles as an innovative classroom. Five currently active outreach programs will incorporate this project's activities to produce a positive scholastic impact on pre-college, undergraduate, and graduate students as well as educators and professionals. Considering diverse perspectives, every student can benefit from an experimental encounter with dynamics: the excitement of demolishing a structure is unfailing.","title":"BRIGE: Infrastructure Health Evaluation via Experimental Techniques","awardID":"0824227","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"I435","name":"Defense Intelligence Agency"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"7680","name":"ENG DIVERSITY ACTIVITIES"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1385","name":"SPECIAL STUDIES AND ANALYSES"}}],"PIcoPI":[377855],"PO":["461803"]},"143930":{"abstract":"There is growing academic and commercial interest in models of<br\/>interactive computing where code segments that lie in the critical<br\/>path of user interaction are split across a network boundary. We use<br\/>the term \"Network-Interactive Computing\" to characterize such models.<br\/>Thin client computing and Ajax-based computing are two<br\/>examples. Unfortunately, introducing network delays into the critical<br\/>path of user interaction can degrade the crispness of system response,<br\/>and hurt usability. Today, we lack the scientific framework to<br\/>quantify the impact of latency on network-interactive computing.<br\/><br\/>This research will establish the scientific underpinnings of<br\/>network-interactive computing with respect to performance and<br\/>usability. Our work will span three areas: (a) Development of metrics<br\/>and benchmarks; (b) Calibration of metrics against user experience;<br\/>and (c) Classification of real-world applications based on their user<br\/>interaction characteristics. We will develop the intellectual tools<br\/>and evaluation techniques to help answer questions such as the<br\/>following: ``Will a legacy interactive application that performs well<br\/>on a local computer degrade unacceptably in a specified<br\/>network-interactive computing environment?''; ``When designing a new<br\/>environment for network-interactive computing, can one predict which<br\/>applications will perform acceptably?''; and, ``Can one develop a<br\/>taxonomy of applications that allows us to easily understand and<br\/>predict the impact of networking changes on interactive performance?''<br\/>Our goal is to establish network-interactive computing on a sound,<br\/>well-validated and replicatable experimental foundation.","title":"CSR: DMSS, TM: Quantifying the Impact of Latency on Network-Interactive Computing","awardID":"0833882","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["492191","475390","413598"],"PO":["565255"]},"144800":{"abstract":"Despite the diversity of social science research, most cyberinfrastructure work has focused on social science research that is similar in approach to the natural sciences. Yet all the social sciences, be they theoretical or empirical, qualitative or quantitative have potential to be transformed with collaborative technology. Meeting this potential will rely upon understanding the diverse forms of intellectual and collaborative work that make up the social sciences in ways distinct from work in the natural sciences. Transforming research on culture, language, philosophy or practice will depend not just on data grids or online archives, but on appropriate technologies that fit with the diverse intellectual and collaborative practice involved in social science research. To meet these opportunities, this project focuses on studying ethnographically the diverse intellectual practices of social science research, using these findings to build new transformative social science cyberinfrastructure. <br\/><br\/>Drawing on science and technology studies as well as computer supported collaborative work, the project will investigate the distinct challenges that social science research faces by exploring the role of theoretical debate and discussion around conceptual questions alongside the diversity in approach, definitions, and methods of research. The research will take a threefold approach: questioning intellectual practice, conducting empirical investigations, and building and testing technological interventions in three ?social science virtual organizations.? Each of the three sites was chosen to represent a distinct form of virtual organization: Vl2 is a major NSF funded multi-site project studying language use; UC Links is a UC wide organization that brings together research and practice around after school clubs; and, XMCA is an organization that brings together an intellectual community around journals and a long lived (25 years) and heavily used email discussion list. The results of this research will be used to develop new collaborative mobile and online systems that assist collaboration. These tools will advance our understanding of how collaboration can be better supported amongst researchers. By improving the productivity of the social sciences through the findings and resulting tools, this project will bring a range of benefits to an important aspect of academic work currently underserved by cyberinfrastructure.","title":"VOSS - Supporting Virtual Organizations In The Social Sciences","awardID":"0838330","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[386280,386281],"PO":["446196"]},"142985":{"abstract":"This CPATH Conceptual Development and Planning project integrates concepts from biology and computer science to create a new track in biological computing. It includes 23 faculty members from 14 academic disciplines with a vision for nurturing future leaders in biological computing. The track recognizes the importance of computing concepts and practice to current thinking in biology as well as the potential for biological processes to inform the development of computer science. The project involves institutional transformation and includes outreach to secondary schools and broader communities.<br\/><br\/>The intellectual merit of the project lies in the importance and currency of the topic and clear need for such changes in computing education to prepare the upcoming generation of computing professionals. The project team includes researchers with significant expertise in both the computing discipline research that underlies the implementation and in educational innovation. The project has the potential for national impact and to provide new models for computing education of the future.<br\/><br\/>The broader impacts of the project lie in the potential to address changing demands on computing professionals and to attract a diverse audience of students. The project includes outreach to high school students and broader communities. The project has the potential to provide quality research-based resources for the preparation of undergraduates for careers in biological computing.","title":"CPATH CDP: Integrating Biology and Computing: Empowering Future Computer Professionals","awardID":"0829607","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["547502","507752",380682,"528208","549542"],"PO":["564181"]},"142996":{"abstract":"The goal of this project is to provide a model for incorporating computational thinking into the undergraduate general education curriculum, and to demonstrate the intrinsic importance of computational thinking as a part of the general education of all undergraduate students. The vision is that \"Computational Thinking becomes a standard general education category in the undergraduate curriculum at all academic institutions nationwide. To meet this goal, the main objective of this CPATH-CDP grant is to develop and assess a particular model for developing pathways of computational thinking throughout the general education (GenEd) curriculum at Towson University. Such pathways would be realized through the development of discipline-specific computational thinking courses by faculty members from various departments across all colleges on campus, supported by a common \"Everyday Computational Thinking\" freshman-level course. As the second-largest university in the state of Maryland with a broad range of academic programs and faculty dedicated to teaching, Towson is an ideal test bed for the proposed model. One of the outcomes of the recent revision of the Towson General Education curriculum is the addition of a three-credit freshman seminar category that all students would need to satisfy. This new category is a perfect place for an introductory computational thinking course. In addition, the committee will also be proposing a junior seminar GenEd category, which will also be perfect placement of the more advanced discipline-specific computational thinking courses. The model that this project will investigate, for infusing general education curricula with concepts of \"computational thinking\", has the potential to dramatically advance the understanding of computing by all undergraduate students, not just those in a computing major. Such understanding not only enables use of computing technology and methods, but is an important intellectual asset for all types of problem solving.","title":"CPATH CDP: Piloting Pathways for Computational Thinking in a General Education Curriculum","awardID":"0829661","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":[380733,380734,"528377","528465"],"PO":["565136"]},"145801":{"abstract":"Existing Bayesian network-based diagnostic approaches often produce either underspecified explanations that do not fully account for given observations, or overspecified explanations that contain variables unnecessary in explaining the observations. This project addresses these limitations by elaborating a new framework, Most Relevant Explanation (MRE), for explanation in Bayesian networks that can automatically identify the most relevant fault(s) for given observations. In particular, the project includes development and evaluation of approximate, efficient diagnostic algorithms for multiple faults based on the MRE framework. Because diagnosis is quite likely the most successful application of Bayesian networks, MRE-based methodologies will significantly improve the current practice of computer-aided diagnosis by improving the comprehensibility and efficiency in areas such as medicine and manufacturing.","title":"SGER: A Framework for Explanation in Bayesian Networks","awardID":"0842480","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["432571"],"PO":["564316"]},"143502":{"abstract":"As the evolution of living species is determined by few basic mechanisms, <br\/>such as genetic heritage, genetic variation, and natural selection, <br\/>it may be that the evolution of large-scale internetworks is similarly <br\/>determined by simple, but still unknown, principles.<br\/>The investigators aim to discover the fundamental character of network <br\/>evolution, the laws that describe it, and ways in which networking <br\/>researchers or innovators can effectively influence this process.<br\/>This research can cause a paradigm shift in the way we understand the <br\/>Internet and on how we attempt to introduce new features and correct its <br\/>problems. The project also has a significant education component, including <br\/>a new course that is cross-listed in the computer science and biology <br\/>graduate programs of Georgia Tech. <br\/><br\/>The first thread of this research focuses on the evolution of the <br\/>Internet Autonomous System (AS) ecosystem. The goal is to better understand <br\/>this complex and dynamic ecosystem, the behavior of entities that constitute <br\/>it, and the nature of interactions between those entities. ASes attempt to <br\/>optimize their utility or financial gains by dynamically changing the ASes <br\/>they interact with, and thus by changing the organization of their ecosystem.<br\/>In the second research thread, the focus is on the evolution of network <br\/>design and architecture and on the laws and principles that determine the <br\/>evolution of large-scale internetworks. The investigators will explore <br\/>questions about the price of evolution in network design, the trade-offs <br\/>between evolutionary and clean-slate architectures and the evolutionary <br\/>implications of layered network organization. The objective is not only to <br\/>understand the evolution of the Internet, but also to provide concrete <br\/>guidelines for the design of evolvable architectures for the Future Internet.","title":"NECO: Towards a theory of network evolution","awardID":"0831848","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["500670","550447"],"PO":["565303"]},"143755":{"abstract":"Large scientific applications have complex communication needs, including intra-machine point-to-point and global communications, cross-machine checkpointing, and data outputs for online validation and remote data display. Coupled scientific models for multidisciplinary investigations further add to this complexity. This multi-purpose, rich, and dynamic nature of data communications in future exascale codes presents the first challenge addressed by this research.<br\/>Further, given the many-core nature of future computer chips and the likely presence of specialized hardware accelerator cores, application-level communications and I\/O face an increasingly complex set of on-chip, cross-node, and cross-machine interconnects. The complex nature of the physical communication infrastructures present in future exascale machines is the second challenge addressed by this research. In summary, the problem facing developers of future exascale applications for scientific discovery is how to effectively manage the complexity of their communication needs while protecting their most critical `core' communications from perturbation.<br\/><br\/>This project will develop higher level, explicit models for the data communications performed in future exascale codes. These models, called C-Models, will describe and implement the communications performed for I\/O for purposes of online analysis, storage, and visualization, and for data movements across coupled application codes, and will also capture the interaction of the data movements implied by all of the above with the internal data communications inherent to each single application. This abstraction and encapsulation of communication complexity is key to taming the complexity of future exascale applications. The C-Model infrastructure will also help protect the critical core communication component of these applications from perturbation, helping to maximize the performance of these applications on leadership-class machines.","title":"Collaborative Research: Actively Managing Data Movement with Models - Taming High Performance Data Communications in Exascale Machines","awardID":"0833062","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["523077","501621","501622","517873"],"PO":["565272"]},"145944":{"abstract":"This SGER addresses the problem of metaphoric inference in Natural Language Understanding (NLU). Language is fundamentally a creative act and NLU systems require extensive semantic knowledge and sophisticated inference methods that are capable of dealing with figurative language. A prototypical example of the power of figurative language in everyday discourse is the ubiquitous use of metaphor to perform most linguistic functions including predication, modification, and reference.<br\/><br\/>To date, no comprehensive computational system addressing all the aspects of metaphor has been implemented or even designed. Our project seeks to remedy this situation by addressing key challenges in building a scalable, open source, model of metaphor. Building on previous modeling work and leveraging results from Cognitive Linguistics, we are exploring techniques to a) design and populate a machine readable metaphor ontology, b) analyze the metaphoric encoding of crucial discourse information, including event structure and communicative intent, and c) use machine learning algorithms for metaphor recognition from textual sources.<br\/><br\/>On a practical scale, this project provides a machine readable ontology of metaphors, allowing programmatic access to the relational structure of the data, enhancing reuse, and fostering incremental development. Coupled with the metaphor recognition tools, this takes us part way (a companion requirement is a corpus annotation effort) toward creating an open and scalable metaphor resource. From the scientific side, this project creates a better understanding of the form and content of metaphoric inference and the first computational framework to explore the automatic processing of this information from text languages and media.","title":"SGER: Toward figurative language interpretation: A pilot study","awardID":"0843275","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["142866"],"PO":["565215"]},"143403":{"abstract":"Energy infrastructure is a critical underpinning of modern society that any compromise or sabotage of its secure and reliable operation will have a prominent impact on people's daily lives and the national economy. Past failures such as the massive northeastern power blackout in August 2003 and the recent Florida blackout in February 2008 have revealed serious defects in both system-level management and device-level designs. This project is aimed at a hardware-in-the-loop reconfigurable system with embedded intelligence and resilient coordination schemes to tackle the vulnerabilities of the power grid. The research in this project will investigate a number of critical issues related to this system, including the design of a hybrid system architecture, the development of advanced power electronic devices with embedded intelligence for reconfiguration of the power grid, the development of control- theoretic algorithms to guarantee real-time adaptation to system changes caused by various attacks, and the threats to existing state estimation algorithms and their defenses.<br\/><br\/>The proposed research can help secure the grid and prevent potential outages from happening. Instead of limiting the system to manage existing devices, the developed system is adaptive to the future power grid in years to come. The power grid is a typical example of complex networks of highly interacting subsystems. Solving these fundamental problems to create a resilient power grid has a direct and immediate impact on this and other critical infrastructures.","title":"CT-M: Collaborative Research: A Resilient Real-Time System for a Secure and Reconfigurable Power Grid","awardID":"0831302","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["553866"],"PO":["497499"]},"144866":{"abstract":"This research investigates the conditions under which three-dimensional virtual environments (3DVEs) best support innovation. Ideally, virtual teams can support innovation by providing the benefits of diverse knowledge acquired from different contexts. However, while traditional information communication technology (ICT) has made virtual teams possible, it does not provide the ?co-presence? that face-to-face communicators use to draw inferences about one another?s knowledge, limiting the understanding and problem solving necessary for innovation. 3DVEs offer an electronic surrogate for face-to-face communication. But, despite the promise of 3DVEs, little is known of how teams operate in 3D virtual worlds or what work practices are more successful. A multidisciplinary team composed of researchers specializing in organization theory and learning in virtual environments has partnered with computer scientists from Sun Microsystems to explore the co-evolution of technical tools and work routines in 3DVEs and to evaluate which technical tools and work routines best support innovation. The team will conduct a comparative analysis of nine project teams at Sun Microsystems: one group of three teams using traditional ICT, a second group of three teams operating in Sun?s 3DVE?Wonderland?and a third group of three teams operating in Linden Labs? 3DVE?Second Life. This research will advance our understanding of virtual teams and sociotechnical systems as well as collaborative innovation and learning in 3DVEs. Society at large will benefit from advancing understanding of the effectiveness of 3DVEs for learning and innovation.","title":"Virtual Teams in 3D Virtual Environments: A Comparative Analysis of Project Team Innovation","awardID":"0838550","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"5376","name":"INNOVATION & ORG SCIENCES(IOS)"}}],"PIcoPI":[386461,386462,386463,386464],"PO":["446196"]},"143304":{"abstract":"Abstract<br\/><br\/>Theoretical Foundations of Collaborative Information Storage and Transmission<br\/><br\/><br\/>This research addresses the theoretical foundations for enabling distributed information storage and communication in next-generation large-scale heterogeneous systems. A key theme underlying this investigation is that of massive collaboration among system nodes in accomplishing this, in contrast to scaling the network by solely throwing infrastructure at the problem. The fundamental challenge is to embrace a distributed systems approach that can rely on the strength of numbers or the system scale itself to build an overall network that is scalable, robust, and capable of meeting performance guarantees. The application scope of this research is broad, and includes next-generation distance learning, and multi-node video conferencing based on peer-to-peer collaboration that has the potential to reach the masses rather than being confined to those privileged to have access to expensive centralized infrastructure. <br\/><br\/>The basis for this investigation is the development of both the theoretical foundations and the architectures and algorithms needed to realize a massively scalable, inherently robust, and quantifiably reliable information storage and transmission systems. A focal point of the study is that of distributed storage and collaborative media delivery paradigms, both with and without delay constraints. In recognition of the bottleneck caused by centralized knowledge and global coordination, this study embraces a distributed approach involving local and randomized individual behavior that result in a highly dynamic system topology. The key challenge is to exploit the scale of the system itself in guaranteeing reliability and global performance guarantees out of these local interactions. The focal point of this study will be on distributed storage systems based on network coding.","title":"Theoretical Foundations of Collaborative Information Storage and Transmission","awardID":"0830788","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["521731"],"PO":["564898"]},"145966":{"abstract":"As the Internet evolves into a global communication infrastructure, <br\/>it is becoming complex and ubiquitous. It is bringing together <br\/>heterogeneous collections of hosts and network devices such as <br\/>wireless mesh networks. A consequence of this ever-increasing <br\/>scale, complexity and agility is that problems of managing this <br\/>infrastructure keep getting worse. Research concentrating on <br\/>streamlining fundamental issues has shown that absence of a single <br\/>end-to-end lightweight hybrid network management solution with the <br\/>necessary levels of context awareness and adaptability, while much <br\/>desirable, has not yet been realized. Consequently, the community <br\/>is still detained from fully gaining from many of the emerging <br\/>applications of the cyber-infrastructure. <br\/><br\/>This research involves an exploratory investigation towards a <br\/>?drive through? network management platform for ?on-the-fly? <br\/>Internet-enabled mobile networks. Objectives include new conceptual <br\/>approaches that couple application, and middleware designs with design <br\/>of network management services; novel theoretical models that tap into <br\/>in-network management characteristics of pervasive computing so that <br\/>missing or conflicting information gathered from multiple participating <br\/>nodes can be tolerated; augmentation of security management algorithms <br\/>with trustworthiness functionalities; theoretical schemes that examine <br\/>epistemic uncertainty by fusing imprecision, uncertainty and event <br\/>synchronization with belief theory; and extensions to archetype <br\/>platforms for supporting autonomic network management and configuration. <br\/>The intellectual merit of the research lies in presenting a first step <br\/>towards synthesis of integrated network management theoretical and design <br\/>solutions that support traditional network management protocols and <br\/>functionalities in addition to the dynamic aspects of infrastructures-less <br\/>networks. Although some risk exists, focused research can overcome the <br\/>current barriers. The broader impact of the research is that this work <br\/>will pave the way to transformative next generation network management <br\/>paradigms and tools.","title":"SGER: A Lightweight Drive-Through Management Service for Internet-enabled Ad-Hoc Networks","awardID":"0843385","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["539039"],"PO":["564924"]},"143425":{"abstract":"Low cost devices, e.g. RFIDs, smartcards, sensor nodes, etc., are becoming crucial for building the next generation pervasive and ubiquitous heterogeneous networks. Given the massive volume, the per-unit manufacturing cost will play a key role in the adoption of these technologies. These devices are constrained in their available computational power and footprint. Yet, their cryptographic units, which tend to be among the most demanding components in the device architecture, need to be hardened against physical tampering. The project is exploiting hardware anomalies for cryptographic ends. Even as chip manufacturing progresses and becomes more precise, one can always expect slight variations along the production line which can be used to distinguish one physical device from any other manufactured on the same production line. The team is developing models, processes and hardware primitives which contribute to the goal of exploiting these individual fingerprints so that they can be used to efficiently identify devices and enable secure, tamper-resilient communication. This initiative is providing low-cost, tamper-resilient cryptography from physical functions, and thereby plays an enabling role in the adoption of a wide array of products and applications to the benefit of the national economy and national security. The results of this project include new physically unclonable function (PUF) constructions with a particular emphasis on constructions which naturally permit reduction to computationally difficult problems, PUF-enabled cryptographic building blocks (such as secure and efficient storage, tamper-resilient state machines, etc.) and PUF-enabled cryptographic primitives (e.g. authentication schemes, block ciphers, pseudo-random generators).","title":"CT-ER: Exploring Physical Functions for Lightweight and Robust Cryptography","awardID":"0831416","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["564461","550273"],"PO":["427499"]},"143546":{"abstract":"Along with the rapid development of Internet Protocol (IP) based technologies and wireless networks, telecommunication and Internet services providers (SPs) are showing great interests in technologies for integrating wireless access networks, wireless backhaul networks, and Internet backbone into a seamless IP solution. One of the fundamental issues towards this ambitious objective is that each network along an end-to-end path needs to be properly dimensioned and managed for quality of service provisioning, for which this project is expected to make significant contributions.<br\/><br\/>Intellectual Merit: This project will develop accurate capacity planning tools, efficient medium access control protocols, and dynamic resource sharing techniques for highest resource utilization in both wireless access and backhaul networks. In particular, all the developments are neatly organized under a generic cross-layer framework, where the complex interplay between the upper-layer IP application data arrival process and the lower-layer wireless medium access control are accurately captured and modeled. <br\/><br\/>Broader Impact: On the education front, this project incorporates theoretical studies in the areas of queueing theory, optimization, and wireless networking, with practical investigations including computer simulations and testbed experiments. This interdisciplinary research will not only provide various training projects to undergraduate and graduate students, but also inspire students to pursue high-quality research with a creative, open-minded, cross-disciplinary perspective. On the industry front, the success of this project will produce techniques that not only facilitate the SPs to squeeze the most out of the wireless networks for highest revenue, but also trigger new customer\/SP interplay models and new network integration models.","title":"NeTS-NEDG: Squeezing the Most Out of Wireless Access and Backhaul Networks: A Generic Cross-Layer Analytical Approach","awardID":"0832093","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["549772"],"PO":["565303"]},"143788":{"abstract":"Most of the traditional HEC applications and current petascale applications are written using the Message Passing Interface (MPI) programming model. The MPI-1 standard provides communication semantics for two-sided operations. The MPI-2 standard added new one-sided communication semantics. However, most of the current candidate petascale applications continue to use the MPI-1 semantics.<br\/>These applications find the available MPI one-sided communication semantics and their implementations in existing MPI-2 libraries very restrictive to exploit performance, scalability and fault-tolerance. The investigators, involving computer scientists from Ohio State University (OSU) and computational scientists from Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), will study and analyze the current restrictions in the MPI one-sided communication semantics, their implementations and usages. Novel solutions will be proposed to alleviate these restrictions so that the next generation ultra-scale systems and applications can be scaled to hundreds of thousands of cores.<br\/><br\/>The investigators will specifically address the following challenges: 1) What are the limitations of using MPI one-sided operations in petascale applications? 2) What extensions are possible to the current MPI one-sided operations to alleviate such limitations? 3) How to design and implement these extensions in an MPI library for emerging ultra-scale HEC systems? 4) How to redesign petascale applications to take advantage of proposed one-sided extensions and their implementations? and 5) What kind of benefits (performance, scalability and fault tolerance) can be achieved by the proposed extensions for petascale applications on the next generation ultra-scale systems? The research will be driven by a set of petascale applications (ENZO, AWM-Olsen, PSDNS and MPCUGLES) from established NSF computational science researchers running large scale simulations on the TACC Ranger and other NSF HEC systems.","title":"Collaborative Research: Extending One-Sided Communication in MPI Programming Model for Next-Generation Ultra-Scale HEC","awardID":"0833155","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["558590"],"PO":["565272"]},"144646":{"abstract":"Proposal Number: 0837871<br\/>PI: Ehab Al-Shaer<br\/>Institution: DePaul University<br\/>Lead<br\/><br\/><br\/><br\/>Title: NSF Workshop on Assurable and Usable Security Configuration<br\/><br\/><br\/>It is estimated that configuration errors at the system or network level enable 65% of the attacks that have been seen, many of which have serious consequences because often a misconfigured system permits attackers to access security-critical objects. <br\/><br\/><br\/>The objective of this workshop is to provide a venue for discussions to eliminate or drastically reduce misconfigurations. It falls on overworked system administrators or the home user to configure their systems, often resulting in seriously misconfigured systems. Research is needed to lead to tools that will drastically reduce the work required to configure systems so that they are not open to attack. The research will involve policy languages that permit users to characterize their security and privacy needs, techniques to reify policies to enforceable rules, and new techniques that support changes to a policy that reflect down to the appropriate changes to rule sets. <br\/><br\/>The workshop involves the participation of industry so that the academic participants will become familiar with policies and rule sets drawn from real enterprises and networks. <br\/><br\/><br\/>The workshop will produce a detailed report to be posted on the NSF web site","title":"NSF Workshop on Assurable and Usable Security Configuration","awardID":"0837871","effectiveDate":"2008-09-15","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["563217"],"PO":["529429"]},"143436":{"abstract":"Energy infrastructure is a critical underpinning of modern society that any compromise or sabotage of its secure and reliable operation will have a prominent impact on people's daily lives and the national economy. Past failures such as the massive northeastern power blackout in August 2003 and the recent Florida blackout in February 2008 have revealed serious defects in both system-level management and device-level designs. This project is aimed at a hardware-in-the-loop reconfigurable system with embedded intelligence and resilient coordination schemes to tackle the vulnerabilities of the power grid. The research in this project will investigate a number of critical issues related to this system, including the design of a hybrid system architecture, the development of advanced power electronic devices with embedded intelligence for reconfiguration of the power grid, the development of control- theoretic algorithms to guarantee real-time adaptation to system changes caused by various attacks, and the threats to existing state estimation algorithms and their defenses.<br\/><br\/>The proposed research can help secure the grid and prevent potential outages from happening. Instead of limiting the system to manage existing devices, the developed system is adaptive to the future power grid in years to come. The power grid is a typical example of complex networks of highly interacting subsystems. Solving these fundamental problems to create a resilient power grid has a direct and immediate impact on this and other critical infrastructures.","title":"CT-M: Collaborative Research: A Resilient Real-Time System for a Secure and Reconfigurable Power Grid","awardID":"0831466","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["527044","527045","521960","492080","554423"],"PO":["497499"]},"143799":{"abstract":"Scientific applications can model interactions of medicines with proteins, predict the behavior of nano-materials, model the climate, and lead to better understanding of physical phenomenon. These applications demand ever greater computational resources, which can only be supplied by new parallel computers with ever increasing capability and complexity. Parallel computing can bring about new breakthroughs only if the complexity of efficient parallel programming can be overcome. Yet developing parallel applications remains significantly more difficult than serial development. Petascale machines with hundreds of thousands(and possibly millions) of processors add to the complexity, as do new sophisticated algorithms and multi-physics applications. <br\/><br\/><br\/>This project is developing a new approach to parallel programming which builds upon the automatic resource management and composibility of the Charm++ framework. This approach includes development of multiple, individually incomplete, programming models. Each model simplifies parallel programming while still covering significant categories of applications. This collection of interoperable models, supported by complete models including Adaptive MPI and Charm++, provides a powerful environment for developing future petascale applications. A compiler framework is being developed which provides a common representation and facilitates compatibility between models. In addition, the vision includes abstractions supported by libraries for commonly needed data types and functionalities. These abstractions will support and interoperate with domain specific frameworks. The results of this project will enable the large community of computational scientists and engineers to harness petascale machines with relative ease in order to generate breakthroughs in scientific discovery and engineering design.","title":"Simplifying Parallel Programming for CSE Applications using a Multi-Paradigm Approach","awardID":"0833188","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7952","name":"HECURA"}}],"PIcoPI":["533380","558488","542046"],"PO":["565272"]},"143205":{"abstract":"PROJECT ABSTRACT<br\/>Cutset Sampling and Processing<br\/>D.L. Neuhoff (P.I.)<br\/>This research project is focused on the development of efficient new methods for sampling and<br\/>processing multidimensional data such as images, video, or spatially distributed sensor data,<br\/>especially the first and last. Traditional methods of digital image processing rely on the taking<br\/>and processing of regularly spaced samples (pixels), typically at the sites of a lattice, for example<br\/>a square grid of points. In contrast, this project is investigating digital image processing<br\/>based on samples taken in a cutset, the simplest example of which is a square grid of lines that<br\/>spans the image. More generally a cutset is defined in terms of a set of neighborhood relations<br\/>among a discrete set of potential sample sites. Cutset sampling is expected to be especially<br\/>effective when the capturing of image edge information is important.<br\/>The principle motivations for cutset sampling derive from the facts that two-dimensional data<br\/>can often be well modeled by Markov random fields and that unsampled pixels of Markov<br\/>random fields can be efficiently estimated with belief propagation algorithms. This research<br\/>project is investigating different forms of cutset sampling, and it is developing reconstruction<br\/>algorithms tailored to cutset sampling, image compression methods based on cutsets, distributed<br\/>sensor network algorithms for scenarios in which sensors are deployed on cutsets, and analysis<br\/>techniques to determine the strengths and weakness of cutset sampling, and to enable comparisons<br\/>with conventional lattice-based sampling.","title":"Cutset Sampling and Processing","awardID":"0830438","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[381337],"PO":["564924"]},"145878":{"abstract":"A genre of robots has begun to move from the pages of science fiction to the research laboratories, and even into society at large. These robots, in various ways and to varying degrees, have a persona, are adaptive and autonomous, and can communicate, learn, use natural cues, and self organize. Such \"personified\" robots, which will become part of people's everyday lives, form the focus of this research wherein a new approach to their design will be explored that builds on a concept the PI terms \"interaction patterns.\" By an interaction pattern the PI means characterizations of essential features of social interaction between humans and robots, specified abstractly enough such that many different instantiations of the pattern can be uniquely realized given different types of robots, purposes, and contexts of use. A simple example of an interaction pattern between humans takes place when we are introduced to a new person. In Western society, we often shake hands, say \"hi,\" exchange our names, and perhaps engage in a little chit-chat about the weather. In other cultures, people might bow to one another, or offer a namaste greeting. We can call this universal social activity an \"Introduction\" and its purpose is to facilitate further social interaction. While the Introduction is never enacted exactly the same way twice, the activity is structured and follows a recognizable pattern. The PI argues that this pattern is but one of many that can be used to help structure human-robot interaction. In the current project, the PI will generate between 50-75 social and moral interaction patterns. He will specify a methodology for designing interaction patterns (which, in part, draws from characterizing successful patterns of interaction in human-human interaction and human-nature interaction), will articulate the optimal levels of characterization between abstraction and concretization for interaction patterns, will show multiple instantiations of individual patterns (to highlight that patterns are not rigid molds but engender a multitude of varied instantiations), will implement technically a sequence of these patterns with an actual robot, either autonomously or (more often) through a \"wizard-of-oz? technique, will establish a beginning set of guidelines for validating interaction patterns, and finally will begin to validate a handful of sequenced patterns. The technical implementation and beginning validation of the patterns will occur using the humanoid robot Robovie from Advanced Telecommunications Research (ATR) in Japan. <br\/><br\/>Broader Impacts: If the PI's approach proves successful, and is then taken up by other laboratories, it will reshape the existing landscape in the human-robot interaction (HRI) research community. Furthermore, it is likely in the near future that the public will raise serious concerns about the introduction of personified robots into modern society; this research will provide a proactive design agenda that recognizes the legitimate societal concerns and shows how we can \"do it right\" with robots.","title":"HCC-SGER: Social and Moral Interaction Patterns with a Personified Robot","awardID":"0842832","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["546850"],"PO":["565227"]},"143227":{"abstract":"This research involves the development of improved methods for<br\/>automatic processing of text documents. The research focuses<br\/>in particular on two specific text-handling problems: lossless<br\/>data compression and classification. Lossless data compression<br\/>is useful for reducing storage and transmission requirements<br\/>for large text documents. It is also an important step in<br\/>sending text securely via cryptography. Classification arises<br\/>frequently in web search problems and also in forensics, where<br\/>the goal is to determine the authorship of an anonymous document.<br\/><br\/>The approach is to draw new insights into these problems by<br\/>using a novel asymptotic regime in information theory, which<br\/>more accurately models real text sources than classical<br\/>models do. Specfically, the investigators consider a regime in which <br\/>the size of the data set and the source alphabet are comparably large. <br\/>It can be shown that most classical information theory techniques<br\/>fail in this \"rare-events\" regime. Nonetheless, new techniques can <br\/>be developed that are tailored to this regime that yield new algorithms <br\/>and insights.","title":"Collaborative Research: Information Theory of the Rare Event Regime","awardID":"0830512","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["523699"],"PO":["564924"]},"143348":{"abstract":"Modern organizations, such as businesses, non-profits, government agencies, and universities, collect and use personal information from a range of sources, shared with specific expectations about how it will be managed and used. Accordingly, they must find ways to comply with expectations, which may be complex and varied, as well as with relevant privacy laws and regulations, while they minimize operational risk and carry out core functions of the organization efficiently and effectively. Designing organizational processes to manage personal information is one of the greatest challenges facing organizations (see, e.g., a recent survey by Deloitte and the Ponemon Institute [TI07]), with far-reaching implications for every individual whose personal information is available to modern organizations, i.e., all of us.<br\/><br\/>This project responds to these challenges by developing methods, algorithms and prototype tools for integrating privacy, compliance, and risk evaluation into complex organizational processes. It explores, articulates and characterizes formally the scope and nature of privacy-expectations of stakeholders as well as those of key regulations, such as HIPAA, GLBA, COPPA, BASEL 2, and Sarbanes-Oxley (SOX). It incorporates the diverse perspectives and areas of expertise of its multidisciplinary research team, which includes three computer scientists, one philosopher, and collaborating researchers from IBM. This industry connection facilitates interaction with product teams that have served complex organizations concerned with business process integrity, information security, privacy, and information risk management. The research builds on \"contextual integrity\" (a philosophical account of privacy) as well as language and risk-based methods for privacy policy specification and enforcement. Extensive training and educational opportunities are provided to undergraduate and graduate students and research results integrated into courses at CMU, NYU, Stanford, and UPenn.","title":"Collaborative Research: CT-M: Privacy, Compliance and Information Risk in Complex Organizational Processes","awardID":"0830949","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[381704],"PO":["565327"]},"144679":{"abstract":"Intellectual Merit: The project explores a transformative networking architecture for the future Internet that truly exploits the physical optical layer in a cohesively integrated fashion. We specifically seek to explore an architectural design that encompasses the capabilities of a fully programmable optical layer. Within this unified architectural platform the optical layer becomes a directly accessible component of the network capable of dynamically morphing to meet the increasingly diverse demands of emerging applications. To realize these capabilities will require breaking the barriers currently separating the underlying optical transport technologies from the networking layers by introducing programmable cross-layer access deeply into the physical layer. The effort will include a design exploration and simulation validation for the uniquely integrated architectural platform evaluated across varied networking applications and traffic scenarios envisioned for the future Internet. <br\/><br\/>Broader Impact: If successful, our exploratory effort could provide a new framework for concretely bridging the gap between the future Internet networking research efforts and the underlying heterogeneous substrate by creating an integrative cross-layer environment that enables deeper access to the physical layer in a context consistent with the diverse capabilities and limitations of the emerging technologies. The architectural approach will enable users and applications at the upper layers of the network to directly drive and exploit emerging heterogeneous technologies, thereby broadening the use to the networking community. In our broader outreach we will coordinate and work closely with the FIND research community to develop this platform in a manner that facilitates the realization of the broader architectural research goals.","title":"Small Grant for Exploratory Research: Creating a Future Internet Network Architecture with a Programmable Optical Layer","awardID":"0837995","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["477270"],"PO":["402055"]},"143238":{"abstract":"CCF- 0830545: A Cooperative Game Theoretic Framework for Collaborative Information Processing in Distributed Sensor Nets<br\/><br\/><br\/><br\/>Distributed sensor networks can be used to monitor or track moving targets, environmental processes, and leaked chemical or biological agents. When distributed sensor nets are used to monitor such phenomenon of interest, each node in the network can come up with an estimate based on its own observation of the process. However, nodes may be able improve their estimates if they were to share information with each other, by forming coalitions. The price for such inter-node collaboration can be the cost of communication resources. Unless only the nodes that are the best candidates join a coalition, there is the drawback of waste of individual node power as well as causing unwanted excessive interference to everyone else. <br\/><br\/>These concerns become critical when we consider, as in this research, sensor nets formed by battery-powered nodes operating in a limited bandwidth. But, a.) How do we define best nodes to join a coalition at any given time in a dynamic sensor net? b.) What should be a fair resource commitment from each node to a given coalition? In this research we formulate the above problem of collaborative information processing in distributed sensor networks as a cooperative game theoretic problem. The research covers different types of sensor network architectures as well as various fairness criteria among nodes in a network. By casting in a cooperative game theoretic framework allows us to apply solution concepts from game theory to achieve different fairness criteria among nodes, in developing collaborative processing schemes. The research is carried out in conjunction with an outreach program that focuses on providing international educational experience to students.","title":"A Cooperative Game Theoretic Framework for Collaborative Information Processing in Distributed Sensor Nets","awardID":"0830545","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["517790"],"PO":["564924"]},"143128":{"abstract":"NUMBER: 0830403<br\/>INSTITUTION: University of California-Irvine<br\/>PI: Goodrich, Michael T. & Eppstein, David A.<br\/>TITLE: Collaborative Research: Algorithms for Graphs on Surfaces<br\/><br\/>Collaborative with:<br\/>NUMBER: 0830149<br\/>INSTITUTION: Brown University<br\/>PI: Tamassia, Roberto<br\/>TITLE: Collaborative Research: Algorithms for Graphs on Surfaces<br\/><br\/>ABSTRACT<br\/>This project addresses fundamental questions on geometric and spatial aspects of graphs and networks, with applications to road networks, sensor networks, computer networks, and social networks. In particular, methods will be developed for constructing effective geometric layouts of networks on surfaces and in three-dimensional space and for analyzing properties of networks by exploiting their geometric structure. <br\/><br\/>The focus of the project is the design and analysis of algorithms for graphs on surfaces in the following three thrust areas: (1) algorithms for embedding graphs on surfaces, including methods for greedy embeddings of graphs to facilitate geometric routing, algorithms for manifold triangulation for a set of points sampled from an embedded surface, and techniques for drawing trees in the plane; (2) algorithms for graphs embedded on surfaces, including the study of connections between partial cubes and integer lattices, the development of algorithms for graphs embedded in non-Euclidean spaces, and the design of methods for solving graph problems on quadrilateral meshes; (3) applications of algorithms for graphs on surfaces, including applications of geometric graphs to computer security and algorithms for road networks.","title":"Collaborative Research: Algorithms for Graphs on Surfaces","awardID":"0830149","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["521367"],"PO":["565157"]},"147616":{"abstract":"Live Updates of Scientifc Workflows<br\/><br\/>ABSTRACT<br\/>Computer systems are being applied to increasingly demanding cross disciplinary applications. The environments in which they function and the resources they manage <br\/>are gradually more diverse, distributed and dynamic. This is particularly observed for <br\/>biomedical applications where the need for flexible, dynamic, data intensive computing<br\/>functionalities are substancially critical. This revolution of modern medicine requires <br\/>novel and efficient approaches to support resource integration and scientific workflows<br\/>(SWF) that capture, analyze, retrieve, integrate, and produce molecular information <br\/>including biological specimens derived from tissue, cells, or blood combined with<br\/>medical records or clinical trial data. Such workflows are increasingly used in e-science<br\/>discovery and are characterized by frequent changes that are part of the scientific <br\/>discovery process. The research develops a uniform approach to support live updates of <br\/>scientific workflows accessing distributed resources. The goal of live updates is to allow <br\/>continuous execution in the presence of changes. The research supports updates of <br\/>both the structure and nature of SWFs processes and the data they access and create<br\/>(produce and consume). The research divides the live update problem for SWFs into four<br\/>main technical aims: software update, persistent storage update, workflow update, and <br\/>evaluation and testing. It develops the mechanisms for updates as well as automatic <br\/>checks for the safety of a planned update. The research impacts information-based<br\/>drug discovery and development, and healthcare decision support systems.","title":"Collaborative: CSR-DMSS, TM: Live Updates of Scientific Workflows","awardID":"0849980","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7793","name":"DATA-INTENSIVE COMPUTING"}}],"PIcoPI":[393307],"PO":["535244"]},"145328":{"abstract":"Abstract <br\/>SGER: Collaborative Research: Contextual Machine Translation <br\/><br\/><br\/>Despite significant progress in machine translation, little work has been done to address large-scale translation of natural multi-party human interactions in real-world contexts. Multi-party interactions typically take place in a context where multimodal events such as hand gestures, head gestures, gaze, body movements, etc. are available and utilized by participants to interpret human language and establish common ground. This observation leads to the hypothesis that modeling multimodal environment and interaction discourse can improve automated language interpretation for the purpose of machine translation. Based on this hypothesis, this exploratory research investigates the role of multimodality in machine translation of multi-party conversations using a subset of the AMI meeting corpus. An appropriate level of multimodal representation is identified focusing on user gestures and presentation slides. Correct referents to referring expressions and correct senses to ambiguous words are annotated and used to evaluate whether and how multimodal context improves reference resolution and word sense disambiguation. The enhanced semantic processing utilizing multimodal information is incorporated in statistical machine translation for English-German. The objective is to conduct proof-of-concept experiments exploring the usefulness of multimodal and discourse information for statistical machine translation in real-world contexts. The results from this exploratory study will provide insights on algorithms and systems for automatic extraction of multimodal information, language interpretation using multimodal information, and incorporation of this information into statistical machine translation. The annotated data will be made available to the research community to facilitate the development of translation technology that produces more natural, human-like translations in real-world interactive situations.","title":"SGER: Collaborative Research: Contextual Machine Translation","awardID":"0840461","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[387675],"PO":["565215"]},"143029":{"abstract":"Behavior-based robotics was established around the idea that robots could be constructed by connecting elementary sensor and actuator modules, without the need to form any internal representation of the world in which they operate. When designed appropriately, such robots, both as individuals and as groups, exhibit complex and seemingly ?intelligent? behaviors, solving challenging tasks in real time, while reacting only to their local environment and obeying sets of local rules. In part, this approach to robotics was inspired by considering natural systems, such as self-organization and adaptability of social insects in their colonies. An analogous approach to initiate the development of the field of behavior-based molecular robotics will be performed over the next three years, leading to groups of molecules that would appear to an observer to show a variety of task-oriented behaviors or some form of purposeful and dynamic self-organization. <br\/><br\/>Individual behavior-based molecular robots are single molecules displaying multiple sensors-actuators. When exposed to artificial landscapes displaying substrates keyed to their sensors-actuators, the molecules start executing elementary steps, determined, in a stochastic sense, by their constantly changing local environments. On some landscapes, called prescriptive, individual molecular robots and their collectives will execute algorithms mimicking wound-up automata with sequence control mechanisms. In contrast, on non-deterministic landscapes, the robots and their collectives will demonstrate properties emerging from internal organization of individual sensors and actuators and through local interactions between molecules and their environments. Importantly, these new interpretations of molecular behaviors will allow radically different experiments from all previous approaches to molecular robotics, while keeping experimental designs realistic, leading to embodiment in the physical world.","title":"Collaborative Research: EMT\/MISC: Behavior Based Molecular Robotics","awardID":"0829805","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["558957"],"PO":["565223"]},"137705":{"abstract":"In 2005, Americans consumed 100 quadrillion BTUs of energy, or almost six times the world wide average per person. Much of this energy consumption is directly related to personal, individual activities such as lighting, heating and cooling. By making simple modifications to these routine activities individuals can conserve energy and help reduce total CO2 emissions. This project leverages Internet technologies to develop a scalable approach to changing behavior regarding environmental footprint (the amount of natural resources or emissions required to support an individual or household). The intent is to use online social networks, cell phones and other technologies to provide a set of ecologically sound, context-sensitive behavioral recommendations. The ultimate goal is to achieve Internet-scale energy behavior change via behavioral research on successful ways to motivate change, environmental impact analysis, and ubiquitous computing that supports objective measurement and context-sensitive communication. <br\/><br\/>The project will use the existing StepGreen.org website as a testbed for three core activities. First, controlled field studies will be conducted to examine the value of personalization and social influence on motivation to change as well as how motivational strategies can accommodate differences among ethnic and cultural groups. Second, inexpensive sensing techniques will be developed that can unobtrusively extract information about changes in daily behaviors. For example, this will be able to track home heating, cooling and transportation behavior through simple financial monitoring of transactions such as electricity bills and gasoline purchases. Finally, the StepGreen deployment, as a large-scale social intervention, must adapt flexibly to contextual information about people (e.g., favored communication modalities, appropriate timing). <br\/><br\/>The multidisciplinary nature of the proposed work enables innovative changes in several scientific domains. By bringing together technical innovation and social science, the proposed work can test theories of social computing in the field, providing a detailed understanding of how online social networks in combination with behavioral intervention strategies can lead to widespread behavioral change. Simultaneously, this work will explore and innovate solutions for large-scale distributed evaluation of Ubicomp technology. By bringing together environmental science and machine learning, the investigators will create the first objective calculator of individual energy use. Finally, the research will benefit environmental decision making by identifying misconceptions about relationships between behavior and energy consumption, educating a broad populace on individual actions they can take to reduce their consumption, and, ideally, achieving reductions in U.S. energy use.<br\/><br\/>Broader impacts. The proposed research will benefit education through the interdisciplinary training of graduate and undergraduate students and through high school outreach as part of Carnegie-Mellon's Green Design Apprenticeship. The work?s focus on different socioeconomic groups will help to highlight the importance of addressing cross-cultural issues in design and behavioral science. Finally, the project will benefit society by helping to address a critical social issue and by providing results that inform other domains in which societal benefits depend on individual behavior change (e.g., health, littering, community action).","title":"HCC-Medium: StepGreen: Mobilizing Social Networks and Context Awareness to Motivate Reduced Energy","awardID":"0803733","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["550147","488866","518044"],"PO":["564456"]},"145218":{"abstract":"Text Analysis and Pattern Detection - Phase II<br\/>Lancaster, Lewis University of California-Berkeley<br\/><br\/><br\/>This project extends the work of IIS-0741556 into new areas. While the purpose remains the exploration of the use of high dimensional visualization to analyze text structure and patterns for scholars in the humanities, This phase of exploratory research will focus on enhancement of the user interface and integration of a statistical analysis toolkit. Also included in the effort are attempts to enhance the functionality and ease-of-use of the user interface. This will be approached by including a context builder for search and retrieval of reference works external to the data repository. In addition, a multi-lingual capability will be explored through collaboration with domestic and international partners. Evaluation plans for the resulting system capabilities will involve numerous beta testing by domain scholars.","title":"SGER: Text Analysis and Pattern Detection - Phase II","awardID":"0840061","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["441922"],"PO":["433760"]},"144008":{"abstract":"Systems currently place a great deal of trust in their underlying components. Unfortunately, due to the complexity of both hardware and software systems, such trust is often misplaced. In particular, a layer will sometimes report that it has succeeded when in fact it has failed. Such a silent fault, or a ?lie? as we will sometimes call it, makes the construction of the next generation of software systems a daunting prospect. In this proposal, we describe the Wisconsin Lie Detection (LID) project, in which we plan to develop methods and techniques to detect lies and thus enable a new generation of robust software. Our basic philosophy is one of skepticism, in which a system generally trusts a layer to correctly complete an operation, but in addition has extra machinery to verify that it indeed has done so. Thus, our major objective is to develop a science of skeptical systems design, including a set of principles and methods systems can employ to be robust to silent faults should they arise. The LID project consists of three major components, all conducted within the domain of storage systems, as we believe storage is an important problem area within which skepticism will be of great value. The first is better understanding the effects of lies on current systems. Thus, we plan to develop a comprehensive lie-injection framework and use it to analyze how silent faults affect systems. Second, we plan to design, implement, and evaluate a set of explicit and implicit lie detectors. Explicit lie detectors expose information to enable higher layers to verify a layer?s operation, whereas implicit detectors utilize covert channels to cope with the lack of explicit assistance. Finally, we will investigate the use of an N-version file system in lie detection and recovery. The LID project will change the landscape of system design and implementation in three fundamental ways. First, our study of existing systems will lend key insights into how systems should be built to withstand silent faults. Second, by developing explicit and implicit lie detectors, we will deepen our knowledge of how to design systems to cope with lies even when no explicit aid to do so exists. Finally, we will see whether the promise of N-version programming [7] can be realized in the problem domain of lie detection; success will alter how we build reliable storage systems.<br\/>A.1 Intellectual Merit<br\/>Intellectual merit and importance: We will advance the state of knowledge in the science of skeptical systems design in three fundamental ways. First, we will develop techniques to understand how systems react to silent failures. Second, we will design, implement, and evaluate both explicit and implicit lie detectors, thus providing insight on how systems can verify the correct operation of underlying layers. Finally, we will develop mechanisms to utilize multiple file systems to improve system reliability in a low-cost manner. Qualifications: We believe we are well positioned to make progress on this demanding problem, as we draw on our expertise in file and storage systems [10, 13, 12, 20, 59, 60, 62, 60, 59, 58], analysis of complex systems [8, 21, 32, 50, 51], and gray-box and related techniques [2, 15, 33, 46, 48]. All three aspects of our previous work feed into our development of the science of skeptical systems design.<br\/>Organization and access to resources: From an organizational viewpoint, our goal is to perform ?low-cost, highimpact? research. Hence, the bulk of funding requested within this proposal is found in human costs. From a computational standpoint, we have access to thousands of machines and multiple storage clusters.<br\/>A.2 Broader Impacts<br\/>Advancing discovery while promoting teaching, training, and learning: In general, we work to give students hands-on training with cutting-edge systems technology. We also plan to incorporate our research into numerous upper-level courses, thus having an impact on all students who pass through the systems courses at Wisconsin. Broadening the participation of underrepresented groups: Our main focus has been to increase female participation in computer science research. In the past seven years, we have graduated five females. We also plan to recruit an additional female in the fall as a part of this project. Enhancing the infrastructure for research and disseminating results: In this proposal, we plan to develop a new skeptical approach to systems design; we believe this viewpoint is critical in the construction of the next generation of systems and thus hope to disseminate as widely as possible. We will do so in three primary ways: through the classic medium of publication, which in the past has impacted the design and implementation of various storage systems including the EMC Centera [32] and NetApp filers [39]; through the development of numerous software artifacts, which we have shared with the open source community and thus improved various code bases; and finally, through our inter","title":"CSR-DMSS-SM: Skeptical Systems","awardID":"0834392","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550389","550390"],"PO":["535244"]},"145229":{"abstract":"Title: Self-Supervised Discriminative Training of Statistical Language Models<br\/>This Small Grant for Exploratory Research is investigating novel methods for discriminative training of statistical language models for application to various human language technologies, such as automatic speech recognition (ASR) and machine translation (MT).<br\/>A language model (LM) is conventionally estimated from a large corpus of text in the target domain via regularized maximum likelihood. Discriminative criteria have been used with some success in ASR, but their immense promise has been curtailed by the requirement of an additional corpus of transcribed speech needed to discriminate between correct word sequences and their incorrect ?cohorts.? This project is exploring ways to discriminatively estimate language models without requiring massive manual annotation, namely, transcribed speech for ASR or parallel text for MT.<br\/>The key idea being explored is that if a large amount of (say) monolingual Chinese text is available, then the MT cohorts of Chinese words and phrases may be accurately estimated by attempting to translate this text into (say) English using an existing MT system and examining which English words and phrases are most frequently in competition with each other. It is not necessary to know which of the competing words or phrases in a cohort set is the correct translation in any particular instance! It suffices to learn who are most often in competition. The investigators are using monolingual English text to explore features that discriminate between observed incidences of each member of a cohort set and its putative competitors; the data for discriminative training are thus derived synthetically. They are investigating if such a discriminatively trained LM specifically targets the most debilitating ambiguities faced by the MT system. The ASR counterpart, with cohort sets derived from automatic transcription of unannotated speech, is also being explored.<br\/>This project benefits both the ASR and MT research communities by exploring statistical language models that can adapt without human intervention to changing tasks or language-use, and that are less reliant on manually annotated data. Advances in ASR and MT in turn will facilitate more effective computer-aided access to information in multiple languages and media.","title":"SGER: Self-Supervised Discriminative Training of Statistical Language Models","awardID":"0840112","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["445173","438318"],"PO":["565215"]},"144019":{"abstract":"With the increasing demand on memory performance by multi-core processors, the memory subsystem has become a new thermal concern along with the processor and the hard disk drive. To address this emerging issue, this project addresses several new, system-level Dynamic Thermal Management (DTM) schemes that coordinate the DRAM thermal management with the processor performance throttling, such as dynamically adjusting the number of active processor cores or scaling the processor's frequency and voltage level based on the memory thermal status. The project also studies coordinated thermal management schemes that consider the thermal requirements from both the processor and the memory subsystem. Thermal-aware OS job scheduling is further considered to smooth memory traffic and DRAM heat generation over time by mixing jobs with different memory demands appropriately. In addition, thermal-aware page allocation is proposed to avoid unbalanced overheating from some memory chips by considering the location of each chip and the memory access demand of each application. These schemes will first be evaluated using simulation and then implemented in OS kernels and evaluated on real systems. To support the memory thermal studies, a simple and accurate thermal model is proposed to estimate the dynamic temperature changes of DRAM memory subsystems. A two-level simulator will be developed to emulate the thermal behavior of memory subsystems. Successfully addressing the thermal concern of memory subsystems will not only ensure safe system operations but also improve the overall system performance, reduce the system manufacturing cost, and improve the system power efficiency.","title":"Collaborative Research: CSR-PSCE, SM: Memory Thermal Management for Multi-Core Systems","awardID":"0834469","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["432712"],"PO":["565255"]},"138816":{"abstract":"The goal of this project is to make progress on computational problems that elude the most sophisticated computers and Artificial Intelligence approaches but that infants solve seamlessly during their first year of life. To this end we will develop a robot whose sensors and actuators approximate the levels of complexity of human infants. The goal is for this robot to learn and develop autonomously a key set of sensory-motor and communicative skills typical of 1-year-old infants. The project will be grounded in developmental research with human infants, using motion capture and computer vision technology to characterize the statistics of early physical and social interaction. An important goal of this project is to foster the conceptual shifts needed to rigorously think, explore, and formalize intelligent architectures that learn and develop autonomously by interaction with the physical and social worlds. The project may also open new avenues to the computational study of infant development and potentially offer new clues for the understanding of developmental disorders such as autism and Williams syndrome.","title":"INT2-Large: Collaborative Research: Developing Social Robots","awardID":"0808767","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["463668","523911","518651","549590"],"PO":["543539"]},"138827":{"abstract":"Proposal No: 0808824 <br\/>Title: Principles for Scalable Dynamic Visual Analytics<br\/>PI name: Jagadish, H. V. <br\/>Inst: University of Illinois Michigan<br\/><br\/><br\/>Abstract:<br\/>The human eye is often capable of identifying interesting patterns and trends from a well-presented data set, whereas computational algorithms may have difficulties with such a task. Yet, there are limits to human ability, both with the scale of the data set in terms of objects and attributes and with dynamic changes over time. This project develops an analytic and computational framework to support the visual analysis of large-scale dynamic data with network structure. <br\/><br\/>The intellectual merit of this project is in the development of a family of operators with which to reduce the size both in terms of objects and attributes of the data set to be visualized; an analysis of the properties of this family of operators to enable their effective use; and the development of algorithms and data structures to support the ef&#64257;cient computation of these operators. By harnessing computational power to assist the human eye in seeing patterns and trends in the data, this project has the potential to transform the way in which large dynamic data sets with network structure are analyzed today. <br\/><br\/>The broader impact of the project lies in the multiple application domains where network data are ubiquitous in their presence. In particular, we plan to focus on two domains to illustrate the proposed framework; biology through protein interaction networks, and national intelligence through social networks of suspect participants. In addition, this interdisciplinary project plows the ground at the boundary of statistics and computer science, and trains graduate students at this interface, an area with great future potential.","title":"Principles for Scalable Dynamic Visual Analytics","awardID":"0808824","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["533266","521130"],"PO":["562984"]},"139619":{"abstract":"Last Modified Date: 07\/21\/08 Last Modified By: Daniel F. DeMenthon <br\/> <br\/>Abstract <br\/>With the ever faster growing number of images and videos, the main bottleneck in extracting the information contained in them is their analysis (indexing) and retrieval. Nowadays image and video search engines are based on textual descriptions, since visual cues are at too low level to provide useful retrieval results when dealing with a large variety of images and videos. For example, if a human submits a query image with the request to find similar images, she focuses on a certain object or a group of objects in the query image. Thus, the meaning of similarity is given by the images that contain similar objects. Therefore, extraction of objects in images (and videos) is a key factor for true progress in content based image\/video retrieval (CBIR). However, object extraction belongs to unsolved problems in Computer Vision (CV). This fact led to the development of a huge number of approaches that try to do CBIR without object extraction. However, although such approaches may be successful in some restricted application domains, in which case low level features may be sufficient to replace object extraction, they have not been successful in general purpose CBIR. The PIs believe solving the object extraction problem will lead to a breakthrough in CBIR. Therefore, the PIs propose to work on object extraction in images. There have been a large number of attempts to solve the object extraction problem in CV, and none provided a satisfactory solution. Why will our approach provide a good solution? A new methodology and a computation framework proposed by the PIs provide solid evidence that the breakthrough in object extraction is possible. <br\/>On the cognitive and geometric modeling side, the PIs propose to use a higher level knowledge of shape similarity and a mid level knowledge of local and global symmetry as cognitively motivated constraints for object extraction. Constraints are essential because object extraction is known to be an ill-posed inverse problem. The human visual system solves this problem very well and we are getting close to a full understanding of how this is done. <br\/>On the computational side, the PIs propose a new framework for a simultaneous estimation of medial axes and the contours. The proposed approach is inspired by the SLAM (Simultaneous Localization and Mapping) approaches in the field of robot mapping. Recent breakthrough solutions in robot mapping are based on the SLAM computation with particle filters. SLAM computation iterates over the processes of localization of the robot in the existing partial map (trajectory estimation), followed by a map update based on new observations and the estimated trajectory. The PIs treat the medial axis as trajectory of a virtual robot and the partial boundary as the map that is composed of edge segments associated with the medial axis. A first successful application of this framework is demonstrated by the PIs in the preliminary results. <br\/><br\/>Project URL: http:\/\/knight.cis.temple.edu\/~shape\/","title":"Collaborative Research: Simultaneous Contour Grouping and Medial Axis Estimation","awardID":"0812167","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["416761"],"PO":["564316"]},"140983":{"abstract":"NSF Proposals 0820088\/0819865<br\/><br\/>Title: Collaborative Research: Passivity-based Architecture for Software Design of Dynamic Networked Systems<br\/><br\/>PIs: Xenofon Koutsoukos (0820088), Panos Antsaklis (0819865)<br\/>Co-PI: Janos Sztipanovits, <br\/><br\/>Real-life Cyber Physical Systems (CPSs), such as autonomous vehicles and building automation systems, are monitored and controlled by networked control systems. In CPSs, the overall system dynamics emerges from the interaction among physical dynamics, computational dynamics, and communication networks. This project aims at addressing the fundamental problems in constructing CPSs caused by network uncertainties, such as time varying delay, jitter, data rate limitations, packet loss and others, by exploiting the inherent safety of passive systems. The project will develop (1) the theoretical foundations for passivity-based design of networked control systems that provide an effective way to interconnect multiple passive systems together and preserve stability and performance in the presence of time varying delays and data dropouts and (2) model-based design processes for developing and analyzing software utilizing the compositionality and the orthogonality across design views stemming from the underlying passivity principles. The research plan includes a software tool-chain for passivity-based design, an implementation of the passivity-based architecture on a distributed hardware platform, and experimental studies for demonstrating the approach. The project serves as an excellent example for the rich societal context and extensive interdisciplinary interactions that future computer scientists and engineers will face as CPS technology is becoming increasingly pervasive.","title":"Collaborative Research: Passivity-based Architecture for Software Design of Dynamic Networked Systems","awardID":"0820088","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["533198","564429"],"PO":["564388"]},"145901":{"abstract":"This SGER project explores a new area of study in which little is known: how skills learned within virtual environments (VE's) such as Nintendo's Wii are transfered to real-life natural environments. Part of what makes this study high-risk is that there many factors to control for to systematically compare skill transfer from and to virtual and real-life environments. The goal of this exploratory project is to investigate the mechanisms transfer through three mixed-methods experiments involving cooking. In this project new ethnographic methods for encoding what and how skills are acquired in VE's will be developed as appropriate (e.g., through video analysis). The PI is a world leader in \"distributed cognition\" and is the ideal person to lead this preliminary investigation of skill acquisition across real and virtual environments.<br\/><br\/>The broader impact of this research is in its potential to uncover new affordances of virtual environments for task acquisition, with implications for VE design. Through validating ethnographic methods for studying virtual to real-world transfer, this project will benefit other researchers. Further, these experiments serve as an important first contribution toward a future body of research that examinines how combining virtual and real-world training\/instructions may best optimize human performance.","title":"SGER: A Study of Following Instructions in Simplified Virtual Environments and Natural Environments: Evidence of Transfer and Comparisons of Cognitive Efficiency","awardID":"0843070","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["443223"],"PO":["564456"]},"144812":{"abstract":"Technology has made possible virtual research organizations involving researchers from different institutions. Virtual research organizations are expected to improve the speed of innovations and the creativity of scientists by bringing expertise to projects that would otherwise be unavailable. This project contributes to the scientific study of this new way of organizing scientific work and examines the problem of collaborating in virtual research organizations at the level of the collaboration and collaborators. The purpose of the research is to better understand how dispersed, interdisciplinary projects succeed, and to explore how virtual organizations might best use new tools and infrastructure. <br\/><br\/>This research builds on the investigators? prior work in understanding the Information Technology Research (ITR) initiative, which included close to 500 interdisciplinary technology research projects supported by the NSF. The proposed work entails an archival analysis of ITR final reports to discern the longer-term impact of the ITR projects and a longitudinal follow-up survey of ITR investigators. This research will contribute to organization science and social network theory, and to a better understanding of science and technology. The results will be useful for social scientists interested in organizations, computer scientists and technologists interested in tools and infrastructure that work, and decision and policy makers who invest in research. <br\/><br\/>Virtual research organizations in science and engineering are essential to the nation?s future and to solving global problems. This research will aid policy understanding of requirements of distributed interdisciplinary collaboration, improve metrics and approaches for evaluating the direct and indirect outcomes of research programs, suggest policies for future programs in innovative scientific research and education, and provide information to organizations, particularly departments and universities, about the institutional policies and practices that best support distributed interdisciplinary research.","title":"VOSS: Collaborative Research: Towards Collaboration Strength in Virtual Research Organizations","awardID":"0838367","effectiveDate":"2008-09-15","expirationDate":"2011-11-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["499499"],"PO":["446196"]},"143723":{"abstract":"Scaling scientific problems to 10,000,000 processors for the next generation HEC systems is today severely challenged by conventional practices of programming models, languages, and their supporting compilation systems. To achieve this goal one must expose greater degree of parallelism and improve parallel computing efficiency than is otherwise feasible with conventional methods such as MPI. <br\/>The goal of this collaborative research project is to dramatically enhance the scalability of challenging physics problems, through the application of an innovative programming model. The strategy is to replace static message-passing course grained processes using global barrier synchronization in a distributed memory space with a model using dynamic message-driven multiple threads using lightweight synchronization objects in a partitioned global address space. Parallelism is to be extracted directly from the large irregular sparse and time varying data structures. Ephemeral user-threads will permit many simultaneous tasks over the data structures, exposing the intrinsic near-fine grain parallelism. System-wide latency will be hidden by overlapping computation with communication through the advanced communication strategy of asynchronous message-driven processing. Consequently this will enable a class of physics problems that cannot currently be done using conventional methods.","title":"Collaborative Research: A Study and implementation of Semantic Constructs for Highly Scalable Leading-edge Scientific Computing","awardID":"0832966","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["545232","545233",382875],"PO":["565272"]},"141424":{"abstract":"This award establishes a humanoid robotics facility at the University of California, Merced to be used by cognitive scientists and engineers to investigate (1) how embodiment constrains models of human cognition, (2) how people naturally interact with humanoid robots, and (3) how the design of robotic control systems can best address the cognitive issues surrounding human-robot interaction. The centerpieces of this facility will be a fully mobile humanoid robot and a humanoid torso equipped with two human-like dexterous arms and a vision system mounted on a fully actuated head. Though much work on humanoid robots has focused on solving fundamental engineering problems associated with robust operation in real world environments, the proposed facility will support research that uses the robot as an instrument to test hypotheses of human cognition or that augments robotic capabilities in a manner sensitive to the cognitive limitations of human-robot coordination. As a scientific instrument, a humanoid robot can be used to present precise, controlled motions and patterns of interaction to human experimental participants, offering new methods for probing human responding in interactive contexts. The embodied perceptual and motor capabilities of such a robot also make it a challenging testbed for computational models of human cognitive processes. Developing cognitive systems that appropriately support human-robot interaction will reify and test our understanding of embodied perception, humanoid motor coordination, and cooperative interaction.<br\/><br\/>The proposed facility is also intended to support interdisciplinary research at the boundary between cognitive science and engineering. Emerging cognitive research on human-robot interaction has the potential to transform robotics research and result in practical innovations in the design of humanoid robot control systems. These additional engineering contributions will be extremely valuable, as robots will be equipped with physical and cognitive abilities that are appropriate for human environments, and with communication and coordination skills that are appropriate for human-robot collaboration. Indeed, robotic assistants have already demonstrated potential in numerous application areas, including physical therapy, care for the elderly and disabled, collaborative work, and astronaut support during space exploration. Though recent technological innovations have allowed for the development of reliable and affordable robotic mechanisms that mimic human bodies, many questions related to the intelligent control of these bodies remain. By supporting the exploration of models of human cognition within a robotic framework, and by generally applying insights from cognitive science to the problems of embodied perception and motor control, practical progress is expected in domains such as locomotion in cluttered environments, spatial awareness and spatial reasoning, dexterous object manipulation, task-oriented attention and perception, imitation and learning, life long adaptation, and multi-modal social interaction and coordination with humans, including the use of natural gestures.<br\/><br\/>This facility will advance the development of integrated research and teaching programs at UC Merced, a new campus located in an economically challenged region with low educational levels. The availability of this equipment will grant students access to unique training opportunities in Cognitive Science, Artificial Intelligence, Motion Planning, Computer Graphics, Computer Animation, Computer Vision, Machine Learning, Robot Algorithms, and Humanoid Robotics.","title":"MRI: Acquisition of Robotic Hardware for Humanoid Research in Cognitive Science and Engineering","awardID":"0821766","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[376312,"543484","502374","543485",376316],"PO":["565192"]},"141314":{"abstract":"Proposal #: CNS 08-21319<br\/>PI(s): Zhao, Wenbing<br\/> Fu, Yongijian; Sridhar, Nigamanth; Yu, Chansu<br\/>Institution: Cleveland State University <br\/> Cleveland, OH 4415-2214<br\/>Title: MRI\/Acq.: Acq. of Equipment to Establish a Secure and Dependable Computing Infrastructure for Research and Education at Cleveland State University<br\/>Project Proposed:<br\/>This project, acquiring server and networking equipment for activities in the area of secure and dependable computing, supports research in enterprise distributed systems, wireless mobile networks, and sensor networks. Projects include<br\/>- Byzantine fault tolerance for long-running, nondeterministic systems,<br\/>- Performability in wireless mobile networks, and<br\/>- Hybrid emulation of wireless sensor networks.<br\/>Novel methods are sought for providing Byzantine fault tolerance to long-running, nondeterministic distributed systems. Performance in highly stressed mobile wireless network is investigated in terms of high node mobility and strong interference, as is understanding of nodes? boundary behavior under extreme conditions seeking novel methods to survive the stress via cooperation. Lastly, a novel framework for hybrid emulation of wireless sensor networks is proposed to enable affordable experimentation at scale. The instrumentation consists of a cluster of servers connected by a managed high-speed switch forming a dynamically-configurable distributed computing testbed, servers to join the PlanetLab Consortium, and networking equipment to connect the testbed with an existing wireless network testbed. <br\/>Broader Impacts:<br\/>This project should have broad impact on security and dependability education. The tools and methodologies produced will be used by collaborators both in academia and industry (e.g., factory automation and highway work-zone safety). The infrastructure and outcomes will be integrated with the education curriculum; courses on fault tolerant computing and performance analysis will be developed. Moreover, the team collaborates with high schools providing research experience for teachers.","title":"MRI: Acquisition of Equipment to Establish a Secure and Dependable Computing Infrastructure for Research and Education at Cleveland State University","awardID":"0821319","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[375811,"557527","558299",375814],"PO":["557609"]},"143503":{"abstract":"This collaborative project investigates a number of fundamental <br\/>problems that are very critical to wireless mesh network (WMN) <br\/>throughput optimization. Wireless mesh networking is believed to <br\/>be the most effective and efficient technology for the last-mile <br\/>data connection to the Internet. The proposed research targets <br\/>to solve the following challenges: 1) Throughput optimization in <br\/>MR-MC WMNs: the complexity of throughput-optimal scheduling, <br\/>real-time logical topology characterization and inference, the <br\/>joint exploitation of both rate diversity and channel diversity, <br\/>and game theory based throughput optimization. 2) Non information <br\/>theory based approaches to throughput optimization in multi-hop <br\/>MIMO-enabled WMNs: stochastically modeling different aspects of <br\/>the mesh network, constructing Network Utility Maximization <br\/>formulations, and applying Information Geometric Programming to <br\/>solve global optimization problems. 3) Biologically-inspired WMN <br\/>design for throughput optimizatio n. 4) A testbed to accomplish <br\/>experimental tasks to validate the effectiveness of the design.<br\/><br\/>This project is in nature multidisciplinary. It requires the <br\/>joint effort from computer science, electrical engineering, <br\/>mathematics, statistics, and biology, and therefore has the <br\/>potential to inspire revolutionary methodologies that could be <br\/>applied to many domains. In addition, the novelty of the proposed <br\/>research has strong impact on wireless research. Furthermore, the <br\/>success of this project is able to enhance the four universities? <br\/>ability to educate, through the multi-disciplinary research effort, <br\/>future scientists and engineers.","title":"Collaborative Research: NEDG: Throughput Optimization in Wireless Mesh Networks","awardID":"0831852","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["531651","560587"],"PO":["564993"]},"143514":{"abstract":"Cross-Layer Designs for Streaming Video in Multihop Wireless Mesh Networks<br\/><br\/><br\/>As unprecedented advances on wireless mesh networks continue, there is an increasing expectation on such networks to support content-rich multimedia communications (e.g., video and voice) in addition to simple data communications. Given the resource constraints and the uncertainties or variable nature of resource availability, quality of service (QoS) provisioning in wireless meshes is a challenging task. These constraints coupled with the requirements for supporting video traffic, layer-specific and isolated solutions are not adequate; hence, arises the need for cross-layer and integrated design approaches.<br\/><br\/>In this project, the researcher will pursue cross-layer approaches to support video streaming over wireless mesh networks. The work will be primarily based on experimental studies, backed by theoretical analysis, and evaluated on an existing wide-area mesh test-bed. Intellectual components of the project are: (a) Experimental characterization of video streaming in wireless mesh networks; (b) Exploring cross-layer schemes for admission control and scheduling of video flows; (c) Exploiting packet retransmissions for quality enhancement of video streaming; (d) Experimental validation of the proposed schemes using the existing test-bed. <br\/><br\/>The outcome of the project will have a broader impact on the society as well as the education and training of the students. Video streaming over wireless mesh would enhance and facilitate remote monitoring facilities and enable new applications. Several parts of the project will be integrated with the educational missions at UC Davis, both at graduate and undergraduate level. Dissemination of the results and inferences will also incite further research in the broader community.","title":"NEDG: Cross-Layer Designs for Video Streaming in Multihop Wireless Mesh Networks","awardID":"0831914","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["550540"],"PO":["557315"]},"143998":{"abstract":"Project Abstract<br\/><br\/><br\/>A common vision of the future is one where our everyday environments are replete with smart cyber physical objects networked to form complicated systems of systems. People will interact with these embedded systems both explicitly and implicitly. The systems will be heterogeneous, need to exist for many years, and operate in the context of real world communication, sensing and failure realities. Many of the systems will be unattended (at least for large periods of time) and often performing very important tasks. The systems will be open in the sense that they will permit access to their functions from humans and other cyber physical systems. The current rapid development and deployment of wireless sensor networks and ubiquitous computing systems and their interactions are exacerbating the need for high confidence embedded systems.<br\/><br\/>Achieving high confidence embedded systems will require new assurance technologies both off-line and on-line. This work addresses on-line run-time assurance. Comprehensive solutions for run time assurances in high confidence embedded systems are developed. The main intellectual contributions are determining how to specify and support at run time a collection of solutions that enable embedded systems to improve confidence and demonstrate application operability. The broad impact of this work is extensive since there is a proliferation of embedded systems being deployed or contemplated for critical applications such as fire fighting, pollution control, disaster response, tracking, military surveillance, and medical assistance.","title":"CSR-EHCS (EHS), SM: Run-Time Assurances for High Confidence Embedded Systems","awardID":"0834299","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550342"],"PO":["561889"]},"141699":{"abstract":"The End User Opt-In Working Group, which is one of five Working Groups dedicated to particular areas of technical design for a suite of experimental network infrastructure currently in its planning stages, holds three workshops each year that, on a continuing basis: 1) identify and resolve design issues, 2) identify areas of technical risk, 3) review documents prepared by the GENI Project Office, and 4) engage in outreach. This award gives the co-chairs funds for their participation in a one-year set of workshops.<br\/><br\/>By having an open, transparent, well-managed design process, the community will have ample opportunities to comment and contribute to the design of a suite of experimental network infrastructure. Engaging the community early in the design process will ensure that the infrastructure will have the functionality and capabilities required by many in the research community. Workshop topics include: detailed open design issues and tradeoffs, risk assessment and mitigation, common vocabulary and models, design and integration activities, use cases, and the evaluation of software and services, for example.<br\/><br\/>The chairs will be responsible for workshop preparation, chairing workshops, results dissemination, and progress reporting. These funds will enable the chairs to recruit a diverse set of attendees, conduct and attend meetings, and broadly disseminate the results.","title":"GENI Working Group Meetings - Opt-in","awardID":"0822954","effectiveDate":"2008-09-01","expirationDate":"2011-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["518545"],"PO":["468236"]},"145824":{"abstract":"The goal of this collaborative exploratory research project (0842546, Clement Yu, UIC and 0842608 Weiyi Meng, SUNY Binghamton) is to investigate two important problems in opinion retrieval. The first is to determine whether a negation word flips the sentiment of an opinionated word and the second is to examine how the opinions about a topic change over time and what causes significant changes. This project aims to demonstrate within 18 months that within two domains (consumer products\/services and medical\/health), whether a negation word flips the polarity of an opinionated word or phrase can be accurately determined when there is a single occurrence of such negation word in a sentence, and the significant time points and words\/phrases that characterize the changes of sentiments for a given topic can be accurately obtained. The research results from this project will lay the foundation in achieving longer-term goals that include determining the polarity of the sentiment of a feature with reference to a given query in any given domain, identifying the events that cause the significant change of sentiments, measuring the intensity of sentiments and its change over time, and studying the interactions between the negation and temporal aspects of opinion retrieval. It is expected that the proposed project can have a significant impact on future search engine technology, information retrieval, text mining, and blog retrieval and analysis. Research results will be incorporated into several courses the PIs teach and students will be recruited to participate in the research activities. Research results will be disseminated through published papers. The project Web site (http:\/\/www.cs.binghamton.edu\/~meng\/OpRetSGER.html) provides access to research results.","title":"SGER\/Collaborative Research: Handling Negations and Temporal Aspects for Opinion Retrieval","awardID":"0842608","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["434747"],"PO":["563751"]},"143404":{"abstract":"This project focuses on the development of a new class of secret key generation and renewal algorithms for securing wireless networks by taking advantage of physical layer characteristics. The basis of the approach is the identification of measurable quantities of the wireless channel between a pair of nodes that are highly correlated exclusively between them (albeit not identical). Capitalizing on this correlation, the project develops algorithms for obtaining a shared key under various adversarial models. <br\/><br\/>This addresses a critical problem area as the broadcast nature of a wireless link provides a natural eavesdropping and intervention capability to an adversary and thus it follows that securing a wireless link is essential to the security of any wireless network. This project undertakes the design, implementation, analysis and testing of several protocols and clarifies the relationship between the channel characteristics and the cryptographic protocols that use them. It also investigates the trade-offs between computational and communication costs, and quantifies the level of security achieved by the key generation and renewal protocols assuming various adversarial models. <br\/><br\/>The broader impact of the project is in the potential of making wireless communications more secure while avoiding costs (both computational and organizational) of cryptography. Given the increased wireless internetworking and the multitude of wireless enabled devices presently carried by an average user, the security of the wireless link becomes one of the most critical aspects of securing computer systems. The reduction in the computation also will translate to a reduction in energy use, important to battery or alternative powered systems. Educationally, the project will present an attractive combination of aspects of signal processing, computer security, algorithms and cryptography and will offer a wealth of problems for student engagement","title":"CT-ISG: Collaborative Research: Key Generation from Physical Layer Characteristics in Wireless Networks","awardID":"0831304","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["381857"],"PO":["565327"]},"143525":{"abstract":"Distributed data storage and access in wireless sensor networks (WSNs) recently has found increased popularity driven by many mission-critical applications. While WSN security has been extensively studied in recent years with focus on network communication security, little attention has been paid on the security and dependability of distributed data storage and access control. However, it is of paramount importance to ensure data security and dependability in the mission-critical applications, where data genuineness and availability can be about life or death. This project is aimed at solving this important challenge. On the one hand, the problem of how to store the sensor network data in a distributed manner while satisfying the requirements of both fault-tolerance and compromise-resilience is studied. The problem of dynamic data security and dependability after the initial data storage is tackled as along the time sensors may be compromised and\/or behave Byzantine failures. On the other hand, researches on distributed and fine-grained data access control are carried out to ensure sensor network data can only be accessed by authorized network users. The worst-case scenario is considered in which not only sensors may be compromised, but also network users may not be fully trusted. Novel symmetric key cryptography (SKC) and public key cryptography (PCK) based approaches are proposed, built on top of the in-depth security analysis of the state-of-the-art SKC solutions and efficiency enhancements over the most recent PKC primitives such as Ciphertext Policy- and Key Policy- Attribute Based Encryption, i.e., CP-ABE and KP-ABE.","title":"NeTS-NECO: Collaborative Research: New Approaches for Secure and Dependable Distributed Data Storage and Access Control in Mission-critical Wireless Sensor Networks","awardID":"0831963","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["565164"],"PO":["565303"]},"143415":{"abstract":"Considering the popularity and wide adoption of social network systems and the competitive edge these systems provide, there has been a rapid growth in use of these systems to access, store, and exchange personal attribute information in distributed and\/or federated environments and this trend is expected to continue. Efficient, secure, and user-centric techniques are important for the successful deployment of such systems. Our goal in this project is to develop a comprehensive and compelling framework SNGuard (Social Network Guard) that satisfies diverse privacy properties, access control issues, identity management requirements, and usage patterns. The vision of dynamic social networks is a complex and highly sophisticated one that requiring ongoing research and analysis to continue concurrent with the changing role and face of digital information creation and usage including personal information and contents in social networks. The principal intellectual products resulting from this project will be the development of novel frameworks to facilitate user-centered privacy management, content management and risk-aware access control, thereby making SNGuard solutions more trustworthy, more reliable, and less vulnerable. This research effort will have broad societal impact by providing a key mechanism to enable new business and community models for the sharing of personal attributes including identity information to safely, easily, and quickly establish social networking environments in cyberspace. In addition to these potential benefits, other anticipated, broad-based benefits to be facilitated by this research include significant influence to K-12 education, international collaboration, and industrial and government partners.","title":"CT-M: Collaborative Research: Securing Dynamic Online Social Networks","awardID":"0831360","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["492724","486272"],"PO":["497499"]},"143536":{"abstract":"This project studies a paradigm in which nodes cooperate by pooling power and bandwidth resources and where flows interact opportunistically to avoid interference and increase network utilization. The PIs will leverage their existing expertise in cooperative and opportunistic communications to analyze the implications for broader networks of communication nodes. In particular, they will instantiate their design philosophy in three ways: <br\/><br\/><br\/>Node Information Management: While previous network analyses considered only isolated aspects of a node (e.g., channel gain), the project studies a comprehensive network state information, which captures not only physical-layer conditions but also higher-layer information such as queue state, processing power, and availability of forwarding routes. <br\/><br\/><br\/>Novel Network Representations: Instead of regarding the network as a simple connectivity graph, the PIs will introduce and develop a network representation which incorporates both temporal and spatial relationships between nodes. The PIs refer to this as the trellis representation of the network, and it will enable us to describe cooperative and opportunistic communication in a wide area network. The trellis will provide a structure in which to identify opportunities for physical layer cooperation, determine the impact of cooperation on neighboring nodes and flows, and opportunistically schedule and route competing flows at fine grained time scales.<br\/><br\/>Distributed Cooperative Discovery: Traditional discovery protocols for determining network connectivity are unable to identify cooperative links. New techniques will be developed that leverage existing discovery protocols to efficiently locate potential cooperative topologies, which are a key to opportunistic communication. These discovery protocols will recover network state information and enable the use of the trellis representation to identify the optimal cooperative route through the network.<br\/><br\/>With these tools, the PIs will develop and analyze protocols for coordinating cooperative and opportunistic communications in heterogeneous networks. The new protocols will expand access in underserved areas while increasing throughput in existing networks. <br\/><br\/><br\/>This research will have a broad impact on education by engaging undergraduate and graduate students in the Rice Center for Multimedia Communication (CMC) laboratory. Cooperative communication will be integrated into several courses at Rice in the wireless communication and networking areas. Software and firmware modules as well as publications will be distributed through the WARP open-access repository<br\/>(\\href{http:\/\/warp.rice.edu\/trac}{http:\/\/warp.rice.edu\/trac}).","title":"NEDG: Cooperative Wireless Networks: From Theory to Urban-Scale Trials","awardID":"0832025","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["542042","548310"],"PO":["557315"]},"143778":{"abstract":"Hardware for High-Performance Computing is advancing at a relentless pace: In the not too distant future we can expect to see systems with over a million of concurrently executing threads, with hardware support for global memory access. On the other hand, we continue to use today the same low-level parallel message passing libraries that we have used in the last 15 years. This causes lower user productivity and does not leverage well modern communication hardware. We propose to explore new language designs that address both problems.<br\/><br\/>It is generally accepted that programming in a shared memory model is easier (at least for initial program development): the ability of each thread to access each variable, using a common name space, reduces much of the burden of distributed memory programming. On the other hand, shared memory programming languages generally allow users to write nondeterministic code (where different outcomes are possible) and do not protect the user from memory races (where accesses to shared variables are not synchronized). This results in subtle bugs that are not reproducible and hard to detect. Furthermore, shared memory languages provide limited support for locality control ? resulting in lack of scalability. Nondeterminism is rarely needed in scientific computing, and scalability is essential. <br\/><br\/>The PIs believe it is possible to develop languages that will support the large majority of programming patterns used in high-performance-computing; will provide the convenience of a shared-memory model; will prevent, by design, nondeterminism and detect races; and will provide user control of locality. The proposed research will explore the design for such a language and the required support technologies.","title":"Deterministic Parallel Programming for High Performance Computing","awardID":"0833128","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["556685"],"PO":["565272"]},"143547":{"abstract":"This project revisits the role of the edge router, and develops novel architectures that lead to the design of an Application Oriented Edge Router. The router is highly ?intelligent? and provides advanced network services by moving packets to the application layer. Examples of such applications (services) are security and user protection, pricing, load balancing, multimedia, peer-to-peer (P2P) and VoIP.<br\/><br\/>Novelty and Technical Merit: <br\/>a. Application aware processing. Multithread algorithms and multi-core scheduling techniques are developed so as to increase throughput and reduce latency while preserving the QoS requirement of an application.<br\/>b. Implementing through network processors. Application processing through programmable network processors (NPs) is also explored. New NP partitioning, processor allocation and scheduling schemes are developed to meet the requirements of various services.<br\/>c. Service classification. The signature based packet classification techniques are extended to differentiate the incoming traffic based on required services. <br\/><br\/>Broader impact: <br\/> a. Industrial impact and technology transfer: The current projects from Cisco and Intel have complimentary effect and synergistic interaction with project. <br\/> b. International collaboration: The active participation of Prof. Bin Liu in the project brings in long-term collaboration between UCR and Tsinghua University. <br\/> c. Educational impact: Benchmark applications and results from the project are widely disseminated. The project also produces high quality Ph.D. graduates and involves Undergraduate students at UCR.<br\/> d. Minority outreach plan: 30-40% of the UCR undergraduate body is minority students. As a result, this project has an amplified and direct impact on the outreach activities at UCR.","title":"NEDG: Application Oriented Edge Routers","awardID":"0832108","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["516924"],"PO":["565303"]},"143437":{"abstract":"With the rapid growth of the Internet, online advertisement plays a more and more important role in the advertising market and has become a billion-dollar business ($19.5 billon in 2007). One of the current and widely used revenue models for online advertising is Pay-per-click (PPC), which involves charging for each click based on the popularity of keywords and the number of competing advertisers. However, the pay-per-click model leaves room for individuals or rival companies to generate false clicks (i.e., click fraud) due to the lack of verifiable engagement in PPC requests. It has been reported that in online ad market, 14.6% are paid to Click Fraud, which has damaged the development and healthiness of online advertising. This project is (1) developing a fundamentally new framework for verifiable clicks and a new way of defining the quality of clicks; and (2) developing filtering-based tools for validating and weeding out suspicious clicks, each of which provides quantifiable guarantees on false positive and negative rates while involving a reasonable processing overhead and space requirements. The new framework promotes transparency and trust between advertisers and online ad businesses and eliminates the need for keeping ?click filters? as trade secrets. A set of portable course materials is also being developed to facilitate the teaching of developed techniques in undergraduate and graduate courses. Some of these results are planned to be licensed for use by online ad businesses.","title":"CT-ER: Detecting Click Fraud in Pay-Per-Click Streams of Online Advertising Networks","awardID":"0831470","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["509531"],"PO":["543481"]},"143206":{"abstract":"Artemov, Fitting, and Nogina will continue their development of Justification Logic, which offers a possible breakthrough in the quest to create a fundamental theory of knowledge, belief, and evidence, and has the potential for significant impact on applications. The celebrated account of knowledge as \"justified true belief,\" which is attributed to Plato, has long been a focus of epistemic studies. About a half-century ago, the notions of knowledge and belief acquired formalization by means of modal logic. However the notion of justification, an essential element of epistemic studies, was conspicuously absent, and this led to well-known deficiencies inherent in modal logics of knowledge.<br\/><br\/>Justification Logic extends the logic of knowledge in three major ways. First, it adds a long-anticipated mathematical notion of justification, making the logic more expressive. We now have the capacity to reason about justifications, simple and compound. We can compare different pieces of evidence pertaining to the same fact. We can measure the complexity of justifications, thus connecting the logic of knowledge to a rich complexity theory, etc. Second, justification logic furnishes a new, evidence-based foundation for the logic of knowledge, according to which `F is known' is interpreted as `F has an adequate justification.' Third, justification logic provides a novel, evidence-based mechanism of truth tracking which can be a valuable tool for extracting robust justifications from a larger body of justifications which are not necessarily reliable.<br\/><br\/>Knowledge, belief, and evidence are fundamental concepts whose significance spans many areas of human activity: computer science and artificial intelligence, mathematics, economics and game theory, cryptography, philosophy, and other disciplines. Justification Logic promises significant impact on the aforementioned areas. In particular, the capacity to keep track of pieces of evidence, compare them, and select those that are appropriate would be a valuable new tool.","title":"Justification Logic and Applications","awardID":"0830450","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[381339,"540120",381341],"PO":["565157"]},"143459":{"abstract":"This collaborative project investigates a number of fundamental <br\/>problems that are very critical to wireless mesh network (WMN) <br\/>throughput optimization. Wireless mesh networking is believed to <br\/>be the most effective and efficient technology for the last-mile <br\/>data connection to the Internet. The proposed research targets <br\/>to solve the following challenges: 1) Throughput optimization in <br\/>MR-MC WMNs: the complexity of throughput-optimal scheduling, <br\/>real-time logical topology characterization and inference, the <br\/>joint exploitation of both rate diversity and channel diversity, <br\/>and game theory based throughput optimization. 2) Non information <br\/>theory based approaches to throughput optimization in multi-hop <br\/>MIMO-enabled WMNs: stochastically modeling different aspects of <br\/>the mesh network, constructing Network Utility Maximization <br\/>formulations, and applying Information Geometric Programming to <br\/>solve global optimization problems. 3) Biologically-inspired WMN <br\/>design for throughput optimizatio n. 4) A testbed to accomplish <br\/>experimental tasks to validate the effectiveness of the design. <br\/><br\/>This project is in nature multidisciplinary. It requires the <br\/>joint effort from computer science, electrical engineering, <br\/>mathematics, statistics, and biology, and therefore has the <br\/>potential to inspire revolutionary methodologies that could be <br\/>applied to many domains. In addition, the novelty of the proposed <br\/>research has strong impact on wireless research. Furthermore, the <br\/>success of this project is able to enhance the four universities? <br\/>ability to educate, through the multi-disciplinary research effort, <br\/>future scientists and engineers.","title":"Collaborative Research: NEDG: Throughput Optimization in Wireless Mesh Networks","awardID":"0831579","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[382009],"PO":["565303"]},"143228":{"abstract":"Natural algorithms are processes that exist in nature and society, processes that do not appear to be explicitly designed or programmed. Typically, one infers the existence of such processes from the results that ensue (e.g. supply and demand balancing in an economic market) as opposed to an explicit presentation of the process (e.g. a procedure for price adjustment). Two questions arise: what are the processes (i.e. what steps do they undertake) and what are their capabilities and limitations? This project approaches the issue from a computational perspective.<br\/><br\/>The main focus will be on economic market equilibria, which can be characterized as prices that balance supply and demand. It is widely believed that economic markets tend to revert to equilibrium states following disturbances, and the issue studied here is when this might be possible. So the first question above amounts to asking what price adjustments might occur in markets to cause this to happen; the second question asks when, in fact, can these adjustments cause convergence to equilibrium, and if convergence can occur, how fast is it?<br\/><br\/>In prior work, a new way of incorporating time in the standard market models was proposed; this seems a necessity for modeling and analyzing price adjustment protocols. This project aims to develop that analysis in multiple dimensions, namely: (1) to account for discreteness of goods and money, (2) to handle dynamic environments, (3) to provide strategic explanations of participant behavior, (4) to incorporate production, (5) to analyze (some) environments in which the Weak Gross Substitutes assumption does not hold. Finally, the project will seek to develop analogous explanations and analyses for other self-governing systems, natural and otherwise, for example in biological systems and in network routing.","title":"Markets and Other Robust Self-Governing Systems","awardID":"0830516","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["518072"],"PO":["565251"]},"143349":{"abstract":"Modern communication networks are constructed using a layered approach, with one or more electronic layers (e.g., IP, ATM, SONET) built on top of an optical fiber network. This multitude of layers is used in order to simplify network design and operations and to enable efficient sharing of network resources. However, this layering also gives rise to certain inefficiencies and interoperability issues. This project explores the impact of layering on network survivability and develops network architectures that are resilient to failure propagation between layers.<br\/> In spite its importance and practicality, to date, very little has been done to understand network survivability in this complex layered environment. The goals of this project is to develop a fundamental theory for understanding cross-layer survivability; and mechanisms for providing survivability in layered networks through the joint design of the network topologies at the different layers. <br\/> The outcomes of this research will include: (i) the development of new metrics for assessing the survivability properties of layered network architectures (ii) the development of mechanisms for enhancing cross-layer survivability through the joint design of the logical and physical topologies. (ii) the development of new network architectures that have good cross-layer connectivity properties. The results of this research will be widely disseminated through publication in conferences, journals and the web. This research will directly impact the way in which future networks will be designed and deployed; and in particular, will lead to network designs that are robust and immune to failure propagation across layers","title":"NECO: Cross-Layer Survivability in WDM-based Networks","awardID":"0830961","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["517543"],"PO":["565303"]},"144339":{"abstract":"\"Garbage collection\" is the quaint name for automatic storage management, a feature common to many modern programming languages, notably Java. It has always been a cause of varying performance, though advances in garbage collection (GC) algorithms and tuning over the past two decades have made it tolerable for some production applications.<br\/>Chaos, as popularized by the saying that the flapping of a butterfly's wings might determine whether a hurricane occurs halfway around the globe, is the study of complex non-linear systems. Chaotic systems are not random, just very complex, requiring different tools to analyze and understand them. Chaos also places bounds on what we can know or predict about a system, and on how we can control it.<br\/>Over the last twenty years there was some evidence collected that GC, and computer systems in general, are chaotic. This SGER project will try to build evidence one way or the other. The implications are significant in terms of the predictability and controllability of this aspect of computer systems. However, the work is also significant as a focused step in understanding the possible chaotic nature of many aspects of computer systems, as a few researchers have begun to collect evidence.","title":"SGER: The Chaotic Behavior of Automatic Memory Management","awardID":"0836542","effectiveDate":"2008-09-15","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["550864"],"PO":["565272"]},"144009":{"abstract":"One of the main challenges in multi-core processor resource management is that existing operating systems (either conventional OS for single core or OS for SMP) are not able to effectively handle the new complexities in multi-core processors. <br\/><br\/>In order to address this challenge, the collaborators will conduct three closely related projects. (1) A hybrid system design and implementation for OS-based cache partitioning will provide efficient software management of shared caches with minimum hardware complexity, and will well define hardware\/software interface of shared cache management. (2) The collaborators will design and implement scheduling algorithms in OS kernels to effectively allocate CPU, caches and memory bandwidth resources to multiprogramming jobs in multi-core processors. (3) A data object locality-aware cache partitioning design and implementation will distinguish the locality strengths of objects and make effective cache allocation decisions. <br\/><br\/>The intellectual challenges of this project are threefold: (1) Hybrid system design involves complex interactions between hardware and the underlined operating system, and demands insightful understanding of existing system structures and innovation to enhance both architecture and the OS kernels. (2) OS-based scheduling in multi-core processors is a fundamental and complex problem in system research. (3) System implementation of proposed algorithms for scheduling and object coordination demands a lot of creative ideas for their seamless integration in the kernels. The broader impact of this project is expected to be significant. Solutions to address critical issues for significant performance improvement in multi-core processors are timely demanded in many application areas. The research training to both undergraduate and graduate students will address the concerns of lacking strong system professionals in IT and computer industries.","title":"Collaborative Research: CSR-PSCE, TM: Effective Resource Sharing and Coordination inside Multicore Processors for High Throughput Computing","awardID":"0834393","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["551992"],"PO":["535244"]},"137629":{"abstract":"This project exercises and expands upon methods for automatic discovery of new representations at multiple temporal and spatial scales. The specific framework generalizes classical harmonic analysis, in particular wavelet-based methods, to graphs and manifolds, thereby greatly extending the scope and the desirable characteristics of this multiscale-analysis framework to domains with arbitrary geometries. This framework, termed diffusion wavelets because it is associated with a diffusion process that defines the different scales, has unique properties relevant to learning, function approximation, compression and denoising. The set of core problems that this project addresses include fast algorithms for construction of multiscale diffusion wavelets, approximation of functions on very large graphs and high-dimensional manifolds, out-of-sample extensions of functions on manifolds and graphs, compression and denoising of functions on data sets, perturbation analysis, and randomized algorithms for multiscale analysis. Challenging application domains are being investigated, including analysis of document corpora, Markov decision processes, and 3D image rendering. In each case, multiscale diffusion analysis yields interpretable and meaningful results. For example, when applied to Markov decision processes, diffusion wavelet analysis yields new optimization methods that dynamically aggregate states and actions at multiple levels of abstraction; and when applied to 3D computer graphics, it yields new compression methods that capture geometric features of objects at multiple resolutions.","title":"RI-Medium: Collaborative Research: Learning Multiscale Representations using Harmonic Analysis on Graphs","awardID":"0803288","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["544475"],"PO":["562760"]},"139609":{"abstract":"With the ever faster growing number of images and videos, the main bottleneck in extracting the information contained in them is their analysis (indexing) and retrieval. Nowadays image and video search engines are based on textual descriptions, since visual cues are at too low level to provide useful retrieval results when dealing with a large variety of images and videos. For example, if a human submits a query image with the request to find similar images, she focuses on a certain object or a group of objects in the query image. Thus, the meaning of similarity is given by the images that contain similar objects. Therefore, extraction of objects in images (and videos) is a key factor for true progress in content based image\/video retrieval (CBIR). However, object extraction belongs to unsolved problems in Computer Vision (CV). This fact led to the development of a huge number of approaches that try to do CBIR without object extraction. However, although such approaches may be successful in some restricted application domains, in which case low level features may be sufficient to replace object extraction, they have not been successful in general purpose CBIR. The PIs believe solving the object extraction problem will lead to a breakthrough in CBIR. Therefore, the PIs propose to work on object extraction in images. There have been a large number of attempts to solve the object extraction problem in CV, and none provided a satisfactory solution. Why will our approach provide a good solution? A new methodology and a computation framework proposed by the PIs provide solid evidence that the breakthrough in object extraction is possible. <br\/>On the cognitive and geometric modeling side, the PIs propose to use a higher level knowledge of shape similarity and a mid level knowledge of local and global symmetry as cognitively motivated constraints for object extraction. Constraints are essential because object extraction is known to be an ill-posed inverse problem. The human visual system solves this problem very well and we are getting close to a full understanding of how this is done. <br\/>On the computational side, the PIs propose a new framework for a simultaneous estimation of medial axes and the contours. The proposed approach is inspired by the SLAM (Simultaneous Localization and Mapping) approaches in the field of robot mapping. Recent breakthrough solutions in robot mapping are based on the SLAM computation with particle filters. SLAM computation iterates over the processes of localization of the robot in the existing partial map (trajectory estimation), followed by a map update based on new observations and the estimated trajectory. The PIs treat the medial axis as trajectory of a virtual robot and the partial boundary as the map that is composed of edge segments associated with the medial axis. A first successful application of this framework is demonstrated by the PIs in the preliminary results.<br\/><br\/>Project URL: http:\/\/knight.cis.temple.edu\/~shape\/","title":"Collaborative Research: Simultaneous Contour Grouping and Medial Axis Estimation","awardID":"0812118","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["541851"],"PO":["564316"]},"143800":{"abstract":"Scaling scientific problems to 10,000,000 processors for the next generation HEC systems is today severely challenged by conventional practices of programming models, languages, and their supporting compilation systems. To achieve this goal one must expose greater degree of parallelism and improve parallel computing efficiency than is otherwise feasible with conventional methods such as MPI. <br\/>The goal of this collaborative research project is to dramatically enhance the scalability of challenging physics problems, through the application of an innovative programming model. The strategy is to replace static message-passing course grained processes using global barrier synchronization in a distributed memory space with a model using dynamic message-driven multiple threads using lightweight synchronization objects in a partitioned global address space. Parallelism is to be extracted directly from the large irregular sparse and time varying data structures. Ephemeral user-threads will permit many simultaneous tasks over the data structures, exposing the intrinsic near-fine grain parallelism. System-wide latency will be hidden by overlapping computation with communication through the advanced communication strategy of asynchronous message-driven processing. Consequently this will enable a class of physics problems that cannot currently be done using conventional methods.","title":"Collaborative Research: A Study and Implementation of Semantic Constructs for Highly Scalable Leading-edge Scientific Computing","awardID":"0833193","effectiveDate":"2008-09-01","expirationDate":"2010-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["559222","563330",383142],"PO":["565272"]},"142976":{"abstract":"This CPATH Community Building project develops a computing education community focused on introducing enterprise computing technologies into undergraduate computing education. Marist College serves as the lead institution in partnership with Illinois State University, North Carolina Central University, University of Arkansas, and Widener University. The team plans to bring together representatives from academia, industry, and non-profit sectors to develop and implement new curricula focused on skills needed to develop and maintain enterprise systems, integrate them into networks of technologically and geographically diverse systems, and design and implement applications that span a network of enterprise systems and distributed systems working together. Students should have the skills to set up and run data centers that minimize energy requirements and meet the ever increasing need for business continuity and disaster recovery. The project includes participation by a wide range of experts to develop cost effective undergraduate curricula based on nonproprietary standards that address essential computing technology principles encountered in predominantly large system environments.<br\/><br\/>The intellectual merit of the project lies in strong collaborative team and committed industry partners. The enterprise computing focus is innovative and important for the nation but also challenging both academically and practically. The project has the potential to produce new research and models in an area of emerging importance to the country.<br\/><br\/>The broader impacts of the project include the development of an educational community to share resources such as new courses and curricula. The implementation of new enterprise computing curricula could impact job prospects and capacity for students across the country through new innovative computing pathways open to a diverse student population in a range of academic disciplines.","title":"CPATH CB: Developing a Professional Community for Introducing the Principles of Enterprise Computing Technologies into the Undergraduate Curriculum","awardID":"0829558","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":[380652,"537212",380654,380655,380656],"PO":["564181"]},"142987":{"abstract":"Computing Education is essential not only for Computer Science and its many sibling disciplines(Computer Engineering, Software Engineering, Information Systems, etc.) but for practically all other academic disciplines. Computers are pervasive today and many professionals develop basic programming skills as a way to express ideas, problems and solutions in computational terms within their own disciplines. It is common to find curricula in the arts (music, graphical design), business (accounting, economics), sciences (biology, chemistry, physics), and social sciences with computational courses in their curriculum. In a way, computing is becoming a requirement of most professional degrees. This project addresses both the separation between computing specialists and to widespread integration of computing concepts, not just the technology but computational thinking, in other disciplines. The project will use technologies now commonly available to permit faculty to collaborate in offering courses that extend the potential reach of experts to a broader audience, as well as a collection of recorded expert lectures. In addition, it will develop a visual, interactive interface to a common framework around which to explore similarities and differences across domains and to enable decisions about educational plan development. The teams will also host workshops to identify innovative approaches to teaching, as well as support initiation of new collaborative course experience and reflect of the utility of the courses. The opportunity computing education is to learn how motivated hands-on learning can engage students and provide opportunities to introduce computing concepts. In addition to aiming for diversity in the groups of participating faculty, the project will extend the reach of computing to disciplines not normally associated with that content and will also represent the scope of their discipline to computing students, providing a broader view of the impact of the discipline as it is applied in creative fields. This project addresses the growing conviction that inter-disciplinary approaches are crucial to revitalizing computing education and offers a solution to the need for broader reach of individual areas of expertise. If successful, this project could spread to national implementations with very great effect.","title":"Collaborative Research: CPATH CB: Distributed Expertise in Enhancing Computing Education with Connections to the Arts","awardID":"0829616","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["83531","498339","498265"],"PO":["565136"]},"144824":{"abstract":"Collaboration and cooperation are essential to innovation and knowledge creation, and virtual teams are a valuable tool for such collaborations. Characteristic features of virtual teams, such as the dispersion and diversity of team members and the reliance on information and communication technology rather than direct interaction, are advantageous but also pose challenges. The purpose of this project is to examine what enables teams to best meet these challenges. We use the concept of trust as an organizing principle for understanding how technostructural and social-psychological factors affect the behavior in virtual teams. <br\/><br\/>The setting for this research is a Massively Multiplayer Online Game (MMOG). Since skills required in such games are similar to those required in work organizations, MMOGs are not only useful for the study of virtual teams but is in itself a valuable education and professional development tool. Travian Games, a designer of MMOGs, has provided unique access to conduct experiments, collect behavioral data, and administer surveys. The research team will pursue two empirical investigations: a series of experiments and a large-scale correlational study involving multi-level, longitudinal data. <br\/><br\/>These studies should yield insight into the development and role of social bonds in virtual teams and the interplay between social dynamics and technostructural factors. Findings will inform literatures on leadership and group dynamics, the management of virtual organizations and information technology. Findings will also provide practical insights into how to design virtual teams and organizations to better enable collaboration and innovation. The methodology involving MMOGs is itself an innovative contribution to both science and practice, as it demonstrates a new platform for conducting social science research and provides insight into using virtual games for leadership and team development.","title":"Collaborative Research: Communication, Trust and Leadership in Virtual Organizations and Teams","awardID":"0838402","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"5376","name":"INNOVATION & ORG SCIENCES(IOS)"}}],"PIcoPI":["483279"],"PO":["519302"]},"143977":{"abstract":"This research project focuses on the intersection of energy management and virtual machine environments. Contemporary energy management algorithms are implemented in operating systems and rely on detailed application, system, and device monitoring to manage devices and improve energy efficiency. Virtual machines make energy management more complicated in several ways. First, virtual machines are lacking application context. Second, guest operating systems lack global information. Third, virtual machines typically present virtualized devices to the guest operating systems, so that a guest operating system no longer has detailed device energy profiles. This project investigates the proper distribution of functionality between the guest operating system, virtual machine, and underlying virtual machine monitor so as to improve overall system energy efficiency. All guest operating systems must collaborate with the virtual machine monitor to exchange information about current context and the device state. System-wide energy efficiency requires global information, as is available in non-virtualized systems. The challenge is to provide the desired information flow while preserving the abstraction provided by the virtualization. Solving this challenge is critical since future systems will rely heavily both on energy efficiency to prolong battery life and reduce energy costs, and on virtualization to provide isolation, portability, and many other benefits. The tools and techniques developed in this project will provide a platform for future research. Furthermore, the goal of this project is to involve students in designing and evaluating energy-efficient virtual machines for various devices through in class assignments as well as independent studies. The projects and assignments will provide students with better understanding of the challenges that they will face in their future careers.","title":"CSR-PSCE,SM: Virtual Watts: Energy Management in Virtual Environments","awardID":"0834179","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383805,"475180"],"PO":["493916"]},"151358":{"abstract":"The goal of this project is to provide a principled way of quantitatively characterizing the effect of disclosing private data. Based on statistical decision theory, the proposed framework incorporates user-defined sensitivity information and identification model into a personalized risk function. The risk is intuitive and interpretable as it is based only on a user-specified loss function and elementary laws of probability and statistics. The proposed framework leads to a more accurate measure of the consequences of popular disclosure policies such as k-anonymity as well as efficient search for novel optimal policies.<br\/><br\/>Currently, private data is being disclosed according to general policies that do not necessarily reflect users preferences. The novel framework will let users obtain a quantitative grasp on the consequences of current data disclosure policies. Due to the simplicity and interpretability of the risk this will apply, in particular, to people lacking in technical or scientific education that otherwise remain uninformed about the use of their private data. Effective dissemination of the research results to industry and the popular press have the potential to transform current disclosure policies to become more focused on serving the needs of the community. The project also aims to enhance graduate and undergraduate education in the interdisciplinary area of statistical approaches to privacy preservation. Outreach efforts include mentoring of minority students in science and technology. The results of this project are disseminated via the web-page http:\/\/www.ecn.purdue.edu\/~lebanon\/privacyRisk.","title":"\"IPS: Decision Theoretic Approaches to Measuring and Minimizing Customized Privacy Risk\"","awardID":"0906643","effectiveDate":"2008-09-09","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7486","name":"INFORMATION PRIVACY & SECURITY"}}],"PIcoPI":["403885"],"PO":["565136"]},"143504":{"abstract":"Wireless networks are increasingly popular as the last-mile solution for a ubiquitous communication infrastructure. This trend in combination with the users? growing interest and falling cost in accessing vast amount of resources in Internet wirelessly has driven the developments of hybrid wireless network architecture such as Internet-based mobile ad hoc networks (IMANETs) and Internet-based vehicular ad hoc networks (IVANETs). However, it brings critical challenges in terms of limited storage space, frequent disconnections due to mobility, as well as unreliability of communication links. Caching a frequently accessed data in a local storage is shown to be an effective technique not only to relieve the network but also to improve data accessibility and availability in the presence of such challenges. The specific goal of this project is to develop algorithms and communication protocols that allow efficient and correct data caching in Internet-based wireless mobile networks. This project investigates cache management and invalidation strategies for IMANETs, and cache invalidation and consistency strategies for IVANETs. The project will make significant advances in understanding and designing an efficient data caching schemes for Internet-based wireless mobile networks. The success of this project will have a much broader impact on Internet-based wireless mobile network education and the corresponding industry, and provide versatile research opportunities to both undergraduate and graduate students. The algorithms and techniques developed in this research will be integrated with the education curricula","title":"Collaborative Research: NEDG: Exploring Data Access in Internet-based Wireless Mobile Networks","awardID":"0831853","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["557527"],"PO":["564777"]},"144956":{"abstract":"This is funding to support attendance by approximately 10 graduate students in a Doctoral Spotlight (workshop) to be held in conjunction with the Tenth International Conference on Multimodal Interfaces (ICMI 2008), which will take place October 20-22, 2008, in Chania, Greece, and is organized by the Association for Computing Machinery (ACM) with co-sponsorship from the Institute of Electrical and Electronics Engineers (IEEE). ICMI is the foremost conference representing the growing interest in next-generation perceptive, adaptive and multimodal user interfaces. Such interfaces represent an emerging interdisciplinary research direction, involving spoken language understanding, natural language understanding, image processing, computer vision, pattern recognition, experimental psychology, etc. They aim at efficient, convenient and natural interaction and communication between computers and human users, and represent a radical departure from previous computing that should ultimately enable users to interact with computers using everyday skills. The main goals of ICMI 2008 are to further scientific research within the broad field of multimodal interaction and systems, to focus on major trends and challenges, and to help identify a roadmap for future research and commercial success. Topics of interest this year include multimodal and multimedia processing, multimodal input and output interfaces, multimodal applications, user modeling and adaptation, multimodal architectures, tools and standards, and evaluation of multimodal interfaces. The 3-day event will bring together researchers from academia and industry from around the world to present and discuss the latest multi-disciplinary work in the field. The invited talks, panels, single-track oral and poster presentations will facilitate interaction and discussion among researchers. Participants in the Doctoral Spotlight will get to showcase their ongoing thesis work, either orally or via posters, in a special \"spotlight session\" during which they will receive feedback from an invited committee composed of approximately half a dozen senior personnel (including the conference General and Program Chairs). As in previous years, students funded under this award will all be U.S. residents enrolled at U.S. institutions of higher education. More information about the ICMI 2008 conference is available at http:\/\/www.icmi2008.org\/.<br\/><br\/>Broader Impacts: The Doctoral Spotlight will give students exposure to their new research community, both by presenting their own work and by observing and interacting with established professionals in the field. It will encourage students at this critical time in their careers to begin building a social support network of peers and mentors. Participants will be selected with the goal of increasing the breadth of participation at ICMI, with priority given first to minority students, female students, students from geographically under-represented states, and finally to students whose advisors or departments have insufficient funds to otherwise support their participation in ICMI.","title":"Support for Student Participation in the International Conference on Multimodal Interfaces '08","awardID":"0838857","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["457546"],"PO":["565227"]},"142657":{"abstract":"Normal vision is not static: time is a key dimension of the natural world we see. The eventual understanding of biological vision requires understanding the neural mechanisms used to recognize objects and actions over time. Thus the focus of the proposed research is to study how the primate visual system recognizes objects and actions in time sequences of images. A meta-goal of this project is to exploit the synergies between computational approaches and physiological experiments to lead to a better understanding of brain function and at the same time to develop better computer vision algorithms. Object recognition in time sequences of images presents a significant challenge for recognition systems, because it requires both selectivity to shape and invariance to changes of appearance in time.. <br\/>This project will extend an existing computational model of the ventral stream by adding temporal dynamics in its model neurons and the ability to process video sequences. It will also expand a working model of the dorsal stream to understand the relative roles that it and the ventral stream play in dynamic visual recognition. At the same time, recordings from single units, and multiple single units, from high level visual areas including IT and regions of the STS will be made in order to characterize the tuning of single neurons to the shape dynamics of specific image sequences. By combining modeling and physiology, this work will search for a computational explanation for how the higher areas of the visual cortex recognize objects and actions over time and how they can learn.<br\/>This integrative effort, which is focused on processing of dynamic perceptual information, can have a significant and direct impact on current theories of autism, dyslexia, and effects of stroke, in addition to directly guiding modeling and engineering efforts in computer vision. The proposed research is tightly coupled to education and teaching, and resources used in the research, including databases of videos, visual stimuli, the modeling software and the experimental data will be made available to the broad scientific community. Information on the project and its progress will be available at http:\/\/cbcl.mit.edu\/projects\/NSF-CRCNS\/index.html","title":"Collaborative Proposal: Object and Action Recognition in Time Sequences of Images: Computational Neuroscience and Neurophysiology","awardID":"0827483","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["523294"],"PO":["564316"]},"143988":{"abstract":"Abstract<br\/>Private and confidential information, e.g., credit card numbers and medical<br\/>records, is increasingly being stored online. Also increasingly this information is exposed due to human errors and malicious attacks. The proposed research addresses this problem through the design and implementation of a new system, called Aeolus. Aeolus will enable construction of applications in a way that greatly reduces the danger of unauthorized release of information. Aeolus includes both a new distributed security model and its support via a distributed platform. The model is simple and high level; the goal is to provide a tool that is easy to use and understand, so that it will be used in practice. In addition, Aeolus allows programmers to use programming languages of their choice and also to use components provided by third parties. The security model is based on information flow control, which prevents leaks of confidential information by controlling what can be done with information. The model contains novel ways to provide safe and precise delegation of authority. The proposed research additionally includes a first proof-of-concept implementation to allow evaluation of the model, a second implementation aimed at improving performance and reducing the size of the trusted base, a development of an architecture and replication techniques for storage servers that prevent leaks while not requiring users to encrypt their information. The work addresses a pressing problem with online information since it will greatly reduce this exposure due to errors and malicious attacks. It will additionally impact society through education, both of students on the project and through courseware.","title":"CSR-DMSS, SM: Aeolus: Secure Support for Preserving Confidentiality and Integrity in a Distributed Environment","awardID":"0834239","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383836],"PO":["565255"]},"143515":{"abstract":"NeTS-NECO: A New Resource Management Paradigm for Sensor Networks with Energy Replacement<br\/>(Principal Investigators: Ness B. Shroff, Emre Koksal, and Prasun Sinha)<br\/><br\/>Recent advances in sensor networks have resulted in a unique capability to remotely sense the environment. These networks can be used to sense natural as well as human-created phenomena (e.g., earthquake, fires, troop movements, radioactive substances). These systems could be deployed in remote or hard-to-reach areas, hence, it is critical that such networks operate unattended for long durations. To that end, new developments in the areas of renewable energy sources suggest that this is feasible. However, the design and control of sensor networks with the added dimension of renewable energy makes the problem of managing these networks challenging and substantially different from their non-replenishment counterparts. The goal of the project is to understand the performance limits of sensor networks with replenishment, to develop simple distributed algorithms and protocols that approach these limits, and validate and fine-tune the results based on experimentation. This ambitious endeavor will be accomplished by a team that brings in expertise that spans physical layer communications, distributed algorithms, control theory, resource allocation, sensor networking, and implementation. <br\/><br\/>Broader Impact: Given that sensor nodes with replenishment are an emerging technology, the techniques developed in this project have the potential to make a significant impact on emerging industry sectors. The PIs will actively share their results by giving academic and industrial seminars and facilitating student internships. Students on this project will learn a wide range of theoretical and experimental methodologies to prepare them for the workforce. The PIs will continue their efforts to recruit and train under-represented students.","title":"NeTS-NECO: A New Resource Management Paradigm for Sensor Networks with Energy Replenishment","awardID":"0831919","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["548182","542039","506208"],"PO":["565303"]},"143405":{"abstract":"The ability to reason about different equational theories is very<br\/>important to the analysis of cryptographic protocols. An understanding of<br\/>how equational properties of a function used by a protocol can be<br\/>exploited by an intruder can be invaluable in finding flaws that might<br\/>otherwise be missed. Numerous examples exist in the literature and even<br\/>in fielded protocols. One very powerful tool for cryptographic protocol<br\/>analysis with equational theories is equational unification. Equational<br\/>Unification was the basis of the NRL Protocol Analyzer (Maude-NPA). Its<br\/>use of these theories allowed it to both reproduce existing flaws and find<br\/>new ones at a level of precision way beyond that available to other tools<br\/>at its time. This suggests that equational unification if properly<br\/>extended, can give support in a similar fashion to analysis of<br\/>cryptographic protocols that use functions that obey more expressive<br\/>equational theories. The aim of this project is to provide a<br\/>laboratory for equational unification that will develop the algorithms<br\/>and techniques that can be used to support the use of equational<br\/>unification in cryptographic protocol analysis. This effort will<br\/>consist of two parts: the development of unification algorithms for<br\/>theories of interest to cryptographic protocol analysis, and the development<br\/>of new techniques for employing unification in cryptographic protocol<br\/>analysis. The project will help in the design and implementation of next<br\/>generation tools for protocol analysis. Tools developed in the project<br\/>will be made available to other researchers working on formal protocol<br\/>analysis methods as well as to protocol designers for experimentation. The<br\/>educational component of the project will involve undergraduate and<br\/>graduate students at both institutions.","title":"Collaborative Research: CT-M: Unification Laboratory for Cryptographic Protocol Analysis","awardID":"0831305","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["402469"],"PO":["529429"]},"143416":{"abstract":"Proposal Title: CT-ISG: Collaborative Research: Key Generation from Physical<br\/>Layer Characteristics in Wireless Networks<br\/>Institution: University of Connecticut<br\/>Abstract Date: 08\/05\/08<br\/>This project focuses on the development of a new class of secret key generation and<br\/>renewal algorithms for securing wireless networks by taking advantage of physical<br\/>layer characteristics. The basis of the approach is the identification of measurable<br\/>quantities of the wireless channel between a pair of nodes that are highly correlated<br\/>exclusively between them (albeit not identical). Capitalizing on this correlation, the<br\/>project develops algorithms for obtaining a shared key under various adversarial<br\/>models.<br\/>This addresses a critical problem area as the broadcast nature of a wireless link<br\/>provides a natural eavesdropping and intervention capability to an adversary and thus it<br\/>follows that securing a wireless link is essential to the security of any wireless network.<br\/>This project undertakes the design, implementation, analysis and testing of several<br\/>protocols and clarifies the relationship between the channel characteristics and the<br\/>cryptographic protocols that use them. It also investigates the trade-offs between<br\/>computational and communication costs, and quantifies the level of security achieved<br\/>by the key generation and renewal protocols assuming various adversarial models.<br\/>The broader impact of the project is in the potential of making wireless communications<br\/>more secure while avoiding costs (both computational and organizational) of<br\/>cryptography. Given the increased wireless internetworking and the multitude of<br\/>wireless enabled devices presently carried by an average user, the security of the<br\/>wireless link becomes one of the most critical aspects of securing computer systems.<br\/>The reduction in the computation also will translate to a reduction in energy use,<br\/>important to battery or alternative powered systems. Educationally, the project will<br\/>present an attractive combination of aspects of signal processing, computer security,<br\/>algorithms and cryptography and will offer a wealth of problems for student engagement","title":"Collaborative Research: CT- ISG Key Generation from Physical Layer Characteristics in Wireless Networks","awardID":"0831366","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["541892"],"PO":["497499"]},"143779":{"abstract":"Peta-scale systems have tens of thousands to millions of cores. To exploit the performance for applications such as climate prediction, environmental modeling, astrophysics, biology, life-sciences, etc, the problems of programming these systems must be solved. Programming language enhancements, compiler techniques and runtime support must be developed to enable computing and knowledge discovery at this unprecedented scale. The challenges faced by a user in programming these machines include performance, power, productivity, and portability, which are inter-related in a complex way. <br\/><br\/> This project entails the design and development of programming language, programming model, and compilation optimizations for I\/O and storage performance and power optimizations. The project is investigating ?What minimal set of changes or enhancements to programming models, programming languages, and what optimizations to compilers and runtime systems are needed to enable better I\/O, file and storage systems performance while optimizing power and improving productivity?? Some specific questions include: What language enhancements can be used by to specifically improve the I\/O and storage performance? Should interfaces be developed that can be used across languages for I\/O? What compiler optimizations are needed? Can the compiler identify transform codes that can inform the I\/O runtime and storage systems on phases to power-down disks to save power at certain times as required? The project tasks include: the design and development of programming-language enhancement; the design of a compilation framework and performance- and power-oriented I\/O optimizations using novel compiler analyses; and the design of a novel hint-handling mechanism within the I\/O stack.","title":"Collaborative Research: Advanced Compiler Optimizations and Programming Language Enhancements for Petascale I\/O and Storage","awardID":"0833131","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["560392","521549"],"PO":["565272"]},"143306":{"abstract":"Claude Shannon's point-to-point information theory is a basis for the design of all modern day communication systems, ranging from cellular communications, cable and DSL modems, statellite communications, compact disks, etc. Extending the theory from point-to-point communication to an entire network of communicating nodes is a holy grail of the communication field. It is expected that such an information theory of networks would have a significant impact for applications such as wireless and sensor networks. Yet, despite significant effort in the past 40 years, only isolated cases have been solved and there is still limited understanding of central issues such as interference, cooperation, broadcast and distributed compression of correlated information.<br\/><br\/>This research advocates a new general approach to attack network information theory problems. The new approach involves three steps: 1) approximate the noisy network with an appropriately chosen deterministic model which focuses on the interaction between the various signals rather than the noise; 2) analyze the analytically simpler deterministic model; 3) translate the insight into finding approximately optimal strategy for the original noisy network with guaranteed performance bound. Significant progress on several canonical long-standing open problems shows the power of the approach: 1) capacity region of the two-user Gaussian interference channel to within 1 bit\/s\/Hz per user; 2) capacity of the Gaussian (single-node) relay channel to within 1 bit\/s\/Hz; 3) capacity of the Gaussian relay network with arbitrary number of relays to within constant gap independent of the SNR's of the links; 4) rate region of the Gaussian multiple description problem to within a constant gap independent of the target distortions of the users.","title":"Information Theory of Networks: A Deterministic Approach","awardID":"0830796","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["523728"],"PO":["564924"]},"141007":{"abstract":"SRS-0820208\/0819845<br\/><br\/>TITLE: Scalable Knowledge-based Middleware For Networked And Mobile Systems<br\/><br\/>PI: Boon Thau Loo (University of Pennsylvania)<br\/>Co-PI: William C. Regli (Drexel University)<br\/>Senior Personnel: Joseph B. Kopena (Drexel University)<br\/><br\/>This research investigates scalable, knowledge-based middleware supporting content-based addressing and routing in mobile, networked systems. It incorporates and integrates two aspects: Ontological reasoning about system resources and declarative networking within routing components. At the application layer is OntoNet, a knowledge-based framework for representing and reasoning on system elements. Declarative, formal techniques provide service discovery and composition, content-based messaging, and distributed querying using OWL-Net, a subset of the OWL description logic. This work includes development of propagation strategies that are efficient and robust in mobile, networked environments. Network layer support is provided by declarative networks, a rule-based framework for compact, high-level protocol specifications. Declarative networking enables rapid prototyping and verification as well as online adaptation and meta-reasoning. This research will include extension of declarative networking to more readily support highly dynamic mobile wireless systems. The intellectual merit of this proposal is development of a unified, declarative framework for distributed organization of knowledge and information in real-world systems. It draws from many areas such as the Semantic Web, databases, and networking. Potential broader impacts are richer and more extensible platforms for real-world networks usable in emergency response, logistics, infrastructure monitoring, and ubiquitous computing.","title":"Collaborative Research: Scalable Knowledge-based Middleware for Networked and Mobile Systems","awardID":"0820208","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["518109"],"PO":["564388"]},"143317":{"abstract":"This project seeks to understand and analyze the global demography of cybersecurity threats and solutions. It is structured as a population study, in three majors parts. First, a real time monitoring infrastructure captures a new dataset on the population characteristics of malicious software, and of security tools, as they are actually found across the world. The monitoring infrastructure can compile frequent statistical estimates of the prevalence of various identified software programs, both malicious and protective, at an identified set of online sites, in a manner that does not compromise the privacy of end users. Second, the data is analyzed for insight to drive enhancements in cybersecurity. For example, analyses can look for correlations between malware prevalence and other factors such as site characteristics and the use of protective software, controlling for various factors. Third, the data set is to be published online in both raw and processed forms, to allow further research across computer security and a range of other fields, while at the same time educating the public at large about cybersecurity threats.","title":"CT-ISG: Population Studies in Computer Security via DNS Monitoring","awardID":"0830838","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["467986"],"PO":["497499"]},"143438":{"abstract":"This project concerns traffic analysis--the practice of learning sensitive <br\/>information from communication patterns, rather than their contents. As<br\/>encryption of data becomes more prevalent, a detailed study of traffic <br\/>analysis is necessary to understand the threats to privacy that patterns<br\/>of communication pose, and to design effective countermeasures. Traffic <br\/>analysis is also important for intrusion detection, to detect attacks and<br\/>abnormalities that are embedded in encrypted traffic.<br\/><br\/>The project will focus on two types of traffic analysis: flow linking, <br\/>where packet timings are used to discover causal relationships between <br\/>network streams, and semantic information extraction, where information <br\/>about the flow contents is leaked through packet sizes and timings. In<br\/>both cases, the goal of the project is to use information, detection, and<br\/>queuing theory to discover the fundamental limits of traffic analysis and<br\/>to design optimal defenses.","title":"CT-ISG: Traffic Analysis: Attacks, Defenses, and Fundamental Limits","awardID":"0831488","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["564825","432547","475205","475207"],"PO":["565327"]},"143559":{"abstract":"Our society is increasingly dependent on the Internet for instant access <br\/>to information. Broadband access can have a very strong positive impact <br\/>on society. The US is lagging behind in providing inexpensive broadband <br\/>access to its citizens. Research is needed to develop next-generation <br\/>broadband access technologies which will lead to new competitive services <br\/>and applications. Novelty: This project is creating new knowledge on the <br\/>design, deployment, and operation of the novel Long-Reach Broadband Access <br\/>Network (LR-BAN) architecture, resource-allocation algorithms, and network <br\/>evolution to accommodate traffic growth. Broader Impact: This project is <br\/>enabling US researchers to lead the development of next-generation Broadband <br\/>Access Networks. This comprehensive and integrated research and education <br\/>project should enhance our nation's educational infrastructure through <br\/>student training and influence the US telecom industry with its findings.<br\/><br\/>Specifically, this project is investigating novel network architectures, <br\/>protocols, and algorithms to design and efficiently operate next-generation <br\/>broadband access networks that can provide extended coverage across local <br\/>and metropolitan regions in telecom networks. The project is developing <br\/>efficient methods for extending the coverage of today?s Fiber-to-the-Home <br\/>(FTTH) networks from their 20-km limit to 100 km and beyond by exploiting <br\/>wavelength-division multiplexing passive optical networks (WDM-PONs) to <br\/>serve a large user base, leading to the Long-Reach Broadband Access Network <br\/>(LR-BAN). The LR-BAN architecture can effectively decrease the number of <br\/>Central Offices in a telecom operator's network and reduce its Operational <br\/>Expenditure. This project is investigating various LR-BAN architectural <br\/>options (such as ?ring-and-spur? architecture); methods to flexibly allocate <br\/>bandwidth on demand to users with a diverse set of requirements (such as <br\/>?multi-thread polling?); and methods to evolve the network toward future <br\/>reliable broadband applications and technologies.","title":"NEDG: Next-Generation Long-Reach Broadband Access Networks","awardID":"0832176","effectiveDate":"2008-09-01","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["518062","518063"],"PO":["565303"]},"141018":{"abstract":"Proposal number: 0820245<br\/><br\/>Title: Certification of Medical Device Software<br\/><br\/>PI: Jens Palsberg<br\/>Co-PI: Majid Sarrafzadeh<br\/><br\/>A medical device should not crash or confuse. A device crash can be anything from inconvenient to life threatening, while confusing device behavior can lead a user to draw an incorrect medical conclusion. While rigorous testing will remain essential, new approaches to design, implementation, and certification have great potential to increase our confidence in medical devices. At the end of the rainbow in this research area lays a design for certifiability paradigm in which programmers think of certification from day one and create designs and code that can be certified by static error checking tools. The project's goal is to develop languages and tools for designing, building, and certifying medical device software in elegant and powerful new ways. The project focuses on a common class of medical devices called medical monitoring devices. The investigators envision a certification tool that can meet challenges related to space bounds, soft-real-time response, life time, and meaningful results. The project's goal is to take a major step towards design for certifiability and to bring closer the day when the FDA will use static error checking tools frequently and routinely. One of the outcomes of the project will be open-source versions of medical device software, along with implementations of the new language and certification tools. The result is a platform for experimenting with software for medical devices that may open up new possibilities for researchers.","title":"Certification of Medical Device Software","awardID":"0820245","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":["547185","558389"],"PO":["564388"]},"143207":{"abstract":"Quickest detection is an important technique to detect the change of probability distribution in a random process being monitored. It is widely used in problems like financial decision making, environmental monitoring and industrial quality control. With the rapid development of networking techniques, there exist pressing demands to carry out quickest detection based on observations from many nodes and make decision at more than one node. Motivated by this demand, this research studies collaborative quickest detection in ad hoc networks, in which nodes exchange observation statistics and make local decisions about distribution change. In contrast to existing theory of decentralized quickest detection, the collaborative quickest detection does not need a data processing center, thus avoiding the round-trip time overhead and possible data congestion. Moreover, collaboration can enhance the agility and robustness of the detection of change. The research involves aspects of statistical signal processing (e.g. detection rule), information theory (e.g. source coding) and networking (e.g. scheduling or broadcast). An important application of collaborative quickest detection is spectrum sensing in cognitive radio systems. In such a system, secondary nodes need to monitor the activity of primary users and should quit the frequency band once primary users emerge. It is essentially a problem of quickest detection since the secondary nodes need to detect the change as quickly as possible. Therefore, this research also involves the design of collaborative quickest detection in cognitive radio networks. Inter-disciplinary essence of the research also lends itself to cross-disciplinary education. A one-semester graduate level course will be devised, which introduces quickest detection, cooperative communication and cognitive radio. This project also expects to attract traditionally underrepresented groups.","title":"Collaborative Research: Collaborative Quickest Detection in Ad hoc Networks with Application in Cognitive Radio","awardID":"0830451","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["531702"],"PO":["564898"]},"143449":{"abstract":"0831530- CT-ISG: I-BLOCK: Understanding and Filtering Malicious IP Traffic<br\/><br\/>Athina Markopoulou and Michalis Faloutsos<br\/>UC Irvine and UC Riverside<br\/><br\/>How can network infrastructure be protected from malicious traffic, such as scanning, malicious code propagation, spam, and distributed denial-of-service attacks? This project investigates mechanisms at the network layer for blocking malicious traffic. <br\/><br\/>One such mechanism is IP filtering: access control lists (ACLs) can selectively block traffic based on fields of the IP header. This mechanism is already available in routers today but, in order to be effective, two issues must be addressed. First, one must identify which IP addresses to block, which requires understanding and detection of malicious activity. A key insight to exploit is that malicious traffic exhibits clustering in both time and address space. Second, filters (ACLs) are a scarce resource, because they are stored in the expensive ternary content addressable memory (TCAM). To decrease the number of filters and therefore the cost, aggregation is used: a single filter blocks an entire range of IP addresses; however, this also blocks legitimate traffic originating from that range. Filter selection becomes an optimization problem that tries to block as many malicious and as few legitimate sources as possible, given a limited number of filters.<br\/><br\/>Outcomes of this project will include: (a) methods for modeling malicious traffic at the IP level (b) cost-efficient filtering algorithms and (c) a prototype to be tested in real networks. The problem is challenging and requires synergy between machine learning, data-mining, optimization and algorithmic techniques. The project can impact networking practice, by providing a comprehensive set of tools that can be deployed on today's Internet architecture.","title":"CT-ISG: I-BLOCK: Understanding and Filtering of Malicious IP Traffic","awardID":"0831530","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["549003","536897"],"PO":["565327"]},"143218":{"abstract":"Abstract<br\/><br\/>In lieu of a formal theory, designing wireless networks often relies on suboptimum rules that under-utilize the available resources. The objective of this research is to build on theoretical foundations of wireless networks by formulating generic optimization problems in a variety of settings. Questions about these problems are posed and answers are subsequently translated to fundamental properties. Among possible questions, this project focuses on the following ones: q1) How difficult are these optimization problems to solve? q2) What structural properties they possess? and q3)What solver(s) should be employed ? Answers to q1) determine whether optimal wireless networking is feasible; addressing q2) translates to architectural principles, e.g., optimal layered architectures; and responses to q3) lend themselves to working wireless network protocols.<br\/><br\/>This project builds on a preliminary result which asserts that separating wireless networks into layers is optimal in the presence of random fading. The approach leverages this principle and related structural properties introduced by fading to develop a formal theory encompassing algorithmic, performance analysis and development issues including: i) dual decomposition stochastic optimization solvers; ii) physical layer innovations in cross-terminal optimization of interference; iii) convergence of on-line stochastic approximation solvers; iv) non-ergodic high-mobility wireless networks; and iv) cognitive radio networks. <br\/><br\/>Successful completion of this research is of immediate interest to the 802.11 evolution of WLANs and the emerging initiatives in mobile ad hoc networks. The vision is ubiquitous incorporation of optimally designed protocols in future generation wireless networks and wireless sensor networks. In a broader sense, advances in the foundations of wireless networks will impact the class of stochastic transport networks which includes the power grid, the water distribution system and the transportation infrastructure.","title":"Theoretical Foundations for Wireless Communication Networks","awardID":"0830480","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["564051"],"PO":["564924"]},"143229":{"abstract":"Problems in the design of low-cost networks are central to our understanding of algorithms. Computing a minimum-cost spanning tree is one of the first graph algorithms to be taught in any standard algorithms course; indeed, the minimum-cost spanning tree algorithm of Boruvka is one of the earliest graph algorithms known. In the last decade or so, similar advances in our understanding of approximation algorithms have occurred through research into generalizations of the minimum-cost spanning tree problem and the Steiner tree problem. This research will consider more recent issues in the low-cost design of networks, and to discover similar, general algorithmic techniques to address them.<br\/><br\/>One central issue now under consideration is that of the role of uncertainty in the specification of the design of the network. One of the ways this is done is by considering a probability distribution over the potential connectivity requirements, leading to stochastic optimization problems; another is that of giving a universal solution, from which a good solution can be derived no matter what requirements are realized. Most prior work in the area assumed that network connections would be purchased, but recent work in infrastructure leasing considers issues in which connectivity requirements are satisfied for shorter periods of time by leasing connectivity from another party instead of building a network.<br\/><br\/>Finally, recent work has considered the general problem of dropping connectivity requirements in the case that they become too expensive to fulfill. This has been explored in simple cases in the past of the prize-collecting Steiner tree problem, but more general models have only recently started to be considered. The intellectual merit of the research lies in finding significant methodological innovations in the course of addressing these issues, and finding simpler, better, more general, and more practical approximation algorithms as a result of this research.<br\/><br\/>Network design problems are increasingly important in a society where reliable communication is essential. In many situations, there is uncertainty about the inputs on which you must compute, since, for example, it is hard to detect failed links and exact network speeds are volatile. If successful, this research will lead to algorithms that can deal with the uncertainty inherent in real-world networks.","title":"Contemporary Issues in Network Design","awardID":"0830519","effectiveDate":"2008-09-01","expirationDate":"2012-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["485172"],"PO":["565251"]},"143119":{"abstract":"Symbolic\/Numeric Linear Algebra Computation<br\/>Abstract<br\/><br\/>The primary characteristic of symbolic computation with large rational numbers is that the computations may be slow but exact -- sometimes more exact than needed. The primary characteristic of numerical computation with floating point numbers is that the computations are fast but approximate -- sometimes more approximate than acceptable. In this research, accuracy and speed of computation are achieved by using hybrids of symbolic and numeric methods. This work involves improvements for linear system solving, matrix inertia computation, and minimal polynomial computation. The methods and implementations that this project creates can be taken up by software systems used widely in science, engineering, and education such as Maple, Mathematica, and Matlab. This research contributes to the fundamental understanding of the interplay between the exact and the approximate.<br\/>The investigators create and demonstrate computational methods to solve several classes of linear systems of equations. Techniques heretofore undeveloped in this arena include matrix bordering techniques and iterative refinement. Additionally, the research improves symbolic-numeric capability to compute inertia of matrices, a measure that is important in control theory and arises in the study of Lie groups. The research includes finessing numerical loss of accuracy and handling of singular cases. The experimental approach to mathematics is enhanced by enabling large problems to be solved, particularly in number theory, combinatorics, algebraic geometry, thus providing data for conjecture formation and for experimental verification of conjectures. For science and engineering, this project creates a capability to solve a class of problems for which no solution method currently exists at all, specifically it is to solve linear systems where (1) numerical methods fail due to ill-condition of the problem instance, yet (2) the exact result is valid and meaningful despite the approximate nature of the input data.","title":"Symbolic-Numeric Linear Algebra Computation","awardID":"0830130","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["541916","451563"],"PO":["565251"]},"143009":{"abstract":"Biological forms of computation present some of the most promising?yet challenging?examples of adaptive mechanisms that we would like to understand well enough to engineer ourselves. However, models for biological processes are becoming increasingly detailed and unwieldy in an effort to reproduce ever-more detailed experimental observations. This research involves the development of mathematical theory, algorithms, and software for efficiently constraining models to data and to analyze their properties mathematically. A sufficiently detailed understanding through mathematical analysis permits the generalization of operating principles that provide biological insights and a basis for engineering similar mechanisms. In particular, the investigators apply these methods to infer adaptive and self-governing properties from detailed dynamical models of excitable neural and cardiac tissue. <br\/><br\/>Although detailed models of physical systems may involve many variables and parameters, mathematical analysis often demonstrates effective lower dimensionality in their operating principles. A decomposition of a complex model to approximate lower-dimensional sub-regimes facilitates analysis by standard techniques from dynamical systems and optimization theory. In contrast to a priori reductions to ?toy? models, software tools monitor and control the sources of error in the approximations, in particular the assumptions underlying the decomposition are validated against global constraints to ensure consistency with the behavior of the full physical system. To study abstract properties of the system such as adaptiveness, decompositions can be made in terms of measurements of qualitative features in the dynamics. These features may be simple or complex according to the needs of the problem. Their formalized definition in software structures enables existing techniques for model optimization and inference to be applied more intelligently, particularly in the context of model behavior that may resemble experimental data only in qualitative terms.","title":"EMT\/BSSE: A Computational Framework for Inferring Self-Regulatory Properties from High-Dimensional Dynamic Models of Biological Systems","awardID":"0829742","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7750","name":"CDI TYPE I"}}],"PIcoPI":[380771],"PO":["565223"]},"136508":{"abstract":"Time integrators are crucial computational tools for studying nonlinear dynamical systems. Numerous time stepping methods have been developed over the years, many of which are now available in off-the-shelf solvers. However energy drifts and numerical dissipation problems present even in highly accurate algorithms still routinely plague engineering applications. Geometric time integrators have been recently proven greatly useful to elucidate and fix these issues in solid mechanics. Yet these contributions have not carried over to the Eulerian setting, where they could impact both the understanding and the reliability of time integrators for computational fluid dynamics. The goal of this research project is thus to develop novel, geometrically-based Eulerian time integrators for the class of problems whose dynamics is described by an action principle, possibly including dissipation and forcing---which encompasses the canonical Euler and Navier-Stokes equations, as well as many other models. Eulerian discretizations of the Hamilton-Pontryagin principle will be explored, and combined with mathematical and numerical tools such as Discrete Exterior Calculus, the semigroup of positive doubly-stochastic matrices, and implicit functions. Resulting integrators are expected, just like in the Lagrangian setting, to respect the structure of the physics, i.e., to introduce no artificial numerical loss of crucial physical quantities such as energy or circulation.<br\/><br\/>The proposed research activities aim at developing an infrastructure for predictive and high-order accurate simulations of fluid-mechanical systems that combine the power of modern applied geometry with modern computational mechanics. In particular, it promises the introduction of novel variational fluid simulation algorithms: this innovative computational approach relies on a multidisciplinary effort drawing upon techniques from geometric mechanics, discrete geometry, numerical analysis, and graphics, thus promising a broad theoretical and practical impact. The development of such variational integrators from a unified geometric standpoint represents a stepping stone for our long-term goal of solving complex physical phenomena such as a flowing dress, a swimming fish or splashing water, the simulation of which requires considerable improvement of the current state of the art to become commonplace. The research experience acquired during this project is to be disseminated to a wide range of audiences through publishing in mathematics, engineering and computer science journals, books, and conferences, as well as on our web sites, in summer schools, workshops, and other educational activities. Outreach efforts at our three institutions include the recruitment of students from underrepresented groups to help with this research project, leveraging existing efforts for enhancing the participation of women and minorities in scientific research.","title":"Collaborative Research: Geometric Time Integrators for Mechanical Dynamical Systems","awardID":"0757092","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7478","name":"DYNAMICAL SYSTEMS"}}],"PIcoPI":["370977"],"PO":["564988"]},"138818":{"abstract":"Elements of uncertainty are inherent to management and analysis of complex image data for scientific and engineering applications. The work builds on previous multidisciplinary work for storage, management, and analysis of biological images of cellular architectures in the vertebrate central nervous system and sub-cellular environments, but the techniques are general and target other areas, such as environmental management, geographical information science, remote sensing and interactive digital multimedia. Imaging is at the cores of many scientific discoveries, with information captured in terms of raw pixel intensities and in multiple channels for color or hyperspectral imagery. The work includes generation of probabilistic measurements and quantified uncertainties from image analysis methods, pattern classification methods generating information that can be stored as probabilistic feature tables and new approaches to visualization of probabilistic information. The proposed work will be integrated within the UCSB BioImage Search and Query environment, part of the campus data infrastructure, and the software developed will be made available as open source.","title":"III-CXT-Large: Working with Uncertain Data in Exploring Scientific Images","awardID":"0808772","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["536697","550914","426604","518684"],"PO":["565136"]},"140996":{"abstract":"Proposal Number: 0820138<br\/><br\/>Proposal Title: Genericity in Network Software: Using Type Systems and Formal Methods to Harness Diverse Theories and Calculi for Scalable and Safe Compositions of Network Services<br\/><br\/>Principal Investigators: Assaf Kfoury, Azer Bestavros, and Abraham Matta<br\/><br\/>This research defines and implements a generic formal framework for writing compositional network specifications and programs; the latter can capture constraints on the contents, form, and representation of communications and interactions. The project contributions include the design of a polymorphic flow language and accompanying algorithms for type-inference and type-checking. Concrete type spaces are defined with which to instantiate this generic framework, reflecting sufficient bounds to ensure correctness of composing systems that can be specified in the framework; examples of such type spaces reflect useful results in coding theory, scheduling theory, control theory, network calculus (among several other disciplines) in which behaviors and properties can be structured into both qualitative and quantitative hierarchies and bounding sets. The result environment will enable network programmers to leverage useful results from valuable but less accessible (to the average network programmer) approaches to the safety and verification of composite software systems, and will integrate these approaches into familiar specification and programming practices.","title":"Genericity in Network Software: Using Type Systems and Formal Methods to Harness Diverse Theories and Calculi for Scalable and Safe Compositions of Network Services","awardID":"0820138","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":["526817","562061","438360"],"PO":["564388"]},"143801":{"abstract":"High-end computing systems are needed to study important, compute-intensive applications such as scientific simulations, multimedia stream processing, and geographical information systems. While these systems are still evolving, it is clear that they will be extremely large and complex, with tens to hundreds of thousands of processors providing a deep hierarchy of systems and resources. This research will develop the theory, techniques, and building blocks that can be used by domain scientists who are not expert parallel programmers to compose efficient applications for such complex systems.<br\/>Hence, the outcomes of the research should greatly increase the number of potential users of high-end machines to include essentially all scientists whose problems could take advantage of such systems. The software resulting from this research, including the testbed applications for important problems in computational biology and physics, will be made publically available.<br\/><br\/>Composition is a natural way to construct and reason about large, complex systems. This research will develop compositional strategies for building applications and for optimizing and controlling the application and its use of system resources.<br\/>This project will use the STAPL (the Standard Template Adaptive Parallel Library) infrastructure for parallel C++ code. STAPL includes of a collection of generic parallel algorithms and distributed containers. In this research, STAPL's existing adaptive capabilities will be further refined and novel techniques will be developed for compositional performance modeling and for providing fault-tolerance capabilities that can be set individually for each container or algorithm instance in the program. A modern programming interface will be designed based on composition of parallel operations that will be modeled on the range abstractions in STAPL and C++0x and directly supported by a high-level compiler.","title":"A Compositional Approach to Scalable Parallel Software","awardID":"0833199","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7952","name":"HECURA"}}],"PIcoPI":["561154","496841","501668"],"PO":["565272"]},"142977":{"abstract":"All recognize the pressing need for more computer science graduates, as well as the often unappealing nature of introductory classes. The goal of this project is to showcase and teach the skills necessary to thrive in computer science, while engaging a diverse student body at a variety of education levels. In order to revitalize undergraduate education in computer science and address the need for an increase in undergraduate majors, this project will use a novel three-fold approach. The community of college professors, high school teachers, and an industrial partner will plan and implement an exciting first exposure to computer science by engaging experiential learning. Interactive Learning Modules, developed by the collaboration team members at different institutions, will serve as a vehicle for bringing innovation to the classroom. These collaboration-based experiences throughout the computer science curriculum are encouraged through a number of mechanisms, including multi-institution teams, games, and research into effective design of curricula and modules. The modules themselves are expected to inspire creation of addition interactive modules, designed by students for students, identifying an aspect of computational thinking, a separate domain of its use and a variety of which in which the modules provides experiences. A summer seminar experience for teachers will provide credit hours as well as experience in using the learning modules. Evaluation will cover both formative and summative aspects.","title":"CPATH CB: Computational Thinking Showcase: Computing Concepts Across the Curriculum","awardID":"0829563","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["449645","393999","393997",380661],"PO":["565136"]},"143967":{"abstract":"The advent of multicore platforms has led to renewed interest in multiprocessor real-time systems, with considerable recent work directed at scheduling issues. Unfortunately, the equally-important topic of synchronization, in comparison, has been somewhat neglected. Indeed, the most influential work on this topic was done decades ago, at a time when multiprocessor real-time applications were deemed to be mostly of \"academic\" interest only. The objective of this project is to re-visit the issue of real-time synchronization in light of the ongoing multicore revolution, with the goal of devising mechanisms that can be practically applied. The fundamental thesis of this project is \"simplicity wins.\" Specifically, real-time synchronization mechanisms, though generally applicable, should be designed with common-case scenarios in mind, using simple techniques that can be reasonably analyzed. This begs the question: What is the common case? To determine this, trace data is collected in this project concerning the synchronization behavior of a wide range of real-time applications. This data is then being used to produce real-time synchronization benchmarks, which will guide development efforts. These efforts consider: real-time multiprocessor locking protocols; techniques for supporting read-mostly synchronization; the use of transactional memory and non-blocking synchronization in real-time applications; and synchronization techniques that can be applied in settings where both real-time and non-real-time components co-exist. Broader impacts include joint research with industry colleagues at AT&T, Honeywell, IBM, Intel, and Sun, and the development of publicly-available open-source software that can be used by other institutions for research and teaching purposes.","title":"EHCS(EHS), TM: Real-Time Synchronization on Multicore Platforms","awardID":"0834132","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["31436","518412"],"PO":["561889"]},"142999":{"abstract":"This project initiates new, unifying directions in Property Testing. <br\/>Property Testing is the area of algorithmic research that attempts to discover global properties of data by by sampling the data probabilistically in very few places. The ``oldest'' property test might be the use of polling to predict the outcome of an upcoming election. Modern research has extended the scope of property tests to a much richer class of properties including tests of linearity (``is the data essentially linear with respect to some parameters\"), multilinearity, low-degreeness, colorability (``is the data describing a graph with small chromatic number\") etc.<br\/><br\/>The goal of this project is to shed light on the question: What makes some properties testable so efficiently, that we do not have to look at the entire data in order to test for it? The thesis underlying the project is that for interesting properties, testability ought to be related to the ``invariances\" shown by the property: i.e., if the data is viewed as a function from some input to some output, then the ``invariances\" are given by a set (a group) of permutations of the input space under which the property remains unchanged. The project explores a variety of potential invariances to consider and studies conditions under which {\\em every} property exhibiting such invariance is testable. <br\/><br\/>The broader impact of the project is to find ways of coping with the data explosion problem faced by many computers, by describing settings where massive data may be analyzed by sampling small pieces.","title":"Invariance in Property Testing","awardID":"0829672","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["425345"],"PO":["565157"]},"142647":{"abstract":"This award supports the preparation and sharing of computational neuroscience data as part of a new component of the CRCNS program aimed at catalyzing rapid and innovative advances in computational neuroscience and related fields. Through a collaborative effort of many laboratories, this project combines knowledge about the nervous systems of several important gastropod model systems, including Aplysia, Clione, Helisoma, Helix (Cantareus), Hermissenda, Limax, Lymnaea, Melibe, Pleurobranchaea, and Tritonia, in an effort to lay the foundation for future comparative neuromics in gastropods.<br\/><br\/>Just as comparative genomics has enabled researchers to understand the evolution and development of organisms in terms of similarities and differences in their genomes, comparative neuromics, the comparison of neurons and their connections across species, offers unprecedented opportunities to increase our understanding of nervous system structure and function. Gastropod molluscs are uniquely suited for neuromic analysis because they have large individually identifiable neurons and work has proceeded for over four decades on species ranging from terrestrial and aquatic pulmonates to marine opisthobranchs, which inhabit diverse ecological niches. This project will consolidate the published knowledge about gastropod neurons and neural circuits into a searchable, extensible knowledge base, to be rendered in a format that can be federated with other knowledge repositories and primary data sources. <br\/><br\/>This project will build on currently available web-based tools and coordinate with other related efforts, such as NeuronBank, Open Biomedical Ontologies, and neuroscience-specific ontologies being developed for the Biomedical Informatics Research Network and the Neuroscience Information Framework. The gastropod user community is involved in providing sources of information and testing the outcome of the project to assure the accuracy of the representation and the usability of the format. Consolidating the vast amount of accumulated knowledge about gastropod nervous systems is essential for the expanded use of these model organisms as genomic tools become available through the Aplysia genome project and other initiatives. This project will serve as a basis for federating other databases including neuronal morphology, neuronal and neural circuit computational simulations, and signal transduction pathways. It will also serve as a proof of concept for larger research communities with more disparate nervous systems.","title":"CRCNS data sharing: Comparative Neuromics of Gastropod Molluscs","awardID":"0827418","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7713","name":"ACTIVATION"}}],"PIcoPI":["487985"],"PO":["564318"]},"143978":{"abstract":"Real-time embedded systems (RTES) found in many real-world applications demand that timing requirements for computations and communications are satisfied. In addition, energy consumption is becoming a major consideration for such systems, because of the proliferation of mobile, wireless, and embedded systems with limited energy resources. Energy consumption significantly impacts the cost, performance, and life time of such systems. This research addresses the lack of system-wide energy conservation approaches by developing integrated scheduling techniques to reduce the overall energy consumption of wireless real-time systems while meeting applications' timeliness requirements. Integration in this context refers to reducing the energy consumption of a system as a whole by maximally exploiting the Dynamic Voltage Scaling (DVS) capabilities of CPU cores and the Dynamic Power Management (DPM) capabilities of other resources and devices. Important results of this work are novel and highly applicable algorithmic techniques for energy-efficient RTES. The broader impacts of this project can be found in both education and research, in addition to its enormous social impacts due to the pervasiveness of real-time systems and environmental concerns. The results of the experimental studies are being made available online for other researchers to pursue their own research and education interests.<br\/>The effort also helps greatly to alleviate the problem of many theoretical system-level power reduction techniques not being adopted due to the lack of sound studies based on actual hardware platforms.","title":"CSR-EHCS(EHS), SM: Collaborative Research: Integrated Energy-Aware Resource Scheduling for Wireless Real-Time Systems","awardID":"0834180","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550624","474129"],"PO":["561889"]},"143626":{"abstract":"A large class of distributed computations in many application domains requires dynamic and adaptive coordination among geographically distributed and autonomous participants.<br\/><br\/>While the foundations of distributed computing have been considerably developed, the question of the value of resources such as partial knowledge, initialization or partial coordination remains unresolved. In many current distributed systems, participants, such as clients, devices, or sensors at any level of granularity, are integrated into a changing network topology to accomplish a global objective despite the fact that the knowledge available at each participant is local, incomplete, imperfect and asymmetric. State-based computation?such as that employed in self-stabilizing protocols?is an exciting computing paradigm that provides an umbrella covering such local computability towards global objectives.","title":"Autonomous Distributed Local Computing Models using Self-Stabilization","awardID":"0832582","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["436244",382532],"PO":["565251"]},"145815":{"abstract":"The goal of this collaborative exploratory research project (0842546, Clement Yu, UIC and 0842608 Weiyi Meng, SUNY Binghamton) is to investigate two important problems in opinion retrieval. The first is to determine whether a negation word flips the sentiment of an opinionated word and the second is to examine how the opinions about a topic change over time and what causes significant changes. This project aims to demonstrate within 18 months that within two domains (consumer products\/services and medical\/health), whether a negation word flips the polarity of an opinionated word or phrase can be accurately determined when there is a single occurrence of such negation word in a sentence, and the significant time points and words\/phrases that characterize the changes of sentiments for a given topic can be accurately obtained. The research results from this project will lay the foundation in achieving longer-term goals that include determining the polarity of the sentiment of a feature with reference to a given query in any given domain, identifying the events that cause the significant change of sentiments, measuring the intensity of sentiments and its change over time, and studying the interactions between the negation and temporal aspects of opinion retrieval. It is expected that the proposed project can have a significant impact on future search engine technology, information retrieval, text mining, and blog retrieval and analysis. Research results will be incorporated into several courses the PIs teach and students will be recruited to participate in the research activities. Research results will be disseminated through published papers. The project Web site (http:\/\/www.cs.binghamton.edu\/~meng\/OpRetSGER.html) provides access to research results.","title":"SGER\/Collaborative Research: Handling Negations and Temporal Aspects for Opinion Retrieval","awardID":"0842546","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[389044],"PO":["563751"]},"152228":{"abstract":"North Carolina Agricultural and Technical State University (NCAT) proposes a novel, 2-way collaboration between HBCUs and research universities in order to increase the number of African Americans who obtain graduate degrees and enter faculty and research careers in computing. Alliance participants include HBCU's - NCAT, Bennett College, Dillard University, Jackson State University, and Norfolk State University - and majority, research universities - the University of Colorado, Georgia Institute of Technology, and Virginia Polytechnic Institute and State University. North Carolina State and the United Negro College Fund Special Programs will provide consulting. <br\/><br\/>The program will have three components. Firstly, it will strengthen undergraduate computing programs at the HBCUs by changing curriculum and pedagogy to include collaborative learning environments, discovery-based and hands-on learning. The Alliance will create research and educational experiences for undergraduates and graduate students providing authentic learning in the form of design, research, industry and workshop experiences. The students will be given social, academic, and career support through peer-, step- and faculty-student mentoring. Secondly, Alliance activities will institutionalize two-way research opportunities and teaching partnerships for students and faculty. This will be done with the development of joint research and teaching, both remotely and through student and faculty visits and exchanges. Faculty will partner on research teams and graduate student supervision. Thirdly, the Alliance will support ongoing research partnerships among faculty members and will provide pathways for HBCU students to pursue faculty careers after graduation. All of these activities are based on a unique, dual feeder model that allows some HBCU undergraduates to pursue Master's study at a research intensive HBCU (NCAT) before going on to Ph.D. study at a research university. The collaborations developed under this Alliance will be meaningful, two-way relationships that will be built from the PIs' experience in facilitating collaborations between research universities and HBCUs.","title":"BPC-A: Collaborative Research: Alliance Between Historically Black Universities and Research Universities for Collaborative Education and Research in Computing Disciplines","awardID":"0909589","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7482","name":"BROADENING PARTIC IN COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7584","name":"ITR-BROADENING PARTICIPATION"}}],"PIcoPI":["510094"],"PO":["561855"]},"143406":{"abstract":"Abstract:<br\/><br\/>This research project focuses on the development of cryptographic mathematical models and constructions that address realistic security requirements at the implementation level. This is a fundamental problem as cryptographic security formalisms are often criticized for lack of relevance given the wide range of attacks available at the implementation level. Indeed, traditional cryptographic attacks are restricted in the way private data can be accessed; hence, the security of systems relying on such constructs is contingent on external non-cryptographic means for enforcing the necessary tamper resilience. Unfortunately, this physical tamper resistance is either too expensive or unreliable. The research extends models of cryptographic attacks to include various forms of private data tampering and access and brings the theory of cryptographic constructions closer to security concerns in practice. In particular, the tamper proofing of a wide set of cryptographic primitives is considered in an extended adversarial setting, such as digital signatures, public key encryption, secure function evaluation, as well as arbitrary cryptographic functions. This research thus explores the boundaries of what is achievable algorithmically and practically through cryptographic means.","title":"CT-ISG: Collaborative Research: Tamper Proofing Cryptographic Operations","awardID":"0831306","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":[381857],"PO":["565327"]},"143527":{"abstract":"The growth of communication networks has profoundly increased <br\/>the availability of information and increased productivity. A <br\/>key requirement in continuing to reap the benefits of connectivity <br\/>is the growth of pervasive wireless networks. Wireless networks <br\/>are resource constrained, primarily due to their broadcast nature. <br\/>Alleviation of this constraint requires an efficient, distributed <br\/>solution to the Medium Access Control (MAC) problem. In this project, <br\/>the investigators study `locally-optimal scheduling and PHY <br\/>adaptation', by incorporating physical layer (PHY) models into <br\/>maximal scheduling for increased accuracy, and priority mechanisms <br\/>for increased efficiency. This will result in the design of <br\/>efficient, distributed MAC protocols, whose guarantees are accurate, <br\/>from the PHY perspective. The results from this research, which is <br\/>training a graduate Ph.D., are disseminated through appropriate <br\/>publications and seminars, to guide the development of future <br\/>wireless network MAC protocols and are incorporated into the <br\/>university?s undergraduate and graduate courses.<br\/><br\/>PHY adaptation and scheduling (MAC) in general ad hoc wireless <br\/>networks, is an NP-hard problem, especially, if distributed <br\/>solutions are desired. In this project, the investigators study <br\/>locally-optimal algorithms, a generalization of maximal scheduling, <br\/>using broad queuing results that prove stability, and adapt them <br\/>to wireless networks, with careful attention paid to the PHY model. <br\/>Theoretical characterization of the performance of these algorithms <br\/>is carried out using graph theoretic and optimization theoretic <br\/>analysis, along with intuition on the design of optimal (though, <br\/>centralized) MAC algorithms. The key ideas of this research, which <br\/>are at the intersection of communication theory, queuing theory, <br\/>graph theory and optimization, are incorporated into university <br\/>courses, using the concepts of Lyapunov functions and fluid models.","title":"NEDG: Locally-optimal Power, Rate Adaptation and Scheduling in Wireless Networks","awardID":"0831973","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["518488"],"PO":["557315"]},"143769":{"abstract":"Recently, the term Data-Intensive SuperComputing (DISC) has been gaining popularity and includes applications that perform large-scale computations over massive datasets. Because of the increasing volume of data analyzed, the amount of computation involved, and the need for rapid or even interactive response, these applications have an ever increasing demand for computational power. <br\/>Starting within the last 2-3 years, it is no longer possible to improve processor performance by simply increasing clock frequencies. As a result, multi-core architectures and accelerators like Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have become cost-effective means for scaling performance. Such architectures are, however, creating a programmability challenge for this class of applications. <br\/><br\/>This project targets a language-independent compiler and runtime framework for enabling data-intensive applications to be scaled on a variety of modern and emerging highly parallel systems. Specifically, the target <br\/>will be cluster of multi-core machines, where each node could additionally have an accelerator like a GPU. <br\/>The system proposed here will built on our prior work on an <br\/>earlier system, FREERIDE (FRamework for Rapid Implementation<br\/>of Datamining Engines). <br\/>Building on the FREERIDE framework, this project has the potential for an impact in the areas of high-end computing, data mining, <br\/>and scientific data processing.","title":"A Language Independent Framework for Compiling Data-Intensive Applications on Highly Parallel Systems","awardID":"0833101","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["558505"],"PO":["565272"]},"141349":{"abstract":"This cross disciplinary project for instrument development combines the fields of computer vision (and computer science overall) with radiation therapy and behavioral sciences. This project is a joint venture among the Medical School, the Psychology Department, and the Institute of Technology. The goal is to develop a minimally invasive, full-body, patient tracking system to be used with the helical tomotherapy system currently in use at the institution. The purpose of this effort is to detect when a patient becomes misaligned during radiation treatment. Adding visual feedback to the helical tomotherapy treatment should improve the safety of the patient, reduce the considerable treatment time, and improve the cure rate. It should be noted that the proposed instrument has a wide variety of implications for other forms of radiation therapy and medical systems that require a specific patient pose with respect to the medical device. In order for these devices to revolutionize the treatment field, the interaction of the patient with the device should be studied from a behavioral sciences point of view. In this project, behavioral sciences will play an important role in capturing the behavioral patterns exhibited when the patient is in treatment, and in devising new patient placement protocols with profound impacts on cancer treatment.<br\/><br\/>During the early stages of this development, the proposed instrument will not be used in human trials, focusing instead on the following tasks:<br\/><br\/>- Development of efficient computer vision algorithms (structured light) for detecting the patient's movements in a non-intrusive way that simultaneously offers the possibility for creating a closed-loop helical tomography instrument,<br\/>- Selection of features, so cumbersome reflective markers are no longer needed,<br\/>- Creation of the probabilistic framework necessary to utilize the features found from multiple calibrated cameras to determine the probability distribution of likely body positions,<br\/>- Incorporation of behavioral science research that utilizes cyber-enabled principles to create patient-friendly medical devices and treatments, and<br\/>- Experimental validation of the proposed instrument and improvement of the instrument with cyber-enabled behavioral science input.<br\/><br\/>Broader Impacts:<br\/><br\/>This project introduces computer vision methodologies\/hardware and cyber-enabled behavioral science research to radiation therapy with the objective of having safer and more effective radiation treatment. It may cause fundamental changes in the way that humans and medical devices interact. Educational programs will enable training to the instrument\/methodologies. Some of these methodologies will be included in the curriculum. A website will be developed for dissemination of findings. Users from around the world will be given access. Outreach programs involving demonstrations to pertinent groups will be created.","title":"MRI: Development of a Vision-based Real-Time Body Motion Tracking Instrument for Advanced Radiation Treatment","awardID":"0821474","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[375961,"544438","421080","557449"],"PO":["557609"]},"143307":{"abstract":"The study of algorithms has yielded numerous techniques that are both<br\/>interesting mathematically and broadly useful. This ranges from<br\/>linear programming to cryptography to geometry to approaches for<br\/>approximately solving optimization problems. The addition of a<br\/>statistical view of the world to the study of algorithms has also been<br\/>a compelling area of study. This project will study better algorithms<br\/>for linear programming as well as, explore and expand on recent work<br\/>in approximation algorithms, and will explore algorithms with a<br\/>statistical view of the world. Recent breakthrough work on<br\/>solving linear systems as well as developments in online optimization,<br\/>make the development of better algorithms for linear programming a<br\/>tantalizing possibility. This project will pursue this goal. A second<br\/>specific area is the study of approximation algorithms which has, in<br\/>recent years, been a primary focus of algorithms research. In<br\/>particular, problems such as finding sparse cuts, disjoint paths, and<br\/>the traveling salesman problem have been fertile ground for developing<br\/>new algorithmic techniques. The researcher has done some of this<br\/>work, and proposes to study this area further. Finally, this project<br\/>will go beyond the ``we know how to solve it'' case of linear<br\/>programming, and the ``we will do as well as we can'' notion of<br\/>approximation algorithms, to a ``how do we do on data generated by the<br\/>world'' view of algorithms. This latter view is especially<br\/>appropriate these days with the growing production of biological data;<br\/>This data is generated accordingly to scientifically validated<br\/>statistical models. Quite interesting work has been done in this<br\/>area, but the field is begging for more involvement from those trained<br\/>in algorithms.<br\/><br\/>The intellectual merit of this project lies in the further development<br\/>of mathematical techniques for algorithms, and the blending of<br\/>traditional ideas with ideas in statistics. The broader impacts lie<br\/>in the many applications of algorithms for the problems addressed in<br\/>this project. Application areas include network design, molecular biology, recommendation systems, and optimization. The researcher will introduce ideas from this project into graduate and undergraduate classes in algorithms as well.","title":"Explorations in Algorithms","awardID":"0830797","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["486616"],"PO":["565251"]},"143428":{"abstract":"A variety of emerging microelectronics applications target portable systems with tight constraints on the related metrics of power, form factor, and longevity. For many of these applications, there are severe constraints on the energy consumption for the electronics in the system. In particular, passive RFID tags rely on power received from readers so low power consumption is necessary to enable long-range reads. Nevertheless, these devices are already widely used for applications with serious security and privacy requirements such as key cards, public transportation tokens, and implantable medical devices. Current design approaches and standard cryptographic primitives fail to satisfy the needs for these systems. As a result, current implementations either resort to \"security-by-obscurity\" and ad hoc solutions that fail to provide adequate security and are frequently broken in practice.<br\/><br\/>This project is developing a comprehensive approach to analyzing, designing, and implementing security and privacy for severely resource-constrained devices such as widely deployed RFID systems.<br\/>The goal is to enable designers to create secure, cost-effective, large-scale RFID-deployments by combining primitives and protocols from a library with known properties and to implement those designs in a principled and efficient manner. The research follows an interdisciplinary approach by bringing together experts in low-energy integrated circuit design, design automation and embedded systems, system and network security, and cryptography. In addition to the research, the project includes a modular RFID security lab course and teaching outreach courses using cryptography and RFID systems to excite middle school students about pursuing science and engineering.","title":"CT-M: Implementable Privacy and Security for Resource-Constrained Devices","awardID":"0831426","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["486395","483825","527785","485695"],"PO":["565327"]},"144407":{"abstract":"Shape analysis is a fundamental problem in computer vision. Object recognition requires comparison of two shapes and determination of similarity. The challenge is that an object?s shape may vary due to articulations, non-rigid deformations, or natural variations between different instances of objects from the same class. It is important to understand how to represent shape, how to match shape allowing for deformation, and how to learn the types of variations that can occur for a particular class of objec5ts. The PI has a number of new approaches to shape matching and has been applying them to the construction of devices for plant species identification. The system can be used to monitor distribution or discover new species. Shape analysis has a wide range of additional applications, from military ones to medical ones. The PI will use the funds to develop new collaborations in statistical models of shape.","title":"Statistical Shape Models to Aid in Plant Species Identification","awardID":"0836823","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["541955"],"PO":["565136"]},"143439":{"abstract":"The objective of this research is to build new measurement-based methods for secure secret key establishment between two wireless devices, without ever communicating the secret key, using diverse physical characteristics of the wireless medium, notably an innovative measurement called temporal link signatures, and using device mobility.<br\/><br\/>The intellectual merits of this research include (i) collection of extensive measurements and characterization of spatial and temporal behavior of wireless link signatures in a variety of indoor and outdoor settings, (ii) determination of the shared secret space in different settings through the use of analytical methods and the measurement data, (iii) novel methods for enhancing the length of the shared secrets to make them robust against brute force attacks, (iv) novel methods for dealing with asymmetric measurements of link signatures, and (v) a complete, implementable methodology, with specific configuration recommendations, for secure secret key establishment in real scenarios.<br\/><br\/>This research will have a significant broad impacts. It generates a large amount of measurement data useful for various future wireless communications and security research; this data is and will continue to be available to the research community through the NSF-funded CRAWDAD repository. The results of this research potentially make mobile applications safer to use in ways that require far less complex management and far less computation than the existing cryptographic methods; these results benefit both security and goals of using mobility in green applications. Finally in broader impacts, this research will be used in conjunction with the PI's existing NSF STEP project for creating learning modules for high school students to demonstrate the problems and limitations of public key cryptosystems and the need for new ideas for secret key establishment.","title":"CT-ISG: Opportunistic Secret Key Exchange Using Wireless Link Characteristics and Device Mobility","awardID":"0831490","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["542075","464564"],"PO":["565264"]},"145408":{"abstract":"The four phase project builds on the successful outcomes of previous Gordon Conferences on Visualization in Science and Education. It brings together roughly 130 leading international researchers and science educators for: two workshops on assessment and design; a Gordon Conference on Visualization in Science and Education; five collaborative Visionary Mini-Grants for exploratory research across interdisciplinary communities; and a post-conference interim workshop for evaluation and planning. The conference has been re-organized in the current proposal to bridge science practice with science education. The sessions are organized around conceptual themes rather than scientific disciplines in order to explore the connections between visualization and understanding. Many of the invited speakers and all of the proposed activities surrounding the conference will be heavily focused on the learning and cognitive aspects of revealing nature and of generating insight.","title":"Revealing Nature, Generating Insight: Gordon Conference, Workshops & Visionary Grants to Guide Research in Science and Education","awardID":"0840726","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7625","name":"REESE"}}],"PIcoPI":["482235"],"PO":["533965"]},"136509":{"abstract":"Time integrators are crucial computational tools for studying nonlinear dynamical systems. Numerous time stepping methods have been developed over the years, many of which are now available in off-the-shelf solvers. However energy drifts and numerical dissipation problems present even in highly accurate algorithms still routinely plague engineering applications. Geometric time integrators have been recently proven greatly useful to elucidate and fix these issues in solid mechanics. Yet these contributions have not carried over to the Eulerian setting, where they could impact both the understanding and the reliability of time integrators for computational fluid dynamics. The goal of this research project is thus to develop novel, geometrically-based Eulerian time integrators for the class of problems whose dynamics is described by an action principle, possibly including dissipation and forcing---which encompasses the canonical Euler and Navier-Stokes equations, as well as many other models. Eulerian discretizations of the Hamilton-Pontryagin principle will be explored, and combined with mathematical and numerical tools such as Discrete Exterior Calculus, the semigroup of positive doubly-stochastic matrices, and implicit functions. Resulting integrators are expected, just like in the Lagrangian setting, to respect the structure of the physics, i.e., to introduce no artificial numerical loss of crucial physical quantities such as energy or circulation.<br\/><br\/>The proposed research activities aim at developing an infrastructure for predictive and high-order accurate simulations of fluid-mechanical systems that combine the power of modern applied geometry with modern computational mechanics. In particular, it promises the introduction of novel variational fluid simulation algorithms: this innovative computational approach relies on a multidisciplinary effort drawing upon techniques from geometric mechanics, discrete geometry, numerical analysis, and graphics, thus promising a broad theoretical and practical impact. The development of such variational integrators from a unified geometric standpoint represents a stepping stone for our long-term goal of solving complex physical phenomena such as a flowing dress, a swimming fish or splashing water, the simulation of which requires considerable improvement of the current state of the art to become commonplace. The research experience acquired during this project is to be disseminated to a wide range of audiences through publishing in mathematics, engineering and computer science journals, books, and conferences, as well as on our web sites, in summer schools, workshops, and other educational activities. Outreach efforts at our three institutions include the recruitment of students from underrepresented groups to help with this research project, leveraging existing efforts for enhancing the participation of women and minorities in scientific research.","title":"Collaborative Research: Geometric Time Integrators for Mechanical Dynamical Systems","awardID":"0757106","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7478","name":"DYNAMICAL SYSTEMS"}}],"PIcoPI":["448655","448656"],"PO":["564988"]},"138819":{"abstract":"This project employs an empirical socio-technical research approach to understand how to gain maximum benefit from decentralized virtual activity systems (DVASs). We see growing, widespread interest in the development and use of decentralized systems and virtual world environments as possible new places for engaging in collaborative work activities. There is widespread interest in stimulating new technological innovations that enable people to come together through social networking, file or media sharing, and massively multi-player online game play. This new generation of networked computing environments seems headed towards increased socialization, interaction, communication, and collaboration that span multiple organizational boundaries as its primary purpose. But how do we get there from here? Is it sufficient to just let the market of entrepreneurial vendors and technological innovators simply decide who needs what? The history of computing reveals a legacy of many failed or problematic efforts to develop and deploy computing systems that arise from a lack of understanding or recognition of the ways in which people's work and social activities are situated in organizational and technological contexts. These contexts configure, constrain, or enable some types of activities to flourish while others are displaced, either unintentionally or intentionally.<br\/><br\/>The core of the research focuses on both the individual and collective study of six critical variables: representations and realities, conflicting policies and practices, relationship work, processes and coordination, privacy and awareness, and security and trust. DVAS research requires a broad interdisciplinary understanding of the problems and a broad and interdisciplinary approach to their solution. This project is a large multi-site, multi-partner research endeavor with an interdisciplinary team, that provides the greatest practical opportunity for generalizable results from comparative analyses of both in-situ field studies and technology prototyping efforts. Five research partners serve multiple roles in this project. First, they serve as a source of real-world problems for the project to tackle. Second, as organizations facing the daily problems of distributed development, the partners view these as practical problems, and they will engage directly with the research team in developing strategies and solutions. Third, they serve as test-beds for early evaluation of proposed new solutions. In other words, working with the research partners ensures a continuous engagement with real world settings at all stages of the project.<br\/><br\/>Research on DVASs will have a vital impact on society. As development and use of DVASs becomes more common practice and as organizations continue to become more decentralized, new methods and policies will need to be identified and tested to enable people to collaborate successfully. This study has economic value as it will help organizations to carry out decentralized work effectively with smoother coordination, so that they can better compete in the global market. DVASs will be resilient to environmental disruptions as collaboration will be able to be conducted from anywhere, anytime, using representations of people, artifacts, and activities. Understanding the unique challenges of work in decentralized and virtual activities, as well as designing DVAS technologies to meet the challenges, will have both theoretical and practical impact. Theoretically, the project will determine the contours of decentralized activity, allowing comparison to other social forms such as traditional hierarchies, communities of practice, and rational bureaucracies. Finally, the results will also benefit higher education as new people entering the workforce will need to gain skills in developing DVASs, and in working in decentralized virtual worlds and environments.","title":"HCC-Large: Decentralized Virtual Activities and Technologies: A Socio-Technical Approach","awardID":"0808783","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["535599","483548","533779","518420","518206"],"PO":["564456"]},"137609":{"abstract":"Spurred by financial scandals and privacy concerns, governments worldwide have moved to ensure confidence in digital records by regulating their retention and deletion. The goal of this project is to develop and explore a database management system (DBMS) architecture that supports a spectrum of approaches to regulatory compliance, thereby extending the level of protection afforded by conventional file-based compliance storage servers to the vast amounts of structured data residing in databases. The key challenge of this work is to provide compliance assurances for the DBMS, even against insiders with super-user powers, while balancing the need for trustworthiness against the conflicting requirements for scalable performance guarantees and low cost. The resulting architecture provides tunable tradeoffs between security and performance, through a spectrum of techniques ranging from tamper detection to tamper prevention for data, indexes, logs, and metadata; tunable vulnerability windows; tunable granularities of protection; careful use of magnetic disk as a cache and of secure coprocessors on the DBMS platform and compliance storage server platform; and judicious retargeting of an on-disk encryption unit. <br\/><br\/>This work enables compliance laws to be applied to business, government, and personal data now stored in databases, increasing societal confidence in such data. A new web course on compliance data management will raise the computer science community's awareness of compliance issues and will help train a new generation of professionals cognizant of these challenges and solutions. The software prototypes and technical papers describing them will be disseminated through the project's web site http:\/\/web.crypto.cs.sunysb.edu\/cdb\/","title":"III-COR Medium: Collaborative Research: Achieving Compliant Databases","awardID":"0803197","effectiveDate":"2008-09-01","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["553485"],"PO":["543481"]},"160500":{"abstract":"Proposal #: CNS 07-08498 07-09946 07-08420 <br\/>PI(s): Preisig, James C Ye,Wei Stojanovic, Milica<br\/> Lee, Freitag Heidenmann, John S.<br\/>Institution: Woods Hole Oceanographic Inst U Southern California Mass Inst Tech<br\/> Woods Hole, MA 02543-1041 Los Angeles, CA 90089-1147 Cambridge, MA 02139-4307<br\/>Proposal #: CNS 07-09005 07-08938 07-08467<br\/>PI(s): Cui, Jun-Hong (June) Levine, Brian; Kurose,James F. Freitag, Lee<br\/> Rajasekaran,Sanguthevar;Shi, Zhijie;Willett,Peter K.;Zhou,Shengli <br\/>Institution: University of Connecticut U of Massachusetts WHOI<br\/> Storrs, CT 06269-1133 Amherst, MA 01003-9242 Woods Hole, MA 02543-1041 <br\/>Title: Collab Rsch:CRD\/IAD:Open Research Testbed for Underwater Ad Hoc and Sensor Networks (ORTUN)<br\/><br\/>This collaborative project, developing the first open testbed infrastructure for the underwater networking community, enables open access with the capability to conduct experiments remotely. The infrastructure, based on open research platforms, consists of a testbed that enables wide and systematic experimental evaluation and comparison of underwater acoustic networks. The work, involving this rapidly deployable testbed that can be shared by the underwater networking community, aims to demonstrate the ability of the facility to facilitate field experiments. The project represents a higher-level collaborative that arose from two collaborative groups. One group developing the facility, the other working mainly on the experiments utilizing the facility. The testbed is expected to be a buoy-based system that can be easily taken to different environments. When operational, these systems will be deployed 5 or 6 times a year. The infrastructure will consist of two types of nodes with different capabilities. The first type of node of the rapidly deployable testbed will offer a fixed physical layer capability using acoustic modems such as the WHOI micromodem or the ISI S-modem to implement a physical layer with limited reconfigurability interfaced to a reconfigurable network processor. This network processor will support algorithm\/protocol implementation and testing at higher network layers. The Network functions on the Fixed Physical Layer testbed will be hosted by a Gumstix processor which will then communicate with physical layer modems such as the WHOI Micromodem or USC\/ISI S-modem via a serial port. Ten to fifteen fixed physical layer nodes will be built including up to 3 gateway nodes. Each gateway node of the testbed will be equipped with wireless RF communication enabling real-time monitoring and control of network performance. The fixed physical layer nodes will be smaller and more easily deployed than the second type of node which is the all-layer node. The all-layer node is a more capable node that will ultimately support algorithm\/protocol implementation and acoustic data collection at all networking layers. In addition to the equipment included in the fixed physical layer nodes (i.e., a gumstix network processor and the ability to support relatively fixed physical layer modems such as the WHOI Micromodem and the ISI S-modem), the all-layer nodes will also include a general purpose data acquisition system (D\/A and A\/D) with substantial disk storage and in-situ processing capability. The MIT r-modem software will be implemented on this general purpose hardware and, along with MATLAB, will enable user implementation and testing of algorithms and the gathering of acoustics data at the physical layer in addition to the testing at higher network layers that it will share in common with the fixed physical layer nodes. Three to five all-layer nodes will be built. The rapidly deployable testbed, using two types of nodes with varying capabilities, should significantly enhance research at all network layers while setting the stage for future infrastructure improvements.<br\/><br\/>Many research groups investigating fundamental questions about how to design such networked systems that utilize acoustic communications in complex underwater environments have had their overall effort significantly slowed by the lack of common means to test and compare protocols under realistic environmental conditions. This infrastructure responds to the need for consensus on analytic or simulation models for underwater networks where researchers need the ability to gather experimental data under real world conditions in order to make progress. <br\/><br\/>The network stack will be modular by design with sockets used to enable cross layer control and communication. The physical, MAC, Network and Application layers will be populated with sample components to enable users test their own algorithms or protocols without having to populate the entire stack. Users will be able to write modules to test their own algorithms or protocols at different layers and selectively replace the sample modules with their own. While the development of the modular architecture and sample modules for the network stack will","title":"Collaborative Research: CRI: CRD: Open Research Testbed for Underwater Ad Hoc and Sensor Networks","awardID":"0946610","effectiveDate":"2008-09-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"024F","name":"GENI CONCEPT\/DEVELOPMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["515773"],"PO":["557609"]},"140931":{"abstract":"NSF Proposals 0820088\/0819865<br\/><br\/>Title: Collaborative Research: Passivity-based Architecture for Software Design of Dynamic Networked Systems<br\/><br\/>PIs: Xenofon Koutsoukos (0820088), Panos Antsaklis (0819865)<br\/>Co-PI: Janos Sztipanovits, <br\/><br\/>Real-life Cyber Physical Systems (CPSs), such as autonomous vehicles and building automation systems, are monitored and controlled by networked control systems. In CPSs, the overall system dynamics emerges from the interaction among physical dynamics, computational dynamics, and communication networks. This project aims at addressing the fundamental problems in constructing CPSs caused by network uncertainties, such as time varying delay, jitter, data rate limitations, packet loss and others, by exploiting the inherent safety of passive systems. The project will develop (1) the theoretical foundations for passivity-based design of networked control systems that provide an effective way to interconnect multiple passive systems together and preserve stability and performance in the presence of time varying delays and data dropouts and (2) model-based design processes for developing and analyzing software utilizing the compositionality and the orthogonality across design views stemming from the underlying passivity principles. The research plan includes a software tool-chain for passivity-based design, an implementation of the passivity-based architecture on a distributed hardware platform, and experimental studies for demonstrating the approach. The project serves as an excellent example for the rich societal context and extensive interdisciplinary interactions that future computer scientists and engineers will face as CPS technology is becoming increasingly pervasive.","title":"Collaborative Research: Passivity-based Architecture for Software Design of Dynamic Networked Systems","awardID":"0819865","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":["460500"],"PO":["564388"]},"143802":{"abstract":"High-end distributed and distributed shared memory platforms with millions of processors will be deployed in the near future to solve the toughest technical problems. Their individual nodes will be heterogeneous multithreading, multicore systems, capable of executing many threads of control, but with relatively little memory per thread, low bandwidth to main memory and deep memory hierarchies.<br\/>A programming model that supports productive, portable, efficient parallel programming both within and across the nodes of these petascale systems is essential if their potential is to be realized. Since it is easier for application developers and tool vendors to extend existing software rather than adopt a new programming language, a programming model based upon a familiar paradigm is highly desirable.<br\/><br\/><br\/>OpenMP is a widely supported shared memory programming model that provides ease of maintenance. It is suitable for programming multicore nodes, but does not address the needs of distributed memory platforms. This research will significantly extend OpenMP so that it can be used to program all levels of a high-end petascale system. In order to accomplish this, the investigators will enhance its existing mechanisms for describing multiple levels of parallelism, provide additional features for specifying synchronization and for achieving high levels of locality, as well as develop a novel I\/O interface. Moreover, they will substantially improve the state of the art in OpenMP implementation technology, enabling high performance between nodes as well as within them. Results will be demonstrated via a state-of-the-art Fortran\/C\/C++ OpenMP compiler, a highly optimized communications library, and a range of large scale applications.","title":"Collaborative Research: Extreme OpenMP: A Programming Model for Productive High End Computing","awardID":"0833201","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["506602","558507"],"PO":["565272"]},"143957":{"abstract":"In the context of concurrency, programming is complicated by a number of issues that need to be addressed, such as mutual exclusion, liveness, and fairness. Petri nets (PNs) are formal models developed in Computer Science for the modeling of concurrent systems. In Control Systems, PNs have been used in the context of supervisory control (SC) of discrete event systems (DES) and powerful theoretical results have been developed. However, these results have not yet been systematically applied to Computer Science problems for which PNs were created. This research would apply SC tools to the automatic synthesis of programming code based on high-level supervisory control program specifications. SC is of interest because various desirable features of programs can be seen in terms of supervisory control specifications. The goal of the research is to reduce programming effort by having more of the higher level requirements implemented automatically by the SC tools. While SC methods have been used to obtain control software, neither the application to the synthesis of concurrent programs nor the application of PN based SC methods have been done so far. PNs are natural models of concurrency that allow the use of a number of efficient supervisory control methods, without excluding other approaches. Another new feature of the work is the extraction of a SC specification from a specification language. This topic is of interest in order to obtain compact and easy to develop specifications, hiding the size of the underlying DES models and the technical details involved in the formulation of SC problems. Finally, the research is unique in the way it handles the SC, in an attempt to take advantage of multiple methods available in the literature. <br\/><br\/>The programming code produced by the SC tools is correct by construction and the programmer has only to manage simpler high-level specifications. The research pursued represents a fresh and novel approach to writing concurrent programs and it involves software development and research on supervisory control methods, specification languages, and code generation strategies. On one hand, the project provides a step forward towards a higher level of automation in the development of concurrent programs. On the other hand, this project will provide a platform for testing, comparing, and developing DES methods in general.<br\/>Parts of the project will involve students at undergraduate level, will also benefit undergraduate students via undergraduate research and design projects. A major part of the proposed work is being carried out at a primarily undergraduate institution.","title":"CSR-EHCS, SM: A Supervisory Control Approach to Concurrent Programming","awardID":"0834057","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["460500",383754],"PO":["561889"]},"142989":{"abstract":"Computing Education is essential not only for Computer Science and its many sibling disciplines(Computer Engineering, Software Engineering, Information Systems, etc.) but for practically all other academic disciplines. Computers are pervasive today and many professionals develop basic programming skills as a way to express ideas, problems and solutions in computational terms within their own disciplines. It is common to find curricula in the arts (music, graphical design), business (accounting, economics), sciences (biology, chemistry, physics), and social sciences with computational courses in their curriculum. In a way, computing is becoming a requirement of most professional degrees. This project addresses both the separation between computing specialists and to widespread integration of computing concepts, not just the technology but computational thinking, in other disciplines. The project will use technologies now commonly available to permit faculty to collaborate in offering courses that extend the potential reach of experts to a broader audience, as well as a collection of recorded expert lectures. In addition, it will develop a visual, interactive interface to a common framework around which to explore similarities and differences across domains and to enable decisions about educational plan development. The teams will also host workshops to identify innovative approaches to teaching, as well as support initiation of new collaborative course experience and reflect of the utility of the courses. The opportunity computing education is to learn how motivated hands-on learning can engage students and provide opportunities to introduce computing concepts. In addition to aiming for diversity in the groups of participating faculty, the project will extend the reach of computing to disciplines not normally associated with that content and will also represent the scope of their discipline to computing students, providing a broader view of the impact of the discipline as it is applied in creative fields. This project addresses the growing conviction that inter-disciplinary approaches are crucial to revitalizing computing education and offers a solution to the need for broader reach of individual areas of expertise. If successful, this project could spread to national implementations with very great effect.","title":"Collaborative Research: CPATH CB: Distributed Expertise in Enhancing Computing Education with Connections to the Arts","awardID":"0829625","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["531150","563440","563440"],"PO":["565136"]},"143506":{"abstract":"Abstract for proposal #0831874 <br\/>Energy constraint has persisted as a fundamental problem in sensor networks, especially when the networks are required to operate for a long time. Although many solutions have been proposed to address the problem, their limitations are salient: resource-conservation schemes can slow down energy consumption but cannot compensate energy depletion; current environmental resource harvesting schemes have low, unstable efficiency due to uncontrollable environmental conditions and technological limitations; incremental deployment may cause environment pollution and may be too costly. This project leverages the emerging wireless energy charging technology to address the problem from a brand new perspective. Specifically, a hierarchical architecture is first proposed to include an energy provision station as the stable source of energy, a few autonomous mobile energy chargers, and a large number of sensor nodes. On top of the architecture, schemes are developed to solve a unique three-tier, long-term, multiple-time and on-line scheduling problem present in resource replenishment. The architecture and schemes are evaluated through prototyping and extensive experiments. The project will potentially transform the current research on energy management in sensor networks, improve the sensor network performance from both practical and theoretical aspects, and thus promote the wider application of sensor networks in structural health monitoring, smart factories, garden\/orchid monitoring, road\/traffic monitoring and so on. It will also train a diverse cadre of young scientists, students and professionals for network research and applications.","title":"Collaborative Research: NeTS-NECO: Energy Replenishment for Wireless Sensor Networks","awardID":"0831874","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[382141,"492059"],"PO":["565303"]},"141328":{"abstract":"Proposal #: CNS 08-21384<br\/>PI(s): Stamos, Ioannis; Flanagan, Mary D.<br\/>Institution: CUNY - Hunter College<br\/> New York, NY 10065-0000<br\/>Title: MRI\/Acq.: Acq. of Range Scanning and Rapid Prototyping Equipment for 3D Urban Modeling<br\/>Project Proposed:<br\/>This project, acquiring instrumentation (laser range scanner and high quality digital cameras) for laser range scanning and rapid prototyping, supports 3D modeling of structures in urban settings. With an efficient combination of all possible sources of information, the work focuses on the acquisition of high-level geometry information and intelligent integration of laser-based models to reconstruct detailed models of urban sites, i.e., digital cities. Laser scanner can produce highly-detailed geometry, whereas color digital cameras can produce photorealistic representations of urban scenes. The work allows<br\/>- Acquiring large amounts of data with the latest generation of laser range scanning and 2D image capturing technology,<br\/>- Generating CAD-quality models by the integration of laser-based with image-based 3D acquisition technology, <br\/>- Building models using the rapid prototyping system, and<br\/>- Attracting students into visual and applied research.<br\/><br\/>Broader Impacts: This project enables the completion of a comprehensive software system for automated photorealistic acquisition of urban environments that will benefit applications such as urban planning, archeology, Google Earth\/Microscope Visual Earth visualizations, disaster recovery, architecture, film industry, and construction industry. The large female and undergraduate minority population at this minority-serving institution is likely to benefit from this work.","title":"MRI: Acquisition of Range Scanning and Rapid Prototyping Equipment for 3D urban modeling","awardID":"0821384","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["496273","409474"],"PO":["557609"]},"143517":{"abstract":"Brewer 0832153<br\/>B: Project Summary<br\/>Collaborative Research: NECO:<br\/>Designing Intermittency-Aware Networked Systems<br\/><br\/>Both the Internet and phone system exhibit deep assumptions about continuous connectivity that are simply not true in much of the world, including rural areas and developing regions in particular. Intermittent network connectivity is a fact of life in most developing countries due to a variety of factors including high usage costs, frequent power outages, network failures and the use of delay tolerant networks. Traditional networked applications are not designed to work in intermittent environments. Although work on delay-tolerant networking (DTN) has created viable low-level networking protocols that support intermittency, it remains difficult to write applications that tolerate disconnections well.<br\/><br\/>This project proposes Intermittent Aware Network Architecture (IANA), a new network platform that can enable a spectrum of intermittent-aware applications. This work will develop several novel applications that show the power of intermittent networking in developing regions and that test and validate the IANA platform. The proposed applications include intermittent versions of web search, collaborative wiki software, automatic teller machine (ATM) support, and voice-messaging cellular phones. These are challenging applications that cover a range of scalability, data sharing, and security issues. Pilot versions of all the four applications will be deployed in India or Africa to understand how well the platform enables real applications in challenging environments.<br\/><br\/>An overarching goal in the design of IANA is to determine the underlying principles in building a generic network architecture that can be both adopted across different types of intermittent networks and be used as a common platform across several intermittent applications. An important related challenge is to determine an appropriate and generic Application Programming Interface (API) for intermittent applications. In pursuit of this larger goal, this work will 1) classify the kinds of intermittent networks, which facilitates appropriate responses to disconnection; 2) develop an intermittent-aware overlay network above DTN that enables application-specific in network optimizations, which in turn enables both better performance and better data distribution and sharing; and 3) develop a range of new APIs that move beyond low-level connection APIs (sockets) to support intermittent sessions, data collection and distribution, data replication and consistency, and mobility.<br\/><br\/>Intellectual Merit: This work will result in a substantial increase in the ability to create applications that work well in intermittent environments. The key contributions include an overlay network to help manage replication, mobility, aggregation and sharing, new higher-level APIs validated by real applications, and an evaluation of the effectiveness of the new platform via large-scale deployments in developing regions.<br\/><br\/>Broader Impact: The target applications have direct value in bridging the digital divide. The collaboration software empowers rural content creation and sharing and facilitates content distribution for education, health care, and emergency response. ATM software can help address the economic inefficiencies due to limited cash flow and limited access to banking services. The voice-messaging phone can extend the impact of the cellular revolution to rural areas and to low-literacy users. The PIs have a history of impactful applications in developing regions and expect similar impact from the proposed deployments and from the free, open-source release of the apps. Field work and new curriculum will also promote development-aware graduate students with strong international exposure. The applications and deployments facilitate inspiring internships, which will encourage diversity and multi-disciplinary research.","title":"Collaborative Research: NECO: Designing Intermittency-Aware Networked Systems","awardID":"0831934","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["390720"],"PO":["565090"]},"143429":{"abstract":"Proposal Number: 0831427<br\/>PI: Paul Barford<br\/>Institution: University of Wisconsin - Madison<br\/>Title: CT-M: Meta-Environments for Experiments on Diverse Topics in Network<br\/>Security<br\/><br\/>Abstract<br\/>In order to address the continued escalation in the diversity, sophistication and quantity of<br\/>malicious activity in the Internet, new methods for systematic testing and evaluation of<br\/>next generation security systems and techniques are required. The objective of this<br\/>research is to investigate and develop meta-environments for Emulab-based testbeds. A<br\/>meta-environment is a set of testbed configurations, tools and processes developed for a<br\/>specific security domain that enables experiments to be conducted in a simplified,<br\/>realistic and consistent fashion. The first component of this research program is focused<br\/>on investigating tools such as traffic generators that can be used across all metaenvironments.<br\/>Next, five specific meta-environments will be investigated and developed<br\/>for the following areas: 1) network intrusion detection, 2) firewalls, 3) honeynets, 4)<br\/>denial of service defense, and 5) security perimeter design. Finally, mechanisms for<br\/>federation between the Wisconsin Advanced Internet Lab, DETER and other Emulabbased<br\/>testbeds that will enable large-scale experiments and transparent access to<br\/>resources will be investigated. The broader impacts of this project are that it will<br\/>simplify and accelerate the use of testbeds for security research, and enable consistent<br\/>comparisons between new security systems and tools. The expected results of this work<br\/>include tools, configurations, and documentation for the five meta-environments and<br\/>testbed federation capability. The project also includes education and outreach activities<br\/>that will develop network security lab exercise materials. These web-based materials will<br\/>be openly available to the community, and will emphasize a hands-on approach intended<br\/>to provide students with practical, relevant experiences.","title":"CT-M: Meta-Environments for Experiments on Diverse Topics in Network Security","awardID":"0831427","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["402290"],"PO":["497499"]},"144419":{"abstract":"The 2008 International Symposium on Information Theory (ISIT)was held at the Sheraton Center in Toronto, Canada, from July 6th to the 11th. <br\/>NSF support for ISIT covered the travel expenses of junior participants from the U.S., such as graduate students, research associates, and new assistant professors. These participants are individuals whose papers had been selected for presentation at the symposium, but who lacked funds and would not be able to attend without travel support. <br\/>The IEEE International Symposium on Information Theory has always been the primary meeting place of the leading researchers in the field. It has a strong tradition of participation by junior researchers that has helped it maintain its reputation as a leading forum for new ideas. This is where researchers get an increased awareness of the relationships between theory and practice and the impact of different kinds of work. It is also where the more application-oriented workers gain realization of the importance of the conceptual structure of the field. <br\/>Typical attendance at these symposia averages about 700 registrants from more than 20 countries. On average each submitted paper is scrutinized by 2 to 3 reviewers, which results in the acceptance of roughly 500 papers out of 1000 papers that are submitted on average each year.","title":"2008 Institute of Electrical and Electronics Engineers International Symposium on Information Theory","awardID":"0836867","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["550597"],"PO":["432103"]},"143209":{"abstract":"Title: Collaborative Research: Leveraging Low-dimensional Structure for Time Series Analysis and Prediction<br\/><br\/>PI: Christopher J. Rozell, Georgia Institute of Technology<br\/>co-PI: Michael B. Wakin, University of Michigan, Ann Arbor<br\/><br\/>Predicting the behavior of complex systems is central to many tasks of great scientific and national importance, including arenas such as meteorology, financial markets and global conflict. Modern science is ingrained with the premise that repeated observations of a dynamic phenomenon can help in understanding its driving mechanisms and predicting its future behavior. The investigators study methods for improving our ability to characterize and predict such systems even when they are very large (i.e., with many interacting factors) or appear highly unordered (i.e., chaotic systems). This research leverages new mathematical results that enable analysts to efficiently capture the simple structure that is often present even in systems that appear very complex. These results lead to improvements and performance guarantees for heuristic prediction methods based on artificial neural networks, which are often used in practice but can sometimes fail inexplicably.<br\/><br\/>Time series prediction is often approached by postulating a structured model for a hidden system driving data generation. This project borrows from recent advances in low-dimensional signal modeling to advance the state of the art in time series analysis and prediction tools when similar low-dimensional structure is present. For linear systems, this research develops efficient estimation strategies that improve upon classical techniques by encouraging sparse solutions. For nonlinear models, this project builds upon Takens' Embedding Theorem, which states that the image of an attractor manifold can be reconstructed using a sequence of time series observations, to guarantee a quantifiably stable embedding of the attractor manifold. Furthermore, this research aims to improve upon and make performance guarantees for reservoir computing methods, where randomly-connected neural networks have been identified as effective mechanisms for predicting chaotic time series.","title":"Collaborative research: Leveraging low-dimensional structure for time series analysis and prediction","awardID":"0830456","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["562955"],"PO":["564898"]},"138809":{"abstract":"IIS - 0808718 <br\/>III-CXT-Core Large: Computer Vision Research: Promoting Paradigm Shifts in Archaeology<br\/>Cooper, David B.<br\/>Brown University<br\/><br\/>This project continues a longer term intellectual program begun under the ITR solicitation and has proven to be innovative in multiple ways. The work is an artful interdisciplinary blend of computer vision, physics, mathematics, algorithm development, efficient computation, graphics and visualization in a rapidly emerging area which might best be termed computational archeology, although the methods are generalized to other domains. The project involves an interdisciplinary team of archeologists and computer vision\/graphics\/visualization researchers developing methods and software for capturing and analyzing archeological data. In this project The Brown University Division of Engineering, Laboratory for Man\/Machine Systems (LEMS), will collaborate with the Brown University Joukowsky Institute for Archaeology and the Ancient World, the nonprofit educational outreach Institute for the Visualization of History, Williamstown, MA, archaeologists at Tel Aviv University, Israel and several computer vision experts from European institutions.<br\/><br\/><br\/>The primary testbed project will be a crusader castle in Israel. There are four sub-projects proposed: a collection system and database for video and 3D data captured on-site during excavation, three-dimensional reconstruction, of both small artifacts and architectural sites, assembly of pottery and glass fragments, and visualization of sites and artifacts. The project builds on and extends earlier work which focused on the Petra archaeological site and assembly of artifact fragments. The advances in computing technologies over the past 5 years in processing power and storage capacity combined with decreases in cost allow the researchers to expand their ambitions and develop more powerful tools and analytic techniques.","title":"III-CXT-Core Large: Computer Vision Research: Promoting Paradigm Shifts in Archaeology","awardID":"0808718","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["520864","550631","408988",369083],"PO":["563751"]},"141900":{"abstract":"The objective of this research is to develop the critical analytical framework for Multiple Input Multiple Output (MIMO) communication in complex channels. Existing MIMO capacity calculation software will be enhanced by including complex channel models for vehicles, lossy biological channels, and highly noisy channels. We will also integrate detector models with the electromagnetic and communication MIMO models, and verify the models via measurement. <br\/><br\/> <br\/>Intellectual Merit:<br\/>Today's MIMO models do not include an accurate representation of non-Gaussian, ultra-reflective, depolarizing, and highly lossy channels seen in many personal communication channels, body-worn or implanted medical communication channels, highly reflective and lossy ('Hyper-Raleigh') channels typical of intra-vehicular communication for sensor networks inside aircraft, cars, buses, trains, ships, etc., most wireless ad-hoc network environments, or the human body scattering channel for medical imaging. This research program will provide more advanced channel models for MIMO. This will enable specialized MIMO design for each application, providing a far greater probability of initial success for the deployed systems. <br\/><br\/> <br\/>Broader Impacts:<br\/>This research addresses unmet communication demand for future wireless devices. In addition to the significant societal, medical, and financial aspects of that potential advancement, the research will be directly integrated into coursework at the University of Utah.","title":"Enabling MIMO Communication for Complex Channels","awardID":"0823927","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"I435","name":"Defense Intelligence Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"J265","name":"Defense Intelligence Agency"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["546109","490693"],"PO":["565185"]},"143803":{"abstract":"The complexity of modern high-end computers has made it exceedingly <br\/>difficult for scientific applications to effectively manage resources <br\/>such as extreme-scale parallelism, single-chip multi-processors, <br\/>and deep hierarchy of shared\/distributed caches and memories. In <br\/>particular, as machines and applications have both evolved to become <br\/>complex and massively parallel, compilers have failed to automatically <br\/>bridge the gap between complex software and diverse hardware platforms.<br\/>Optimization models for parallel computing have lagged far behind <br\/>those for serial applications, and conventional compilers are <br\/>increasingly unable to accommodate emerging high-end architectures.<br\/><br\/>This research develops a new optimization model that allows<br\/>1) developers to effectively interact with advanced optimizing <br\/>compilers to provide both domain-specific knowledge and high-level <br\/>optimization strategies (e.g., directions to enable new or choose <br\/>amongst differing parallelization strategies); 2) computational <br\/>specialists to easily define arbitrary domain-specific transformations <br\/>to directly control performance optimizations to their code; <br\/>3) architecture-sensitive optimizations to be easily parameterized <br\/>and empirically tuned to achieve portable high performance.<br\/>The optimization model is supported with an integrated environment <br\/>that contains two main components: ROSE, a C\/C++\/Fortran2003 <br\/>source-to-source optimizing compiler developed at DOE\/LLNL; and POET, <br\/>a transformation language together with an empirical optimization <br\/>engine developed at UTSA. This framework permits different levels <br\/>of automation and programmer intervention, from fully-automated tuning <br\/>to semi-automated development to fully programmable control. The research <br\/>targets both the optimization needs of computational kernels and the more <br\/>general requirements of whole program optimizations. The framework is <br\/>integrated as an external development mechanism for the widely-adopted <br\/>ATLAS library and is connected with empirical tuning research under <br\/>DOE SciDAC program to improve the efficiency of large-scale scientific <br\/>applications.","title":"Programmable Code Optimization and Empirical Tuning For High-end Computing","awardID":"0833203","effectiveDate":"2008-09-01","expirationDate":"2012-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["563382","538267",383153],"PO":["565272"]},"141625":{"abstract":"This award to the Joint Quantum Institute (JQI) provides support to enable high-risk, speculative experimental and theoretical activity in the control and use of quantum coherence and entanglement - the basis of a second quantum revolution. The combination of Information Theory with Quantum Mechanics has created the new field of Quantum Information Science (QIS), which has the potential to revolutionize how information is processed and stored. Quantum coherence and quantum entanglement are universal to all quantum physical systems, in much the same sense that the processing of information is independent of the information's physical embodiment. Future success in propagating quantum coherence, preparing complex quantum entangled states, and managing decoherence will likely hinge upon the interconversion of quantum coherence among various physical platforms, for example: solid-state photon sources, individual ions, ultracold atomic gases in optical lattices, and superconducting devices.<br\/><br\/>The Physics Frontiers Center (PFC) at JQI will facilitate the integration of Atomic, Molecular and Optical (AMO) and Condensed Matter (CM) systems for the study of QIS through three major activities: (1) Correlated and Topological Matter with Cold Atoms will create and investigate topological and other novel forms of quantum-correlated matter in cold atomic and molecular systems. Using probes of single-particle properties and measures of quantum entanglement, this activity will provide insight into the emergence and dynamics of exotic phases in real or artificial condensed-matter systems. (2) Supercircuits at the AMO\/CM Interface will bring together AMO and CM techniques and perspectives to treat \"supercircuits\": superconducting electrical circuits and mechanical circuits of atomic gas superfluids. A combination of AMO\/CM couplings and AMO\/CM analogies will provide new insights into supercircuits and superfluidity as well as develop new tools for quantum information. (3) Quantum Optics with Semiconductors and Atoms will investigate methods to transfer coherence and create entanglement between matter and light and to produce complex many-body entanglement in semiconductors and atomic systems.<br\/><br\/>The emergence of QIS has propelled a convergence of CM and AMO physics. Graduate students and post-doctoral associates supported through the PFC will be trained at that interface. A the high school level, the PFC will institute a program to enrich the teaching of physics and mathematics in Prince George's County (PG) high schools, including offering paid summer research internships to as many as six PG County Public School teachers. The teachers will work alongside scientists in PFC projects and will meet weekly with JQI fellows to discuss their summer experiences, current science topics, and issues they confront in their classrooms. The PFC will have an informative, educational, and topical web site, an active seminar series, a program of JQI travelling lecturers, and an annual open house. The PFC will support, maintain, and expand the Physics Lecture Demonstrations at UMD, and develop new channels to disseminate findings to both the scientific and non-technical communities.","title":"Joint Quantum Institute: Processing Quantum Coherence","awardID":"0822671","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1646","name":"PHYSICS FRONTIER CENTER"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1710","name":"CONDENSED MATTER PHYSICS"}}],"PIcoPI":["544655",376952,"401463",376954,376955],"PO":["564422"]},"142846":{"abstract":"CNS ? 0828923<br\/>Birman, Kenneth <br\/>Cornell University<br\/>FIND: Distributed Application Management Service (DAMS)<br\/><br\/>Since its inception, the Internet has been a difficult environment for supporting applications offering security, reliability, or other trust properties. Yet we?re seeing a wave of network applications that need to maintain private data (for example, sensitive medical records) or play roles in which correct, fault-tolerant behavior is critical (for example, air traffic control). Each developer has been forced to invent his or her own workarounds to compensate for the limitations of the existing Internet. The central premise of this research is that this represents a dangerous trend and that the research community must do what it can to create new and better options.<br\/><br\/>Intellectual Merit: Cornell University researchers will develop technologies aimed at supporting a new kind of trustworthy distributed computing service on the Internet. The project will have a foundational intellectual impact by showing that what have previously been treated as a dozen separate, often ill-defined functions can be unified into a single conceptually elegant abstraction (distributed role delegation), and by developing the needed science and mathematics to reason about this mechanism. This project aims to help create the next generation of mission-critical networked applications ? systems that can be entrusted with sensitive roles, because they embody solutions shown to have the needed properties<br\/><br\/>Broader Impact: This project seeks to create a useful solution that developers worldwide today lack by creating a powerful, extensible technology that will achieve theoretically optimal performance and scalability, consistent with the constraints imposed by application-specific consistency requirements.","title":"FIND: Distributed Application Management Service (DAMS)","awardID":"0828923","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["495285","466565"],"PO":["565090"]},"142979":{"abstract":"Behavior-based robotics was established around the idea that robots could be constructed by connecting elementary sensor and actuator modules, without the need to form any internal representation of the world in which they operate. When designed appropriately, such robots, both as individuals and as groups, exhibit complex and seemingly ?intelligent? behaviors, solving challenging tasks in real time, while reacting only to their local environment and obeying sets of local rules. In part, this approach to robotics was inspired by considering natural systems, such as self-organization and adaptability of social insects in their colonies. An analogous approach to initiate the development of the field of behavior-based molecular robotics will be performed over the next three years, leading to groups of molecules that would appear to an observer to show a variety of task-oriented behaviors or some form of purposeful and dynamic self-organization. <br\/><br\/>Individual behavior-based molecular robots are single molecules displaying multiple sensors-actuators. When exposed to artificial landscapes displaying substrates keyed to their sensors-actuators, the molecules start executing elementary steps, determined, in a stochastic sense, by their constantly changing local environments. On some landscapes, called prescriptive, individual molecular robots and their collectives will execute algorithms mimicking wound-up automata with sequence control mechanisms. In contrast, on non-deterministic landscapes, the robots and their collectives will demonstrate properties emerging from internal organization of individual sensors and actuators and through local interactions between molecules and their environments. Importantly, these new interpretations of molecular behaviors will allow radically different experiments from all previous approaches to molecular robotics, while keeping experimental designs realistic, leading to embodiment in the physical world.","title":"Collaborative Research: EMT\/MISC: Behavior-Based Molecular Robotics","awardID":"0829579","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["527861"],"PO":["565223"]},"151328":{"abstract":"An effective document representation is a crucial text processing component and without it, even the most sophisticated methods and models perform poorly. Current document representations such as the bag of words or Markov n-gram models ignore nearly all sequential information and focus instead on the histogram of words or short phrases. The proposed work develops sequential representations for documents that go beyond bag of words and Markov models and effectively capture a wide range of sequential information. The main idea behind these representations is to use smoothing techniques to transform the word sequence into smooth curves representing sequential content through changes in the local word histogram. By varying the amount of smoothing, the proposed representations interpolate between different sequential resolutions, thus conveniently capturing sequential details at varying levels of granularity. The proposed work provides improved document analysis, including the classification, segmentation, and summarization of documents. Furthermore, it enables visualizing the sequential trends in documents thus leading to the emergence of computer-assisted document browsing technology. In addition to computer experiments validating improved modeling accuracy, the project involves a series of user studies thus demonstrating the wide applicability of the project.<br\/><br\/>Broader impacts include the development of visualization tools that will assist users in reading and browsing documents thus potentially helping millions of people to quickly and effectively absorb textual information. Other education components include assisting foreign language learning, strengthening the computational aspects of the statistics program at Purdue and mentoring minority students.<br\/><br\/><br\/><br\/>http:\/\/www.stat.purdue.edu\/~lebanon\/research\/projects\/multiResDocuments\/","title":"CAREER: Multiresolution Representations of Documents","awardID":"0906550","effectiveDate":"2008-09-09","expirationDate":"2014-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["403885"],"PO":["565136"]},"143738":{"abstract":"The most widely used parallel languages require users to worry about mundane details. High performance programs written in these languages unnecessarily require both algorithmic and architecture expertise interposed in the same program text. This lack of separation overly complicates programs. Furthermore, the programmers have to explicitly, or implicitly, choose a data and computation distribution. This reduces the compatibility, malleability, portability, and maintainability of the optimized programs. This research addresses how programmers can express parallelism in computation by directly allowing experts to create reusable software constructs or \"bricks.\" <br\/><br\/>The research explores the PetaBricks compositional language, in which automatic parallelism extraction and locality recognition will become tractable. The major task is the building of the compiler to map the algorithmic parallelism and locality to near optimal utilization of the resources. <br\/><br\/>The language will be composed of base cases and compositions. They can be composed recursively to solve large problems. The ordering and granularity of these compositions will be managed by the compiler and runtime framework to allow programs to adapt. The research introduces Patlo: a pattern transformation language for optimization where domain experts can program patterns and corresponding transformations for algorithm-specific and architecture-specific optimizations.","title":"PetaBricks: A Language and Compiler for Scalability and Robustness","awardID":"0832997","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["547366",382922],"PO":["565272"]},"142649":{"abstract":"Last Modified Date: 08\/01\/08 Last Modified By: Daniel F. DeMenthon <br\/><br\/>Abstract <br\/>Normal vision is not static: time is a key dimension of the natural world we see. The eventual understanding of biological vision requires understanding the neural mechanisms used to recognize objects and actions over time. Thus the focus of the proposed research is to study how the primate visual system recognizes objects and actions in time sequences of images. A meta-goal of this project is to exploit the synergies between computational approaches and physiological experiments to lead to a better understanding of brain function and at the same time to develop better computer vision algorithms. Object recognition in time sequences of images presents a significant challenge for recognition systems, because it requires both selectivity to shape and invariance to changes of appearance in time.. <br\/>This project will extend an existing computational model of the ventral stream by adding temporal dynamics in its model neurons and the ability to process video sequences. It will also expand a working model of the dorsal stream to understand the relative roles that it and the ventral stream play in dynamic visual recognition. At the same time, recordings from single units, and multiple single units, from high level visual areas including IT and regions of the STS will be made in order to characterize the tuning of single neurons to the shape dynamics of specific image sequences. By combining modeling and physiology, this work will search for a computational explanation for how the higher areas of the visual cortex recognize objects and actions over time and how they can learn. <br\/>This integrative effort, which is focused on processing of dynamic perceptual information, can have a significant and direct impact on current theories of autism, dyslexia, and effects of stroke, in addition to directly guiding modeling and engineering efforts in computer vision. The proposed research is tightly coupled to education and teaching, and resources used in the research, including databases of videos, visual stimuli, the modeling software and the experimental data will be made available to the broad scientific community. Information on the project and its progress will be available at http:\/\/cbcl.mit.edu\/projects\/NSF-CRCNS\/index.html","title":"Collaborative Proposal: Object and Action Recognition in Time Sequences of Images: Computational Neuroscience and Neurophysiology","awardID":"0827427","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[379746],"PO":["564316"]},"146906":{"abstract":"This project identifies implicit assumptions in contemporary IT design rooted in industrialized consumer culture through an ethnographic investigation of the arrival of broadband IT in Change Islands, Newfoundland, a remote, subsistence-oriented, working-class community with close ties to the environment. This exploratory work claims that the current drivers are inappropriately oriented towards a non-sustainable future, and the fresh perspective that a group of users whose particular needs and patterns of life are not well-known to IT designers can identify a more sustainable future for IT design. This project starts with the recognition that we must substantially rethink our relationship to the environment. The project develops an understanding of the marginal ties of this particular community to an industrialized consumer culture to highlight the aspects of everyday IT design which are predicated on industrialized orientations such as mass production and consumption of consumer goods and a disconnect with the natural environment. In response to this understanding, a design case book is developed to reflect a sustainable approach to human-computer interaction (HCI). Design directions that arise from taking Change Islanders? lifestyle and perspectives seriously contribute to the fundamental rethinking of IT practice which will be an essential requirement of sustainable HCI.","title":"SGER: Rethinking Drivers for IT: Lessons from a Newfoundland Fishing Village","awardID":"0847293","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["517929"],"PO":["565227"]},"143518":{"abstract":"This collaborative project investigates a number of fundamental <br\/>problems that are very critical to wireless mesh network (WMN) <br\/>throughput optimization. Wireless mesh networking is believed to <br\/>be the most effective and efficient technology for the last-mile <br\/>data connection to the Internet. The proposed research targets <br\/>to solve the following challenges: 1) Throughput optimization in <br\/>MR-MC WMNs: the complexity of throughput-optimal scheduling, <br\/>real-time logical topology characterization and inference, the <br\/>joint exploitation of both rate diversity and channel diversity, <br\/>and game theory based throughput optimization. 2) Non information <br\/>theory based approaches to throughput optimization in multi-hop <br\/>MIMO-enabled WMNs: stochastically modeling different aspects of <br\/>the mesh network, constructing Network Utility Maximization <br\/>formulations, and applying Information Geometric Programming to <br\/>solve global optimization problems. 3) Biologically-inspired WMN <br\/>design for throughput optimizatio n. 4) A testbed to accomplish <br\/>experimental tasks to validate the effectiveness of the design. <br\/><br\/>This project is in nature multidisciplinary. It requires the <br\/>joint effort from computer science, electrical engineering, <br\/>mathematics, statistics, and biology, and therefore has the <br\/>potential to inspire revolutionary methodologies that could be <br\/>applied to many domains. In addition, the novelty of the proposed <br\/>research has strong impact on wireless research. Furthermore, the <br\/>success of this project is able to enhance the four universities? <br\/>ability to educate, through the multi-disciplinary research effort, <br\/>future scientists and engineers.","title":"Collaborative Research: NEDG: Throughput Optimization in Wireless Mesh Networks","awardID":"0831939","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["438415"],"PO":["564993"]},"143408":{"abstract":"Information and Communications Technology (ICT) infrastructure failures and cyber attacks are realities that can have catastrophic societal effects. Information Assurance (IA) can be defined as the operations undertaken to protect and defend ICT systems by ensuring their dependability and security. There is a critical need for systematic IA methods that enable ICT systems to adapt and survive any type of disruption or attack. A major hurdle in the development of IA techniques is the lack of models and metrics which enable one to determine the effectiveness of IA mechanisms. This exploratory project seeds a collaborative effort between three PIs at different institutions: Duke University, University of Missouri Kansas-City, and the University of Pittsburgh focused on the development of metrics and models that will allow one to quantitatively study the technical aspects of information assurance (IA) for the network component of the ICT infrastructure. The basis of the approach is to unify attack trees, attack graphs, privilege graphs and fault trees into a common scalable framework with a well defined set of metrics and application scenarios. Extensions of the basic model that include state information, stochastic properties and rewards via Markov chains and stochastic Petri nets, enabling a wider variety of attack and fault scenarios are being studied. The impact of the models and metrics developed is that they provide the techniques and tools necessary to determine the effectiveness of IA mechanisms and allow one to detect bottlenecks and to evaluate the tradeoffs between levels of information assurance, performance and cost.","title":"Collaborative Research: CT-ER MiMANSaS: Metrics, Models and Analysis of Network Security and Survivability","awardID":"0831325","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["451391"],"PO":["543481"]},"143529":{"abstract":"Very little research exists in understanding the risks to users of powerful web features, as seen from a user-centric point of view. This project seeks to fill this void by creating a framework for this understanding by studying three classes of known (and urgent) threat: misuses of secure socket layer (SSL), web bugs (a technique for spying on the user of web page), and open redirects (a technique for transferring the user to a different web page perhaps without detection). These are costly and urgent current threats; results of preliminary research were presented at the USENIX 2008 Workshop on Offensive Technologies and an article in the Washington Post on July 16, 2008 reported on this paper. <br\/><br\/>The project envisions the three specific problems as representing three major classes of the users' web risks and thus leading to a user-centric security framework. The broad research activities include: <br\/>1. Measurement: through a combination of active Web crawling and passive observation, the research is quantifying the nature and extent of the three threats, and in the process developing techniques for accurately identifying their occurrences. <br\/>2. User-centric correlation studies: When multiple web sites subscribe to the same web bug hosting company, that (malicious) company can track user behavior across multiple sites. This is one example of a correlation threat. The project also studies the correlated threats to user security and privacy. <br\/><br\/>The broader impacts of the project include:<br\/>1. User security and privacy on the Web: The project advances a user-centric view of security and privacy on the Web. This is a current and urgent social need. The research group has already developed several user-side tools that help prevent or at least reveal problem, and it will continue to develop and publish these to the public domain. 2. Open access to data: crawling the Web is very resource intensive and few sources make this data publicly available. The group makes its data available to the research community.","title":"NeTS-ANET: SSL Misuse, Web Bugs, and Open Redirects: Prevalence, Implications, and Defense","awardID":"0831988","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["491568"],"PO":["565090"]},"145608":{"abstract":"This is a 12-month standard award to support a doctoral symposium program at the ACM Symposium on User Interface Software and Technology (UIST), to be held in Monterey, CA (October 18-22, 2008). This will bring together approximately 8 dissertation-stage doctoral students from diverse areas that include traditional graphical user interfaces, virtual and augmented reality, multimedia, new input and output devices, CSCW, and ubiquitous computing, among others. It includes on day of talks and interaction with 3 distinguished senior faculty mentors, followed by events integrated throughout the conference.<br\/><br\/>Intellectual Merit<br\/>The doctoral symposium is intended to help expand the participation of young researchers currently pursuing a Ph.D. studies in the area, giving their innovative work wider exposure in the community, helping to foster a sense of community among these young researchers, and providing an opportunity to obtain feedback and guidance from senior members of the research community in an interactive and supportive environment. This will be accomplished in a number of ways. Visibility will be increased by publication of student position papers in the UIST Conference Companion, and by presentation of posters at a special session the first night of the conference. Community building will include beginning the symposium with an informal social event to establish a friendly and interactive atmosphere before formal presentations are made, and the inclusion of substantial time for discussion among the participants. Finally, guidance and feedback will be provided by senior members of the community with substantial experience working with Ph.D. students in the area, again structured with substantial time planned for interactive discussion.<br\/><br\/>Broader Impact<br\/>The UIST doctoral symposium brings together highly talented students and several senior researchers, facilitating the development of a social network that will play a major role in the career success of new researchers. Since both faculty and students are diverse on several dimensions (research topics, methodological approaches, national and cultural background), the students' horizons are broadened at a critical stage in their professional development.","title":"Workshop: User Interface Software and Technology 2008 Doctoral Symposium","awardID":"0841481","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["506686"],"PO":["564456"]},"151724":{"abstract":"Wireless networks bring mobility to users by allowing them to change location without disrupting connectivity. On one hand, this opens space for a wide variety of location-aware services. On the other hand, it also poses a concern to users as to what extent their location can, or should, be identified in different scenarios.<br\/><br\/>The main focus of the research is to address the proper identification of mobile device locations in wireless networks. There are two sides of the problem: the accurate determination of a mobile device's location in applications such as E911 services, and the effective protection of location information against adversaries which intend to intrude mobile device users' privacy. This project takes a systematic view of both sides of the problem, explores a solution space that spans both ends of wireless communication, and delivers solutions for various application scenarios. The sticking point is to determine the minimum location information necessary for the intended network service by understanding, analyzing, structuring, and controlling the complex interactions among different entities in the system including end-devices, base stations, and (potential) adversaries across application, network, and physical layers. Both control plane and data plane are considered.<br\/><br\/>The intellectual merit of this project is to form a scientific foundation for proper location identification.","title":"Collaborative Research: WN: Proper Location Identification in Wireless Networks","awardID":"0907964","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["539562"],"PO":["564993"]},"140603":{"abstract":"Cross-hybridization is singly the most important source of noise in gene expression measurements. Many current approaches to assess gene signal fit data to statistical models; in contrast, the investigators will develop a thermodynamics-based algorithm to compute the hybridization free energy between probe and specific (i.e. intended) target as well as non-specific (i.e. non-intended) target molecules. This new approach takes into consideration the fact that thermodynamics of hybridization for two molecules in aequeous solution is different that than where one molecule (probe) is tethered to a glass slide. A new probe-specific position-dependent hybridization partition function will be computed by a dynamic programming algorithm PPH using free energy parameters from labs of Turner, Santalucia, and Sugimoto. The partition function accounts for (Boltzmann-weighted) sum of all possible partial as well as complete hybridizations betwenn probe and target, including secondary structure of both probe and target. Applying PPH to all probes and all (specific and non-specific) targets is not computationally feasible, so the algorithm PPHx will be developed to compute the hybridization free energy between probe and a Markov model representing all non-specific targets. Ensemble free energies are immediately obtained from partition function values Z, and can be used to derive concentrations of messenger RNA from microarray fluorescence intensity values. <br\/><br\/>High-density oligonucleotide arrays (gene-expression arrays, tiling arrays, single-nucleotide polymorphism arrays, microRNA arrays, etc.) constitute a powerful tool in molecular biology for the discovery of genes and their function, with far-reaching applications in population biology, systems biology, pathobiology and other fields. In this method, fluorescently tagged cRNA or cDNA derived from messenger RNA is washed over a glass slide to which hundreds of thousands up to millions of short cDNA probes are attached. Hybridization between fluorescently tagged molecules and probes occurs, non-hybridized molecules are removed, and fluorescence intensities are measured by an optical scanning device. Despite technical improvements in various commercial platforms, it is still not possible to infer messenger RNA concentrations from microarray fluorescence intensity values, due to noise from cross-hybridization (also called non-specific binding). A computer algorithm to compute the cross-hybridization free energy will be developed and implemented, directly allowing one to estimate cross-hybridization effects. <br\/><br\/>Findings from this research will be made publicly accessible through a web server and distribution of source code, by journal publications, and presentations in meetings. Due to the prevalent use of microarray technology, this research will have a very broad impact on molecular biology (genetics, genomics, systems biology, population biology) as well as to disease pathology and drug dosage and design. Educational impact at the undergraduate and graduate level will be ensured by courses and training opportunities offered at Boston College, which provide special opportunities for females and minorities, with additional outreach provided in the weekly MIT Bioinformatics Seminar.","title":"Physically modeling cross-hybridization error in gene expression microarrays by a novel Boltzmann partition function algorithm for probe-specific position-dependent free energy","awardID":"0817971","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7334","name":"MATHEMATICAL BIOLOGY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["538594"],"PO":["230845"]},"141978":{"abstract":"The objective of this research is to formulate design principles for and demonstrate, via prototyping, the feasibility of a new sensor network of everyday objects based on radio frequency identification (RFID) system components. The vision exploits the advantages of RFID while addressing two key challenges, namely the lack of sensor integration onto RFID tags and the need for careful redesign of the transceivers (i.e, down and up links as well as the multiple access protocol). Central to the success of the proposed research are two key advances already in place at University of Washington-Intel Research Labs. First, a new class of passive RFID tags, called wireless identification and sensing platforms (WISPs), integrated with appropriate sensors and designed for enhanced power harvesting are available. Second, a software-defined reader (SDR) that allows innovation of link and medium access control (MAC) protocols is available.<br\/><br\/>With respect to intellectual merit, the research pursues an integrated systems solution to RFID network design at various interacting levels of abstraction. As an example, the research considers coupled circuit and electromagnetic simulation that characterizes the back-scattered uplink signal at the reader, which in turn can suggest link and MAC layer components, such as modulation and coding as well as collision avoidance dedicated to interference mitigation.<br\/><br\/>With respect to broader impacts, the research has the potential to accelerate academic research into RFID networks via freeware distribution of the SDR code and limited availability of WISPs as well as dissemination via topical tutorials. Successful execution of the project can move RFID technology from its intended application of reading tag IDs within a supply chain environment toward the more ambitious goal of realizing a ubiquitous \"internet of things.\"","title":"Realizing the Internet of Things via RFID Sensor Nets","awardID":"0824265","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1504","name":"GRANT OPP FOR ACAD LIA W\/INDUS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["543393","529904",377870],"PO":["564728"]},"142968":{"abstract":"Living cells constantly monitor their surroundings for physical and chemical changes and respond by controlling genetic and biochemical processes inside the cell. These processes involve sophisticated sensing and information processing that are very different from the conventional silicon-based counterparts. The investigators will develop novel biologically-inspired systems capable of sophisticated molecular sensing and elementary computations in cell-free conditions. The cell-fee gene switches and circuits could be used as a part of larger systems or devices that use biological elements to achieve complex functions, such as biosensors and diagnostic devices. Additionally, the project will enable deeper understanding of how RNAs control gene expression in living cells, which will further enhance our ability to engineer synthetic gene circuits with complex functions.<br\/><br\/>The research will focus on the development of RNA-based molecular sensors and gene circuits that function in cell-free conditions. The investigators will develop new techniques to isolate RNA switches and circuits from a large pool of mutants in the laboratory. Complex circuits will be built using the simpler parts generated by the techniques. The gene switches and logic gates that will be developed during the project period will be reusable as parts in other biomolecular devices and systems, thus they will be deposited in a public database. The proposed educational component of the project will provide an exceptional opportunity for undergraduate students to engage in real problem solving activities in small teams, both in the classroom and in the laboratory at University of California, Davis.","title":"EMT\/BSSE: RNA switches and logic gates for biomolecular computation in vitro","awardID":"0829536","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["524275"],"PO":["565223"]},"144817":{"abstract":"Technology has made possible virtual research organizations involving researchers from different institutions. Virtual research organizations are expected to improve the speed of innovations and the creativity of scientists by bringing expertise to projects that would otherwise be unavailable. This project contributes to the scientific study of this new way of organizing scientific work and examines the problem of collaborating in virtual research organizations at the level of the collaboration and collaborators. The purpose of the research is to better understand how dispersed, interdisciplinary projects succeed, and to explore how virtual organizations might best use new tools and infrastructure.<br\/><br\/>This research builds on the investigators? prior work in understanding the Information Technology Research (ITR) initiative, which included close to 500 interdisciplinary technology research projects supported by the NSF. The proposed work entails an archival analysis of ITR final reports to discern the longer-term impact of the ITR projects and a longitudinal follow-up survey of ITR investigators. This research will contribute to organization science and social network theory, and to a better understanding of science and technology. The results will be useful for social scientists interested in organizations, computer scientists and technologists interested in tools and infrastructure that work, and decision and policy makers who invest in research.<br\/><br\/>Virtual research organizations in science and engineering are essential to the nation?s future and to solving global problems. This research will aid policy understanding of requirements of distributed interdisciplinary collaboration, improve metrics and approaches for evaluating the direct and indirect outcomes of research programs, suggest policies for future programs in innovative scientific research and education, and provide information to organizations, particularly departments and universities, about the institutional policies and practices that best support distributed interdisciplinary research.","title":"VOSS: Collaborative Research: Towards Collaboration Strength in Virtual Research Organizations","awardID":"0838385","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["519226"],"PO":["519302"]},"144608":{"abstract":"NSF and other Federal agencies will require further understanding of concepts, methods, and means for Exascale computing systems before formulation of future programs in enabling technologies, architectures, and methodologies. Such understanding will drive the unknowns to be determined, the challenges to be addressed, strategies to guide, and tasks to be performed on the way to Exascale system realization. The objective of realizing systems capable of sustained Exaflops computing by 2018 suggests that sponsored R&D programs dedicated to this goal be started no later than 2011 to provide sufficient time to complete the ensemble of necessary research, design, and implementation tasks. <br\/>The NSF Point Design Study for Exascale Computing is a collection of three tasks that will look at three different regimes of computing by experts in those areas. While each is stand alone, they will benefit strongly through cooperation and sharing of results, each being guided by the insights derived from the others. These are 1) applications and programming models, 2) system architecture and operating systems, and 3) hardware technology and component design. <br\/><br\/>This SGER will study applications and programming models aspect of the Point Design Study for Exascale Computing.","title":"Programming Models and Application Requirements for an Exascale Computing Point Design Study","awardID":"0837719","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["557487"],"PO":["565272"]},"143519":{"abstract":"Network Virtualization Using Dynamic FPGA Reconfiguration<br\/><br\/>As the Internet grows and evolves, increasingly diverse network applications will be deployed to accommodate business and social needs. These increasingly diverse network applications undoubtedly will exacerbate the demand for a spectrum of network services. Often, network applications call for strikingly divergent performance requirements in terms of security, predictability, and throughput. Although physically separate networks could be constructed to meet these varied performance constraints, in many cases, a common physical substrate is needed to minimize equipment investment, operating cost, and power consumption. Several virtual network implementations which share network nodes and links have recently been introduced. To allow for the rapid allocation of system resources and a flexible programming environment, these systems are deployed in general-purpose processors and the physical resources required by each virtual network are dynamically allocated by an operating system. Although this approach has been shown to be feasible, the serial nature of general-purpose processors limits virtual network performance. In this project, a new hardware-based approach to virtual network construction that provides scalability and programmability will be developed. The planned computing platform uses a reconfigurable field-programmable gate array (FPGA) to implement one or more individual unique routers that have been customized to specific virtual network needs. As the number of virtual networks and their characteristics change, the hardware in the FPGA can be reconfigured to support the updated requirements. The management of the deployed virtual routers is performed by a resource manager which is executed on an accompanying microprocessor. To evaluate the approach, a series of software tools and hardware modules will be created.<br\/><br\/>Intellectual merit: This project represents an aggressive effort to develop an integrated environment for the creation and deployment of reconfigurable virtual networks. This coordinated effort takes advantage of advances in FPGA systems, FPGA resource management, and network router development to allow for the creation of scalable, high performance virtual networks. A virtual network architectural layer will be developed to address dynamically changing network requirements. Architectural resources will be managed by a new resource allocation algorithm that will allow for the effective use of the available FPGA area under virtual network performance constraints. This algorithm will run periodically to allow for dynamic rebalancing of FPGA resources. All hardware and software components of the system will be designed to allow for effective system use by users that are unfamiliar with FPGA design. The real-time performance of the system will be evaluated in the PIs? networking laboratory at the University of Massachusetts, Amherst under realistic workloads.<br\/><br\/>Broader impact: The potential for broader impact from this work is substantial due to the ubiquity of networking and the scalability of the solution. The researchers plan to develop two specific programs to broaden the impact of the work including: 1) a new undergraduate curriculum in virtual networking, focused on scalable real-world systems, and 2) application of the new technologies in the NSF-sponsored Engineering Research Center for Collaborative and Adaptive Sensing of the Atmosphere (CASA) which already has an elaborate infrastructure in place to impact meteorologists, emergency managers, and under-represented groups at the University of Puerto Rico.","title":"XPLR: Network Virtualization Using Dynamic FPGA Reconfiguration","awardID":"0831940","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["564706","549952"],"PO":["564993"]},"135907":{"abstract":"This project will furnish several 'building blocks' for data interoperability within linguistics and all disciplines which use language data. The work will be based on the General Ontology for Linguistic Description (GOLD), a machine-readable information structure which allows allows computers to process and 'understand' linguistic concepts and the relations among them. Using GOLD, the project will develop an extensive network of ontology-aware lexical items drawn from sixteen different projects and over 3000 languages. Thus, computers will be able to understand the relationship between linguistic categories across languages, and interpret what their linguistic function is when they appear in texts. In addition, the project will develop a set of low-barrier data requirements which lexicon creators can implement in order to join this ontology-based network. It will also create architecture to integrate network data into frameworks developed by major international standards initiatives. Finally, the project will establish DevSpace, an online facility designed to promote continuing information- and resource-sharing among linguists and developers interested in augmenting the network with additional tools and services.<br\/><br\/>Such a project is important because cross-linguistic language data is central to many research communities. Language history and language comparison can provide critical insights into the genetics, culture, migrations, and contacts of human populations. And natural language data is indispensable to major computational research initiatives, such as multilingual text processing. In providing linguistically interpreted lexical data from so many underdescribed languages, LEGO will ultimately aid in meaning extraction from texts even of languages far too small to justify a full-scale natural language processing system. Thus from both a computational perspective and a Humanities and Social Sciences perspective, the LEGO project will create a research resource of remarkable breadth and diversity, one which will serve multiple disciplines.","title":"INTEROP: Lexicon Enhancement via the GOLD Ontology (LEGO)","awardID":"0753321","effectiveDate":"2008-09-01","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"1311","name":"LINGUISTICS"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7701","name":"DATA INTEROPERABILITY NETWORKS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["529966","529965","564749"],"PO":["564500"]},"140978":{"abstract":"Proposal Number: 0820061\/0820034\/0820230<br\/><br\/>TITLE: Design and Run-time Techniques for Physically Coupled Software<br\/><br\/>PIs: Ramesh Govindan (USC), Rajesh Gupta (UCSD), Mani Srivastava (UCLA), and Paulo Tabuada (UCLA)<br\/><br\/>ABSTRACT:<br\/><br\/>Many real-world systems are deeply embedded in the physical world and their operational behavior is determined in large part by a tight coupling between the system components and the physical environment. This project seeks to establish the scientific principles governing software for such physically-coupled systems by focusing on four challenges in the context of distributed sensing and control applications: 1) Support for physical context in the form of programming structures that enable application software to explicitly capture the state of the physical world as an observable in an embedded computation; 2) Formal methods for composing software modules that indirectly interact with each other through the physical world, and a run-time safety supervisor that provably enforces correctness of composition; 3) Programming structures to enable design and verification of applications with resource provisioning that is driven by and adapts to physical-world dynamics; 4) System software support for sharing physically-coupled sensor and actuator resources in distributed settings. In addition, educational techniques targeting the teaching of topics in physically-coupled computational systems are being explored by creating shared educational content in the form of self-contained reusable modules.","title":"Collaborative Research: Design and Run-time Techniques for Physically Coupled Software","awardID":"0820061","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":["515835","526859"],"PO":["564388"]},"142969":{"abstract":"QUANTUM AND CLASSICAL COMPLEXITY OF CONTINUOUS PROBLEMS<br\/><br\/>ABSTRACT<br\/><br\/><br\/> The investigators are studying the following general question: If physicists and chemists succeed in building quantum computers, which continuous problems arising in science and engineering can be solved much faster on a quantum computer than on a classical computer? Examples of continuous problems are path integration, the Schr\u00f6dinger equation, high-dimensional approximation, continuous optimization, and<br\/>integral equations. To obtain the power of quantum computation for continuous problems one must know the computational complexity of these problems on a classical computer. This is exactly what the investigators have studied for decades in the field of information-based complexity.<br\/><br\/> The classical complexity of many continuous problems is known due to information theoretic arguments. This may be contrasted with discrete problems such as integer factorization where one has to settle for conjectures about the complexity hierarchy. Among the issues the investigators will study are the following:<br\/><br\/>1. For the foreseeable future the number of qubits will be a crucial computational <br\/>resource. The investigators have shown that modifying the standard definition of quantum algorithms to permit randomized queries leads to an exponential improvement in the qubit complexity of path integration. The investigators propose to exploit the power of the randomized query setting. For example, are there exponential improvements in the query complexity for other important problems?<br\/><br\/>2. A basic problem in physics and chemistry is to compute the ground state <br\/>energy of a system. The ground state energy is given by the smallest eigenvalue of the time-independent Schr\u00f6dinger equation. If the number of particles in the system is p, the number of variables is d = 3p. In the worst case classical setting, the problem we study suffers the curse of dimensionality. The curse is broken in the quantum setting. The<br\/>investigators want to determine if the randomized classical setting suffers the curse of dimensionality. If it does, a quantum computer enjoys exponential speedup for this problem. This would mark the first example of proven exponential quantum speedup for an important problem.<br\/><br\/> 3. The Schr\u00f6dinger equation is fundamental to quantum physics and quantum chemistry. Solving this equation for quantum systems with a large number of variables would have a huge payoff for many applications. The investigators propose to study algorithms and initiate the study of the computational complexity of the Schr\u00f6dinger equation in the worst case and randomized settings on a classical computer and in the quantum setting","title":"Quantum and Classical Complexity of Continuous Problems","awardID":"0829537","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["516897","516898"],"PO":["565251"]},"143509":{"abstract":"This collaborative project investigates a number of fundamental <br\/>problems that are very critical to wireless mesh network (WMN) <br\/>throughput optimization. Wireless mesh networking is believed to <br\/>be the most effective and efficient technology for the last-mile <br\/>data connection to the Internet. The proposed research targets <br\/>to solve the following challenges: 1) Throughput optimization in <br\/>MR-MC WMNs: the complexity of throughput-optimal scheduling, <br\/>real-time logical topology characterization and inference, the <br\/>joint exploitation of both rate diversity and channel diversity, <br\/>and game theory based throughput optimization. 2) Non information <br\/>theory based approaches to throughput optimization in multi-hop <br\/>MIMO-enabled WMNs: stochastically modeling different aspects of <br\/>the mesh network, constructing Network Utility Maximization <br\/>formulations, and applying Information Geometric Programming to <br\/>solve global optimization problems. 3) Biologically-inspired WMN <br\/>design for throughput optimizatio n. 4) A testbed to accomplish <br\/>experimental tasks to validate the effectiveness of the design. <br\/><br\/>This project is in nature multidisciplinary. It requires the <br\/>joint effort from computer science, electrical engineering, <br\/>mathematics, statistics, and biology, and therefore has the <br\/>potential to inspire revolutionary methodologies that could be <br\/>applied to many domains. In addition, the novelty of the proposed <br\/>research has strong impact on wireless research. Furthermore, the <br\/>success of this project is able to enhance the four universities? <br\/>ability to educate, through the multi-disciplinary research effort, <br\/>future scientists and engineers.","title":"Collaborative Research: NEDG: Throughput Optimization in Wireless Mesh Networks","awardID":"0831902","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["531713"],"PO":["564993"]},"144929":{"abstract":"SGER: Exploiting Alternative Packagings of Source Meaning in Statistical Machine Translation<br\/><br\/>Current approaches in statistical machine translation (MT) miss a key<br\/>fact: the source language sentence is not the only way the author's meaning could have been expressed. The idea that the source sentence is just one of various ``packagings'' of underlying meaning was, of course, one familiar motivation for interlingual approaches to translation; however, interlingual semantic representations have generally been abandoned as notoriously difficult to define, and equally difficult to obtain accurately with broad coverage once defined. In this project, we are revisiting the idea of \"packagings\" of meaning, but exploring it in practical ways consistent with current practice in statistical MT. Unlike semantic transfer or interlingual<br\/>approaches, we encode alternatives as source paraphrase lattices, a representation that allows us to exploit generalizations about the source language while still maintaining the surface-to-surface orientation that characterizes the statistical state of the art. Our exploratory work focuses on capturing syntactic and semantic variation using Lexicalized Well Founded Grammars (LWFG), a recent formalism that balances expressiveness with practical and provable learnability results. We are quantifying and characterizing the information available in source paraphrase lattices, assessing the value of shallow paraphrasing, and exploring the relative promise of deeper techniques for source paraphase generation using LWFG and other constraint-based grammatical frameworks. The ability to capture<br\/>generalizations via source paraphrase may open new possibilities in the translation of minority and endangered languages, which lack training corpora on the scale necessary to support standard statistical MT techniques.","title":"SGER: Exploiting Alternative Packagings of Source Meaning in Statistical Machine Translation","awardID":"0838801","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["514912"],"PO":["565215"]},"150858":{"abstract":"Abstract<br\/><br\/>Title: Integrating the Local and Global Structure of Natural Scenes<br\/>PI: Michael Lewicki, CMU<br\/>Co-PI: Aude Oliva, MIT<br\/><br\/><br\/>One of the fundamental problems in modeling human vision is understanding the visual cues and computations that underlie the perception of natural visual scenes. Recent studies have suggested that there exist important aspects of scene perception which do not depend on the recognition of objects in the scene and are more global or holistic in nature. The objective of this proposal is to use integrated theoretical and experimental approaches to gain insight into the information processing that underlies the representation of natural scenes and the computation of their global and spatial layout properties. The research will be driven by the theoretical hypothesis that visual system representations at both a local and global level are adapted to the statistical structure of the natural images and scenes. This project will investigate local structure of natural images by developing hierarchical statistical models of local textures and testing to what extent human observers are sensitive to the same statistical features. The spatial structure of natural images will be investigated by developing statistical models that identify scene regions over which there are smooth changes in the local texture distribution and comparing the resulting segmentation to that of human observers. The global structure of natural scenes will be investigated by developing a statistical model that learns holistic, statistical representations, with the aim to evaluate scene depth and spatial layout information as human observers do.<br\/>The broader impact of this work is that it will develop theoretical models that can be directly tested at a perceptual level and are also sufficiently detailed that they could lead to testable models of the <br\/>underlying neural mechanisms. Furthermore, it will be essential to <br\/>understand the computational principles underlying human perception in order to emulate their behavior in machines and also to better understand our own visual experience.<br\/><br\/>URL: http:\/\/www.cnbc.cmu.edu\/nsf-natural-scenes\/","title":"Integrating the Local and Global Structure of Natural Scenes","awardID":"0905017","effectiveDate":"2008-09-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["483749"],"PO":["564318"]},"142905":{"abstract":"Biological interaction networks are graphs in which nodes represent molecules, such as genes or proteins, and edges represent interactions among the molecules. These networks underly and govern the mechanisms of cellular decision-making, and play major roles in understanding causes of different diseases, as well as designing effective, targeted therapeutics. Advances in biotechnologies are amassing large amounts of these types of data from multiple organisms. One powerful tool for analyzing such data and elucidating functional components in them is through comparative analysis by means of network alignment. Roughly speaking, aligning a pair of networks, typically from two different organisms, entail finding the ?evolutionary correspondence? between the nodes of the networks. This pairwise alignment is also generalizable to multiple<br\/>networks. The rationale behind aligning a set of networks is identifying subsets of these networks that are conserved across organisms, which are natural candidates for functional molecular components.<br\/><br\/>This research will develop models and algorithms for aligning interaction networks and identifying conserved elements via a probabilistic framework that uses hidden Markov models (HMMs). Despite the success of HMMs in a variety of biological applications, mainly in the sequence analysis area, they have not been considered in the realm of network alignment. The reason for this has been that while HMMs naturally apply to one-dimensional data, such as sequences or columns of a matrix, they do not apply to multidimensional data, such as graphs. To achieve this goal, the investigator will conduct research in two areas. First, the investigator will formulate an HMM-based framework for alignment multiple biological interaction networks. This part entails devising strategies for identifying a mapping among nodes of the networks and devising schemes for handling matches, mismatches, insertions, and deletions in these networks. Second, the investigator will design novel algorithms that enable the HMM to ?operate? in a multi-dimensional space.","title":"SGER: NET HMMs and Their Applications to Biological Network Alignment","awardID":"0829276","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["541859"],"PO":["565223"]},"142927":{"abstract":"The value of computer-generated protein structural models in biological research and practice relies critically on their accuracy. However, development of high-resolution computational approaches that can reliably produce protein structural models with or close to experimental quality remains an unsolved problem, though significant advances have been made in the past ten years. The main difficulties include the tremendously large and complex protein conformation space and, more importantly, the absence of scoring functions with satisfactory accuracy as well as sensitivity. <br\/><br\/>In this project, the research seeks to answer a challenging question ? can one still model protein structures with high accuracy using the existing scoring functions which are potentially insensitive and inaccurate? Different from the common approaches of globally optimizing a scoring function describing the conformational energy, the investigators explore a new direction to model protein structures via efficiently sampling the common low score regions in multiple carefully-selected knowledge-based, physics-based, or regression-based scoring functions. This new approach addresses the scoring function insensitivity problem based on the assumption that the native or native-like conformations should satisfy most of the existing good scoring functions by yielding low score values. Sampling multiple scoring functions allows toleration of insensitivity and deficiency in individual scoring functions and identification of conformations that can best satisfy most scoring functions, which will eventually lead to significant resolution improvement. The investigators verify this sampling strategy by applying it to a proof-of-concept ab initio protein loop structure prediction problem. The work involves integrating multiple scoring functions, including triplet torsion angle score, physical energy, distance-based potential, loop closure score, and others, into the sampling scheme with the goal of reliably predicting loop backbone structures with near experimental resolution. The computational tools for loop structure prediction are being made available as a software package to the protein modeling research community.","title":"SGER: A Novel Multi-Scoring Functions Sampling Approach to Impove Protein Modeling Resolution and It's Applications in Protein Loop Structure Prediction","awardID":"0829382","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["482837"],"PO":["565223"]},"141905":{"abstract":"The objective of this project is to investigate and develop a gene library-based resource allocation methodology for effective and efficient time-sensitive large-scale multi-rate system integration over communication networks. The Impaired Driver Electronic Assistance Testbed at the North Carolina Center for Automotive Research serves as an accident prevention demonstration project. The approach is to use the gene library to classify and detect abnormalities in vehicle movements in various traffic environments and to provide optimal real-time sampling rate adaptation and emergency intervention. An artificial immune system is used to optimize the gene library, which can then be used in real time and can adapt to its environment to realized optimal solutions.<br\/><br\/>With respect to intellectual merit, this research pursues a vertical integration of multiple layers of systems dynamics (from networks, to distributed agents, and to users) with the goal to provide efficient and reliable system operation by considering bandwidth constraints, hybrid structures, and different control topologies for adaptive real-time optimal resource allocation for distributed sensor, actuator, and controller agents over communication networks.<br\/><br\/>With respect to broader impacts, this project is a synergistic integration of theoretical analyses with industrial applications. Joint research, bidirectional education programs, and commercialization serve as a three-tiered structure for the project. Outreach and dissemination efforts include industrial internships for students, summer high school student experiences, inter-institutional visits, and public seminars. Recruitment of underrepresented groups is actively pursued. The research has the potential to reduce vehicle accident-related costs from more than $250 billion per annum in the United States while enabling older drivers to retain mobility and quality of life.","title":"Collaborative Research: GOALI: AIS gene library based real-time resource allocation on time-sensitive large-scale multi-rate systems","awardID":"0823952","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1504","name":"GRANT OPP FOR ACAD LIA W\/INDUS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["557729",377674],"PO":["564728"]},"140816":{"abstract":"Proposal Number 0819333<br\/><br\/>Title: Design and Evaluation Methodologies for Enhancing ERP System Usability<br\/><br\/>PI: Wendy Lucas<br\/>Co-PIs: Jennifer Xu, Heikki Topi, Tamara Babaian<br\/><br\/>The effective adoption and use of Enterprise Resource Planning (ERP) systems are profoundly affected by their poor usability characteristics. The goal of this research is to improve the usability of ERP systems in order to lessen user training time and increase user performance and satisfaction. Using collaboration theory as a unifying framework, the proposed engineering processes and methods for analyzing human-computer interaction and performing system design and evaluation include (1) design principles and field studies for enhancing system usability, (2) concrete implementations that embody and illustrate the application of these design principles to ERP system interfaces, and (3) evaluation methodologies for measuring collaborative properties. This research will also investigate the validity of three scientific principles for creating and analyzing software for real-world systems: (1) Greater collaborative strength of software yields greater usability, (2) Embedding knowledge about the users, tasks, processes, domains, usage logs, and its own interface components into an ERP system strengthens its collaborative capabilities, and (3) Usage logs are the true reflection of reality. Outcomes from the project will include new methods and approaches for evaluating and enhancing collaborativeness and usability that are applicable to the design of large-scale real-world software systems.","title":"Design and Evaluation Methodologies for Enhancing ERP System Usability","awardID":"0819333","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}}],"PIcoPI":[374236,374237,"468220",374239],"PO":["564388"]},"140927":{"abstract":"SRS-0820208\/0819845<br\/><br\/>TITLE: Scalable Knowledge-based Middleware For Networked And Mobile Systems<br\/><br\/>PI: Boon Thau Loo (University of Pennsylvania)<br\/>Co-PI: William C. Regli (Drexel University)<br\/>Senior Personnel: Joseph B. Kopena (Drexel University)<br\/><br\/>This research investigates scalable, knowledge-based middleware supporting content-based addressing and routing in mobile, networked systems. It incorporates and integrates two aspects: Ontological reasoning about system resources and declarative networking within routing components. At the application layer is OntoNet, a knowledge-based framework for representing and reasoning on system elements. Declarative, formal techniques provide service discovery and composition, content-based messaging, and distributed querying using OWL-Net, a subset of the OWL description logic. This work includes development of propagation strategies that are efficient and robust in mobile, networked environments. Network layer support is provided by declarative networks, a rule-based framework for compact, high-level protocol specifications. Declarative networking enables rapid prototyping and verification as well as online adaptation and meta-reasoning. This research will include extension of declarative networking to more readily support highly dynamic mobile wireless systems. The intellectual merit of this proposal is development of a unified, declarative framework for distributed organization of knowledge and information in real-world systems. It draws from many areas such as the Semantic Web, databases, and networking. Potential broader impacts are richer and more extensible platforms for real-world networks usable in emergency response, logistics, infrastructure monitoring, and ubiquitous computing.","title":"Collaborative Research: Scalable Knowledge-based Middleware for Networked and Mobile Systems","awardID":"0819845","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7724","name":"SOFTWARE FOR REAL-WORLD SYSTMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["560676"],"PO":["564388"]},"141907":{"abstract":"The objective of this project is to investigate and develop a gene library-based resource allocation methodology for effective and efficient time-sensitive large-scale multi-rate system integration over communication networks. The Impaired Driver Electronic Assistance Testbed at the North Carolina Center for Automotive Research serves as an accident prevention demonstration project. The approach is to use the gene library to classify and detect abnormalities in vehicle movements in various traffic environments and to provide optimal real-time sampling rate adaptation and emergency intervention. An artificial immune system is used to optimize the gene library, which can then be used in real time and can adapt to its environment to realized optimal solutions.<br\/><br\/>With respect to intellectual merit, this research pursues a vertical integration of multiple layers of systems dynamics (from networks, to distributed agents, and to users) with the goal to provide efficient and reliable system operation by considering bandwidth constraints, hybrid structures, and different control topologies for adaptive real-time optimal resource allocation for distributed sensor, actuator, and controller agents over communication networks.<br\/><br\/>With respect to broader impacts, this project is a synergistic integration of theoretical analyses with industrial applications. Joint research, bidirectional education programs, and commercialization serve as a three-tiered structure for the project. Outreach and dissemination efforts include industrial internships for students, summer high school student experiences, inter-institutional visits, and public seminars. Recruitment of underrepresented groups is actively pursued. The research has the potential to reduce vehicle accident-related costs from more than $250 billion per annum in the United States while enabling older drivers to retain mobility and quality of life.","title":"Collaborative Research: GOALI: AIS gene library based real-time resource allocation on time-sensitive large-scale multi-rate systems","awardID":"0823960","effectiveDate":"2008-09-01","expirationDate":"2012-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1504","name":"GRANT OPP FOR ACAD LIA W\/INDUS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":[377678,377679],"PO":["564728"]},"139480":{"abstract":"Technology trends show that future multiprocessor system-on-chips will integrate tens of processor cores and tens of on-chip resources. To exploit inherent parallelism in such systems, multiple jobs can be run concurrently on different processor cores, each using the abundant on-chip resources. As a result, considerable interactions among processor cores and resources may result in deadlocks and would be the most critical issue faced by such systems. Software approaches to detect and resolve deadlock in such systems take longer time and may even fail to be timely invoked. <br\/><br\/>This research investigates a novel technique to inherently equip these systems with a hardware mechanism that can detect deadlocks very fast so that an OS can react in minimal time. The central idea behind this methodology is to transform one form of complexity (e.g., graph) into another form of complexity (e.g., matrix) that can then be parallelized in hardware. This project develops various parallel hardware-oriented deadlock detection algorithms and their variations, which will be not only faster but will also dramatically reduce the run-time complexity from at most linear down to constant. Moreover, the project also provides proofs of correctness and run-time complexity of these algorithms. These proofs are essential for wide spread adoption in practical applications. The research is of significant value to the reliability of many real-time systems such as medical robots, automobiles and avionics. Finally, this project extends devised data structures and algorithms to potential algorithms in various other areas that utilize corresponding graph oriented data structures such as the Floyd-Warshall algorithm; the manipulation of B-tree, T-tree, R-tree and their variations; digital logic optimization; and Petri-net models.","title":"CPA-CSA: Development of Parallel Reduced Run-Time Complexity Hardware-Oriented Deadlock Algorithms with Proofs and Extensions to Other Areas","awardID":"0811448","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["383996"],"PO":["559883"]},"139580":{"abstract":"The goal of this project is to turn existing location-based services to be preference- and context-aware. Enabling preference- and context-awareness will make location-based services more convenient to use by casual users and make the best use of technology advances in location-detection devices, wireless communication, and mobile computing. The project achieves its goals using the following approaches: (1) define a context taxonomy and build a system architecture to guide location-based query processors to be preference- and context-aware, (2) extend the SQL language with new constructs to define\/manipulate preference and context, (3) develop new query processing techniques that integrate user preferences, user context, and database context, (4) develop query processing techniques that can make use of the surrounding user peers, (5) integrate the awareness of the continuous changes in the environmental context, like changes in the road network, into query processors, and (6) take into account data incompleteness and uncertainty in real life scenarios. PhD students pursue research in this project. Publications, technical reports, software and experimental data from this research will be disseminated via the project web site (http:\/\/www.cs.umn.edu\/~mokbel\/ContextAware).","title":"III-COR-Small: Collaborative Research: Preference- And Context-Aware Query Processing for Location-based Database Servers","awardID":"0811935","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["518156"],"PO":["565136"]},"139591":{"abstract":"The goal of this project is to develop technology in support of the visual analytics process. The complex process of analysis in domains from scientific discovery to homeland security consists of cycles of hypothesis generation, evidence gathering, evidence organization, and hypothesis resolution. The developed discovery management technology provides support, both visually and computationally, not only for all analysis stages in isolation, but also for tight integration among them. The computational infrastructure enables analysts to capture, visualize, manage, analyze, and interactively explore patterns and discoveries (\"nuggets\") of the visual analytics process. Tools for the modeling and management of nuggets and their complex interrelationships form the foundation of the infrastructure. Methods for nugget generation include explicit identification and confirmation by user, implicit capture based on analysis of user logs, and automated discovery using statistical and data mining techniques.<br\/><br\/>Computational and interactive visual methods enable analysts to efficiently validate, annotate, classify, organize, and purge nuggets over time. Visual representations of hypotheses, evidence, and nuggets help analysts explore their data, manage their discoveries, and organize their reasoning processes. The resulting technology enables analysts from diverse domains ranging from biomedical discovery to homeland security to visually explore data, to form hypotheses, and to swiftly manage their discoveries as the exploration process proceeds. Project insights are infused back into an educational agenda using a rich repertoire of mechanisms, ranging from special-topics courses to undergraduate and K-12 project-based student activities. The project Web site (http:\/\/davis.wpi.edu\/~xmdv) is used for results dissemination.","title":"III-COR-Small: Managing Discoveries in Visual Analytics","awardID":"0812027","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["543519","543520"],"PO":["563751"]},"139581":{"abstract":"Other than the PI's prior work, the current state of the art in human-robot interaction does not include algorithms which closely tie engagement behaviors with a sophisticated model of task-oriented collaboration. Thus the most \"engaging\" current humanoid robots typically lack the ability to collaborate on complex tasks. Conversely, current robots with sophisticated task skills are typically stilted and unnatural to interact with. The PI's ultimate goal in this research is to develop new fundamental computational principles and algorithms which will significantly improve the ability of autonomous robots to collaborate with humans in a broad range of situations in the home, in commercial functions, and in hazardous environments. The specific focus of this project is on engagement, the process by which two (or more) participants in a collaboration initiate, maintain and terminate their perceived connection. Examples of generally applicable engagement behaviors include looking, nodding, pointing and body stance. The PI's working hypothesis, supported by his prior work, is that the proper interpretation and generation of these engagement behaviors by a robot is crucial to the overall effectiveness of its collaboration with humans. The research method will combine the study of engagement in human-human collaboration with the implementation and evaluation of algorithms for human-robot engagement and collaboration using an experimental humanoid robot. In the evaluation, a human will first teach the robot a new procedural skill and then the robot will teach the same skill to a different human. The main scientific output of the project will be a broadly applicable algorithm for initiating, maintaining and terminating engagement in the context of human-robot collaboration. For example, this algorithm will include rules which specify, relative to the state of the collaboration, when and where the robot should look, nod, point and face, and how to interpret the corresponding behaviors by humans. A secondary contribution of the project will be to demonstrate a collaboration-based approach to naturally instructing a robot.<br\/><br\/>Broader Impacts: Better engagement and collaboration abilities will make it easier for robots to assist the handicapped and the elderly, will make robots more effective in search and rescue operations, and will open up new markets for robots in sales and entertainment. The results of this project will be widely disseminated both through scientific publication and free distribution of the implementation code. The experimental humanoid robot designed by the PI for this work is also commercially available at a reasonable cost. This dissemination will facilitate the results being applied by the designers and builders of future human-robot systems.","title":"HCC-Small: Engagement and Collaboration in Human-Robot Interaction","awardID":"0811942","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["448731"],"PO":["565227"]},"139471":{"abstract":"As general-purpose computing moves into the age of pervasive parallelism, programmability becomes the key hurdle limiting the effective use of available computing resources. Transactional memory promises to simplify parallel programming for application programmers. However, research in Transactional Memory is being seriously hampered by the lack of a reusable open source infrastructure. The project will develop the key pieces necessary to overcome this situation: A transactional memory library built out of highly decomposed pieces will provide reusable and replaceable parts suitable for investigating tradeoffs in software TM implementations. Standardized interfaces will allow libraries conforming to the interfaces to be used in a variety of environments. TM-aware run-time analysis tools, particularly profilers and debuggers, will provide the necessary tool support for TM implementors and application programmers to understand and improve the performance of software using transactions. Interesting benchmarks, in a variety of high-level languages, will move forward our understanding of TM performance characteristics.","title":"CPA-SEL-T: Collaborative Research: Unified Open Source Transactional Infrastructure","awardID":"0811405","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["517262"],"PO":["523800"]},"147094":{"abstract":"With the continued explosive growth in electronic information, both<br\/>business enterprises and high performance computing systems are being<br\/>structured as large ensembles of interconnected modular<br\/>computing\/storage modules or blades. These systems form the next<br\/>generation data centers and supercomputers and a major impediment to<br\/>their successful wide-spread deployment is the growing capital cost as<br\/>well as power and cooling costs. A dominant contributor to these costs<br\/>is the memory subsystem that can exceed the costs of the processing<br\/>core and consume as much as 25%-40% of the energy in high end<br\/>configurations. Currently, the architecture of data centers limits how<br\/>physical memory in data centers can be shared by different programs in<br\/>the system. Consequently, systems must be designed to accommodate the<br\/>worst case peak memory demand and is therefore unduly expensive both<br\/>in terms of capital cost as well as power. <br\/><br\/>The proposed research explores a novel solution for the reduction of memory system related costs by enabling memory to be shared across blades in a dynamic, demand-driven, fashion. The key idea is the tight integration of memory subsystem with the network subsystem. The result is a significant increase is memory efficiency and an attendant drop in the total memory requirements of a data center with little to no performance penalty. This approach is made feasible by exploiting recent advances in chip integration where the memory controllers and high performance communication interfaces are integrated on the same die. Thus cost and power can be more effectively managed than previously feasible by tracking actual memory demand rather than statically provisioning for worst case memory demand.<br\/><br\/>To maintain its leadership in computing, U.S. industry requires both<br\/>technical advances such as those proposed here, new companies that<br\/>leverage these advances, and new employees with the skill sets needed<br\/>for rapid technology and product development. The proposed work offers<br\/>suitable technical elements to offer ways to engage students in<br\/>Computer Science and Engineering topics, to arrest the ongoing decline<br\/>in the U.S. enrollment in CS and CmpE, to create new employment<br\/>opportunities and to leverage the creativity and entrepreneurial<br\/>nature of the U.S. student population. Existing relationships with<br\/>technology companies such as IBM, HP, and Intel through an existing<br\/>NSF I\/UCRCat Georgia Tech will provide pathways to influence the system<br\/>community.","title":"SGER: Dynamic Partitioned Global Address Spaces for Future Large Scale Systems","awardID":"0847991","effectiveDate":"2008-09-15","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["556660"],"PO":["559883"]},"147171":{"abstract":"Recent advances in wireless and micro-electronics technologies <br\/>have made these technologies functionally accessible and available <br\/>to a wide range of different applications. One of the fast growing <br\/>field?s is wireless sensor networks technologies, whose applications <br\/>from military, such as monitoring enemy and military consignments, <br\/>to traffic surveillance, environment and building structures monitoring. <br\/>An important characteristic of sensor networks is the limited power <br\/>supply of the sensor nodes. These nodes are usually powered by batteries, <br\/>and therefore it may not be possible to recharge or replace the batteries <br\/>after their deployment.<br\/><br\/>This proposed research aims at addressing the maximal lifetime scheduling <br\/>problem for sensor surveillance networks in a dynamic environment in which <br\/>the positions of the targets are dynamic and each sensor has different <br\/>surveillance range. Specifically, the research involves an exploratory <br\/>investigation of finding a novel family of efficient scheduling algorithms <br\/>(that offer a wide range of tradeoffs among cost, scalability and energy <br\/>consumption) for a sensor surveillance network. Its purpose is to monitor <br\/>a set of moving targets, such that all the targets are being monitored by <br\/>the sensors in the network at any point in time and at the same time the <br\/>lifetime of the sensor surveillance network is maximized. In this scheduling <br\/>algorithm, a sensor can switch off to save energy when it is not its turn to <br\/>monitor a target. The proposed project also develops distributed monitoring <br\/>mechanisms and scanning methods to obtain a sensor?s locations information. <br\/>An analytical framework to theoretically evaluate the scheduling solution <br\/>from the point of view of tradeoffs between cost, scalability and energy <br\/>consumption is also proposed. The analytical results will be evaluated by <br\/>extensive simulations and experimental measurements.","title":"SGER: A Novel Optimization Scheduling Technique for Sensor Surveillance Networks","awardID":"0848273","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["468323"],"PO":["432103"]},"139450":{"abstract":"Recently, the microprocessor industry has moved toward multicore microprocessor designs as a means of utilizing the increasing transistor counts in the face of physical and micro-architectural limitations. <br\/>Unfortunately, providing multiple cores does not directly translate into performance for most codes. To make use of multicore, many new languages have been proposed to ease the burden of writing parallel programs, yet the programming effort involved in creating correct and efficient parallel programs is still far more substantial than writing the equivalent single-threaded version. A more attractive approach is to rely on tools, both compilers and runtime optimizers, to automatically extract threads from sequential applications. Unfortunately, despite decades of research on automatic parallelization, most techniques have only been effective in the scientific and data-parallel domains. With recently gained insight, the investigators showed that the limits of prior thread-extraction approaches are not fundamental. By applying known and new compilation techniques in a systematic manner, the investigators found that SPEC CINT2000, among the most sequential of codes, has abundant scalable parallelism.<br\/><br\/>In this project, the team of investigators is taking the initial steps toward developing the techniques necessary to build an automatic thread extraction framework. These techniques include developing static transformations that extract parallelism and quantifying the opportunities for dynamic optimization. The system will ultimately consist of a series of static transformations and compiler-inserted hints combined with a run-time optimization component. This framework will address the multicore challenge by reliably extracting parallelism from a wide range of applications without burdening the programmer with what should remain to be low-level implementation details.","title":"CPA-CPL-T: Collaborative Research: Revisiting the Sequential Programming Model for Multicore Systems","awardID":"0811302","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["438733"],"PO":["565272"]},"139461":{"abstract":"With the emergence of the multicore architecture comes the promise of integrating enormous computing power in a single chip, thereby enabling parallel computing in all types of platforms including handheld computers and desktop machines. Providing proper software support for applications is critical to harness the true power of this architecture. An inherent characteristic of multicores that presents a significant obstacle is runtime variation: reliability, energy\/thermal behavior and process variation will vary across identically designed components of a multicore, producing a negative impact on application power consumption and performance. Runtime variation has been identified as one of the key problems that could block further scaling of circuits if not properly addressed. <br\/><br\/>This research project is developing an advanced execution system, called a Robust Execution Environment (REEact), that dynamically mediates, controls and adapts an application's execution to the runtime resource landscape originating from runtime variations. It employs a combination of techniques in adapting both the hardware resources and the application software code to overcome the impact of runtime variations. At the hardware level, it adapts the resources, such as setting the speed\/voltage of a node on the multicore. At the software level, REEact dynamically optimizes code, taking into account performance and power consumption due to runtime variations. It elicits the help of the OS in determining what resources to use in running the application. REEact informs the OS about information it dynamically discovers about latency, power, and application behavior. REEact is built as multi-layer hierarchical runtime system that interacts with the parallel application, the OS, and the underlying multicore architecture to ensure that maximum performance is achieved.","title":"CPA-CPL-T: REEact: A Robust Execution Environment for Fragile Multicore Systems","awardID":"0811352","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["560516"],"PO":["565272"]},"139582":{"abstract":"Navigation sensors are found not only on robots, but also on passenger vehicles, portable devices, such as cell phones, and on wheel-chairs and white-canes that aid people with disabilities. For several applications, sensor-fusion algorithms are employed to combine measurements from multiple sensors rigidly attached to the same vehicle, or spatially dispersed and mobile. In either case, their relative spatial configuration must be known. This introduces the need for sensor-to-sensor extrinsic calibration. Moreover, in order for sensor measurements to be useful for high-precision navigation, their placement on the host robot must be determined, which brings about the problem of sensor-to-body calibration.<br\/><br\/>To this day, very little is known about how to rigorously perform sensor-to-sensor and sensor-to-body extrinsic calibration. Typically, the transformations between the sensor and\/or robot frames of interest are estimated using expensive calibration equipment, or found through approximate manual measurements and use of CAD plots. The objectives of this research effort are to develop rigorous methods for motion-induced sensor-to-sensor and sensor-to-body calibration, and to conduct a detailed theoretical study of their performance. The results of this work will significantly improve the quality and reduce the effort needed for multi-sensor calibration, thus providing valuable tools for designing and implementing navigation systems.","title":"RI-Small: Motion-induced Extrinsic Calibration of Rigid and Reconfigurable Networks of Sensors","awardID":"0811946","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["553283"],"PO":["543539"]},"139472":{"abstract":"Recent years have seen the emergence of large-scale data clusters consisting of hundreds of thousands hard drives in data-intensive computing. Because failure of a disk could cause loss of valuable data or information, the direct and indirect costs of ever often disk failures are becoming increasingly critical issues in the deployment and operation of these computational platforms. Recent studies conclude that the trend in data-intensive computing is towards much higher disk replacement rates in the field than the Mean-Time-To-Failure estimates of the manufacturer datasheet would suggest, even in the first years of operation. Hence, the recovery becomes state of storage. <br\/><br\/>Existing storage recovery solutions successfully developed optimal and near-optimal parallelism layouts such as declustered parity organizations at small-scale storage architectures. There are very few studies on multi-way replication based storage architectures that are equally important but significantly different from erasure code based storage architectures. Moreover, it is difficult to scale up to a large size because current placement-ideal solutions have a limited number of configurations. Lastly, fast recovery demands efficient reverse data lookup, which is not well studied in current scalable data distribution schemes. The investigators develop methods and tools for achieving fast recovery by exploiting optimal and near-optimal parallelism techniques, and distributed hash table and reverse hashing techniques to improve the scalability of reverse data lookup in high-performance storage systems. The proposed research, if successful, will have broad impact in both fault-tolerance computing and high-performance computing community by providing a scalable and fast storage recovery solution.","title":"CPA-ACR: Fast Recovery Using Optimal and Near-Optimal Parallelism in Data-Intensive Computing","awardID":"0811413","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["464413"],"PO":["565272"]},"139593":{"abstract":"This project is motivated by the needs of disaster response teams, which include efficient waste removal, humanitarian aid, electronic group buying, air cargo transport, and power transmission planning. However, the strategies that the project explores apply to many cooperative work environments. <br\/><br\/>The research focuses on the problem of forming coalitions in an environment in which tasks are dynamic, agents are presented with inaccurate and unreliable information, agent trust is used to evaluate offers, and a combination of leadership and incentives are used to improve the efficiency of coalition formation and operation. The project is (1) investigating simple local strategies that lead to desirable group behavior, (2) evaluating the scalability and adaptability of these strategies, (3) showing that through mechanism design the coalition framework can temper bias and increase the global good without restricting the algorithms used by self interested agents, and (4) showing that leadership increases efficiency.<br\/><br\/>The project is developing of algorithms and adaptive strategies to aid artificial (virtual, machine) agents in dynamic and unknown environments, but the research has implications for human society, which include applications to emergency response and other cooperative work, and also for theorizing about the way individuals and groups respond to the many forms of bias in areas such as promotions, hiring, and college admissions.","title":"COAL: Coalition Organization with Agent Leadership","awardID":"0812039","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["393997"],"PO":["562760"]},"139483":{"abstract":"Project ID: CCF - 0811454<br\/>Title: eSilicon Calibrated Scan Based Timing Tests for Delay Defect Detection<br\/>PI name: Singh, Adit D. <br\/>Institution: Auburn University, Alabama<br\/><br\/><br\/><br\/><br\/>Abstract<br\/>Microscopic manufacturing flaws, such as the localized narrowing of interconnection lines, via voids, and pin holes in insulating gate oxide, are a major reliability concern in nanometer digital integrated circuit technologies. Such defects are difficult to detect during production testing because they often do not cause catastrophic malfunction in the circuit; instead, circuit switching delays may be marginally increased along the signal paths containing the defect. Since a complex chip contains billions of circuit paths of varying lengths, and circuit clock timing is designed to accommodate the longest path delay, small delay defects on short paths can remain hidden within circuit timing slacks during post manufacturing testing. However, such defects can often cause errors under worse case operating conditions or degrade further under the stress of field operation to cause early life reliability failure. These small delay defects can potentially be detected through more aggressive timing tests, using faster than rated clock frequencies, to capture and expose any erroneous response due to the excessive delays along short paths. This requires the use of the scan design-for-test methodology which provides full access to the internal flip-flops in sequential designs, and can thereby allow circuit timing to be observed for single cycle operation. Scan delay tests check if signal propagation delays along the targeted paths in the circuit, activated by two vector delay test patterns, fall within the applied ?launch? to ?capture? clock period. However, because single cycle scan tests operate the circuit in a manner which is different from normal multi-cycle sequential operation, there is increasing evidence that the observed circuit timing may not reflect true circuit delays in normal continuous functional operation. Factors such as power supply noise, coupling and cross talk from abnormal switching activity, variations in die thermal profile, and ?clock stretching? can all significantly impact scan test timing. Normal manufacturing process variations further add to these test timing uncertainties, making the detection of small delay defects extremely challenging. This research takes the view that it may be impractical to observe true circuit functional timing with sufficient accuracy using a surrogate scan based timing test to reliably detect small delay defects. Instead, such defects are better detected by identifying abnormal switching delays (timing anomalies) through a comparison of timing test results from a matched population of parts. Here the scan tests are not used to test circuit timing against a fixed rated clock (e.g. as required for speed binning), but only to identify anomalous parts that display abnormal delays relative to the statistical norm for the population. Critically, this relative rather than absolute timing evaluation eliminates the need for the scan timing tests to accurately match functional timing. The impact of common mode factors such as power supply and coupling noise, clock stretching, etc. is factored out by the comparison test. Non-functional test inputs can also be used by the test, with multiple fast clocks, to achieve high coverage of small delay defects, even on short paths. Preliminary research has experimentally shown that if test responses from adjacent die are compared, delay defects of size less than 5% of the critical path can be reliably detected. This research is developing and evaluating a number of new methodologies to make such silicon ?calibrated? scan delay testing practical and commercially attractive. The new delay test methodologies will be evaluated using specially designed test chips. A longer term goal of the proposed research is to carry out experimental studies, in partnership with industry, to validate the methodologies developed on volume production parts. <br\/><br\/>This research project will help improve the research capabilities of Alabama, an EPSCoR state. Results from the research will also be incorporated in advanced courses taught by the PI, both at Auburn and off campus, through the IEEE Test Technology Education Program. The PI works closely with the Historically Black Tuskegee University located near Auburn, including cooperating on an ongoing joint NSF Computing Research Infrastructure (CRI) grant in the VLSI testing area. Tuskegee students take graduate courses at Auburn University, and some are jointly advised by Auburn faculty. Tuskegee Ph.D. students, who have taken graduate courses with the PI, will be encouraged to participate in this research. This will further strengthen the research interaction between Auburn and Tuskegee, which is helping to attract minority U.S. citizens to graduate studies the computing field.","title":"Silicon Calibrated Scan Based Timing Tests for Delay Defect Detection","awardID":"0811454","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["550443"],"PO":["562984"]},"143190":{"abstract":"Abstract for NSF Proposal 0830381<br\/>Project Title: Perturbation Codes: A New Class of Linear<br\/>Convolutional Codes<br\/>Linear convolutional codes are widespread in data communication systems and data<br\/>storage systems. Each such code processes data at a \u00afxed rate and with a \u00afxed delay, so that<br\/>the fraction of errors (called average distortion) in the reconstructed data will be acceptably<br\/>small. The primary problem of linear convolutional code design is to \u00afnd an optimal code,<br\/>which is a code yielding the minimum average distortion among codes in a \u00afnite set of feasible<br\/>codes. In most cases, with past techniques, this problem could only be solved approximately,<br\/>yielding an almost optimal code (a code with near minimum average distortion), the set of<br\/>feasible codes being too large for the average distortion of codes to be examined individually.<br\/>In this project, we investigate a new technique, called perturbation theory, which reduces the<br\/>set of feasible codes to a much smaller set of codes called perturbation codes. In many cases,<br\/>an optimal code will be one of the perturbation codes, and it can be found in a reasonable<br\/>amount of time.<br\/>Each linear convolutional code in the set of feasible codes is described via a parity check<br\/>matrix over the binary \u00afeld, consisting of a \u00afxed number of rows and columns. A perturba-<br\/>tion class of codes consists of codes whose parity check matrices can be arranged so that a<br\/>certain number of rows at the top remain \u00afxed. Any code will lie in more than one pertur-<br\/>bation class of codes, depending upon which rows of the parity check matrix are held \u00afxed.<br\/>Under certain conditions, there will be a natural group acting on the parity check matrices<br\/>of a perturbation class of codes. Using the group structure, a subset of each perturbation<br\/>class of codes is selected. A code will be declared to be a perturbation code if it is among the<br\/>selected codes in a certain number of perturbation classes containing the code. There is \u00b0ex-<br\/>ibility in the de\u00afnition of perturbation code. In this project, we investigate how to properly<br\/>delineate the perturbation code concept so that an optimal code can be found from among<br\/>a small set of perturbation codes, for source and channel models of su\u00b1cient symmetry.","title":"Perturbation Codes: A New Class of Linear Convolutional Codes","awardID":"0830381","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["480652"],"PO":["564924"]},"143080":{"abstract":"NSF 08-517 - Emerging Models and Technologies for Computation (EMT)<br\/>EMT\/MISC Nanogrid Implementation of<br\/>Massively Parallel Algorithms<br\/>PI: Dan Hammerstrom, PSU; Co-PI: Richard Granger, Dartmouth;<br\/>Co-PI: Konstantin Likharev, SUNY Stony Brook<br\/>Abstract<br\/>Our goal is to investigate the use of radically new implementation technology to enhance Intelligent Computing (IC) with new algorithms and new architectural and design techniques. We are focusing this work on recognition problems in computer vision. The algorithms we are studying have their origin in neuroscience, but they are significant abstractions of those algorithms, with the goal of retaining the essence of the computation while dropping many of the biological details. The 1st assumption is that these algorithms constitute a promising approach to achieving improved levels of IC. The 2nd assumption is that scaling to very large networks is a necessary requirement of intelligent computing, and the algorithms we are using do scale. The 3rd assumption is that CMOS will never give us the algorithm scaling we need at a reasonable cost\/performance for scaled implementations of these algorithms. Thus we need to move to a far denser medium. Our 4th assumption then is that hybrid CMOS \/ nanogrids (CMOL)CMOL is the most promising implementation technology on the horizon.<br\/>Consequently the goal of the research proposed here is to implement massively parallel, statistically based algorithms in CMOL, which is our solution to the general problem. Our approach is to take a real application with a number of different algorithmic stages and study the mapping of that stage to CMOL. The design spectrum for each stage will be based on a concept we call virtualization. The performance\/price of a particular implementation is determined by the degree of virtualization, which in turn is determined by the algorithm and its dynamic behavior.","title":"EMT\/MISC Nanogrid Implementation of","awardID":"0829947","effectiveDate":"2008-09-01","expirationDate":"2012-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":[380952,"456863",380954],"PO":["562984"]},"144070":{"abstract":"This research project studies the feasibility of digital, programmable, software-based control of micro-electromechanical systems (MEMS). MEMS are extremely small physical structures capable of movement, lithographically combined with similarly-scaled electrical circuits to produce micron-scale mechanical devices. Examples of MEMS are tunneling accelerometers, gyroscopes, and micro-mirrors. Currently, analog controllers are used to control MEMS because of the very small time constants required to control devices at this scale. This project has the goal of developing a new style of embedded, software feedback controllers that communicate to sigma-delta over-sampled analog interfaces via high sample rate delta encoding. These designs are the result of research in metric-based controller decomposition that specifically seeks to achieve the potential of multi-rate, over-sampled signal processing. Direct benefits of this work are digital controllers suitable for MEMS integration that match the requirements and potential of these designs, delivering performance\/power ratios that are not possible currently. Although the chosen implementation strategy is FPGA based, the decomposition strategy should lead to very small designs, competitive in power to current low-end microprocessors, with dozens to hundreds of times the performance achieved by current software based control using digital signal processing (DSP) platforms. The broader impact of this research project is the development of practical techniques for the design and modeling of over-sampled controllers for MEMS devices that will have direct impact in medical prosthetics, distributed sensing, and the vast range of applications that can benefit from battery-operated sensor array devices. The project seeks educational impact in the form of graduate curriculum development and of efforts to enable substantive undergraduate involvement in research.","title":"CSR-EHCS(CPS), TM: Low-Power Digital MEMS Feedback Control","awardID":"0834805","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[384054,"496514","527086"],"PO":["561889"]},"147491":{"abstract":"Project summary<br\/><br\/>The workshop proposed here is intended to bring together the relevant NSF research communities to discuss NetSE activities undertaken by the computing research community and the recently announced NetSE program, and to facilitate future collaboration by providing a workshop environment for researchers from different communities to meet. <br\/><br\/>The Community Computing Council (CCC) created the NetSE Council. In addition to this workshop, the NetSE Council will hold five topical, invitational workshops on aspects of NetSE. One of the goals of this workshop is to allow the open community of potential proposers to hear about the research agenda and results of these different workshops. This meeting will tie together the external planning process of the NetSE Council with the internal planning process within NSF in a timely manner consistent with the schedule imposed by the NetSE solicitation deadlines.<br\/><br\/>The NetSE solicitation describes a new structure for research related to the future of networking, and calls for new concepts, new approaches, and new ways of doing research. Because of the novelty of this solicitation, it is important that potential proposers have the opportunity to hear about the program from the relevant NSF program officers, learn about the preliminary work that has been done to frame and define this initiative, to talk to potential collaborators, and to ask questions that will help them better position their own research interests.<br\/><br\/>Intellectual merit<br\/><br\/>While the main objective of the workshop is informational, we anticipate that the integrated presentations concerning the various NetSE Council workshops and the following breakout sessions can lead to fresh insights and research ideas. It is the goal of this workshop to excite the relevant research communities about the new approaches to research that are embodied in NetSE.<br\/><br\/>Broader impact<br\/><br\/>The primary goal of this workshop is broad impact?outreach across a broad set of research communities, broader than would normally respond to a solicitation concerning network design. The goal of NetSE is to engage research across multiple disciplines within CISE, and to reach out even beyond CISE to the social sciences, law and humanities to get their points of view about the future of our global networks. This workshop, with broad outreach, is a central component of that objective.","title":"NetSE Workshop; September2008; Arlington, VA","awardID":"0849472","effectiveDate":"2008-09-01","expirationDate":"2009-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["565000"],"PO":["565090"]},"147161":{"abstract":"This project creates and evaluates new visualization techniques of models from a concept generator to stimulate the designer to generate concepts not originally posed by designer or computer. This technology is expected to enable the development of creative solutions to design problems that would otherwise go undiscovered. The three challenges to achieving effective visualization for enhancing creative design at the concept generation phase are: 1) how to cluster the many concept variants returned from an automated concept generation algorithm into a manageable set of representative concepts that spans the design alternative space; 2) how to visually represent the option space to the designer so that it enhances creativity; and 3) how to measure the impact of the visualization schemes on designer creativity. This exploratory research, if successful, offers the opportunity to transform how we design products and systems and how we guide any designer to a creative new product.","title":"SGER Collaborative Research: VisualizeIT - Measuring the Impact of IT-Enabled Concept Generation on Designer Creativity","awardID":"0848244","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["564650"],"PO":["424970"]},"138460":{"abstract":"Abstract<br\/>FODAVA-Partner: Visualizing Audio for Anomaly Detection<br\/>Mark Hasegawa-Johnson, Thomas Huang and Camille Goudeseune<br\/>The goal of this proposal is to transform large audio corpora into a form suitable for visualization. Specifically, this proposal addresses the type of audio anomalies that human data analysts hear instantly: angry shouting, trucks at midnight on a residential street, gunshots. The human ear detects anomalies of this type rapidly and with high accuracy. Unfortunately, a data analyst can listen to only one sound at a time. Visualization shows the analyst many sounds at once, possibly allowing him or her to detect an anomaly several orders of magnitude faster than ?real time.? This proposal aims to render large audio data sets, comprising thousands of microphones or thousands of minutes, in the form of interactive graphics that reveal important anomalies at a glance. Data transformations will include signal processing, statistical modeling, and visualization. Signal processing will seek to characterize all of the ways in which the difference between two audio signals may be ?important,? including, for example, spectral differences, rhythmic differences, and differences in the impression made on the auditory cortex of a human listener. Statistical modeling will seek to characterize the range of audio events that are ?normal? or easily explicable, so that we may precisely measure the degree to which a potential anomaly is abnormal or inexplicable. Visualization methods will render measures of abnormality, and information about the signal characteristics of each anomaly, in a form suitable for rapid browsing. Two testbeds are proposed. The ?multi-day audio timeline? will be a portable application, visually similar to a nonlinear audio editing suite, which will allow the analyst to rapidly zoom in on potentially anomalous periods of time. The ?milliphone? will be a three-dimensional visualization tool for command and control centers. Audio recordings from one thousand security microphones scattered throughout a city or a large industrial site will be rendered in the form of brightly colored visible threads reaching skyward from a map of the secure region. The analyst will be able to listen to the audio recorded on any microphone by touching its thread; by touching the thread at different heights, the analyst will be able to audit different periods of time. The brightness, color, and thickness of each thread will display the abnormality and signal characteristics of the audio signal at each point in time. Data transformation research will map related types of abnormality to related color\/brightness codes, so that important anomalies, and anomalies confirmed by multiple microphones, are immediately visible to the trained data analyst.<br\/>This research seeks broad impact in the area of security analysis. Video cameras are routinely used for security monitoring of industrial sites, government installations, day care centers, and nursing homes. Data analysts and guards routinely browse the video recorded by up to twenty surveillance cameras simultaneously, fast-forwarding through uninteresting periods of time. Microphones would be used in the same applications, if they were useful; but there is at present no way for a data analyst to rapidly and accurately audit the signals from many different microphones. The proposed techniques will give guards and data analysts new data transformation and visualization techniques that will help them to rapidly identify dangerous situations signaled by inexplicable but anomalous audio signals.","title":"FODAVA-Partner: Visualizing Audio for Anomaly Detection","awardID":"0807329","effectiveDate":"2008-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7703","name":"FOUNDATIONS VISUAL ANALYTICS"}}],"PIcoPI":["550184","426305",368125,368126],"PO":["562984"]},"139583":{"abstract":"The goal of this project is to turn existing location-based services to be preference- and context-aware. Enabling preference- and context-awareness will make location-based services more convenient to use by casual users and make the best use of technology advances in location-detection devices, wireless communication, and mobile computing. The project achieves its goals using the following approaches: (1) define a context taxonomy and build a system architecture to guide location-based query processors to be preference- and context-aware, (2) extend the SQL language with new constructs to define\/manipulate preference and context, (3) develop new query processing techniques that integrate user preferences, user context, and database context, (4) develop query processing techniques that can make use of the surrounding user peers, (5) integrate the awareness of the continuous changes in the environmental context, like changes in the road network, into query processors, and (6) take into account data incompleteness and uncertainty in real life scenarios. PhD students pursue research in this project. Publications, technical reports, software and experimental data from this research will be disseminated via the project web site (http:\/\/www.cs.umn.edu\/~mokbel\/ContextAware).","title":"III-COR-Small: Collaborative Research: Preference- And Context-Aware Query Processing for Location-based Database Servers","awardID":"0811954","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["486458","486458","486459","486459"],"PO":["565136"]},"139594":{"abstract":"Online recommender systems are widely deployed as tools to guide users towards items they will like. There is a growing concern that recommender systems may be manipulated by people with a vested interest in having certain items recommended (or not recommended). This is exacerbated as it is often easy for a manipulator to create multiple online accounts to execute an attack. The goal of this project is to develop general techniques for the design of manipulation-resistant recommender systems as well as specific solutions for applications in which such a recommender could have a significant impact. The applications studied include Internet sites that use user-provided ratings or tags to recommend items to users, and a recommendation system for job candidates that aggregates informal information from former employees and colleagues. <br\/><br\/>The project uses three research methods: (1) Theoretical modeling and analysis of provably robust mechanisms, building on techniques from economic mechanism design and online learning; (2) Simulations of such mechanisms against known attack strategies; and (3) Empirical tests on recommendation datasets. The results and software will also be used to enrich a Masters-level course on recommender systems, and related courses.<br\/><br\/>A manipulation-resistant recommender algorithm will be valuable to numerous Internet organizations, allowing them to offer reliable recommendations without obtaining and verifying personal information. The development of a recommendation mechanism for job candidates will directly benefit both employers and candidates, especially those who lack formal credentials. Algorithms and simulation software developed through this project will be made publicly available via the project Web site (http:\/\/icd.si.umich.edu\/MRRS) that will include additional information on the project.","title":"III-Small: Manipulation-Resistant Recommender Systems","awardID":"0812042","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["539967","505391"],"PO":["563751"]},"139484":{"abstract":"PI: Zhang, Tong & Parhi, Keshab<br\/>Proposal No: 0810992 & 0811456<br\/>Title: Collaborative Research: CPA-DA: Noise-Aware VLSI Signal Processing: A New Paradigm for Signal Processing Integrated Circuit Design in Nanoscale Era<br\/>Institution: Rensselaer Polytechnic Institute & University of Minnesota<br\/><br\/><br\/>ABSTRACT<br\/>The objective of this proposal is to develop a new noise-aware design methodology that can maximize the error resilience of signal processing integrated circuits. As CMOS technology approaches its end-of-roadmap physical limit, there have been increasing levels of environmental and process variations, and susceptibility to noise, which make it a challenge to maintain the historical yield and reliability. This proposal will develop methodology and approaches that tackle this grand challenge in the context of signal processing integrated circuits implementation. The intellectual merit of this proposal lies in the research theme of leveraging the unique characteristics of signal processing functions to substantially improve the tolerance to noise. There are two major parts to this project: developing noise analysis techniques for signal processing integrated circuits, and exploring design space for noise-aware VLSI signal processing. In particular, this research will develop analysis techniques that can quantitatively estimate how variations in signal processing integrated circuits may affect the signal processing performance. This research will further explore the design space for noise-aware VLSI signal processing where the objective is to minimize the noise-induced signal processing performance degradation at minimal energy consumption and\/or silicon cost. <br\/><br\/>The proposed research program represents the first step towards exploring a new research area. If successful, it will have broad impact on the semiconductor industry and national economy in both the near term and long term: In the near term, it will generate considerable economic benefit by improving the noise tolerance and the effective yield of signal processing integrated circuits. From another perspective, it will enable more aggressive CMOS scaling for implementing signal processing integrated circuits, which will be greatly beneficial since the signal processing functions are typically very hardware resource demanding. In the long term, this research will shed light on signal processing system implementation using post-silicon nanotechnology, such as molecular electronics, where a significant degree of noise is presumably inevitable. The education objective of this proposal is to promote the education of VLSI signal processing, the inter-disciplinary area linking semiconductor and signal processing\/communication, to a wider spectrum of students.","title":"Collaborative Research: CPA-DA: Noise-Aware VLSI Signal Processing: A New Paradigm for Signal Processing Integrated Circuit Design in Nanoscale Era","awardID":"0811456","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["550262"],"PO":["562984"]},"139374":{"abstract":"The text available on the Web and beyond embeds unprecedented volumes<br\/>of valuable structured data, \"hidden\" in natural language. For<br\/>example, a news article might discuss an outbreak of an infectious<br\/>disease, reporting the name of the disease, the number of people<br\/>affected, and the geographical regions involved. Keyword search, the<br\/>prevalent query paradigm for text, is often insufficiently expressive<br\/>for complex information needs that require structured data embedded in<br\/>text. For such needs, users (e.g., an epidemiologist compiling<br\/>statistics, as reported in the media, on recent foodborne disease<br\/>outbreaks in a remote country) are forced to embark in labor-intensive<br\/>cycles of keyword-based document retrieval and manual document<br\/>filtering, until they locate the appropriate (structured) information.<br\/>To move beyond keyword search, this project exploits information<br\/>extraction technology, which identifies structured data in text, to<br\/>enable structured querying. To capture diverse user information needs<br\/>and depart from a \"one-size-fits-all\" querying approach, which is<br\/>inappropriate for this extraction-based scenario, this project<br\/>explores a wealth of structured query paradigms: sometimes users<br\/>(e.g., a high-school student in need of some quick examples and<br\/>statistics for a report on recent salmonella outbreaks in developing<br\/>countries) are after a few exploratory results, which should be<br\/>returned fast; some other times, users (e.g., the above epidemiologist<br\/>investigating foodborne diseases) are after comprehensive results, for<br\/>which waiting a longer time is acceptable. The project develops<br\/>specialized cost-based query optimizers for each query paradigm,<br\/>accounting for the efficiency and, critically, the result quality of<br\/>the query execution plans. The technology produced will assist a vast<br\/>range of users and information needs, by enabling efficient, diverse<br\/>interactions with text databases -- for sophisticated searching and<br\/>data mining -- that are cumbersome or impossible with today's<br\/>technology. The research and educational components of the project<br\/>will rely on -- and encourage -- a tight integration of three<br\/>complementary Computer Science disciplines, namely, natural language<br\/>processing, information retrieval, and databases. The project will<br\/>also provide data sets and source code, for experimentation and<br\/>evaluation, to the community at large over the Web (http:\/\/extraction.cs.columbia.edu\/).","title":"III-COR-Small: Beyond Keyword Search: Enabling Diverse Structured Query Paradigms over Text Databases","awardID":"0811038","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["451238"],"PO":["563751"]},"129221":{"abstract":"This proposal develops, deploys and evaluates a prototype pervasive dynamic oceanographic ecosystem that integrates sensors, networks, observatories, and computational algorithms to enable dynamic data driven application systems research (DDDAS) in oceanography and in particular the study of anoxia and hypoxia off the coast of New Jersey. <br\/>The research findings are also incorporated in cross-disciplinary research curricula (across SOE, FAS and MCS) to provide students with the skills needed by the rapidly expanding network of research and applied observatories being constructed. New research methods in distributed resource allocation, distributed simulation environments, nonlinear dynamic system theory for swarming and computer vision learning methods are used to achieve the proposed goals. The result and impact of this DDDAS framework, is to integrate computational infrastructure that includes computers, networks, data archives, instruments, observatories, experiments, and embedded sensors and actuators, to address important national and global challenges such as<br\/>(1) safe and efficient navigation and marine operations, (2) efficient oil and hazardous material spill trajectory prediction and clean up, (3) monitoring, predicting and mitigating coastal hazards, (4) military operations, (5) search and rescue, and (6) prediction of harmful algal blooms, hypoxic conditions, and other ecosystem or water quality phenomena.","title":"CSR-CSI: DDDAS-The Pervasive Dynamical Ecosystem for Oceanographic Research","awardID":"0720836","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["421129","522465","528215","493379"],"PO":["493916"]},"145490":{"abstract":"Multi-scale, multi-modal, multi-site science, such as terrestrial observatories and other dynamic and adaptive sensor-based application, high resolution sub-cellular imaging, or genomic analyses of communities of organisms, builds large data sets that can serve as the basis for in silico exploration and analysis. Often the data collections represent a wide range of disciplines, yet there are often a limited number of interpretations, guided by an individual scientist?s expertise. Intuitive exploration that transcends disciplinary boundaries and expertise could enhance the process of interdisciplinary collaboration, as well as drive the process of discovery in new directions. This proposal seeks to develop a dynamic metadata-based approach for intuitive, interactive, and immersive multi-scale, multi-modal data exploration application to a wide range of data sets and their associated metadata. The original focus will be on metagenomics data from the Global Ocean Survey. The development of the environment will enable a different dynamic sorting and sifting of data rather than relying solely on known or expected features. Such open-ended exploration occurs not only in observational science, but also in tasks ranging from network intrusion detection. Students will be engaged in the research and collaboration, with dissemination in public cultural activities, as well as through more traditional academic venues.","title":"SGER: Metadata-Driven Approach to Discovery-Oriented Exploration of Massive Data Sets","awardID":"0841031","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["557608","561353"],"PO":["565136"]},"143070":{"abstract":"Collaborative Research: EMT\/QIS:<br\/>Quantum Algorithms and Post-Quantum Cryptography<br\/>Project Summary<br\/><br\/>Cryptography is the basic infrastructural element enabling privacy and <br\/>security for electronic transactions. However, when a large-scale <br\/>quantum computer is finally built, it will force us to abandon <br\/>established methods of cryptography, such as RSA and Diffie-Hellman, <br\/>which are in common use today. The proposed research will further <br\/>this line of disruptive quantum algorithmic research; but it also aims <br\/>to erect a new framework of secure post-quantum cryptography, in order <br\/>to maintain this societally critical infrastructure.<br\/><br\/>The most attractive approach for salvaging modern cryptography would <br\/>be to develop classical cryptosystems for which we have compelling <br\/>evidence of security even in the face of quantum adversaries. Recent <br\/>work by the PIs and their collaborators has shown that certain <br\/>algebraic problems possess hardness properties relevant even for <br\/>quantum algorithms. We propose to strengthen and leverage these <br\/>results in order to develop cryptographic schemes which can be carried <br\/>out by today's computers, but which will remain secure even against <br\/>quantum attack in the future.<br\/><br\/>In tandem with this effort, we propose to develop new quantum <br\/>algorithms for breaking cryptosystems based on conjugacy in the braid <br\/>group. This is one of the few remaining classical cryptosystems which <br\/>has not already been shown to be vulnerable to quantum attack.<br\/><br\/>Our research program is also directly integrated with graduate student <br\/>training at all four institutions, undergraduate educational <br\/>innovation, educational outreach, and broad scientific dissemination.","title":"Collaborative Research: EMT\/QIS: Quantum Algorithms and Post-Quantum Cryptography","awardID":"0829917","effectiveDate":"2008-09-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["486281"],"PO":["565157"]},"143081":{"abstract":"EMT\/NANO: Broadcast Optical Interconnects for<br\/>Global Communication in Many-Core<br\/>Chip-Multiprocessor<br\/>Alan Mickelson (Principal Investigator)<br\/>Dejan Filipovi\u00b4c<br\/>Won Park<br\/>Li Shang<br\/>Manish Vachharajani<br\/>Electrical and Computer Engineering<br\/>University of Colorado<br\/>Abstract:<br\/>Computer performance scaling is critical to advances in a wide range of fields, including<br\/>medicine, telecommunications, climate science, weather forecasting, engineering, and design.<br\/>With the rise of multicore systems and increasing processor-core count as the hardware<br\/>mechanism for increased compute power, parallel program performance is now critical to<br\/>continue this performance scaling. This work emphasizes latency reduction as this will be<br\/>the most important performance impediment as the number of processors increases. Electrical<br\/>interconnections can support the large information bandwidth over small distance, for<br\/>example, between neighboring processors. Packet switching can share bandwidth between<br\/>processors at a cost of latency that increases with inter-processor spacing. This research<br\/>involves the use of nano scale optics to implement an optical broadcast network that can<br\/>address all processors during a single clock cycle. Media access control will partition information<br\/>between the high bandwidth point to point messages to be packet switched and<br\/>latency bound synchronization messages to be optically broadcast.<br\/>The investigators address the problem of a hybrid on-chip interconnection network both<br\/>experimentally and theoretically. The bulk of the experimental work will involve fabricating<br\/>and demonstrating the operation of nanophotonic broadcast network. The broadcast<br\/>network will consist of a two dimensional slab waveguide region addressed with optical antennas<br\/>that are fed and read out from nanophonic channel waveguide components. Optical<br\/>sources will be placed off-chip and input to the netwrk through silicon on insulator waveguides.<br\/>Wavelength division multiplexing will provide multi simultaneous channel operation<br\/>through the single transceiver per processor interconnect. The media access control that<br\/>partitions inter processor communication into high bandwidth packet switched communications<br\/>and latency bound synchronization messages will be simulated using experimental data<br\/>and phenomenological models of the actual interconnection network.","title":"EMT\/NANO: Broadcast Optical Interconnects for Global Communication in Many-Core Chip-Multiprocessor","awardID":"0829950","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["559847","523624",380958,380959,"485555"],"PO":["565157"]},"144060":{"abstract":"ABSTRACT<br\/>With the advances in the hardware technology and ever-decreasing cost of imaging circuitry, cameras have become<br\/>ubiquitous in our lives. The large amounts of video data generated by multiple cameras necessitate automatic<br\/>analysis of activities from video. With the introduction of smart cameras, it has now become viable to install<br\/>many spatially-distributed cameras interconnected by wireless links. A smart camera is a stand-alone unit that<br\/>not only captures images, but also includes a processor, memory and communication interface. This project<br\/>takes a holistic view of the problems that need to be solved to build scalable, battery-powered wireless smart-<br\/>camera networks (Wi-SCaNs). It focuses on providing a unifying solution to perform tasks distributively, and<br\/>communicate in a P2P fashion over wireless links while making e\u00b1cient use of limited resources, such as energy<br\/>and bandwidth. This research explicitly considers the challenges of wireless channels, and addresses the issues<br\/>of scarce resources, P2P protocol design and channel usage priorities simultaneously. The expected outcomes of<br\/>this project include light-weight vision algorithms that are portable to embedded smart cameras, characterization<br\/>of the energy-bandwidth-delay tradeo\u00aes in Wi-SCaNs, e\u00b1cient resource allocation strategies, and P2P wireless<br\/>communication protocols that leverage the knowledge of the wireless channel conditions. The outcomes will have<br\/>an important positive impact, because they will fundamentally advance the automatic activity analysis solutions<br\/>by providing installation of any number of cameras anywhere, and will \u00afnd wide-ranging applications including<br\/>surveillance in military, commercial or public scenarios, elderly care and wildlife monitoring.<br\/>1","title":"CSR-DMSS,SM: Cooperative Activity Analysis in Wireless Smart-Camera Networks (Wi-SCaNs)","awardID":"0834753","effectiveDate":"2008-09-01","expirationDate":"2012-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["512215","542021"],"PO":["535244"]},"143092":{"abstract":"This research studies new methods for constructing<br\/>economical plans to deploy wireless sensor nodes of<br\/>multiple types and form a reliable wireless sensor<br\/>network to monitor, throughout a given time period,<br\/>a large set of targets spread across a geographical<br\/>space. It initiates and carries out a thorough<br\/>analytical investigation on deploying multiple types<br\/>of sensors for constructing a wireless sensor network<br\/>that satisfies, simultaneously, the min-cost, the<br\/>lifetime, the fault-tolerance, and the connectivity<br\/>requirements. It integrates concepts and methods in<br\/>linear programming, convex programming, reliability<br\/>theory, graph theory, and approximation algorithms<br\/>to model these requirements and find efficient solutions.<br\/>Results of this research provide a substantial impact on<br\/>large-scale, terrestrial and underwater, wireless sensor<br\/>network applications and communication protocol designs.<br\/><br\/><br\/>More specifically, this research seeks analytical models<br\/>to provide, with minimum monetary cost on sensor nodes,<br\/>reliable sensor placements and time scheduling to hinge<br\/>on spatial-temporal information and power consumption.<br\/>Despite failures and limited power supply of sensor<br\/>nodes, the network must guarantee that each spatial<br\/>target be watched by at least one active sensor at any<br\/>given moment during a pre-determined period of time<br\/>required by the underlying application. This implies<br\/>sensor placements must satisfy the spatial-temporal<br\/>coverage, the fault-tolerance, and the min-cost<br\/>requirements for single-hop networks; as well as the<br\/>additional connectivity requirement for multi-hop<br\/>networks, which ensures that each sensor node is<br\/>connected to a base station through a communication path.","title":"TF-SING: Collaborative Research: Reliable Spatial-Temporal Coverage with Minimum Cost in Wireless Sensor Network Deployments","awardID":"0829993","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["505880","450698"],"PO":["564924"]},"144071":{"abstract":"Ordinary PCs are widely employed for large scale scientific computing today. Released<br\/>in 2004, BOINC middleware is managing over 500,000 volunteered PC nodes and<br\/>providing computation power to around 30 scientific research projects including<br\/>Rosetta@home, Climateprediction.net, and IBM World Community Grid. The main<br\/>attraction of such \"virtual clusters\" is that computing is \"free\" as minimal additional<br\/>hardware and personnel resources are needed. The potential for exploiting such idle<br\/>cycles is immense since well under 1% of the world's 1 billion PCs are currently<br\/>participating.<br\/>Currently idle PCs are exploited for sequential and independent parallel tasks, but not<br\/>communicating parallel tasks, a common HPC paradigm. The central goal of this project<br\/>is to achieve robust execution of communicating parallel applications on networked<br\/>ordinary PCs. The challenge is that ordinary desktops are \"volatile\", i.e., their<br\/>availability changes suddenly and frequently based on desktop owner's actions.<br\/>Checkpointing of parallel applications, the state of the art in fault tolerant scientific<br\/>computing, is not sufficient for high failure rate environments.<br\/>This project is developing the VolPEx framework (Parallel Execution on Volatile nodes)<br\/>that employs managed redundancy as the core mechanism to achieve seamless<br\/>forward application progress in the presence of routine failures. The canonical execution<br\/>model consists of two or more concurrent replicas of each process with failed process<br\/>replicas regenerated on-demand from healthy ones. The following communication APIs<br\/>are provided for application development.<br\/>1. Virtual Dataspace: An abstract API for asynchronous anonymous Put\/Get<br\/>communication among tasks. The BOINC programming model of independent tasks is<br\/>being extended with the dataspace API to allow inter-task communication<br\/>2. Volpex MPI: A subset implementation of MPI with a communication layer customized<br\/>for execution on volatile nodes.<br\/>The validation of this research will include execution of selected parallel applications on<br\/>100s of nodes across campus LANs and 1000s of nodes across the globe under<br\/>Volpex\/BOINC.<br\/>The ability to transform ordinary PCs into a virtual cluster to run parallel codes will have<br\/>wide ranging impact. Virtually all scientists will get access to HPC while the need for<br\/>clusters and the cost of purchasing, operating, and maintaining clusters will diminish.","title":"CSR-PSCE, SM: Collaborative Research: VOLPEX: A Framework for Parallel Execution on Volatile Nodes","awardID":"0834807","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550108"],"PO":["493916"]},"146183":{"abstract":"Biology is now an information science, increasingly dependent on the array of biological resources, such as expert-curated databases. The knowledge in these databases ultimately derives from literature, but the transfer requires experts to read, extract and structure the information for the database, which is both labor and expertise expensive. There has been significant progress in performance of text mining and information extraction tools to support the process, much of which has been demonstrated in the context of challenge evaluations and assessment. However, all of these have measured results such as precision and recall, rather than focusing of whether the tools help the intended end users. The PI will work closely with database curators to assess the impact of the tools and develop evaluation methods to guide tool developers. The measures will be used in upcoming text mining evaluations, for the first time making the evaluation process interactive and enabling the evaluation of utility and usability.","title":"SGER: Utility and Usability of Text Mining for Biological Curation","awardID":"0844419","effectiveDate":"2008-09-01","expirationDate":"2011-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[389934,389935],"PO":["565136"]},"139550":{"abstract":"The focus of this proposal is to evaluate the use of instantaneous, chip-wide global information for increasing performance and reducing power and cost in large-scale, chip multi-processors (CMPs). Using global information contradicts conventional thinking because it has been assumed that traversing the entire chip requires many clock periods. Recently developed circuits, however, enable single-cycle, chip-wide signaling, suggesting new possibilities of combining global and local information simultaneously. Further, similar ideas have not been previously implemented in large, traditional multi-chip systems because design flexibility, interconnect configurability, and transistor performance improvements exist only on chip.<br\/><br\/>Preliminary exploration has identified both the required interconnect circuits and opportunities for hardware and software to take advantage of up-to-date global information. The research plans to exploit new circuit techniques that will enable the proposed network-on-chip (NOC)with hybrid interconnect (NOCHI) architecture, which has both a data plane using conventional interconnect techniques and an ultra low-latency control plane with a global interconnect. Because NOCHI ignores established design patterns, understanding the circuit properties is essential for the architectural studies, which include: new algorithms that can better control and regulate power consumption within the network and across cores; improved interaction of the cores with the off-chip memory system to reduce demands on on-chip network resources; the ramifications of global information on cache coherence and management; and software controlled, ultra low-latency efficient synchronization for multicores.","title":"Collaborative Research: CPA-CSA: CMP Architecures with Global Communication","awardID":"0811798","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["432706"],"PO":["366560"]},"139561":{"abstract":"Improving programmer productivity is a crucial challenge, <br\/>especially with the emergence of multi- and many-core processors. <br\/>High level mathematical equations serve as declarative specifications <br\/>for a large class of compute- and data- intensive parts of programs. <br\/>The expressivity of such equations is greatly enhanced when users are allowed to specify collective operations called \"reductions:\" <br\/>associative and commutative operators applied to sets of values. <br\/>Another common feature of such equational programs is \"reuse:\" at different points in a set, the same intermediate value is (re)- computed or used. Certain extremely effective program optimizations are possible when equational programs have both reductions and reuse. <br\/>These optimizations yield new equations with lower computational complexity (e.g., a quadratic time equation can be transformed into one with linear complexity). This project investigates how to perform these simplifications automatically and optimally. The PIs will develop and deploy a tool called Reduction Simplification Engine (RSE) that implements such optimizations techniques. Traditional compiler optimizations simply seek a few percentage points of performance gains, at best a small, constant factor. On the other hand, the optimizations performed by the RSE have significantly more benefits since they reduce the asymptotic complexity of the program.","title":"CPA-CPL: The Reduction Simplification Engine","awardID":"0811852","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["528018"],"PO":["565272"]},"149033":{"abstract":"This project will improve the process of patent search by combining methodologies for measuring the patent space developed at the University of Michigan School of Information with methodologies developed at IBM's Almaden Research Center for browsing and exploring topics and concepts within a large document collection. This combined methodology, called Patent Cartography, will leverage multiple taxonomies, related terms, network analytics, visualization, and user interaction to navigate, explore, and map the patent space. <br\/><br\/>As the rate of patenting has increased, so too has the problem of patent thickets, or dense webs of overlapping intellectual property rights that an organization must hack its way through in order to commercialize new technology. In certain industries characterized by cumulative innovations and multiple blocking patents, the existence of such densely concentrated patent rights can have the perverse effect of stifling innovation rather than encouraging it. A recent Federal Trade Commission report notes that in certain industries, the large number of issued patents makes it virtually impossible to search all the potentially relevant patents, review the claims contained in each of those patents, and evaluate the infringement risk or the need for a license. For many firms the only practical response to this problem of unintentional and sometimes unavoidable patent infringement is to file hundreds of patents each year so as to have something to trade during cross-licensing negotiations. In other words, the only rational response to the large number of patents in a given field may be to contribute to it. <br\/><br\/>Given that 200,000 US patents are issued each year, with new patents issuing each week, any attempt to analyze and comprehend the dynamic topology and interconnectedness of patent space will almost certainly have to be based on information technology. The process of patent search, however, has not progressed very much even with the advent of searchable patent databases. Although automated, the patent search process itself has not been reengineered and thus remains functionally similar to the patent search process developed in the nineteenth century. <br\/><br\/>While end-users do not have the necessary tools to conduct exhaustive patent searches, professional patent searchers are overwhelmed with an ever increasing workload driven by an increased awareness of the importance of intellectual property. The need for effective end-user patent search capabilities will only increase over time, yet the traditional search process must be reengineered for that need to be met. <br\/><br\/>Using sets of existing professionally-generated patent searches, this project will compare end-user searches conducted with existing tools with other searches conducted with the new methodologies. Several different classes of end-users have been identified, with different levels of previous knowledge and experience with the patent system, and this project will compare the search results produced by each class of user with the professional patent search results. At the conclusion of the study, we will have not only a better understanding of the applicability of Patent Cartography and visualization techniques to the process of patent search, but also an understanding of the impact of familiarity and experience with the patent system in harnessing the potential of Patent Cartography. <br\/><br\/>This project will expand the boundaries of document retrieval beyond simple keyword and category search into multidimensional analysis combining mixed initiative data and text mining with visualizations. The project will also combine content and network analysis and evaluate visualizations based on both proximity and dependency data. Search processes and search strategies will also be examined in the context of large document collections. The project will also develop models of task-based information searching, which has been generally under-explored. Finally, the project will develop an ethnographic account of the patent search process by observing patent search experts. <br\/><br\/>This project will extend the scope of research on searching, analyzing, and visualizing large document collections. Patents are one exemplary form of large digital library repositories that contain both structured and unstructured data to which these techniques can be applied. Other sources include Medline, Research Abstracts, repositories of the World Wide Web, and other enterprise repositories. This search process could be leveraged in health care, life sciences, pharmaceuticals, market research, competitive intelligence, knowledge discovery, and tracking technology maturity and innovation to name just a few.","title":"Patent Cartography: Improving the Process of Searching Through the Patent Thicket","awardID":"0855352","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6857","name":"DIGITAL LIBRARIES AND ARCHIVES"}}],"PIcoPI":["400168"],"PO":["564456"]},"138033":{"abstract":"Haptic interaction refers to the form of interaction with a real or virtual environment based on the sense of touch. As the user experiences and interacts with an environment through a haptic system, the haptic system directly or indirectly alters the user's perception and motor control. Therefore, understanding the effects of the haptic system on the perception and motor control is important.<br\/><br\/>The purpose of this study is the development of a computational model of the human haptic sensory-motor system for quantitatively capturing the user performance in haptic manipulation tasks. The developed computational model, which we call the haptic e-model, will be an objective tool for evaluation of haptic systems with emphasis on human operator's sensory-motor performance. Specifically, the developed haptic e-model will be composed of a benchmark suite of representative haptic manipulation tasks, task performance measures, and models for human perception and motor control.<br\/><br\/>The haptic e-model will be used as a benchmark evaluation model by the research community to evaluate effectiveness and performance of haptic systems, components, and algorithms. It will also be a computational tool that can be easily integrated into the design cycle of haptic systems, and facilitate quantitative analysis of design choices throughout system development.","title":"RI-Small: The Haptic E-Model: A Computational Model of Human Sensory-Motor Performance in Haptic Manipulation","awardID":"0805495","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["460484"],"PO":["543539"]},"139386":{"abstract":"Recently, the microprocessor industry has moved toward multicore microprocessor designs as a means of utilizing the increasing transistor counts in the face of physical and micro-architectural limitations. <br\/>Unfortunately, providing multiple cores does not directly translate into performance for most codes. To make use of multicore, many new languages have been proposed to ease the burden of writing parallel programs, yet the programming effort involved in creating correct and efficient parallel programs is still far more substantial than writing the equivalent single-threaded version. A more attractive approach is to rely on tools, both compilers and runtime optimizers, to automatically extract threads from sequential applications. Unfortunately, despite decades of research on automatic parallelization, most techniques have only been effective in the scientific and data-parallel domains. With recently gained insight, the investigators showed that the limits of prior thread-extraction approaches are not fundamental. By applying known and new compilation techniques in a systematic manner, the investigators found that SPEC CINT2000, among the most sequential of codes, has abundant scalable parallelism.<br\/><br\/>In this project, the team of investigators is taking the initial steps toward developing the techniques necessary to build an automatic thread extraction framework. These techniques include developing static transformations that extract parallelism and quantifying the opportunities for dynamic optimization. The system will ultimately consist of a series of static transformations and compiler-inserted hints combined with a run-time optimization component. This framework will address the multicore challenge by reliably extracting parallelism from a wide range of applications without burdening the programmer with what should remain to be low-level implementation details.","title":"CPA-CPL-T: Collaborative Research: Revisiting the Sequential Programming Model for Multicore Systems","awardID":"0811065","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["518034"],"PO":["565272"]},"143390":{"abstract":"ABSTRACT:<br\/><br\/>This research project advances the understanding of security and privacy in pervasive healthcare by testing technological methods of securing implantable medical devices and by evaluating human factors through patient studies. The most fundamental question is how to balance the opposing goals of safety and effectiveness with security and privacy of wireless, implantable medical devices.<br\/> Future pervasive healthcare devices --- including prescription-based devices like pacemakers, drug pumps, and neurostimulators --- will not only contain long-range wireless capabilities, but will have sophisticated processors and the ability to interact with other devices on the person or in the person's environment. These innovations could vastly improve healthcare, but also introduce unique security and privacy risks not well understood from the perspectives of cyber security, medicine, public policy, and society. A primary challenge is understanding how to overcome the fundamental tension between security and privacy and traditional goals of safety and effectiveness. A second challenge is learning how to design usable, intuitive security mechanisms that patients and doctors will accept and understand. Approaches include novel, zero-power security mechanisms that do not drain the healthcare device's battery, and surveys of patients and other users of pervasive healthcare technologies to learn about patient perceptions and expectations.<br\/> Blending computer science with healthcare, the project prepares computer science students for interdisciplinary security research via both research and coursework. Further demonstrating broader impact, the project reaches out to under-represented minorities and women in computing at community colleges in Western Massachusetts.","title":"CT-ISG: Improving Security and Privacy in Pervasive Healthcare","awardID":"0831244","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["554404"],"PO":["565327"]},"143280":{"abstract":"Proposal: CCF- 0830691<br\/>Institution: Polytechnic University of New York<br\/>PI: Aronov, Boris<br\/>Title: Understanding Geometric Arrangements: Unions and Beyond <br\/><br\/>ABSTRACT<br\/>The research centers on a class of geometric problems involving unions of objects. A variety of much-studied problems in combinatorial and computational geometry can be cast in these terms. Building on significant previous experience investigating the connections between combinatorics and algorithmics, the investigators explore combinatorial problems arising in the union-of-objects questions in order to develop new, simple, and powerful combinatorial tools and to refine those already available. Refining the techniques developed so far, the work expands the scope of problems to encompass questions of optimization in overlays of objects.<br\/><br\/>Throughout the investigation, the researchers emphasize the following theme: Developing new, more powerful tools for dealing with combinatorial problems. Discovery of easier proofs of and streamlined approaches to ``classical'' problems makes research results significantly more accessible to students and non-specialists and ultimately more relevant to its claimed beneficiaries, the practitioners in the application areas. Moreover, they are very likely to foster further progress in the field.","title":"Understanding Geometric Arrangements: Unions and Beyond","awardID":"0830691","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["518464"],"PO":["565157"]},"144380":{"abstract":"Collaborative Research:<br\/>0836656 (Peter Doerschuk, Cornell University)<br\/>0836649 (Bud Mishra, NYU)<br\/>0836720 (Sanjoy Mitter and Emery Brown, MIT)<br\/>Title: Discovery of Succinct Dynamical Relationships in Large-Scale Biological Data Sets<br\/><br\/><br\/>ABSTRACT:<br\/><br\/>Many types of information in neuroscience and molecular biology can be described as a set of measurements taken repeatedly as some index changes its value. In some situations, such as transcriptomic data measuring gene activities, the index is time while in other situations, such as in genetics association study, the index is position in a genomic DNA sequence and, in any case, the complete collection of data is referred to as a time series. Inference is the process of taking such time series, probably corrupted by errors, and computing answers to the following sorts of questions: (1) What is the system that generated the time series? For instance, if the system is known to be a differential equation of a specific type, what are the parameter values in the differential equation? (2) Given a completely specified system and a time series, did that system generate that time series? For instance, if a biologist has hypothesized a system that describes gene expression for a particular set of genes and then measures expression data, is the data compatible with the system, or equivalently, the hypothesis? (3) Given two time series, were they generated by the same system? For instance, if the pattern of nerve firings in a neural system is recorded in two different experimental situations, is the pattern the same or is it different? The four Principal Investigators are focused on three different biological application domains at three different biological scales: (1) the phenotyping of animal and human ethanol-consumption behavior (whole organism scale), (2) the pattern of action potentials measured on ensembles of neurons (cell-population scale), and (3) the time course of gene expressions as governed by the regulatory circuits of the cell (cellular scale). The types of challenges that are encountered in these applications include the following characteristics: the information is distributed over long periods of time rather than concentrated in time; the systems include delays and feedback paths; and the systems are highly nonlinear, including switching behavior, rather than linear. The major methodologies that will be developed and combined to solve inference problems in these application areas are: (a) information theory and stochastic control, (b) multi-scale approaches to learning the geometry of the data, and (c) computer algebra and symbolic computation. For example, to deal with the presence of delay and feedback in neuroscience systems, especially in the context of the interaction between information and stochastic control, requires a fundamental rethinking of classical information theory as it is employed in technology-based communication systems.<br\/><br\/>As the cost of computing decreases, computing becomes increasingly pervasive. A major purpose of pervasive computing is the real-time collection of high-dimensional time series of very diverse types of data including biological, medical, financial, communication systems status, power systems status, etc. The project will provide computational algorithms and software to analyze this data in more sophisticated ways and thereby extract more sophisticated information. Action taken upon this more sophisticated information, e.g., personalized medicine based on individualized genomic information or more accurate and flexible control of power systems thereby avoiding blackouts, will have important human and economic benefits to society. An important component of the project is educational, e.g., three graduate students working on the project will receive tuition and stipend and an unrestricted number of undergraduates will participate through a variety of ways, e.g., project courses. By attracting talented students to science and technology and providing challenging research experiences, the project will have important work force benefits to society.","title":"Collaborative Research: CDI-Type II: Discovery of Succinct Dynamical Relationships in Large-Scale Biological Data Sets","awardID":"0836720","effectiveDate":"2008-09-15","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7751","name":"CDI TYPE II"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["495282","507053"],"PO":["562984"]},"143060":{"abstract":"Behavior-based robotics was established around the idea that robots could be constructed by connecting elementary sensor and actuator modules, without the need to form any internal representation of the world in which they operate. When designed appropriately, such robots, both as individuals and as groups, exhibit complex and seemingly ?intelligent? behaviors, solving challenging tasks in real time, while reacting only to their local environment and obeying sets of local rules. In part, this approach to robotics was inspired by considering natural systems, such as self-organization and adaptability of social insects in their colonies. An analogous approach to initiate the development of the field of behavior-based molecular robotics will be performed over the next three years, leading to groups of molecules that would appear to an observer to show a variety of task-oriented behaviors or some form of purposeful and dynamic self-organization. <br\/><br\/>Individual behavior-based molecular robots are single molecules displaying multiple sensors-actuators. When exposed to artificial landscapes displaying substrates keyed to their sensors-actuators, the molecules start executing elementary steps, determined, in a stochastic sense, by their constantly changing local environments. On some landscapes, called prescriptive, individual molecular robots and their collectives will execute algorithms mimicking wound-up automata with sequence control mechanisms. In contrast, on non-deterministic landscapes, the robots and their collectives will demonstrate properties emerging from internal organization of individual sensors and actuators and through local interactions between molecules and their environments. Importantly, these new interpretations of molecular behaviors will allow radically different experiments from all previous approaches to molecular robotics, while keeping experimental designs realistic, leading to embodiment in the physical world.","title":"Collaborative Research: EMT\/MISC: Behavior-Based Molecular Robotics","awardID":"0829896","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["550110"],"PO":["565223"]},"143071":{"abstract":"Quantum Control of Polar Molecules for Quantum Information and Quantum Computing<br\/><br\/>Abstract:<br\/><br\/>Quantum computers promise to revolutionize our ability to perform computation and solve complicated problems. However, quantum computers are difficult to build. It remains unclear what quantum systems will eventually be used for implementation of a ?scalable? quantum computer, which needs to operate on a large number of quantum bits (qubits) in order to outperform classical computers. This research develops a novel technology using quantum coherent control to produce and manipulate a large number of cold polar molecules. Such polar molecules have many appealing features to be used as qubits. The investigators use all-optical methods to achieve parallel yet local and individual control of many polar molecules. This work can lead to new approaches toward scalable quantum computing. Graduate and undergraduate students will participate in this research in an interdisciplinary team and learn at the forefront of optics and photonics, atomic and molecular physics, and quantum information science. <br\/><br\/>This research develops a novel technology using quantum coherent control in the production and manipulation of cold polar molecules, motivated by potential applications of such polar molecules in quantum information processing and computing. The investigators combine two powerful techniques in modern molecular physics, photoassociation and phase coherent control, to explore novel methods of quantum control of polar molecules, using an optically-based approach to achieve local yet massively-parallel individual control of both the density and orientation of polar molecules. This work could lead to the development of new protocols and schemes of scalable quantum information processing and computing.","title":"Quantum Control of Polar Molecules for Quantum Information and Quantum Computing","awardID":"0829918","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7948","name":"QUANTUM COMMUNICATION"}}],"PIcoPI":["490636","474923"],"PO":["565157"]},"145381":{"abstract":"Title: SGER: Collaborative Research: Exploring the Role of Word Senses in Subjectivity Analysis <br\/><br\/>Approaches to subjectivity and sentiment analysis often rely on manually or automatically constructed lexicons. Most such lexicons are compiled as lists of words, rather than word meanings (senses). However, many words have both subjective and objective senses, which is a major source of ambiguity in subjectivity and sentiment analysis. <br\/><br\/>The goal of this exploratory research project is to address this gap, by investigating novel methods for subjectivity sense labeling, and exploiting the results in sense-aware subjectivity analysis. Specifically, the project targets two research objectives. The first objective is to develop new methods for assigning subjectivity labels to word senses in a taxonomy. The second objective is to explore contextual subjectivity disambiguation techniques that will effectively make use of the word sense subjectivity annotations. By achieving these objectives, the project is expected to contribute to the understanding of the connections among subjectivity, word senses, and contextual subjectivity analysis, which will serve as a stepping stone for continued research efforts in this area. <br\/><br\/>The resources created in this project will be made available to the research community, which will help advance the state of the art in automatic sentiment and subjectivity analysis.","title":"SGER:Collaborative Research: Exploring the Role of Word Senses in Subjectivity Analysis","awardID":"0840632","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["409250"],"PO":["565215"]},"143082":{"abstract":"DNA is well-known for its role in biology as the genetic material. In recent years, however, DNA has begun to be used as a material for creating technology. In particular, DNA can be used to make complex nanometer-scale patterns which, in turn, can be used as templates to arrange nanometer-scale devices. For example, DNA patterns might be used to organize nanowires and nanoswitches to create computer circuits much smaller, cheaper, and faster than current semiconductor computer chips.<br\/><br\/>Recently the investigators invented a method called DNA origami, whereby a long DNA strand is folded into any desired pattern. The method is powerful but has limitations: current DNA origami only contain 200 pixels, which means they can organize at most 200 different devices---not enough to create a complex circuit. In practice it takes 10-20 pixels to align a single carbon nanotube wire on DNA origami, so the most complex device created using DNA origami is a field effect transistor composed of two crossed carbon nanotube wires. Another difficulty is that DNA origami are made in solution, but must be used on surfaces like silicon. Transferring DNA origami to silicon currently results in random placement and orientation, but to build circuits DNA origami must positioned accurately.<br\/><br\/>The investigators are interested in overcoming these limitations. They are working on: (1) combining DNA origami into larger patterns with larger numbers of pixels by treating DNA origami as puzzle pieces that fit together based on \"stacking interactins\", (2) precisely placing and orienting DNA origami on lithographically-defined sticky patches on silicon, and (3) using DNA origami to organize multiple carbon nanotubes to create more complex circuits, such as NAND logic gates.","title":"EMT\/NANO: Integration of DNA nanotechnology with nanoelectronics","awardID":"0829951","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["558957","480791","549587"],"PO":["565223"]},"144061":{"abstract":"Innovations in computing technology, decreasing costs, and advances in miniaturization have made it physically and economically possible to incorporate computers into a wide variety of devices ranging from diminutive wireless sensors to large consumer appliances. This proliferation of computing devices has served to create a world full of possibly millions of such computers, scattered ubiquitously across physical spaces. These devices in combination represent a computational fabric which has the potential to enable an ambitious new class of applications that can exploit the resources in the physical world and also affect the environment around them. At present however, devices exist in isolation because their software and hardware architectures are incompatible and they have not been designed with interoperability in mind. This project envisages the design and implementation of a software system that (1) will allow large collections of heterogeneous devices to interoperate via the use of standard interfaces, (2) will enable the representation of a (potentially changing) set of such devices as a single composite device to simplify programming, and (3) will support the construction of complex applications from small modular pieces that run across multiple devices. If successful, the project will achieve a seamless integration of the computing and physical worlds in a manner that is highly sensitive to the needs of individual, groups, and organizations. Intellectual advances associated with the project will be incorporated into the computer science curriculum and will be disseminated as open source software.","title":"CSR-DMSS, TM: A Substrate for Personalized Computing In the Real World","awardID":"0834755","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[384032,"556687","556687"],"PO":["565255"]},"144182":{"abstract":"A Novel Information Model For Efficient Routing Protocols in Delay Tolerant Networks with General Mobility<br\/><br\/>This project supports the research on a novel information model for efficient routing protocols in delay tolerant networks with general mobility. Delay Tolerant Network (DTN) is a type of wireless mobile network that does not guarantee the existence of a path between a source and a destination at any time. It has broad applications in sensor-based, satellite, underwater acoustic networks, and Internet connectivity in remote places. In this project, nodes mobility cannot be predicted. Routing in DTNs with general mobility poses unique challenges due to the uncertainty and the time-varying nature of network connectivity. To facilitate the routing, a novel information model using the least square methods and their variations are explored. The techniques involve the information that each node should gather and process so that the message can be delivered to the destination as soon as possible even though the location of the destination is unknown and unpredictable.","title":"SGER: A Novel Information Model for Efficient Routing Protocols in Delay Tolerant Networks with General Mobility","awardID":"0835834","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["303431","547822"],"PO":["402055"]},"147460":{"abstract":"This CISE Special Project award provides funding to the Association for Computing Machinery for the organization of a high-level summit to bring together the main computing societies in the United States to address the future of undergraduate computing education on a national scale. The ACM Education Board is to lead the activity. The meeting will focus on the collaborative efforts to revitalize undergraduate computing education. Underlying this meeting is the current perception of a crisis in undergraduate computing education with decreasing enrollments, adherence to outdated traditional models for undergraduate education and a need to foster innovation across computing education to provide exciting educational pathways for the computing professionals of the future. Through collaboration and discussion with leaders of the professional societies the group plans to develop a shared understanding of the main issues, identify key steps to address the current situation, and develop agreement about responsibilities for a range of actions leading to a substantial transformation and revitalization of undergraduate computing education.<br\/><br\/>The intellectual merit lies in the efforts to address the major issues impacting undergraduate computing education on a national scale. The participating groups, the ACM Education Board and the other participating professional organizations, represents a powerful alliance informed by experience as computing professionals and service in professional organizations as representatives of the computing education community uniquely qualified to undertake a project of this scope and to implement a national vision for transformation of computing education in the United States. Through these activities, the computing community should develop promote a better understanding of computing, promote relevant research, support excellence in computing education, and emphasize the range of opportunities available in computing to the national as a whole.<br\/><br\/>The broader impacts lie in the potential to set the stage for a comprehensive national strategic agenda for improving computing education across the nation. Developing shared national goals and strategies among the major computing organizations can impact the potential and opportunities for all citizens and thus address the national need for a diverse group of computing professionals for the future.","title":"Supporting a Community Building Meeting with US Computing Societies","awardID":"0849355","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1139","name":"RSCH EXPER FOR UNDERGRAD SITES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7482","name":"BROADENING PARTIC IN COMPUTING"}}],"PIcoPI":["521278","468220"],"PO":["564181"]},"144072":{"abstract":"Healthcare is 16% of the United States Gross Domestic Product. Technology in the form of medical devices as well as health informatics is critical to improving the quality and reliability of care and the reduction of cost. Medical devices are increasingly controlled by software, and failures in the software can often be life-threatening.<br\/>Medical devices are instances of cyber-physical systems that operate in both the physical and virtual world. The scientific challenge is to demonstrate that the combined cyber-physical system is safe for its intended purpose. The Teleolog project takes on the challenge of certifying the safety of SRI's M7 system for robotic telesurgery. The<br\/>M7 medical robot allows a surgeon to perform telesurgery by manipulating the master controls in order to mobilize two robot arms at the slave station connected through a network. The project takes an explicit, evidence-based approach that leans heavily on formal verification technology for modeling and analyzing the electronic and physical components, the human user, the software control, as well as the interaction between these components. The certification effort reengineers the M7 software to develop a formal assurance case for the safety and resiliency of the system. The models, formats, tools, and certification techniques developed during the certification of M7 can be employed in the certification of other cyber-physical systems. The project also develops course material, case studies, and training kits for educating students in the design and analysis of cyber-physical systems, evidence-based software certification, and resiliency engineering.","title":"Collaborative Research: CSR-EHCS(CPS), TM: Teleolog: Certified Software for Medical Robotics","awardID":"0834810","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["555363",384061,384062],"PO":["561889"]},"146030":{"abstract":"Existing wiki technology has successfully encouraged the collaborative development of content from a distributed and large set of individuals. However, a typical wiki does not encourage the community to actively collaborate or communicate with each other, unless they are required to collaborate. This project identifies a framework for addressing the shortcomings in typical wiki technology that encompasses ideas from social computing to develop a more community focus for the CreativeIT wiki. This project creates a socio-technical environment to support researchers, practitioners, and students to become a vibrant community focused on Creativity and IT, based on the formative evaluation of the existing CreativeIT wiki and related research in the field. A theoretically grounded and empirically evaluated framework is developed for the creation and evolution of a participation culture. Grounding the proposed research in a real problem provides a unique opportunity to assess the strengths and weaknesses on an ongoing basis. This research effort contributes to broaden the theme of Creativity and IT, involves more people to participate, and sustain the effort in the long-term future. The project identifies insights, mechanisms, guidelines, and support environments for participation cultures.","title":"SGER: Increasing Participation and Sustaining a Research Community in \"Creativity and IT\"","awardID":"0843720","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["483372","483373"],"PO":["424970"]},"137681":{"abstract":"The intent of this research is to develop a wireless mobile computing paradigm consisting of cost-effective wireless broadband network, mobile phones, and relevant applications for underserved urban communities. To date, 3G cellular networks, the proposed deployment of urban-scale mesh networks, and penetration of data-ready mobile phones, provide an initial thrust towards this vision. However, this paradigm invites drastically different user experiences and usage patterns from both traditional personal computers based computing and cellular telephony. Consequently, fundamental research questions arise regarding the design of wireless mobile computing to support users from the targeted communities. <br\/><br\/>This project addresses these questions systematically, leveraging a CRI supported deployment of a large-scale open-access wireless broadband network in Pecan Park, an underserved Latino community in Houston, Texas and the distribution of experimental Wi-Fi capable mobile phones to establish a wireless mobile infrastructure there. The longitudinal study of this community's adoption and use of this network infrastructure will be performed using remote data collection and ethnographic fieldwork. The results will drive an iterative, value-centered optimization of the technology ranging from user-centered mobile computing design to community-driven network management. This approach utilizes multidisciplinary techniques including mobile computing, networking, human-computer interaction, and cultural anthropology to develop new research methods relevant to this emerging research domain. <br\/><br\/>Broader Impact. The experimental deployment in Pecan Park will provide low-cost access to information and communication technologies for its residents. Its success will demonstrate the potential of wireless mobile computing to address the digital divide for our nation's urban poor. It will produce lessons and insights for future deployments of wireless mobile infrastructures in other underserved urban communities, both nationally and internationally. The project will provide research opportunities for undergraduate and graduate students, primarily from underrepresented groups.","title":"HCC-Medium: Collaborative Research: Understanding and Optimizing Wireless Mobile Computing for Underserved Urban Communities","awardID":"0803555","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[365970,"488351"],"PO":["564456"]},"139650":{"abstract":"The wide use of the internet coupled with the steadily decreasing cost in computing and storage has led to an expansion of the data that users expect to retrieve from simple numeric and alphanumeric, to include images,audio, video, where the retrieval criterion is one of similarity. An inherent difficulty with similarity retrieval is deciding on a criterion for the similarity. <br\/><br\/>Intellectual Merit<br\/><br\/>This proposal explores issues involved in retrieval that is based on several criteria of similarity:<br\/><br\/>(1) In the spatial data context, similarity is usually defined in terms of proximity in spatial position. Instead, this research will examine similarity in relative spatial orientation. Thus there will be a focus on groups of objects, In particular, attention will be paid to position-independent indexes that find use in applications involving pictorially-specified queries to symbolic image databases.<br\/>(2) Issues involved in finding the k nearest types of objects rather than the k nearest objects will be explored. At present, it is the type of the objects that is of interest rather than their identity. This differentiation is similar to the classic distinction made in spatial databases between location-based and feature-based queries.<br\/>(3) Also of interest will be finding similarity between sets of objects where the similarity measure is the maximum of the minimum distance between objects in the two sets (the Hausdorff distance). Of particular interest will be methods that are incremental so that data is obtained as quickly as possible without waiting for the algorithms to complete.<br\/><br\/>Broad Impacts<br\/><br\/>Position-independent indexing has application to pictorially-specified image databases as well as to databases of moving objects which can be used for searches that include traffic data. The Hausdorff distance is applicable to multimedia databases as is the focus on retrieval by type rather than individual objects. Via a collaboration with the National Cancer Institute, the research team will investigate what makes a good similarity measure for bioinformatics applications.","title":"III-COR-Small: Similarity Criteria Issues in Similarity Retrieval","awardID":"0812377","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["551003"],"PO":["563751"]},"147064":{"abstract":"This SGER proposal concerns the accumulation and representation of skills and control knowledge by robots that interact with unstructured environments. There has been comparatively little work on representations that capture re-useable knowledge in robotics---an issue that lies at the heart of many future applications. Thus, this SGER represents a potentially transformative technology and addresses significant gaps in the state-of-the-art for which the payoff, despite the risk, is extremely high. We aim our 1 year study on learning techniques that accumulate knowledge related to grasping and manipulation. We shall extend pilot studies and build prototypes for self-motivated learning techniques and generative models for manipulation and multi-body contact relationships. The approach relies on learning to discover and exploit structure over the course of several staged learning episodes; from sensory and motor knowledge concerning the robot itself, to controllable relationships between the robot and external bodies, to multi-body contacts involved in tasks like stacking and insertion. The project has three principal technological goals: to advance the state-of-the-art of robotic manipulation and knowledge representation; to extend machine learning methods toward intrinsically motivated, cumulative, and hierarchical learning; and to advance computational accounts of the longitudinal processes of sensorimotor and cognitive development in humans and machines.","title":"SGER: Hierarchical Knowledge Representation in Robotics","awardID":"0847895","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[391847,"513288"],"PO":["403839"]},"139585":{"abstract":"The goal of this research to to develop a new generation of robust, highly<br\/>effective unsupervised and semi-supervised models of meaning<br\/>extraction. Our key methological insights include the use of<br\/>global methods of inference to simultaneously consider many linguistic<br\/>aspects of the task, the use of rich typed lexical dependencies,<br\/>and the semi-supervised use of structured data from the web, such<br\/>as dictionaries, thesauruses, encyclopedias, and so on. We are<br\/>extracting meaning at three levels: word meaning, propositional<br\/>meaning, and conceptual meaning. At the word level, we are learning<br\/>lexical relations like hyponymy (leptin is-a hormone), synonymy,<br\/>and others both from raw text and from structured sources like on-line<br\/>dictionaries. At the propositional level, we are learning<br\/>predicate-argument structure using a global unsupervised clustering<br\/>model as well as developing semi-supervised methods of learning semantic frame<br\/>extractors. At a larger structural level we are inducing scripts<br\/>and structured narrative relations between verbs. These rich models<br\/>of meaning will have a broader impact by providing a critical step<br\/>towards the creation of systems with true language understanding<br\/>capabilities. The results could thus impact the creation of<br\/>natural-language applications in every field, from educational or<br\/>tutorial applications, to information extraction tasks like legal<br\/>discovery, to conversational agents. All deliverables of this<br\/>project will be available on the web: WordNet expansions, induced<br\/>frames and scripts, our temporal event classifier, and semantic<br\/>role, frame, and script inducers.","title":"RI-Small: Unsupervised Learning of Meaning","awardID":"0811974","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["517444","384241"],"PO":["565215"]},"143490":{"abstract":"ANET: Mobius: A Multi-Tier Socially-Aware Network Infrastructure<br\/><br\/>Abstract<br\/><br\/>This research explores the benefits of embedding social knowledge in network protocols and services to support mobile social computing. Specifically, it investigates 1) which mobile social computing problems can be solved with socially aware networks; 2) what social information is amenable to being captured and utilized by these networks (assuming privacy preserving capabilities); 3) what mechanisms and system architectures are necessary to enable dynamic network adaptation to geo-social context changes; 4) how to leverage these mechanisms to design socially-aware protocols and services; 5) how to define and enforce individual and global privacy policies, in general and for accessing social context; and 6) which programming abstractions provide both flexibility and simplicity for rapid mobile social computing application development. <br\/><br\/>This research will lead to a self-organizing, self-adaptive, community-oriented, two-tier network infrastructure for mobile social computing. The mobile human-centric tier runs mobile applications and collects geo-social context information. The peer-to-peer system tier runs services in support of mobile applications and adapts to the geo-social context to enable energy-efficient, scalable, secure, and reliable applications. For large-scale evaluation, this project uses the NSF-sponsored SmartCampus testbed with hundreds smart phones distributed to students.<br\/><br\/>This research will expand the understanding of mobile social computing, an area of great practical relevance to the society at large. To spur the dissemination of results, the source code is made publicly available. Noteworthy educationally is that both institutions, University of South Florida and New Jersey Institute of Technology, are among the national leaders in the percentages of graduates from under-represented groups, and the researchers have specific plans to attract students from these groups to research opportunities in the project. Finally, the foundational results of the project are integrated in an inter-disciplinary studio course that creatively explores design ideas in mobile social computing.","title":"Collaborative Research - ANET: Mobius: A Multi-Tier \\\\Socially-Aware Network Infrastructure","awardID":"0831785","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["432066",382087],"PO":["565090"]},"143270":{"abstract":"Multi-platform Synthetic Aperture Imaging in Complex Environments via Microlocal Techniques<br\/>Multiple advances in diverse fields are expected to transition synthetic-aperture imaging technology from a dedicated single platform requiring an operator to a large number of small platforms operating autonomously. Such a swarm of sensors is expected to provide orders-of-magnitude performance gains relative to a single, dedicated platform. They are also expected to operate in complex environments involving dynamically changing scenes and multiple scattering. Such multi-platform synthetic-aperture imaging systems pose a number of challenges to image formation in addition to those involving wave propagation in complex environments. First, the requirement for scalability implies that the computational resources at each platform are limited and that moreover there may not be perfect phase coherency between platforms. This means the reconstruction algorithms have to be fast, decentralized and be able to handle phase errors. Second, the autonomy of the platforms implies non-ideal conditions from the perspective of image reconstruction: the sensors may be traversing arbitrary trajectories, and transmitting varying waveforms, etc. These challenges rule out the use of standard tomographic methods.<br\/>This research involves developing theoretical foundations and corresponding constructive algorithms to address the challenges of multi-platform synthetic-aperture imaging. The fundamental developments of this project are applicable to all scattered-field-based synthetic-aperture imaging modalities, including RF and acoustics. Central to the project is microlocal analysis. This theory leads to powerful Generalized Filtered-BackProjection (GFBP) techniques that can accommodate non-ideal imaging conditions and wave propagation models for complex environments. This project investigates innovative extensions to microlocal techniques and integrates them with inverse scattering theory and statistical estimation and detection theory. Specifically, the investigators study GFBP algorithms for imaging in dynamically changing and multiple-scattering environments; analytic autofocus methods and fast GFBP algorithms.","title":"Multi-platform Synthetic Aperture Imaging in Complex Environments via Microlocal Techniques","awardID":"0830672","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":[381486,"518474"],"PO":["564898"]},"143281":{"abstract":"Wireless technology offers a unique mixture of connectivity, flexibility, and freedom. It plays an instrumental role in bridging the gap between mobile devices and established communication infrastructures. Today, wireless technology is being embraced with increasing vigor. This trend is reflected in the growing interest in multihop wireless networks. Wireless systems have the potential to fulfill the long-standing promise of pervasive computing and ubiquitous network access. This research initiative seeks to provide a foundation for the next radical advance in information technology: building reliable wireless multihop networks that can support delay-sensitive applications such as voice over internet protocol, video conferencing, remote control and gaming.<br\/><br\/><br\/>The overarching goal of this project is to combine the concepts of queueing analysis and coding theory to enable reliable service for delay-sensitive traffic. An information theoretic perspective on wireless communication has traditionally ignored the bursty nature of practical sources and channels. By focusing on asymptotic throughput limits, information theory often overlooks the role of delay in service quality. Delay distributions, probabilities of decoding failure, and buffer occupancy profiles all have major impacts on the perceived quality of communication links. Understanding the interplay between resource allocation, code design, and system performance necessitates a new mindset and a different system perspective. Queueing-aware communication and code design are especially important in emerging multihop wireless networks, where the adverse effects of channel variations and delay get compounded by the successive relaying of information packets. This project seeks to identify a new methodology for the analysis and the design of wireless communication systems and networks based, in part, on large-deviation theory and novel coding paradigms.","title":"Fundamental Limits in Delay-Constrained Wireless Communication","awardID":"0830696","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["551055","551056",381516],"PO":["564924"]},"143171":{"abstract":"Title: Collaborative Research: Leveraging Low-dimensional Structure for Time Series Analysis and Prediction<br\/><br\/>PI: Christopher J. Rozell, Georgia Institute of Technology<br\/>co-PI: Michael B. Wakin, University of Michigan, Ann Arbor<br\/><br\/>Predicting the behavior of complex systems is central to many tasks of great scientific and national importance, including arenas such as meteorology, financial markets and global conflict. Modern science is ingrained with the premise that repeated observations of a dynamic phenomenon can help in understanding its driving mechanisms and predicting its future behavior. The investigators study methods for improving our ability to characterize and predict such systems even when they are very large (i.e., with many interacting factors) or appear highly unordered (i.e., chaotic systems). This research leverages new mathematical results that enable analysts to efficiently capture the simple structure that is often present even in systems that appear very complex. These results lead to improvements and performance guarantees for heuristic prediction methods based on artificial neural networks, which are often used in practice but can sometimes fail inexplicably.<br\/><br\/>Time series prediction is often approached by postulating a structured model for a hidden system driving data generation. This project borrows from recent advances in low-dimensional signal modeling to advance the state of the art in time series analysis and prediction tools when similar low-dimensional structure is present. For linear systems, this research develops efficient estimation strategies that improve upon classical techniques by encouraging sparse solutions. For nonlinear models, this project builds upon Takens' Embedding Theorem, which states that the image of an attractor manifold can be reconstructed using a sequence of time series observations, to guarantee a quantifiably stable embedding of the attractor manifold. Furthermore, this research aims to improve upon and make performance guarantees for reservoir computing methods, where randomly-connected neural networks have been identified as effective mechanisms for predicting chaotic time series.","title":"Collaborative research: Leveraging low-dimensional structure for time series analysis and prediction","awardID":"0830320","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["501968"],"PO":["564898"]},"143061":{"abstract":"This interdisciplinary research investigates creating dynamic models of biological systems with predictive power that is beyond the capabilities of current generation of descriptive, static models. A biological system such as a cell can be conceptualized as a complex integrated network of biochemical reactions. Metabolic reactions are an important class of reactions performing essential cellular functions such as energy generation, biosynthesis, and harmful waste and byproduct elimination. Predictive models of cellular metabolism offer broad benefits as tools for both basic and applied research. In the context of public health, metabolic models can be used to integrate new laboratory and clinical data on drug efficacy, to compare healthy and diseased tissues, and to predict potentially harmful side effects of new drugs under development. Metabolic models also play an essential role in biotechnology as they enable the design and optimization of genetically engineered microbial cells that produce industrially useful bulk and value-added chemicals (e.g. biofuels).<br\/><br\/>This research focuses on two innovative modeling techniques for metabolic networks. The driving principle for both techniques is integration of structural and functional analyses. The first technique is multi-resolution structural modeling, which combines top-down modularization and bottom-up functional abstraction of individual reactions. The second technique compensates for incomplete information during mathematical modeling. This work investigates a co-estimation of reaction rate law functions and relevant parameters for each dominant reaction set while using noisy data for model calibration. The two modeling techniques and their associated algorithms are tested on experimental data collected from cultures of liver cells (hepatocytes), a representative and well-studied model system with biochemical complexity and relevance to public health. The modeling techniques obtained through this study feature generic aspects, e.g. mathematical abstraction of directed and time-varying interactions, which apply not only to metabolic networks but also other types of important biochemical (e.g. signaling, gene regulatory, etc.) networks.","title":"Multi-Resolution Partitioning and Kinetic Function Estimation for Dynamic Biochemical Network Model Development","awardID":"0829899","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["539728","557200"],"PO":["565223"]},"144392":{"abstract":"NSF and other Federal agencies will require further understanding of concepts, methods, and means for Exascale computing systems before formulation of future programs in enabling technologies, architectures, and methodologies. Such understanding will drive the unknowns to be determined, the challenges to be addressed, strategies to guide, and tasks to be performed on the way to Exascale system realization. The objective of realizing systems capable of sustained Exaflops computing by 2018 suggests that sponsored R&D programs dedicated to this goal be started no later than 2011 to provide sufficient time to complete the ensemble of necessary research, design, and implementation tasks. <br\/>The NSF Point Design Study for Exascale Computing is a collection of three tasks that will look at three different regimes of computing by experts in those areas. While each is stand alone, they will benefit strongly through cooperation and sharing of results, each being guided by the insights derived from the others. These are 1) applications and programming models, 2) system architecture and operating systems, and 3) hardware technology and component design. <br\/><br\/>This SGER will study hardware technology and component design aspect of the Point Design Study for Exascale Computing.","title":"Enabling Technology and Hardware for an Exascale Point Design Study (Exahardware)","awardID":"0836759","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":[385020],"PO":["565272"]},"144040":{"abstract":"Improving driving safety has been a top objective for vehicular ad hoc networks (VANETs). However, the lack of sensing in the absence of cars ahead on the road and frequent network disconnections specific to VANETs could lead to driving hazards: These networks cannot detect dangerous road conditions with good accuracy and cannot guarantee timely propagation of alert messages. To address these problems, this research proposes to merge inexpensive wireless sensor networks (WSNs) with VANETs to create a VANET-WSN symbiotic architecture. In this architecture, sensor nodes are deployed along road sides to detect dangerous road conditions and facilitate timely information sharing among vehicles; in return, VANETs provide richer computation, communication, storage, and power resources to help WSNs overcome their resource constraints. On top of this symbiosis, more effective on-road information systems for safe driving can be deployed. The main objective of this project is to explore the design and implementation of a prototype VANET-WSN symbiotic system and conduct research to test and measure the feasibility and performance of the proposed architecture. Additionally, a number of real applications built and evaluated on top of this architecture help identify and address architectural challenges in supporting application development and deployment. This research is anticipated to produce first-hand experimental results that will lead to a more accurate and comprehensive understanding of the scientific and engineering principles behind the VANET-WSN symbiotic architecture, and thus will contribute to developing new and practical technologies for significantly improved driving safety.","title":"Collaborative Research: CSR-DMSS: On-road Real-time Information Systems for driving safety atop VANET-WSM symbiosis","awardID":"0834585","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383971,"492076"],"PO":["535244"]},"143072":{"abstract":"Project Summary<br\/>Objectives: The objective of our proposal is co-exploration of simulation and design of single or few electron nanoscale quantum devices into binary decision diagram based reconfigurable logic architectures for realization of robust, ultra-low power computing systems. The approach is based on split-gate quantum nanodots as the decision node elements while incorporating functional reconfigurability. This research will primarily focus on the device architecture design to demonstrate its viability in realizing logic architectures with sufficient logic depth, functional robustnesss, and energy delay product approaching the quantum limit.<br\/>Intellectual merit: Key contributions expected from this research are: 1) Simultaneous design and optimization of nanoscale quantum devices to implement system level logic architecture operating with record power-delay product approaching the quantum limit of kT\/q*ln(2) 2) Detailed quantum simulations of single and cascaded devices to facilitate design of reconfigurable BDD-based logic circuits for achieving reliable ultra low power operation. The proposed research is transformative since it will expand our fundamental understanding of physics and operation of compound semiconductor based nanodevices in restricted geometries in novel multiple gate configurations, as well as design and implementation of energy efficient reconfigurable BDD logic architectures.<br\/><br\/>Broader Impact: The results from this research will foster new directions at the intersection of material science, electrical engineering and computer engineering extending mesoscopic devices to logic architectures. The proposed research directly addresses the ongoing quest in the semiconductor industry for longer term solutions to energy efficient technology scaling. Undergraduate and graduate students, involved in this research, will get versatile training in cross-disciplinary areas in preparation of their careers in the burgeoning nanoelectronics industry. The device models, nanofabrication techniques and design methodologies developed will be used in creating courseware which will be available through our website for use by other educators, researchers and industrial practitioners.","title":"EMT\/NANO: Co-Exploration of Device and System Architecture for Quantum NanoElectronics","awardID":"0829926","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["457083","549542"],"PO":["565157"]},"157482":{"abstract":"The use of location-aware devices, such as cell phones with GPS or objects with RFID (Radio Frequency Identification) tags, is exploding in a number of emerging spatio-temporal applications. Traditional database management systems (DBMS) are not designed to handle such applications, especially if the application requires managing a large number of moving objects. The goal of the COMET (Continuous Management of Evolving Trajectories) project is to design, implement, and build a database management system for managing large repositories of continuously evolving trajectories data sets. Since in these environments location updates are issued continually, the DBMS must support extremely efficient methods for dealing with updates. In addition, to allow querying on previous locations of the moving objects, the DBMS must keep track of past trajectories. As time passes these trajectories continue to increase in length, and with large and often increasing number of moving objects, the database size can increase dramatically. Consequently, the backend DBMS must deploy scalable techniques to deal with increasing data sizes, and increasing number of mobile objects. The key focus of the COMET project is on developing efficient and scalable methods for querying on past, present, and future locations of moving objects, and on scalable trigger mechanism in this environment. The expected results of this project may have a strong impact on emerging notification-based applications, such as emergency response systems, in which critical data needs to be disseminated to a physical mobile user based on the user's current and changing spatial location. The project will also train a number of graduate and undergraduate students. The project Web site (http:\/\/www.eecs.umich.edu\/~jignesh\/comet) will be used for making the COMET software, all developed applications, and real user movement data freely available to a broad research community.","title":"COMET: An Efficient and Scalable Trajectory Data Management System","awardID":"0929988","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["533271"],"PO":["563727"]},"143083":{"abstract":"This exploratory study will analyze and understand the ways in which players of World of Warcraft, a popular multiplayer game, engage in creative collaboration. World of Warcraft is a massively multiplayer online role playing game with ten million players worldwide. The proposed research is novel in locating creativity in the context of collaboration in a distributed online space. Most creativity research is laboratory based. It takes the individual as the unit of analysis. This research will examine creativity as a collaborative act, and will investigate creativity in a distributed online context. <br\/><br\/>The research will focus on modding - the creation and distribution of player-created software modifications that extend the game - as an act of creative collaboration. What is the effect of collaboration on creativity? What motivates players to maintain engagement? How does the game software itself support or hinder collaboration? What interaction tools do players use to undertake creative collaboration? What can be learned from creative collaboration in games about mediated collaboration in general? Can these principles be translated to other environments such as work, or does the very context of \"play\" have inherent qualities that cannot be easily translated? <br\/><br\/>The increasing confluence of work and play in games and virtual worlds is a topic of growing interest in industry and the military. The practices of millions of young people are being shaped by participation in multiplayer games. Players will take these practices into the workplace and military service. Investigating how creativity is enabled by collaborative online practices is vital to our understanding of how work and military service can be reshaped to encourage and sustain creative activity in these arenas.","title":"NSF: SGER: Creative Collaboration in an Online Game","awardID":"0829952","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["483548"],"PO":["564456"]},"144062":{"abstract":"Cyber-physical systems will soon become ubiquitous. One of the major challenges that such systems pose is that the control algorithms that modulate the interaction with the physical world have traditionally assumed availability of unlimited computational resources. However, in cyber-physical systems, control tasks are executed on shared processors that can only provide time-varying and uncertain computational resources. Moreover, the real-time scheduling of control tasks provides new difficulties in this context given the sensitivity of control performance to jitter and latency caused by the time varying availability of computational resources. These are new problems that reside at the interface between control algorithms and real-time scheduling policies. This research provides a new joint control and task scheduling approach that maintaining a guaranteed control performance under time-varying processor availability while optimally utilizing the available computational resources. More specifically, this project aims at the design of novel anytime receding horizon control algorithms that provide progressively better performance as more computational time is provided and redistribute the available computational resources online based on the control performance and on the quality of service of other real-time tasks.<br\/>By addressing a fundamental bottleneck in the design of cyber-physical systems, this project directly impacts the society in profound ways. New classes of applications of receding horizon control in real-time environments, such as networked vision-based control and complex SCADA systems will become possible with economic and societal benefits. This project also focuses attention at the challenges at the intersection of control and real-time computation, and thus fosters collaboration between researchers in these two communities, also leading to new educational material.","title":"CSR-EHCS(EHS), SM: Collaborative Research: An Anytime Approach to Real-Time Embedded Control","awardID":"0834771","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["526859"],"PO":["561889"]},"145283":{"abstract":"This workshop brings together topics of robotics and creativity in the context of the premier artificial intelligence conference in the United States. The annual AAAI Conference is a venue that brings together American roboticists and AI experts. For over a decade-and-a-half, the AAAI has held the Robotics Workshop and Exhibition as part of the overall Conference. This established venue provides a unique opportunity for the community to formulate roadmaps that leverage and apply America?s strengths in artificial intelligence to robotics. The 2008 AAAI Robotics Exhibition and Workshop (Chicago July 13-17, 2008) emphasizes multi-disciplinary research in the area of ?Robotics and Creativity?, which will provide novel opportunities to establish new paradigms merging AI and robotics. The workshop will emphasize research in which robots display creativity as well as cases where creativity is combined with design and engineering to stimulate robotics research at a variety of educational levels. A focus on creativity can lead to new paradigms for robotics research. A tangible output of the workshop is a White Paper describing a research roadmap for the community.","title":"Robotics and Creativity Workshop (Part of AAAI 2008)","awardID":"0840358","effectiveDate":"2008-09-01","expirationDate":"2009-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["503250"],"PO":["424970"]},"144073":{"abstract":"Healthcare is 16% of the United States Gross Domestic Product. Technology in the form of medical devices as well as health informatics is critical to improving the quality and reliability of care and the reduction of cost. Medical devices are increasingly controlled by software, and failures in the software can often be life-threatening.<br\/>Medical devices are instances of cyber-physical systems that operate in both the physical and virtual world. The scientific challenge is to demonstrate that the combined cyber-physical system is safe for its intended purpose. The Teleolog project takes on the challenge of certifying the safety of SRI's M7 system for robotic telesurgery. The<br\/>M7 medical robot allows a surgeon to perform telesurgery by manipulating the master controls in order to mobilize two robot arms at the slave station connected through a network. The project takes an explicit, evidence-based approach that leans heavily on formal verification technology for modeling and analyzing the electronic and physical components, the human user, the software control, as well as the interaction between these components. The certification effort reengineers the M7 software to develop a formal assurance case for the safety and resiliency of the system. The models, formats, tools, and certification techniques developed during the certification of M7 can be employed in the certification of other cyber-physical systems. The project also develops course material, case studies, and training kits for educating students in the design and analysis of cyber-physical systems, evidence-based software certification, and resiliency engineering.","title":"Collaborative Research: CSR-EHCS(CPS), TM: Teleolog: Certified Software for Medical Robotics","awardID":"0834812","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":[384064],"PO":["561889"]},"146141":{"abstract":"Humans use intricate vocal and visual orchestrations to encode and communicate intent and emotions. Understanding and utilizing these expressive emotional elements hence is key to facilitating any human experience, including for developing human-centered communication technologies. The vocal instrument is central to the expressive human communication capability. While its use in speaking, singing and other forms of vocalizations have been studied well, the actual expressive mechanisms of vocal productions are less completely understood. For example, what vocal tract mechanisms do humans use to produce speech sounds conveying anger versus happiness? How do singers produce different sounds with different emotions? New technology tools, such as fast magnetic resonance imaging, combined with novel computational capabilities, such as statistical machine learning, offer ways for gaining insights into, and measuring and modeling, these processes in ways that were not possible before. <br\/><br\/>This Small Grant for Exploratory Research focuses on investigating the articulatory vocal production mechanisms of expressive human communication. It has two specific near term goals. The first goal aims at experimental data collection of emotional speech production using real-time magnetic resonance imaging. The goal focuses on pilot analysis of the collected data, both image and audio, and provide insights into the emotional modulation of speech mechanisms and its consequences on the audio signal. The work will provide the necessary foundation for a detailed research study on emotional human speech production. <br\/><br\/>The intellectual merit of the project lies in the use of novel methods for examining and modeling emotional speech production. It aims to discover details of how the human vocal process is modulated to encode emotional expressions and how such knowledge can be incorporated in the design of emotional speech processing and generation by the machine. Expressive aspects of information however have been largely ignored in the technology realm with the primary focus thus far put on content than style; human machine interfaces are limited in their emotional cognizance. The study of emotional human speech promises new directions in human language communication research and its applications. <br\/><br\/>The interdisciplinary approach to the problem leads to its broad impact along several dimensions including the innovative experimental and computational approaches that can impact several disciplines including engineering, computer science, psychology and linguistics, the use of the project as a vehicle for graduate and undergraduate training, and the dissemination of novel imaging data that is hitherto not available in the scientific community.","title":"SGER: Exploring Emotional Vocal Productions Through the Use of Real-Time Magnetic Resonance Imaging","awardID":"0844243","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["515830"],"PO":["565215"]},"146262":{"abstract":"This project studies a new integrated data-driven planning and control scheme for life science automation. Particular attention is paid for the objective of designing a highly robust and intelligent system to achieve the safety and reliability.<br\/><br\/>The research here intends to (i) create a framework for facilitating analytical integration of planning and control, (ii) adopt a two degree-of-freedom controller for robust and intelligent life science automation, (iii) test and evaluate of the obtained results via simulation, (iv) evaluate the proposed glucose controller by a quantitative grading system measure, and (v) establish contact with Professor Alan Permutt at Washington University Medical School to prepare for experimental studies. <br\/><br\/>The expected benefits are (i) an advanced nontrivial planning algorithm to improve safety and reliability of the design of life science system, (ii) an advanced control software for life science automation, and (iii) a science base for the development of automated life science systems. Additionally this research will significantly enhance our general understanding about nonlinear dynamics and control of a complex system and will have a specific impact on the bio-engineering fields in terms of their research and development.","title":"SGER: Robust Intelligent Automation for Life Science Systems","awardID":"0844706","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["399195"],"PO":["564316"]},"137682":{"abstract":"The intent of this research is to develop a wireless mobile computing paradigm consisting of cost-effective wireless broadband network, mobile phones, and relevant applications for underserved urban communities. To date, 3G cellular networks, the proposed deployment of urban-scale mesh networks, and penetration of data-ready mobile phones, provide an initial thrust towards this vision. However, this paradigm invites drastically different user experiences and usage patterns from both traditional personal computers based computing and cellular telephony. Consequently, fundamental research questions arise regarding the design of wireless mobile computing to support users from the targeted communities.<br\/><br\/>This project addresses these questions systematically, leveraging a CRI supported deployment of a large-scale open-access wireless broadband network in Pecan Park, an underserved Latino community in Houston, Texas and the distribution of experimental Wi-Fi capable mobile phones to establish a wireless mobile infrastructure there. The longitudinal study of this community's adoption and use of this network infrastructure will be performed using remote data collection and ethnographic fieldwork. The results will drive an iterative, value-centered optimization of the technology ranging from user-centered mobile computing design to community-driven network management. This approach utilizes multidisciplinary techniques including mobile computing, networking, human-computer interaction, and cultural anthropology to develop new research methods relevant to this emerging research domain. <br\/><br\/>Broader Impact. The experimental deployment in Pecan Park will provide low-cost access to information and communication technologies for its residents. Its success will demonstrate the potential of wireless mobile computing to address the digital divide for our nation's urban poor. It will produce lessons and insights for future deployments of wireless mobile infrastructures in other underserved urban communities, both nationally and internationally. The project will provide research opportunities for undergraduate and graduate students, primarily from underrepresented groups.","title":"HCC-Medium: Collaborative Research: Understanding and Optimizing Wireless Mobile Computing for Underserved Urban Communities","awardID":"0803556","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["548310","548312","490853","467978"],"PO":["565342"]},"148352":{"abstract":"Wireless networks bring mobility to users by allowing them to change location without disrupting connectivity. On one hand, this opens space for a wide variety of location-aware services. On the other hand, it also poses a concern to users as to what extent their location can, or should, be identified in different scenarios.<br\/><br\/>The main focus of the research is to address the proper identification of mobile device locations in wireless networks. There are two sides of the problem: the accurate determination of a mobile device's location in applications such as E911 services, and the effective protection of location information against adversaries which intend to intrude mobile device users' privacy. This project takes a systematic view of both sides of the problem, explores a solution space that spans both ends of wireless communication, and delivers solutions for various application scenarios. The sticking point is to determine the minimum location information necessary for the intended network service by understanding, analyzing, structuring, and controlling the complex interactions among different entities in the system including end-devices, base stations, and (potential) adversaries across application, network, and physical layers. Both control plane and data plane are considered.<br\/><br\/>The intellectual merit of this project is to form a scientific foundation for proper location identification.","title":"Collaborative Research: WN: Proper Location Identification in Wireless Networks","awardID":"0852673","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["464575"],"PO":["564993"]},"139651":{"abstract":"Despite the enormous effort that goes into application design, end users invariably encounter workflow scenarios that are poorly supported by even the most developed, commercial-quality applications. Lack of support for end-user extension increases the burden on application developers to exhaustively anticipate the needs of all end users and then to commit enough resources to address their needs, which often includes re-implementing solutions already found in other applications. This project develops the feasibility of externally retrofitting, without altering, running applications with new interfaces and functionality made possible by using a combination of application-independent pixel-level recognition techniques and more specialized techniques for inspecting data structures exposed by the window system or application. Users will be able to make annotations to those running applications, including hand-drawn ink, typed text, diagrams, interactive widgets, or even links to other application user interface components. Registration techniques will then be researched to associate the annotations with specific elements of an application or document so that such an annotation can be made to appear perhaps only in one place in a specific file, or whenever a certain application runs. This supports a variety of practices, including: integrating functionality from different applications, enriched collaboration, task or user customized interfaces, and adding new fine-grained user interface elements to applications. <br\/><br\/>Broader Impact. Making applications more malleable by embracing customization as a first class and general notion could fundamentally shift application design. Instead of relying solely on inherently slow development of one-size-fits-all applications, end-users could become de facto participants in application design, somewhat analogous to the relationship between traditional print-copy publishing and blogging. Rather than waiting, possibly years, for even simple revisions to fix workflow inefficiencies, users will be empowered to make limited modifications to applications almost as easily as marking up a document, for example by freely rearranging applications? interfaces, or adding diagrammatic annotations displayed over documents which do not natively support them.","title":"HCC-Small: End-User Retrofitting of Applications By Recognizing Text and UI Components","awardID":"0812382","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["448703"],"PO":["565227"]},"137594":{"abstract":"Participants in human-human conversation often entrain to one another, adopting the vocabulary and other behaviors of their partners.<br\/>Evidence of this has been found from laboratory studies and observations of real life situations. We are investigating many types of entrainment in two large corpora of human-human conversations to improve system behavior in Spoken Dialogue Systems (SDS). We want to discover which types of entrainment occur generally across speakers and which seem to be speaker-specific, which types of entrainment can be reliably linked to task success and perceived naturalness, and which types of entrainment can be automatically modeled in SDS.<br\/><br\/>Our research has importance for the construction of better SDS.<br\/>Currently, research SDS have attempted to entrain users to system vocabularies to improve speech recognition accuracy: Since users are likely to employ the same vocabulary in their answers that systems use in their queries, systems have a better chance of recognizing user input correctly if they can predict word usage. However, there has been little attempt to create SDS that entrain to user behavior, despite evidence that human beings rate humans and systems that behave more like them more highly than those that do not. Our work focuses on determining which types of system entrainment to users will be most important to users and most feasible for SDS. Our results will be disseminated through papers and presentations at speech and language conferences. We will also provide publicly available annotated corpora for future research by others.","title":"RI-Medium: Collaborative: Corpus-Based Studies of Lexical, Acoustic-Prosodic, and Discourse Entrainment in Spoken Dialogue","awardID":"0803148","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["511614"],"PO":["565215"]},"139541":{"abstract":"Most engineered artifacts, such as bridges and nuclear power plants, are tested by subjecting them to operating conditions and observing results.<br\/>Software is different. It manifests dynamic behavior when running on computers, and software quality (with respect to achieving specified<br\/>behavior) is normally tested that way. But software also can be considered purely symbolic -- a sequence of instructions -- and hence can be subjected to mathematical proof of correctness. Achieving such \"verified software\" has been identified as a \"grand challenge\" for computing research. The work of this project's interdisciplinary team of software engineers and logicians focuses on the thesis that practical, scalable, automated software verification is feasible, one component at a time, by combining careful language design with recent advances in automated theorem proving. The plan is to evaluate this thesis empirically by generating the logical verification conditions for a benchmark suite of software components like those used in computing courses and commercial software, and proving them automatically. The project's significance will derive from its proof of concept that the verified software grand challenge can be conquered, and from a better understanding of what the next generation of software engineers need to be taught to produce verified software.","title":"CPA-SEL: Collaborative Research - Continuing Progress Toward Verified Software","awardID":"0811748","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["508269"],"PO":["564388"]},"147054":{"abstract":"One of the most critical problems in Internet security is <br\/>the Denial-of-Service (DoS) attack, which aims to make a <br\/>service unavailable to legitimate clients. In this project, <br\/>we consider a sophisticated attack, called service-level DoS <br\/>attack, which is very difficult to identify as malicious <br\/>requests can be made arbitrarily similar to legitimate ones <br\/>and can bypass the network-based defense systems. We propose <br\/>a novel framework to detect the attackers based on the group <br\/>testing (GT) technique which can overcome the limitations of <br\/>current detection approaches. More specifically, this project <br\/>seeks to investigate the following challenges: 1) Dynamic <br\/>threshold model is studied to handle the legitimate bursts <br\/>and variance in the number of clients on each server; 2) <br\/>Legitimates and malicious requests are similar, required <br\/>new testing design without examining each request one by <br\/>one or tightly specifying legitimate behaviors; 3) In <br\/>addition, the study of the proposed model evokes a new <br\/>type of GT, called Size Constraint Group Testing (SCGT) <br\/>which requires an in depth analysis of matrix construction <br\/>complexity. This mathematically rigorous framework helps <br\/>to minimize the false positive and false negative of detection, <br\/>which is the main problem currently for any existing defense <br\/>mechanisms.","title":"SGER: A New Approach for Identifying DoS Attackers Based on Group Testing Techniques","awardID":"0847869","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["432405"],"PO":["564777"]},"139684":{"abstract":"The Social Web is changing the way people create and use information. <br\/>Sites like Flickr, Del.icio.us, Digg, and others, enable users to <br\/>publish and organize content and participate in communities. <br\/>The information they create while interacting with content and <br\/>other users is called social metadata. Tags, one example of social <br\/>metadata, were introduced as a means for individuals to organize their <br\/>content by assigning freely-chosen keywords to it. Some Social <br\/>Web sites now also allow users to organize content hierarchically. <br\/>The photosharing site Flickr, for example, allows users to group <br\/>related photos in sets, and related sets in collections. Although<br\/>social metadata lacks formal structure, it captures the collective <br\/>knowledge of the community. Once extracted from the traces left by <br\/>many users, such collective knowledge will add a rich semantic layer <br\/>to the content of the Social Web. This project will develop a <br\/>probabilistic framework to combine diverse types of social metadata <br\/>to construct a common concept hierarchy. In addition, the methods <br\/>developed by the project will use social relations, in the form of <br\/>community participation, to discover community-specific vocabularies <br\/>and concepts, and identify facets of multi-dimensional concepts.<br\/><br\/>In the future, Social Web sites and data management tools will allow <br\/>users to express ever richer types of knowledge, including complex <br\/>predicates and semantic relations. The ability to aggregate <br\/>individually expressed knowledge into a unified whole will <br\/>transform the way people use information. Global concept hierarchies, <br\/>for instance, can help users visualize how their content relates <br\/>to that of others and allow for more efficient browsing, search <br\/>and discovery. By linking content to a common concept hierarchy, <br\/>the methods developed by the project could also be used to integrate <br\/>disparate data and align it across domains. The proposed work, <br\/>therefore, addresses one of the more important emerging questions <br\/>in AI, namely, how to harness the power of collective intelligence.<br\/><br\/>For further information about this project, please see the project <br\/>Web site at http:\/\/www.isi.edu\/~lerman\/projects\/folksonomy.html.","title":"III-COR-small: Harvesting Concept Hierarchies from Social Data","awardID":"0812677","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["517887"],"PO":["560586"]},"139575":{"abstract":"In the past few decades biochemists have discovered that practically all fundamental molecular design problems are about the geometric relationship of complex molecules. Massively parallel and distributed versions of such algorithms have been recently developed in hope of finding such proteins with shear brute force. For example, implementing screensavers that harvest idle CPU time from PC users worldwide to provide sufficient computing power to solve the protein prediction problems. So far, all these attempts have produced very modest improvements. This project takes a radically different approach: the molecular folding problem is cast as a massively distributed 3D puzzle game, and encourages people to work together with computers to find the solutions to current open problems including cures for cancer, AIDS, and discovery of novel biofuels.<br\/><br\/>The fundamental idea is to employ user-assisted optimization for protein design, and formulate and present it as a competitive game played by thousands of people. The intention in this project is that people will play the game beacause it is fun (it looks like a fun 3d puzzle and not like some biochemistry textbook), it is addicting, it is competitive, it is collaborative (players can work together in groups to solve a problem), has impact (players want to get credit for the drug that cures cancer). The impact of this proposal is in its addressing of a number of fundamental areas: 1) how to best develop games to maximize human ability to discover novel proteins beyond what is currently possible with computation-only approaches; 2) determining the guiding principles of a successful molecular design game; 3) how to best generalize game-development principles to the widest possible range of biochemical problems; 4) revealing what is learned from the way people play the game, and how these strategies could be \"distilled\" towards developing stronger automated approaches.","title":"HCC-Small: Protein Design Through Massively Distributed Video Games","awardID":"0811902","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["549589"],"PO":["564456"]},"139498":{"abstract":"The PLASMA project lays the ground work for a new generation of dense linear algebra libraries that achieve the fastest possible time to an accurate solution on multicore systems by efficiently using all the processors that such systems make available. To work within this design space and leverage the power of million way parallelism, PLASMA researchers are combining new, highly parallelizable algorithms, a programming and execution model that can exploit massive task parallelism, and a flexible memory management facility that helps optimize data locality across a range of different computing platforms. PLASMA's design space is also conditioned by the fact that, in order to support the broadest possible range of computational science problems, the PLASMA library framework must be able to scale both up and down, running at all levels of the platform development chain.<br\/><br\/>The PLASMA project focuses on the following two objectives:<br\/>- Explore new, highly parallelizable algorithms: PLASMA researchers have shown that a wide range of algorithms (e.g. Cholesky, LU and QR factorizations) can be expressed as algorithms by tiles, greatly improving parallel performance of these operations on multicore processors. They are currently extending this concept to a broad range of linear algebra algorithms.<br\/> - Develop an algorithm description abstraction for expressing parallelism: Building on the well established concepts from dataflow architectures, PLASMA researchers are developing high-level abstractions for algorithm description that make task dependencies explicit and therefore facilitate scheduling on multicore systems. Since all the information about parallelism in the algorithm is contained in its dependency graph, which is a Direct Acyclic Graph (DAG), the PLASMA approach replaces a programming language algorithm definition with a graph-based algorithm definition.","title":"Collaborative CPA-ACR-T: PLASMA: Parallel Linear Algebra Software for Multiprocessor Architectures.","awardID":"0811520","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["558541"],"PO":["565272"]},"143370":{"abstract":"Protecting the confidentiality and integrity of sensitive information is central to trustworthy computing. This project focuses on one aspect of the problem, namely, the difficulty of developing software that satisfies critical information flow properties. The approach of secure information flow analysis is to do a static analysis, usually in the form of a type system, on a program prior to executing it, with the goal of proving that it does not leak any information from its high inputs to its low outputs; this is formalized as a property called noninterference. But noninterference is widely recognized to be too restrictive in practice -- often we need to have low output that depends on high input. In implementations, such deliberate leaks of information can be allowed through an explicit declassify construct, which functions like a type cast to circumvent the typing rules. But declassification, while expedient, throws into question what is then ensured by the analysis.<br\/><br\/>One promising disciplined approach to relaxing noninterference is to develop a quantitative theory of information flow that lets us talk about \"how much\" information is leaked. Such quantitative theories are being studied in a variety of contexts, including secure information flow, anonymity protocols, and side-channel analysis, and there is an emerging consensus to base such theories on the concepts of Shannon entropy and mutual information. But a useful theory of quantitative information flow must provide appropriate security guarantees: if the theory says that an attack leaks x bits of secret information, then x should be useful in calculating bounds on the resulting threat. Unfortunately, it can be argued that the standard theories actually fail to provide such guarantees, because a random variable can have arbitrarily large Shannon entropy even if it is highly vulnerable to being guessed.<br\/><br\/>This project will therefore explore an alternative foundation for quantitative information flow based on a concept of vulnerability (closely related to Bayes risk) and which measures uncertainty using Renyi?s min-entropy, rather than Shannon entropy. The goal is to develop the new foundation both theoretically and practically. The main technical challenge will be to develop type-based static analyses that can be used to guarantee that programs satisfy desired quantitative information flow policies. More broadly, this project aims to help to enable the disciplined development of software with guaranteed information flow properties, and to educate students about programming for secure information flow.","title":"CT-ISG: New Foundations for Quantitative Information Flow","awardID":"0831114","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["528350"],"PO":["565264"]},"143491":{"abstract":"To counter the inefficiencies of the current spectrum usage, regulatory bodies, all over the world, are exploring ways to deregulate the spectrum market by<br\/>allowing flexible dynamic spectrum access (DSA) in a broad range of spatio-temporal scale. Recent advances in radio technology have given an impetus to this trend. For DSA to fulfill its promise of economic and societal impact, wireless services based on DSA must be commercially successful, and a tangible spectrum market must evolve that can be supported by technology. This research project will build a realistic DSA architecture for cellular networks supported by appropriate market mechanisms in an integrated fashion that is both technically and economically viable and efficient. This is a truly trans-disciplinary approach spanning the fields of wireless networking and systems, algorithmics, economics, simulation and modeling, which leads to a deeper understanding of the dynamics of the spectrum market by (i) realistic modeling of various market entities (i.e., buyers, sellers, and the market mechanisms), (ii) dynamic spectrum demands and bids based on innovative and realistic population dynamics models, and (iii) new and robust market clearing mechanisms with provable performance guarantees. The results will be validated using large-scale simulations, and experiments on a prototype test bed with reconfigurable radio hardware. In addition to fostering new topics in trans-disciplinary education, this project will offer insights into market driven spectrum sharing, provide useful tools for policymakers, and ultimately guide spectrum policy decisions in DSA technology. This will, in turn, open up new business opportunities in the use of wireless spectrum.","title":"Collaborative Research: NECO: A Market-Driven Approach to Dynamic Spectrum Sharing","awardID":"0831791","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["486433","563568"],"PO":["557315"]},"143271":{"abstract":"Computing approximate solutions to NP-hard optimization problems is an idea that is attractive both in theory and practice. Semidefinite programming (SDP) has become an indispensable tool in this area. It has led to new and better algorithms, and recently, thanks in part to the PI's prior work, more combinatorial algorithms that actually avoid SDP. The connections of SDP to high-dimensional geometry, metric embeddings, fourier transforms, and probabilistically checkable proofs are also at the center of some of the exciting work in theoretical computer science in recent years.<br\/><br\/>The project will work on ideas to (a) apply SDP to design new approximation algorithms for a host of problems, such as graph coloring, metric traveling salesman, graph partitioning, vertex cover; (b) use insights from SDP to design efficient combinatorial algorithms; (c) apply SDP insights (and a ``constructive'' version of SDP duality discovered recently by the PI and his student) to new settings such as decoding error-correcting codes, compressed sensing, and analysing network algorithms and swarm algorithms; (d) explore connections between SDPs, high-dimensional geometry, fourier transforms, and probabilistically checkable proofs.<br\/><br\/>SDP-inspired primal-dual approaches are an important new extension of traditional approaches based upon linear-programming duality, and developing their uses promises to have transformative impact. Development of new approximation algorithms for central problems like graph partitioning and metric TSP may also transform the field.<br\/><br\/>During this project new courses will be designed at the undergraduate and graduate level to teach these new ideas in an accessible way.","title":"New Directions in Semidefinite Programming and Approximation","awardID":"0830673","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["549429"],"PO":["565157"]},"143392":{"abstract":"Considering the popularity and wide adoption of social network systems and the competitive edge these systems provide, there has been a rapid growth in use of these systems to access, store, and exchange personal attribute information in distributed and\/or federated environments and this trend is expected to continue. Efficient, secure, and user-centric techniques are important for the successful deployment of such systems. Our goal in this project is to develop a comprehensive and compelling framework SNGuard (Social Network Guard) that satisfies diverse privacy properties, access control issues, identity management requirements, and usage patterns. The vision of dynamic social networks is a complex and highly sophisticated one that requiring ongoing research and analysis to continue concurrent with the changing role and face of digital information creation and usage including personal information and contents in social networks. The principal intellectual products resulting from this project will be the development of novel frameworks to facilitate user-centered privacy management, content management and risk-aware access control, thereby making SNGuard solutions more trustworthy, more reliable, and less vulnerable. This research effort will have broad societal impact by providing a key mechanism to enable new business and community models for the sharing of personal attributes including identity information to safely, easily, and quickly establish social networking environments in cyberspace. In addition to these potential benefits, other anticipated, broad-based benefits to be facilitated by this research include significant influence to K-12 education, international collaboration, and industrial and government partners.","title":"CT-M: Collaborative Research: Securing Dynamic Online Social Networks","awardID":"0831247","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["563413"],"PO":["497499"]},"143282":{"abstract":"This research takes a multiuser information theoretic approach to investigate innate theoretical<br\/>connections that exist between multiuser source and channel coding, information theoretic<br\/>network security, and combinatorial tree packing algorithms in theoretical computer science. It is<br\/>of compelling interest to the theory as well as the engineering practice of network source and<br\/>channel coding, and network security, in emerging wireless technologies.<br\/>Addressing broad classes of network source and channel models with correlation and<br\/>cooperation, the investigators study explicit and precise characterizations of basic structural<br\/>connections between multiuser data compression, channel coding, network security and<br\/>combinatorial tree packing, and the underlying role of common randomness (i.e., coordinated<br\/>randomization). The research develops information theoretic principles for associated signal<br\/>processing algorithms. It applies source coding and channel coding techniques for providing<br\/>information theoretic network secrecy in encrypted communication, and investigates points of<br\/>contact between combinatorial tree packing algorithms and network security.","title":"Common Randomness, Multiuser Secrecy and Tree Packing","awardID":"0830697","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["561385"],"PO":["564924"]},"143293":{"abstract":"This research involves the design and analysis of efficient algorithms for survivable routing <br\/>in next generation networks. Current research in this area are either heuristic based that <br\/>lacks performance guarantee, or computationally intensive that lacks scalability. The PI <br\/>concentrates on efficient survivable routing in a very important class of network topologies <br\/>known as minimum isolated failure immune networks. Efficient algorithms will be designed for <br\/>survivable routing in such networks. This transformative research will provide a solid <br\/>theoretical foundation for survivable routing in next generation networks.<br\/><br\/>The INTELLECTUAL MERIT of this research lies in exploring a new dimension of research in <br\/>survivable routing in next generation networks, using combinatorial optimization and <br\/>algorithms as tools of investigation.<br\/>The research will extend the frontiers of knowledge in survivable networks in the following <br\/>directions: (1) improving heuristic based or integer linear programming based algorithms <br\/>for survivable lightpath routing in mesh network to polynomial time optimal algorithms for <br\/>survivable lightpath routing in minimum isolated failure immune networks; (2) extending <br\/>heuristic based QoS-aware redundant tree schemes to QoS-aware redundant tree schemes with <br\/>provably good performance; (3) extending heuristic based double-failure protection to <br\/>guaranteed double-failure protection.<br\/><br\/>In terms of BROADER IMPACTS, this research will lead to (i) integration of research into <br\/>educational experiences of students at the undergraduate and graduate levels, especially <br\/>students from underrepresented groups;<br\/>(ii) unification of seemingly diverse disciplines which have common underlying mathematical <br\/>foundation; (iii) dissemination of research results through presentation and publication <br\/>in high-quality conferences and journals.","title":"SING: Efficient Survivable Routing in Next Generation Networks","awardID":"0830739","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7939","name":"WIRELESS COMM & SIGNAL PROCESS"}}],"PIcoPI":["517891"],"PO":["564924"]},"144030":{"abstract":"As the scale and complexity of parallel systems continue to grow, failures are inevitable. For years research focused on pre-failure prediction and tolerance - predicting failures and taking precautionary actions before failure occurrence. Despite progress on failure prediction, unexpected failures occur in practice, especially in modern systems with unprecedented sizes and complexities. Relying on pre-failure prediction and tolerance alone is insufficient for fault management because of the inevitability of failures. Just as failures need to be carefully avoided and managed when they occur, post-failure diagnosis and recovery is of equal importance and has a profound impact on almost every aspect of parallel computing. The goal of this research project is to develop RAPS, a Recovery Aware Parallel computing System for post-failure diagnosis and recovery. The research focuses on how to quickly and effectively resume parallel computing after a failure has occurred. The ultimate goal is to seamlessly integrate post-failure diagnosis and recovery with pre-failure prediction and tolerance as a compound fault management solution for parallel computing. The approach consists of (1) development of new diagnosis mechanisms for fast failure detection and root cause analysis, (2) development of system-wide orchestration for recovery coordination, (3) design of new recovery techniques for quick restoration of parallel applications, and (4) a comprehensive evaluation. The results of this project can significantly improve the productivity of parallel systems. This project also enhances the CS curriculum at IIT and broadens the participation by underrepresented groups.","title":"CSR-PSCE,SM: Recovery Aware Parallel Computing","awardID":"0834514","effectiveDate":"2008-09-01","expirationDate":"2013-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["557489","550730"],"PO":["535244"]},"143073":{"abstract":"Collaborative Research: EMT\/QIS:<br\/>Quantum Algorithms and Post-Quantum Cryptography<br\/>Project Summary<br\/><br\/>Cryptography is the basic infrastructural element enabling privacy and <br\/>security for electronic transactions. However, when a large-scale <br\/>quantum computer is finally built, it will force us to abandon <br\/>established methods of cryptography, such as RSA and Diffie-Hellman, <br\/>which are in common use today. The proposed research will further <br\/>this line of disruptive quantum algorithmic research; but it also aims <br\/>to erect a new framework of secure post-quantum cryptography, in order <br\/>to maintain this societally critical infrastructure.<br\/><br\/>The most attractive approach for salvaging modern cryptography would <br\/>be to develop classical cryptosystems for which we have compelling <br\/>evidence of security even in the face of quantum adversaries. Recent <br\/>work by the PIs and their collaborators has shown that certain <br\/>algebraic problems possess hardness properties relevant even for <br\/>quantum algorithms. We propose to strengthen and leverage these <br\/>results in order to develop cryptographic schemes which can be carried <br\/>out by today's computers, but which will remain secure even against <br\/>quantum attack in the future.<br\/><br\/>In tandem with this effort, we propose to develop new quantum <br\/>algorithms for breaking cryptosystems based on conjugacy in the braid <br\/>group. This is one of the few remaining classical cryptosystems which <br\/>has not already been shown to be vulnerable to quantum attack.<br\/><br\/>Our research program is also directly integrated with graduate student <br\/>training at all four institutions, undergraduate educational <br\/>innovation, educational outreach, and broad scientific dissemination.","title":"Collaborative Research: EMT\/QIS: Quantum Algorithms and Post-Quantum Cryptography","awardID":"0829928","effectiveDate":"2008-09-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["402689"],"PO":["565157"]},"144052":{"abstract":"Cyber-physical networks (CPNs) are tight integrations of computational and physical components with interactions and interdependencies that are much more varied and complex than what occurs in traditional monitoring and control systems. Building on research in software architectures, this project will develop a proof of concept for a novel architecture description language with annotations and views that will provide a unified framework for the design of CPNs. Initial algorithms that leverage the structural features of component-based architectural descriptions of CPNs will also be developed and demonstrated. Using environmental monitoring and control of buildings as the target application domain, existing formalisms and methods will be extended to incorporate a wider set of cyber-physical features aimed at supporting compositional analysis and verification. Prototype design algorithms will be developed in the context of new performance metrics that capture critical features and design tradeoffs among the objectives and constraints for computation, communication and control in CPNs. The concept of networks of verification and design problems derived from multiple views of CPN architectures will also be developed, making it possible to assess and leverage tradeoffs that cut across current cyber-physical boundaries created by disparate mathematical formalisms and dissimilar methodologies in engineering and computer science. As a proof of concept, methods will be developed to compose and iterate verification and design results over a restricted class of networks of interdependent abstractions and problems. These preliminary results will be applied and evaluated in the context of the design of energy efficient buildings. This research will lay the foundations for developing a general set of architectural tools and algorithms for the compositional analysis, design, and verification of CPNs.","title":"CSR-EHCS(CPS),TM: Architectures, Abstractions and Algorithms for Cyber-Physical Networks","awardID":"0834701","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["460548","485947","553646"],"PO":["493916"]},"144063":{"abstract":"While the server-client model has been a successful architecture for many Internet applications such as web and e-mail services, it does not scale with the number of clients. The scalability problem arises from either the computational capability or the bandwidth of a server which limits the number of clients that can be served at any given time. The research will investigate novel network coding techniques that facilitate the development of highly scalable and reliable distributed systems for media storage and streaming over the Internet using the peer-to-peer platform. The research will 1) explore the Hierarchical Network Coding, a novel coding technique for distributed storage and streaming of multimedia data, 2) characterize the necessary data redundancy level for supporting high quality media streaming applications over the Internet, 3) investigate the efficient data regeneration technique for maintaining the high level of data availability in the presence of peer departures, and 4) study the practical network coding based protocols for efficient media delivery via multiple routes. Beyond the technical contributions, the research has several broader impacts. In particular, the research will move toward making network coding a practical approach for increasing performance and efficiency in decentralized networks. This will include the development of publicly available source codes for network coding techniques and protocols. The technologies developed in this research will be used to start new ventures in multimedia delivery via content distribution and P2P networks. The generality of research will enable to other researchers to explore and apply its results to other types of distributed systems. The project will produce several Masters and PhD students with expertise in networking and coding. Also, the project will contribute to fostering the collaboration between networked systems and coding theory researchers.","title":"DMSS: Network Coding Techniques for Scalable Distributed Media Storage and Streaming","awardID":"0834775","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7793","name":"DATA-INTENSIVE COMPUTING"}}],"PIcoPI":["399757"],"PO":["565255"]},"136693":{"abstract":"The principal objective of the proposed work is to research new methods, which provide a means for the efficient modeling and simulation of the behavior of complex bio-polymeric systems. These systems often have important phenomena taking place at multiple spatial and temporal scales (levels). Systems where fine scale (small and rapid, e.g. motions of individual atoms) phenomena contributes significantly to coarse scale (large and slow; e.g. gene expression depends to significant degree on the shape(conformation) of the molecule) behavior are common in macro-molecular processes and are essential to human health.<br\/><br\/>To gain true insight into the behavior and control of important cellular processes, one must understand the physical principles and mechanisms that underlie them. Physics based modeling and simulation will play a critical role towards gaining such an understanding. Unfortunately, these molecular systems are so computationally costly that they cannot currently be modeled and simulated to an adequate level (in accuracy and duration). Because many of the important aspects of these molecular processes change significantly during the process of interest, the model itself must be similarly able to adapt so that it can accurately represent the important process, while remaining fast and cost effective.<br\/><br\/>The proposed work is devoted to this end. This work involves to production of an adaptive, multi-level modeling strategy, utilizing advanced multibody methods. Physics-based internal metrics guide the division of the system model into regions, each with its own local temporal and spatial set of scales. The region boundaries and model types, which may vary from atomistic (fine scale) to continuum (coarsest scale), are then dynamically (adaptively) adjusted, as needed to capture important system behavior at minimum computational cost. The underlying FDCA and ODCA formulations produce equations which are inherently divided into such regions (subdomains). Additionally, the overall structure of these equations are those of a binary-tree, so the resulting formulation is highly conducive to effective parallel computer implementation. <br\/><br\/>The impact of this work will be a great increase in the rate and extent to which such complex molecular dynamic systems may be modeled and analyzed. This will allow the analyst to treat far more complex systems in a more cost, time, and resource effective manner than is currently possible, thus leading to greater understanding. Examples of such systems where the proposed adaptive multiscale strategy should be particularly suitable are biopolymeric systems which include RNA, DNA, and proteins. The proposed framework is expected to provide a means to obtain greater insight into and understanding of key biomolecular processes, which may contribute greatly to our learning to modify and control such processes in the future. Such understanding and ability could significantly impact human health in many positive respects.","title":"Framework for the Adaptive Multiscale Modeling of Biopolymers","awardID":"0757936","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"7478","name":"DYNAMICAL SYSTEMS"}}],"PIcoPI":["508246"],"PO":["564988"]},"137683":{"abstract":"The genomes of individuals alive today are derived from some common ancestor(s) by the actions of mutations and meiotic recombinations. Recombination mixes two homologous chromosomes in an individual to produce a third recombinant, mosaic chromosome, consisting of alternating segments of the two homologs. A recombinant chromosome is then passed on to a child of the individual. Therefore, the derivation of genomes in a current population, from some ancestral genome(s), cannot be represented by a tree, but rather must be represented by a directed acyclic graph, called a Phylogenetic Network or Genealogical Network, or Ancestral Recombination Graph (ARG). Explicitly knowing the historically correct genealogical network that derived extant genomes, or knowing critical features of the network, would greatly facilitate the solution of fundamental problems in biology, and has important practical applications, for example in association mapping in populations, a technique to find genes affecting diseases and important economic traits. However, since we cannot directly examine the past, we must computationally deduce a genealogical network, or features of it, from genomes that we can examine in populations today. The development of algorithms for such computation requires significant interaction of ideas from biology, computer science, graph-theory, mathematics, and algorithm and software engineering. This interdisciplinary project, conducted by computer scientists in collaboration with a population geneticist, is focused on developing efficient algorithms to infer and exploit complex genealogical histories under a variety of biological models of the evolution of genomic sequences and of genetic traits, using different types of existing and emerging biological data. These algorithms will be implemented in software that can be used to study fundamental biological questions, and applied to practical problems such as association mapping of complex traits. The central thesis of the project is that explicit genealogical networks can be efficiently computed, and that these networks capture enough of the true history (even if the networks don?t capture all of it) to allow researchers to more effectively answer fundamental biological questions, and more effectively solve practical biological problems. The project also addresses fundamentally new algorithmic problems and biological applications, driven by new kinds of population variation data that are becoming available, new areas of biology where population data is becoming available, new biological models that have been recently proposed for the evolution of sequences and genetic traits in populations, new understanding of different genomic variations that affect important traits, and biological controversies and questions about the nature (and even the existence) of recombination, and about its role in other biological phenomena. This work will contribute to algorithmic computer science and also to several areas of biology, particularly population genetics. The algorithms and software that will be developed will allow biologists to deduce complex genealogical histories, to better understand the role of recombination, and to address both fundamental biological problems and applied practical problems. The software will be disseminated on the web, along with slides and videos of lectures on the algorithms underlying the software. The project will allow the joint mentoring of graduate students and post-doctoral researchers by advisers from both computer science and biology. The participation of researchers from both computer science and biology makes the research more visible to their respective communities, encouraging other interdisciplinary<br\/>research.","title":"III-CXT-Medium: Collaborative Research: Inference of Complex Genealogical Histories in Populations: Algorithms and Applications","awardID":"0803564","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["518691","447975"],"PO":["565136"]},"146164":{"abstract":"With the proliferation of wireless sensor networks and mobile devices enabled by global positioning systems (GPSs), the volume of real-time geo-referenced streams being collected is large and continues to increase. Individual readings from sensors represent discrete sampling points, whereas the phenomena that sensor networks monitor (e.g., floods, fires, and ocean currents) are often spatially and temporally continuous. <br\/><br\/>This project aims at bridging the impedance mismatch of discrete sensor readings and the continuous phenomena. Specifically, we will explore incremental methods to detect and maintain evolving regions from discrete sensor readings in real time. This task is challenging and risky because (1) human intervention, which is important for region detection, needs to be minimal for the targeted monitoring applications; and (2) the alerting nature requires real-time responses, especially in disastrous situations when volumes of data are often high. The quality of service (QoS) requirement in terms of response time and accuracy of the regions detected needs to be balanced.<br\/><br\/>A novel idea of virtual sensor insertion will be explored to improve the accuracy of region detection. To reduce human intervention, the system will be equipped with a learning ability by using and maintaining statistics needed for incremental polygonization. Measurements in information retrieval will be explored creatively for identifying qualitative region evolvements and creating region evolvement graph, which will result in a reduced number of alerts sent to users.<br\/><br\/>The expected results will bridge the semantic gap of discrete readings and natural phenomena as well as provide a foundation for future work in geo-stream processing. Once the results are integrated into a geo-stream processing system, users can monitor evolving regions without being confined to querying discrete readings. The work will help sustain the growth of and support important time-critical applications such as disaster response and surveillance. Graduate students will be trained on various aspects of geo-stream processing. The project Web site (http:\/\/www.cse.unt.edu\/~huangyan\/eRegion) will be used for results dissemination.","title":"SGER: Detecting and Maintaining Evolving Regions from Spatially and Temporally Varying Observations for Monitoring and Alerting","awardID":"0844342","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["451487"],"PO":["563751"]},"147495":{"abstract":"This proposal requests funding to cover the costs of running a workshop to permit students funded under the NSF FIND program to come together to present their work to each other, discuss potential collaboration, and identify important open problems for future research. The FIND program is distinctive from many other NSF programs, both in its technical emphasis and its approach to research. The FIND program seeks to involve the research community in a collective exploration of what the Internet of 15 years from now should be. This project requires a long-range perspective, the invention of new concepts not fettered by the constraints of today?s networks, an understanding of the larger social, economic and legal issues that arise from the interplay between the Internet and society, and a willingness to collaborate on larger visions that build on the ideas developed in individual research grants. Because of the value that the FIND program places on integration and collaboration, it is important that those working on the parts of the program have opportunities to meet and discuss objectives, approaches and opportunities. The FIND program has held five prior meetings for PIs funded by FIND, as well as others associated with the program. However, the size of these meetings has precluded including students in the meetings. The FIND planning committee, and the community of FIND researchers generally, have said that it is very important to give students an opportunity to get together in the same way that faculty have been encouraged to do. It is important for the careers of the students, especially those who will enter academia when they graduate, and as well important for the health of the FIND program. These students, as they perform their research, will greatly benefit from having in their minds the larger picture of the goals of FIND. This meeting is planned to occur immediately following the ACM SigComm conference in Seattle this August, which many of these students would attend, in order to reduce the cost and overhead of travel.<br\/><br\/>Intellectual merit<br\/>The agenda of this workshop has been centered on the identification and discussion of open questions?aspects of a future network that are not yet being explored as part of the FIND program or elsewhere. While the students will be invited to present their own work (via a poster session and ?lightning talks? in breakout sessions) the challenge that will be put to the working groups of the meeting is to assemble a list of important open problems. Such a list will prove valuable both to the students as they plan their own future research.<br\/><br\/>Broader impact<br\/>The broader impact of this workshop will be to facilitate and further the careers of students, especially students who will enter academia when they finish their graduate work. The objective is to generate excitement and motivation, a sense of purpose and coherence, and a collegial context for a future generation of network researchers.","title":"Workshop: FIND Student Collaboration Meeting","awardID":"0849483","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["565000"],"PO":["565090"]},"145196":{"abstract":"This public health informatics Small Grant for Exploratory Research is a small-scale, exploratory, high-risk proposal that is potentially transformative in its research collaborations between the US and China. The proposal team is building on the momentum gained from two US-China public health informatics workshops held in Beijing in March 2008. New partnerships are expected to emerge. The outcomes have the potential to transform approaches to public health informatics, not only in US and China, but potentially across the globe, in particular in exploring transnational social networks and taking advantage of the kinds of data collection and integration methodologies and technologies employed by the two countries' public health agencies. It affords the opportunity to provide new and longer-term research and educational programs between US and Chinese institutions and practitioners.","title":"SGER: Transnational Public Health Informatics Research: US-China Collaboration","awardID":"0839990","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[387279],"PO":["565136"]},"139630":{"abstract":"III-COR-Small: Efficient Matching for Large Real-World Schemas and <br\/>Ontologies<br\/><br\/>PI: Maria Cruz<br\/><br\/>This project aims at bridging across heterogeneous data and will<br\/>impact applications in multiple fields, including emergency<br\/>management, biomedicine, digital government, and environment. In<br\/>particular, this project extends the state of the art in schema and<br\/>ontology matching, and therefore in data integration, by testing and<br\/>evaluating methods and strategies that establish relationships among<br\/>semantically related concepts in heterogeneous data sources.<br\/><br\/>The following research issues are addressed: (1) Design of methods and<br\/>algorithms for schema and ontology matching that operate at different<br\/>levels of granularity (e.g., concept, structure); (2) Development of a<br\/>prototype of an integrated system that supports the visualization and<br\/>manipulation of large schemas and ontologies in addition to the<br\/>developed matching methods and algorithms; (3) Test and evaluation of<br\/>the above methods and system prototype in terms of their<br\/>effectiveness, including accuracy (precision, recall) and efficiency<br\/>(execution time). From an educational viewpoint, this project impacts<br\/>the design of courses and the research training of graduate and<br\/>undergraduate students, and of a postdoc.<br\/><br\/>Further information about this project can be found at:<br\/>http:\/\/www.cs.uic.edu\/~ifc\/grants\/SchemaOntologyMatching\/","title":"III-COR-Small: Efficient Matching for Large Real-World Schemas and Ontologies","awardID":"0812258","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["554456"],"PO":["560586"]},"137694":{"abstract":"The work to be carried out under this award will develop the needed infrastructure and apply it to study realistic compact binary systems that can give rise to gamma ray burst phenomena. To this end the required physics modules will be incorporated in a new infrastructure framework able to exploit the new generation of high performance computers. The implementations of this infrastructure will allow study of the system in depth, with the goal of predicting and interpreting aspects of observations of electromagnetic and gravitational\u00a0waves. <br\/><br\/>This work serves to advance computational and numerical techniques for an exciting scientific problem, as well as the creation and dissemination of advanced tools for distributed computing. The research to be carried out has as its goal to give an unprecedented description of possible (short) gamma ray burst systems and to develop a new computational model to efficiently utilize petascale computing. Finally, this study involves the training of postdoctoral researchers, graduate students, and undergraduates.","title":"Collaborative Research: Simulating Neutron Star-Black Hole Inspirals: From Binaries to Accretion and Jets","awardID":"0803629","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"7244","name":"COMPUTATIONAL PHYSICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["563330","383142","559222","559222"],"PO":["564326"]},"139641":{"abstract":"In a query-by-humming system, a user sings part of a melody and the computer identifies the songs that contain the melody. In a sign spotting system, a sign language user searches for occurrences of specific signs in a video database of sign language content. These are two example applications where users want to retrieve the best matching subsequences in a time series database given a query sequence. This project is developing methods for efficient subsequence matching in large time-series databases using the popular Dynamic Time Warping (DTW) distance measure. Embeddings are being designed that partially convert the subsequence matching problem into the much more manageable problem of similarity search in a vector space. This conversion allows leveraging the full arsenal of vector indexing and metric indexing methods for speeding up subsequence matching. The proposed methods will be applicable in a wide variety of time series domains, including, e.g., stock market modeling, seismic activity analysis, and sensor-based health monitoring. To showcase the commercial, social, and educational impact of the research, the project will produce three demonstration systems: a query-by-humming system, a handwritten document search-by-keyword system, and a sign spotting system. The results of the research are being integrated into these systems to achieve efficient retrieval in the presence of large amounts of data. The creation and dissemination of large, real-world datasets for these three systems will be an additional contribution of the project.","title":"III-COR-Small: Collaborative Research: Time Series Subsequence Matching for Content-based Access in Very Large Multimedia Databases","awardID":"0812309","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["550881"],"PO":["563751"]},"138794":{"abstract":"Last Modified Date: 08\/04\/08 Last Modified By: C.S. George Lee <br\/><br\/>Abstract <br\/>The goal of this project is to make progress on computational problems that elude the most sophisticated computers and Artificial Intelligence approaches but that infants solve seamlessly during their first year of life. To this end we will develop a robot whose sensors and actuators approximate the levels of complexity of human infants. The goal is for this robot to learn and develop autonomously a key set of sensory-motor and communicative skills typical of 1-year-old infants. The project will be grounded in developmental research with human infants, using motion capture and computer vision technology to characterize the statistics of early physical and social interaction. An important goal of this project is to foster the conceptual shifts needed to rigorously think, explore, and formalize intelligent architectures that learn and develop autonomously by interaction with the physical and social worlds. The project may also open new avenues to the computational study of infant development and potentially offer new clues for the understanding of developmental disorders such as autism and Williams syndrome.","title":"INT2-Large: Collaborative Research: Developing Social Robots","awardID":"0808653","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["552551"],"PO":["543539"]},"147022":{"abstract":"International travel support is requested for U.S. based graduate strudent participants to attend the 2008 International Conference on Data Mining (ICDM 2008), which will be held in Pisa, Italy, December 15-19, 2008. ICDM has established itself as the world's premier research conference in data mining. It provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative, practical development experiences.<br\/>The conference covers all aspects of data mining, including algorithms, software and systems, and applications, as well as related areas such as data management, machine learning and bioinformatics. The conference proceedings are published by IEEE. The conference seeks to continuously advance the state of-the-art in data mining. With the growth of the Web, the Internet, and data intensive technologies such as Sensor Networks, and Bioinformatics, Data Mining is an extremely important area in Information Technology. A strong representation of U.S. researchers and students at the Conference is useful in maintaining U.S. competitiveness in this important area. The total number of ICDM participants in the past has been in excess of 300, with a majority of the participants from the U.S., then Europe and Asia. It is expected to provide scholaships to 15 U.S. based graduate student participants. This grant will partially support the travel costs for the qualified U.S. based graduate student participants.","title":"Proposal for Supporting US-Based Graduate Students to Attend the 2008 IEEE International Conference on Data Mining (ICDM 2008), Pisa, Italy December 15-19, 2008","awardID":"0847772","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["430635"],"PO":["433760"]},"148353":{"abstract":"CAREER: A Theoretical Foundation for Achievability and Optimization in <br\/> Privacy-Preserving Data Mining<br\/><br\/>Data mining has been successfully applied to support a variety of applications, including marketing, weather forecasting, medical diagnosis, and homeland security. Mining data without violating the privacy of data being mined, however, is still a critical challenge. How to mine patients\u00d5 personal information, for example, is an ongoing problem in healthcare applications. Emerging privacy legislation, such as the Health Insurance Portability and Accountability Act (HIPAA), as well as the heightened public concerns about privacy protection, require immediate and resolute attention from the computing community on the protection of private information in data mining.<br\/><br\/>This research involves the understanding, analysis, and optimization of the tradeoff between privacy protection, accuracy of data mining, and system resources in privacy-preserving data mining. The methodology is to establish a solid theoretical foundation that defines the requirements for privacy protection in data mining, identifies the domain of privacy-preserving strategies, and determines the achievability of such strategies. This theoretical foundation enables the design and optimization of privacy-preserving data mining algorithms that are realistic, generic, and efficient. The research results of this project have broader impacts on the nation\u00d5s higher education system and high-tech industries. The ability to mine private data without violating the privacy of data owners is a must for a wide variety of corporations, universities, hospitals, and government agencies. Similarly, theoretically and empirically validated means to protect privacy in data mining would benefit all privacy-concerned individuals at large. The impact of this project also extends to academia through educational efforts, including graduate and undergraduate student training, curriculum development, seminars, and outreach.","title":"CAREER: A Theoretical Foundation for Achievability and Optimization in Privacy-Preserving Data Mining","awardID":"0852674","effectiveDate":"2008-09-01","expirationDate":"2013-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["464575"],"PO":["565157"]},"139531":{"abstract":"A combination of factors has resulted in an industry-wide move to build multiple cores on a single chip to sustain the rate of growth in the computing power of processors. At the same time, high-level programming systems (e.g., scripting languages) have gained tremendous popularity motivated by dramatically higher programming productivity that such systems afford compared to traditional programming languages, such as C\/C++ or Java. Unfortunately, these two trends combine to increase the gap between application software and the underlying hardware, which negatively impacts hardware utilization. An important reason is the critical nature of memory subsystems on the modern multi-core processors and a lack of any systematic methods to derive the memory behavior of programs written in high-level programming systems. This research aims at addressing the problem by developing a theoretical model to estimate the memory behavior of programs written in MATLAB and implementing the model in a prototype compiler.<br\/><br\/>A quantitative metric that past researchers have found useful in memory-related optimizations is reuse distance. Defined as the volume of data accessed between two successive references to a memory location, the goal is to transform programs to lower most of their reuse distances to below a threshold (usually, related to cache size).<br\/>By restricting the computation of reuse distances to locations accessible at the source-level it is possible to define source-level reuse distance. The metric, even though approximate, enables crucial analysis for high-level programming systems that often benefit remarkably from source-level transformations. This research develops efficient algorithms for source-level reuse distance analysis, validates it against the actual program behavior, and implements it in a MATLAB compiler to drive memory-related optimizations, especially those related to array accesses.","title":"CPA-CPL: Addressing the Memory Bottleneck in High-Level Programming Systems","awardID":"0811703","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["487796"],"PO":["565272"]},"139652":{"abstract":"Abstract:<br\/><br\/>The PIs have recently developed a discriminatively trained deformable part based model for detecting objects in images. This model achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. Our entry in the 2007 object detection challenge won in six classes. Modifications since the 2007 challenge have improved our results so that we now outperform the best 2007 challenge results in ten of twenty classes including the person class. These accomplishments rely on a newly developed approach to discriminative training for latent variable models which we call a latent SVM (LSVM). We propose to extend this methodology in a variety of ways. The research will focus on deeper latent information such as subclassification (mixture models), three dimensional pose, and figure\/ground segmentations. They will also use class hierarchies, visual words, and hierarchical object models with parts and sub-parts. We also propose a general methodology for using SVM training to train models, such as geometry-based 3D models, which are highly nonlinear in model parameters. All aspects of this research are strongly tied to empirical performance -- no method will be adopted unless it actually improves the state of the art. The goal has been, and will continue to be, to improve the state of the art through the use of semantically deeper models and improved general purpose machine learning methods.<br\/><br\/>Progress on this project can be found at http:\/ttic.uchicago.edu\/ ~dmcallester\/objects.html","title":"RI-Small: Collaborative Research: Discriminative Latent Variable Object Detection","awardID":"0812428","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["432698"],"PO":["564316"]},"139663":{"abstract":"In many real world scenarios, speech recognition and speaker identification systems must deal with simultaneous speech from several talkers, i.e., speech mixtures representing conversations in natural environments. Users of cochlear implants encounter problems in separating speakers in multi-speaker environments, because of the loss of fine temporal structure. Thus, a crucial preprocessing step for such systems is the segregation of speech according to its constituent sources. The project is the first part of this process which involves the recognition of the number of speakers and the separation of their pitch tracks based on the periodic portions of the speech signal (i.e., voiced regions). Since different speakers have characteristic pitch ranges as a consequence of vocal cord physiology, pitch tracks can be used to help separate the combined signal into different speech streams. Current popular multi-pitch tracking approaches are susceptible to artifacts caused by the interaction between the periodic regions of the different speech signals. Consequently, the periodicity of the combined signal can be different from that of the individual components. The major new idea is the extension of an existent periodicity and pitch estimation process to higher dimensions, arriving at a multi-dimensional periodicity function which is not susceptible to the harmonic interaction artifacts. Preliminary results show that the multiple pitch tracks obtained are accurate even when one speaker is considerably more dominant than the other speaker. The approach is easily generalized to non-speech audio and it should be robust in noisy channels. The outcome of this project will be used in a future project where the actual speech streams will be separated from each other based on the multi-pitch information.","title":"RI: Extension of the APP detector for multipitch tracking and speaker separation","awardID":"0812509","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["508500"],"PO":["565215"]},"139311":{"abstract":"Today, we are faced with three apparently conflicting problems: users are afraid to collaborate unless they have fine-grained control over how their data are accessed by others; many shared environments, especially the new ones, do not offer such controls because of the difficulty of implementing them; those that do offer such controls provide access mechanisms that are difficult to understand and use and result in users being assigned wrong access rights. In this project the PI will investigate the idea of using special-purpose collaborative environments to distribute access, with the goal of developing a general model of access distribution that captures in-use and promising mixed-initiative schemes that have so far been defined only in an application-dependent fashion. The model will be defined using several new kinds of application-independent objects such as access requests and grants that capture the information exchanged in a mixed-initiative system. It will be compatible with existing authorization models including object-based models, in which copies of objects are granted, and rights-based models such as role-based access control, in which (potentially revocable) rights to the object granted. In the PI's approach, the initiative in distributing access rights to shared objects can be taken by information guardians, information consumers, and tools that act as agents of the guardians and the consumers. Information consumers are responsible for sending access requests to information guardians; their agents will (partially or completely) automate this task for them. Information guardians are responsible for authorizing access; their agents will automate this task for them. The PI will identify a general architecture for implementing his model, in which the access-awareness in existing collaboration and communication tools is kept low. In addition, the PI will develop programming abstractions that make it easy to implement the model using the architecture. He will use the abstractions to add mixed-initiative access control in several target systems, which will include both complex widely-used traditional file systems and distributed web services; this experience will help the PI evaluate the programmability of the abstractions. Finally, he will perform field and lab studies to compare alternative approaches to distribute access supported by the general model. <br\/><br\/>Broader Impacts: If successful, this work will open up a new research area focusing on collaborative mixed-initiative access control, and show that collaborative systems are not only a liability for access control but also an asset. Project outcomes will lead to significant improvement in the usability and programmability of fine-grained authorization mechanisms, thereby facilitating a large number of collaborations that would otherwise not take place. They will also afford a better understanding of the similarities and differences between different access distribution schemes and the consequences of using them. In the short term, the project will develop research and teaching software consisting of two main components: layers on top of widely-used file systems that provide several new access distribution schemes, which can be evaluated by usability researchers and demonstrated in classes on security; and programming abstractions allowing the incorporation of these schemes in new shared environments implemented using web services, which can be used in both class and research projects.","title":"HCC-Small: Collaborative Mixed-Initiative Access Control","awardID":"0810861","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["533215"],"PO":["565227"]},"139674":{"abstract":"This project is developing a new foundation for workflow and business <br\/>process management, called the \"artifact-based semantic workflow model\", <br\/>and working in this context to develop new theoretical results, <br\/>techniques and algorithms for specifying, designing, evolving, and <br\/>implementing workflows. The new foundation is based on two fundamental <br\/>premises. The first is to use the artifact-based approach to workflow <br\/>pioneered at IBM Research. This approach is data-centric rather than <br\/>process-centric, and allows to structure workflows according to the <br\/>desired life-cycle of key business artifacts (or entities) that are <br\/>manipulated by a workflow. The second premise is to use techniques from <br\/>semantic web services to enable the declarative specification of <br\/>workflows based on the semantics of the workflow services (or tasks) to <br\/>be performed and underlying goals of the business managers. This is <br\/>fundamentally different from most current approaches to workflow <br\/>specification, which are procedural. <br\/><br\/>A major thrust of the proposed <br\/>research is to develop technical results (e.g., techniques, algorithms, <br\/>and tools) to enable the automated construction of workflows, starting <br\/>from from a specification of the artifacts to be manipulated, the <br\/>individual services that might be applied to them, and a goal to be <br\/>achieved (expressed using a logic formula rather than as a flowchart).<br\/><br\/>More information can be found from the project web page <br\/>(http:\/\/www.cs.ucsb.edu\/~su\/NSF\/0812578).","title":"III-Small-COR: Automatic Construction of Artifact-based Workflows","awardID":"0812578","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["389214","389215"],"PO":["543481"]},"139587":{"abstract":"This is an analytical history project examining FastLane, the pioneering system created to manage review of research proposals submitted to the National Science Foundation. It will advance the history of contemporary computing and contribute to the development of the human-centered computing field. A rigorous evaluative history of NSF?s FastLane is feasible at this moment, owing to accessibility of the designers of the system as well as the distinct communities of users, including NSF legacy users, staff at Sponsored Projects offices, and principal investigators. Substantial advance work has been carried out in contacting NSF staff (concerning both document collection and oral histories) and in designing and developing new research methods that will facilitate collecting qualitative, interview-type data from the large population of varied users. The research explores the human-centered computing (HCC) theme of how government agencies respond to and shape the introduction of new information technologies, and it will develop insights into the design and use of other complex computing systems. The project will also contribute to the interdisciplinarity of HCC by exploring the process of design, models for learning, modes of communication, and the social, cultural, and ethical contexts of computing.<br\/><br\/>Evaluation of the development of FastLane, an early cyberinfrastructure (CI), will yield insights and lessons for contemporary designers, managers, and users of CI. The research will generate insights into the interaction of computing systems with complex cultural and organizational processes. The project will also develop novel research tools for collecting rich qualitative data on these processes, including a web-based interview platform, a Wiki-site designed for the FastLane community, and semi-automated oral-history transcription. The research is carefully designed to permit an evaluation of FastLane?s impact on the participation of diverse researchers and diverse educational institutions in the nation?s research infrastructure. This project will create, analyze, and archive important data on the research participation of several historically under-represented groups, including HBCU institutions and EPSCoR states.","title":"HCC: Small: Designing and Using FastLane: Distilling Lessons for Cyberinfrastructures","awardID":"0811988","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["485962","485963"],"PO":["564456"]},"139477":{"abstract":"In modern computers with multiple CPUs per chip, multiple levels of<br\/>computer memory are shared among the CPUs. This sharing may cause<br\/>varying response times for computer applications, potentially<br\/>affecting their usability. For instance, a video streaming<br\/>application that has unpredictable response times due to interference<br\/>from other applications may be essentially unusable. Thus, the<br\/>computer system must attempt to allocate shared memory resources<br\/>equitably to increase response consistency. Virtual Private Machines<br\/>(VPMs), in which shared computer resources are partitioned to give<br\/>the illusion that a program is running on a single physical CPU<br\/>running in isolation, are one method to provide response<br\/>consistency. This research involves the development of software<br\/>modeling techniques to predict the memory requirements of applications<br\/>run on modern computers and using those predictions to guide memory<br\/>resource allocation to provide consistent, predictable performance.<br\/><br\/>The focus of this research consists of the development of<br\/>reuse-distance-based memory locality analysis for multi-threaded<br\/>applications to predict the resource requirements of those<br\/>applications. This research will apply the new memory locality models<br\/>to manage the cache and bandwidth requirements for multi-threaded<br\/>applications run on a VPM within a multi-programmed<br\/>environment. Specifically, this research will examine reuse distance<br\/>for Partitioned Global Address Space (PGAS) applications written in<br\/>Unified Parallel C (UPC). The research will investigate the effects of<br\/>the number of threads used per application on per-thread data size and<br\/>the effect on reuse distance of shared data caused by thread<br\/>interaction. The result will be novel solutions to memory locality<br\/>analysis for multi-threaded applications while allowing the compiler<br\/>to predict and specify the resources needed to give multi-threaded<br\/>applications targeted performance.","title":"CPA-CPL: Feedback-Directed Resource Management in Virtual Private Machines","awardID":"0811427","effectiveDate":"2008-09-01","expirationDate":"2010-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["382991","530278"],"PO":["565272"]},"139389":{"abstract":"Proposal No: 811082<br\/>Title: Robust Performance Characterization in Complex VLSI Design Under Variations <br\/>PI name: Davoodi, Azadeh <br\/>Institution: University of Wisconsin-Madison<br\/><br\/>ABSTRACT:<br\/>Design of Integrated Circuits is highly impacted by the imperfections of the nano-scale manufacturing process. These imperfections translate into variations in the characteristics of devices and their interconnections. These variations tend to become more random and less systematic as technology further scales down into sub-45nm domain. Only partial statistical information might be known about these variations, such as their average, variance and range. Furthermore, depending on the stage in the hierarchical design flow, correlations in the variability of the components on the chip may be partially known. Given this partial information, robust performance estimation is required to obtain more predictability during the design process. A robust prediction should include all scenarios that match with the partially available variability data. The PI proposes to investigate the novel applications of two statistical and optimization-based approaches for robust modeling of the performance of VLSI circuits. The PI suggests investigating the applicability of these approaches for large circuit sizes and large number of die-to-die and within-die variations at different stages of the design flow.<br\/><br\/>Successful implementation of the proposed research allows a more predictable design flow which can make a significant contribution in the development of next generation Integrated Circuits by shortening design cycles, thereby allowing a faster time-to-market. Reducing the time-to-market can prevent losses of 5-10% magnitude per month in major manufacturing industries. In addition, the PI plans development of a graduate-level course which will likely appeal to the students outside the Computer Engineering area towards meeting their secondary and minor area requirements, at the University of Wisconsin-Madison. This research will catalyze the PI?s efforts to increase the diversity in engineering by encouraging women to study computer engineering and choose it as a career, either at industry or academia.","title":"CPA-DA: Robust Performance Characterization in Complex VLSI Design Under Variations","awardID":"0811082","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["469654"],"PO":["562984"]},"143360":{"abstract":"Information and Communications Technology (ICT) infrastructure failures and cyber attacks are realities that can have catastrophic societal effects. Information Assurance (IA) can be defined as the operations undertaken to protect and defend ICT systems by ensuring their dependability and security. There is a critical need for systematic IA methods that enable ICT systems to adapt and survive any type of disruption or attack. A major hurdle in the development of IA techniques is the lack of models and metrics which enable one to determine the effectiveness of IA mechanisms. This exploratory project seeds a collaborative effort between three PIs at different institutions: Duke University, University of Missouri Kansas-City, and the University of Pittsburgh focused on the development of metrics and models that will allow one to quantitatively study the technical aspects of information assurance (IA) for the network component of the ICT infrastructure. The basis of the approach is to unify attack trees, attack graphs, privilege graphs and fault trees into a common scalable framework with a well defined set of metrics and application scenarios. Extensions of the basic model that include state information, stochastic properties and rewards via Markov chains and stochastic Petri nets, enabling a wider variety of attack and fault scenarios are being studied. The impact of the models and metrics developed is that they provide the techniques and tools necessary to determine the effectiveness of IA mechanisms and allow one to detect bottlenecks and to evaluate the tradeoffs between levels of information assurance, performance and cost.","title":"CT-ER: Collaborative Research: MiMANSaS: Metrics, Models and Analysis of Network Security and Survivability","awardID":"0831055","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["531589"],"PO":["427499"]},"143481":{"abstract":"Collaborative Research:<br\/>A Mathematical Framework for the Performance Evaluation of Large-Scale Sensor Networks<br\/><br\/>Jeffrey P. Kharoufeh and Rusty O. Baldwin<br\/><br\/> <br\/>This grant provides funding for creating and analyzing mathematical models of large-scale wireless sensor networks (WSNs). A WSN is a collection of sensing devices linked via a wireless transmission medium for the purpose of sensing and conveying information about objects, their surroundings, and their interactions, without human intervention. When the information is time sensitive, the network?s performance capabilities are critical to meeting quality-of-service guarantees. However, assessing network performance is difficult because the location of the data is generally not known a priori, or it may not exist in the network at all, sensors have limited energy reserves and fail when these reserves are exhausted, sensors may fail due a harsh operating environment, and communication channel quality may vary with time, impacting the timely delivery of critical data. These characteristics distinguish sensor networks from other communication networks and significantly complicate the task of modeling and predicting their performance over time. If successful, the results of this research will lead to fundamental principles, effective algorithms and optimization schemes that will enhance the design, analysis, control and realization of WSNs. Using a queueing network-based model, specific performance parameters will be investigated including the network energy expenditure, average time required to respond to queries and the query success rate (the proportion of queries that are answered on time). The sensitivity of the network?s performance to query deadline distributions, sensor failures and the network?s environment will also be assessed. Finally, optimization of the network?s parameters will be investigated to ensure resilience in various operating environments.","title":"Collaborative Research: NECO: A Mathematical Framework for the Performance Evaluation of Large-Scale Sensor Networks","awardID":"0831707","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["540691"],"PO":["564993"]},"143250":{"abstract":"Solvent interactions play a critical role in determining the structure and<br\/>function of biomolecular systems. However, accurate modeling is a challenging<br\/>task due to long-range correlations and vast numbers of solvent atoms and ions.<br\/>For very large systems, one method for reducing this expense is the application<br\/>of an implicit solvent model which replaces the explicit atoms and ions with a<br\/>dielectric continuum. One of the more accurate implicit solvent models is<br\/>described by a generalized Poisson equation or Poisson-Boltzmann equation with<br\/>point charge source terms. Although the generalized Poisson approach is<br\/>considered in very many cases to be sufficiently accurate, most current methods<br\/>for approximating its solution have been deemed prohibitively expensive for<br\/>very large systems and for applications requiring rapid repetitive evaluation.<br\/><br\/>This project centers on the creation and analysis of algorithms of unsurpassed<br\/>effectiveness for approximating the solution to the generalized Poisson<br\/>equation (GPE) and its extensions. The approach involves solving for an<br\/>\"effective charge distribution\" given by the Laplacian of the solution<br\/>to the GPE. The corresponding electrostatic potential can be recovered using<br\/>a fast N-body solver. Broader impact of this project will be realized through<br\/>the incorporation of novel algorithms in simulation software. Better methods<br\/>and software in the hands of scientists will enable biomolecular simulation of<br\/>much larger systems with improved accuracy which will result in contributions<br\/>to society. In addition, this project provides an opportunity for a graduate<br\/>student to engage in research that spans computer science, physical science,<br\/>and mathematics.","title":"Collaborative Research: Laplacian-Centered Poisson Solvers and Multilevel Summation Algorithms","awardID":"0830582","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["433979"],"PO":["565157"]},"145560":{"abstract":"Proposal Number: 0841273<br\/>PI: Zhuoqing M. Mao<br\/>Institution: University of Michigan Ann Arbor<br\/>Lead<br\/><br\/><br\/><br\/>Title: NSF Workshop on Unwanted Traffic<br\/><br\/><br\/>The objective of this workshop is to provide a venue for discussion about the important and vexing problem of denial of service attacks, as exemplified by the recent attacks on Estonia and Georgia. Such attacks are all but impossible to mitigate with existing technology. The participants of this workshop will attempt to identify new solutions to prevent and\/or cope with denial of service attacks, perhaps involving fundamentally new network security architecture. Industry involvement in the workshop is vital to its success, as there is a clear need for network agents, such as ISPs to cooperatively identify unwanted traffic and to block such traffic, even if some traffic of their clients is erroneously blocked.<br\/><br\/>The workshop will produce a detailed report to be posted on the NSF web site.","title":"NSF Workshop on Unwanted Traffic","awardID":"0841273","effectiveDate":"2008-09-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["549845"],"PO":["529429"]},"143382":{"abstract":"Modern organizations, such as businesses, non-profits, government <br\/>agencies, and universities, collect and use personal information from <br\/>a range of sources, shared with specific expectations about how it <br\/>will be managed and used. Accordingly, they must find ways to comply <br\/>with expectations, which may be complex and varied, as well as with <br\/>relevant privacy laws and regulations, while they minimize <br\/>operational risk and carry out core functions of the organization <br\/>efficiently and effectively. Designing organizational processes to <br\/>manage personal information is one of the greatest challenges facing <br\/>organizations (see, e.g. a recent survey by Deloitte and the Ponemon <br\/>Institute [TI07]), with far-reaching implications for every <br\/>individual whose personal information is available to modern <br\/>organizations, i.e. all of us.<br\/><br\/>This project responds to these challenges by developing methods, <br\/>algorithms and prototype tools for integrating privacy, compliance, <br\/>and risk evaluation into complex organizational processes. It <br\/>explores, articulates and characterizes formally the scope and nature <br\/>of privacy-expectations of stakeholders as well as those of key <br\/>regulations, such as HIPAA, GLBA, COPPA, BASEL 2, and Sarbanes-Oxley <br\/>(SOX). It incorporates the diverse perspectives and areas of <br\/>expertise of its multidisciplinary research team, which includes <br\/>three computer scientists, one philosopher, and collaborating <br\/>researchers from IBM. This industry connection facilitates <br\/>interaction with product teams that have served complex organizations <br\/>concerned with business process integrity, information security, <br\/>privacy, and information risk management. The research builds on <br\/>\"contextual integrity\" (a philosophical account of privacy) as well <br\/>as language and risk-based methods for privacy policy specification <br\/>and enforcement. Extensive training and educational opportunities are <br\/>provided to undergraduate and graduate students and research results <br\/>integrated into courses at CMU, NYU, Stanford, and UPenn.","title":"Collaborative Research: CT-M: Privacy, compliance and information risk in complex organizational processes","awardID":"0831178","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["475098"],"PO":["565327"]},"144361":{"abstract":"Collaborative Research:<br\/>0836656 (Peter Doerschuk, Cornell University)<br\/>0836649 (Bud Mishra, NYU)<br\/>0836720 (Sanjoy Mitter and Emery Brown, MIT)<br\/>Title: Discovery of Succinct Dynamical Relationships in Large-Scale Biological Data Sets<br\/><br\/><br\/>ABSTRACT:<br\/><br\/>Many types of information in neuroscience and molecular biology can be described as a set of measurements taken repeatedly as some index changes its value. In some situations, such as transcriptomic data measuring gene activities, the index is time while in other situations, such as in genetics association study, the index is position in a genomic DNA sequence and, in any case, the complete collection of data is referred to as a time series. Inference is the process of taking such time series, probably corrupted by errors, and computing answers to the following sorts of questions: (1) What is the system that generated the time series? For instance, if the system is known to be a differential equation of a specific type, what are the parameter values in the differential equation? (2) Given a completely specified system and a time series, did that system generate that time series? For instance, if a biologist has hypothesized a system that describes gene expression for a particular set of genes and then measures expression data, is the data compatible with the system, or equivalently, the hypothesis? (3) Given two time series, were they generated by the same system? For instance, if the pattern of nerve firings in a neural system is recorded in two different experimental situations, is the pattern the same or is it different? The four Principal Investigators are focused on three different biological application domains at three different biological scales: (1) the phenotyping of animal and human ethanol-consumption behavior (whole organism scale), (2) the pattern of action potentials measured on ensembles of neurons (cell-population scale), and (3) the time course of gene expressions as governed by the regulatory circuits of the cell (cellular scale). The types of challenges that are encountered in these applications include the following characteristics: the information is distributed over long periods of time rather than concentrated in time; the systems include delays and feedback paths; and the systems are highly nonlinear, including switching behavior, rather than linear. The major methodologies that will be developed and combined to solve inference problems in these application areas are: (a) information theory and stochastic control, (b) multi-scale approaches to learning the geometry of the data, and (c) computer algebra and symbolic computation. For example, to deal with the presence of delay and feedback in neuroscience systems, especially in the context of the interaction between information and stochastic control, requires a fundamental rethinking of classical information theory as it is employed in technology-based communication systems.<br\/><br\/>As the cost of computing decreases, computing becomes increasingly pervasive. A major purpose of pervasive computing is the real-time collection of high-dimensional time series of very diverse types of data including biological, medical, financial, communication systems status, power systems status, etc. The project will provide computational algorithms and software to analyze this data in more sophisticated ways and thereby extract more sophisticated information. Action taken upon this more sophisticated information, e.g., personalized medicine based on individualized genomic information or more accurate and flexible control of power systems thereby avoiding blackouts, will have important human and economic benefits to society. An important component of the project is educational, e.g., three graduate students working on the project will receive tuition and stipend and an unrestricted number of undergraduates will participate through a variety of ways, e.g., project courses. By attracting talented students to science and technology and providing challenging research experiences, the project will have important work force benefits to society.","title":"Collaborative Research: CDI-Type II: Discovery of Succinct Dynamical Relationships in Large-Scale Biological Data Sets","awardID":"0836649","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7751","name":"CDI TYPE II"}}],"PIcoPI":["417590"],"PO":["562984"]},"145582":{"abstract":"This project creates and evaluates new visualization techniques of models from a concept generator to stimulate the designer to generate concepts not originally posed by designer or computer. This technology is expected to enable the development of creative solutions to design problems that would otherwise go undiscovered. The three challenges to achieving effective visualization for enhancing creative design at the concept generation phase are: 1) how to cluster the many concept variants returned from an automated concept generation algorithm into a manageable set of representative concepts that spans the design alternative space; 2) how to visually represent the option space to the designer so that it enhances creativity; and 3) how to measure the impact of the visualization schemes on designer creativity. This exploratory research, if successful, offers the opportunity to transform how we design products and systems and how we guide any designer to a creative new product.","title":"SGER Collaborative Research: VisualizeIT - Measuring the Impact of IT-Enabled Concept Generation on Designer Creativity","awardID":"0841379","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["540515",388419],"PO":["565227"]},"143041":{"abstract":"Quantum Control of Impurity States in a Semiconductor<br\/>Brage Golding and Mark Dykman<br\/>Michigan State University<br\/>E. Lansing, MI 48824<br\/>ABSTRACT<br\/>Quantum information is a new and rapidly developing interdisciplinary area of research<br\/>that resides at the interface between quantum physics and information science. It<br\/>addresses some of the most challenging fundamental problems in science and technology<br\/>while seeking to advance our understanding of the unique properties of the quantum<br\/>world. Among the predictions in this field is that of an exponential speedup of computers<br\/>that operate on purely quantum principles. If quantum properties are to be exploited for<br\/>qualitatively new capabilities, it is necessary to build systems that exhibit long-lived,<br\/>controllable quantum states. In addition, the devices should be compatible with<br\/>conventional electronics and photonics. The devices based on controlled impurity states<br\/>in semiconductors stand out as particularly attractive, since they should be scalable,<br\/>operate at high speed, and take advantage of existing semiconductor technology.<br\/>The quantum systems studied in this experimental and theoretical research are based on<br\/>acceptors in a semiconductor such as silicon. These are substitutional dopants, whose<br\/>atomic-like properties are well-known and invariant. The two states of a qubit are the<br\/>lowest orbital states of the acceptor, a result of splitting of the ground state by strain and<br\/>electric field. The distance between the energy levels is in the range of a few GHz and<br\/>each qubit is individually controlled by a gate electrode. Single-qubit operations can be<br\/>performed with short microwave pulses whereas two-qubit operations are enabled by the<br\/>long-range dipolar interaction between the acceptors, which allows the qubit-qubit<br\/>distance to be more than 100 nm. The readout of the final state of the system is based on<br\/>quantum nondemolition optical measurement of light scattering by an exciton bound to<br\/>the acceptor.","title":"EMT\/QIS: Quantum Control of Impurity States in a Semiconductor","awardID":"0829854","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["398856",380852],"PO":["565157"]},"143162":{"abstract":"Project Abstract<br\/>The algorithmic complexity (efficiency) of many geometric algorithms depends mainly on the size<br\/>(combinatorial complexity) of the structure to be computed. The Principal Investigators study a<br\/>variety of problems originating in robotics, computer graphics, cellular networking, bioinformatics,<br\/>etc., that belong to this category. To bound the complexity of the corresponding structures, they<br\/>often need sophisticated techniques drawn from several branches of mathematics and theoretical<br\/>computer science. In most cases, the bulk of the work is devoted to the study of arrangements of<br\/>curves and surfaces in Euclidean spaces, which lies at the heart of the field. During the process,<br\/>they develop important new results in several classical mathematical disciplines ranging from Helly<br\/>theory to Tur\u00b4an- and Ramsey-type extremal graph theory. Specifically, the PIs are studying:<br\/>(1) Combinatorial, topological and algorithmic problems related to structures in arrangements<br\/>(lower envelopes and cells, levels, vertical decompositions, incidences with points, etc.) of surfaces<br\/>in higher dimensions.<br\/>(2) Applications of these results to geometric optimization and range searching, to various visi-<br\/>bility and intersection problems in computer graphics, to generalized Voronoi diagrams in higher<br\/>dimensions, to motion planning in robotics, and to many other geometric problems at large.<br\/>(3) Combinatorial, topological, and algorithmic problems involving planar arrangements of seg-<br\/>ments or curves, including graph drawings.<br\/>1","title":"Geometric Arrangements and their Algorithmic Applications","awardID":"0830272","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[381203,381204,381205],"PO":["565157"]},"143283":{"abstract":"Over the past 50 years, error-control coding has been employed with spectacular success by the communications and data storage industries to achieve performance trade-offs that would have been otherwise impossible. What has been recognized only recently, however, is that coding theory could be just as useful in applications other than communications and storage. In particular, this project is concerned with numerous problems that arise in the area of digital circuit design. The problems studied come from a wide spectrum of technologies, ranging from nanoscale circuits and memory chips to more conventional VLSI architectures. In each case, these problems are inherent to the physics of the underlying medium or system. In each case, the project aims to show that sophisticated coding --- based upon methods and ideas deeply rooted in algebraic and combinatorial coding theory --- offers a significant advantage, thereby enabling circuit designers to achieve system trade-offs that would have been otherwise impossible.<br\/><br\/>Specifically, the research carried out in this project can be roughly subdivided into the following four focus areas:<br\/><br\/> Development of new coding schemes for efficient addressing and<br\/> correction of manufacturing defects in next-generation memory<br\/> nano-devices, in particular the nano-wire crossbar;<br\/><br\/> Advanced coding techniques for high-density flash memories, based<br\/> upon ground-breaking recent ideas of floating codes and rank-modulation<br\/> coding;<br\/><br\/> Development of coding schemes to reduce power dissipation and to avoid<br\/> cross-talk in VLSI circiuts, with particular emphasis on both on-chip<br\/> and off-chip buses;<br\/><br\/> Applications to circuit design of the techniques developed in a range<br\/> of well-known combinatorial problems in coding theory, including<br\/> covering arrays, separating codes, intersecting codes, and qualitatively<br\/> independent set families.","title":"Collaborative Research: Coding for Nano-Devices, Flash Memories, and VLSI Circuits","awardID":"0830699","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["518024"],"PO":["564924"]},"144020":{"abstract":"This project aims to develop theory and algorithms for distributed sensing from high-dimensional, non-Euclidean data corrupted with noise and outliers. The development of such a distributed sensing framework faces several critical challenges. For instance, most distributed algorithms such as \"consensus\" proceed by locally averaging low-dimensional Euclidean data to obtain a global average. In most applications, however, data are high-dimensional, and plagued with noise and outliers. Moreover, the goal is not necessarily to average the measurements, but to reach a consensus on a model inferred from the measurements. Since the estimation of such models often involves optimization on manifolds, nearly all algorithms for solving these problems are centralized, and require resources not available in wireless sensor nodes. This project offers a significant paradigm shift in distributed sensing based on novel robust consensus algorithms on manifolds. The first goal is to develop distributed sensing algorithms based on geometric control, graph theory, and machine learning, for processing data related by parameters lying in Riemannian manifolds. The second goal is to develop distributed sensing algorithms based on robust statistics and machine learning, that are robust to noise, and outliers. The third goal is to apply these robust consensus algorithms on manifolds to several distributed localization problems in wireless sensor networks.<br\/><br\/>The development of robust distributed estimation techniques on manifolds can impact many application areas, such as surveillance, security, tele-immersion, space exploration, environmental monitoring, and assisted home living. Such applications require professionals trained at the intersection of hybrid, embedded, and networked systems, robotics, sensor networks, control theory, computer vision and machine learning. The multidisciplinary expertise of the team will foster training at the intersection of these disciplines.","title":"Collaborative Research: CSR-EHCS(EHS), TM: Distributed Sensing via Robust Consensus on Manifolds","awardID":"0834470","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[383914,"530045","555788"],"PO":["561889"]},"144262":{"abstract":"This is a SGER request that addresses both industry's and academia's need for computer professionals with solid foundational knowledge in computational thinking. Declining computer science enrollment at all levels is making such a need hard to meet, with the result that experienced practitioners are not fully realizing their potential because they may lack formal training in computer science. <br\/><br\/>By and large, suitable alternatives to mastery on computing foundation do not exist for the population of computing practitioners. Taking a large number of undergraduate courses in foundational computer science topics is inappropriate because of the time commitment and the fact that such courses are oriented towards students with little experience or domain expertise. Moreover, most graduate courses typically assume this knowledge as a prerequisite. <br\/><br\/>This SGER project proposes to address this critical need by condensing the foundational knowledge into a set of modules that streamlines its presentation and relates it to practical engineering examples, targeting students with some experience in software development. Such a course will help bridge the gap between the practical nature of todays software systems and the foundations needed to apply advanced techniques and theories to solve real problems. <br\/><br\/>Intellectual Merit: This project will address the current gap between engineering practice and educational materials related to computational thinking and computer science foundations by: <br\/>1. Distilling the essential elements of computational thinking for practicing engineers, <br\/>2. Providing ways to relate those concepts to practical applications, <br\/>3. Packaging the material in modules that can satisfy diverse backgrounds and needs. <br\/><br\/>This project team proposes to develop a course that directly addresses the requirements outlined above. Specifically, it will distill and package the set of core, foundational concepts of computational thinking that have stood the test of time and are relevant broadly to professionals working in software development or other disciplines that have a significant computational component, for example computational biology, computational astronomy, etc. <br\/><br\/>An evaluation strategy will be put in place to assess the quality and effectiveness of the project. The two main areas of evaluation will be evaluation of course content, and evaluation of course delivery.","title":"SGER: Computational Thinking for Practicing Engineers","awardID":"0836133","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[384651,"485947","565238"],"PO":["493916"]},"143052":{"abstract":"Researchers are developing computational elements and systems based on nucleic acids for use in biodetection applications. The elements are based on oligonucleotides (short single-strands of DNA) and deoxyribozymes (oligonucleotides that enzymatically act on other oligonucleotides). These elements are organized into information-processing systems as gates, cascades, amplifiers, and larger application-specific circuits. <br\/><br\/>Biodetection applications of the elements and systems being developed include gene expression profiling (in basic molecular biology research), human genetic screening (in public health), pathogen detection (in containment of infectious diseases in the field), and civil defense (in rapid detection of bioterrorism). The direct display devices the researchers are developing provide a read-out that is easy to interpret without specialist training and allows immediate practical decisions to be made; for instance, in the field, health care providers can select individual patient treatment, and public health professionals can select options for containment of infectious disease outbreaks. <br\/><br\/>Researchers are solving four practical problems. To reduce the expense of laboratory implementation procedures, they are studying oligonucleotide interaction models and devising libraries for modular circuit construction. To make the information-processing components interface to the relevant biodetection applications, they are building robust and selective biosensor modules. To make the visual display of biodetection results easy to use, they are experimenting with different display designs, with encoding schemes based on colors and patterns, including alphanumerics. Finally, to make the deployment of devices practical, they are establishing clear and easy to follow procedures for integrating with specific biological applications. <br\/><br\/>As a prototype, researchers are developing a biodetector for the class of mosquito-borne flaviviruses. The prototype allows identification of at least 11 different flavivirus species, among them the Yellow Fever virus and the West Nile virus, using equipment that is readily deployable in the field.","title":"Collaborative Research: EMT\/MISC: Making Molecular Computation Practical for Biodetection Applications","awardID":"0829881","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["550110"],"PO":["565223"]},"143294":{"abstract":"Abstract<br\/>Localization involves the estimation of the precise location of an object based on various forms of relative position information available of the object. Source and sensor localization is a fundamental capability broadly useful in a number of emerging applications. For example a network of sensors deployed to combat bioterrorism, must not only detect the presence of a potential threat, but also must pinpoint the source of the threat. Similarly, in pervasive computing, locating an errant mobile user permits the computer network to identify the most appropriate server with matching capabilities for the user. Likewise, in sensor networks individual sensors must know their own positions, so as to route packets, detect faults, and sense and record events. There is also an emerging multibillion dollar wireless localization industry. This research will address issues that hold the key to fast efficient localization.<br\/>The investigators will adopt a three pronged approach. First, various optimum estimates will be investigated under a variety of practical signal models. These include maximum likelihood and minimum variance estimates. Theoretical performance limits will be determined. Secondly, the investigators will generalize and analyze various algorithms that obtain these estimates efficiently with low complexity. Specifically, the investigators will study optimal localization involving minimization of non-convex cost functions that admit multiple local minima. To overcome the problem of multiple local minima, the investigators will develop a relaxation framework based on convex optimization to obtain fast near optimal solutions. Finally, the proper placement of wireless sensors and anchors impacts both the accuracy and the complexity with which localization is performed. Thus, the investigators will study optimum anchor placement to aid both these attributes. These investigations are critical to the understanding of the theoretical foundation of wireless source localization, and will fundamentally impact its broad applications.","title":"Collaborative Research: Robust Low Complexity Approaches to Source Localization and Sensor Placement in Wireless Networks","awardID":"0830747","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7750","name":"CDI TYPE I"}}],"PIcoPI":["541994"],"PO":["564898"]},"144031":{"abstract":"Over 6.4 million automotive accidents occur in the US annually. Odds of someone being in an accident this year are 1 chance in 16. Any information that warns of problems along the road(s) ahead can therefore potentially save lives and reduce the frequency and\/or intensity of accidents. The vehicle of tomorrow is the programmable-networked vehicle. In our view, the networked vehicle of the future is one of the most complex Cyber Physical Systems (CPS) with active trajectory control, active navigation and on-line maintenance. V2V wireless networks are a special class of networked-CPS where the maximum relative speeds are in excess of 80m\/s, the node density can span over 9,000 vehicles\/mi^2 and, most importantly, the dynamics of the vehicle, the environment, driver reaction and interaction with other vehicles need to be considered in every communication and control decision. To meet these timeliness and coordinated communication requirements, we are developing a new set of networking capabilities that can lay the foundation for dynamic vehicular networks designed to make driving safer, more efficient and more enjoyable. This project is aimed at the design, analysis, implementation and evaluation of vehicular networks that will enable a wide range of applications including V2V and V2I communication for: (a) Bounded-latency broadcast protocols for active networked safety alerts (b) Protocols and algorithms for Real-Time collision avoidance (c) Native protocols for secure V2V and V2I communication. Real-time research in V2V networks will be the first step toward developing a Spatio-Temporal Real-Time theory and network protocols with wide-area time synchronization.","title":"Collaborative Research: CSR-EHCS(CPS), TM: AutoMatrix: Large-scale Test-bed and Real-Time Protocols for Vehicle-to-Vehicle Wireless Networks","awardID":"0834517","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["534440"],"PO":["561889"]},"144042":{"abstract":"Improving driving safety has been a top objective for vehicular ad hoc networks (VANETs). However, the lack of sensing in the absence of cars ahead on the road and frequent network disconnections specific to VANETs could lead to driving hazards: These networks cannot detect dangerous road conditions with good accuracy and cannot guarantee timely propagation of alert messages. To address these problems, this research proposes to merge inexpensive wireless sensor networks (WSNs) with VANETs to create a VANET-WSN symbiotic architecture. In this architecture, sensor nodes are deployed along road sides to detect dangerous road conditions and facilitate timely information sharing among vehicles; in return, VANETs provide richer computation, communication, storage, and power resources to help WSNs overcome their resource constraints. On top of this symbiosis, more effective on-road information systems for safe driving can be deployed. The main objective of this project is to explore the design and implementation of a prototype VANET-WSN symbiotic system and conduct research to test and measure the feasibility and performance of the proposed architecture. Additionally, a number of real applications built and evaluated on top of this architecture help identify and address architectural challenges in supporting application development and deployment. This research is anticipated to produce first-hand experimental results that will lead to a more accurate and comprehensive understanding of the scientific and engineering principles behind the VANET-WSN symbiotic architecture, and thus will contribute to developing new and practical technologies for significantly improved driving safety.","title":"Collaborative Research: CSR-DMSS: On-road Real-time Information Systems for driving safety atop VANET-WSM symbiosis","awardID":"0834593","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["492059"],"PO":["535244"]},"144163":{"abstract":"CDI Type-I: Quantum Diffusion and Quantum RandomWalks<br\/>in Physical Systems<br\/>Alexander Russell (PI), Robin C?ot\u00b4e<br\/>University of Connecticut<br\/>B. Project Summary<br\/>A shocking theoretical discovery of the late 90?s demonstrated that a computational apparatus directly harnessing the laws of quantum mechanics could dramatically outpace any classical computer for a number of important computational problems. This instigated a broad, ongoing effort both to implement such systems and to understand their full computational power.<br\/>This project focuses on quantum random walks and quantum diffusion. Quantum random walks are important algorithmic tools appearing, for example, in the the most efficient known quantum algorithms for basic problems such as element distinctness (that is, the problem of determining if an element?such as a name or a number?appears twice in a long list) and evaluation of certain logic circuits. Quantum random walks are particularly attractive from the standpoint of implementation as they are presumably simpler to faithfully implement than a general purpose quantum computer. In particular, they possess a direct connection to quantum diffusion, an area of ultracold atomic physics. Our goal is to give the first rigorous analysis of a realistic quantum random walk and, thus, a clear indication of a candidate quantum system for implementing such walks.<br\/>Intellectual merit. In this proposal, we intend to join forces of two new subfields of computer science and physics, namely quantum random walks (QRW) and ultracold atomic systems. More precisely, we will explore how the tools developed to investigate quantum random walks in simple models can be adapted to more realistic situations corresponding to physical systems of interest. Conversely, we will study realistic physical systems that could be engineered to correspond to models solvable with quantum random walks.<br\/>These two complementary approaches will lead to insight about the complex underlying behavior of systems where quantum diffusion is crucial. Additionally, a large class of systems in condensed matter physics and<br\/>atomic, molecular, and optical physics ? such as high-temperature superconductors or quantum magnets ? are thought to be described by models, such as the Bose-Hubbard model or one of its many generalizations,<br\/>where quantum diffusion plays a key role. We plan to explore how one can control such systems so that simplified experimental setups, such as ultracold Rydberg atoms or ultracold atom-ion mixtures, can be exploited to mimic the simple models where the solutions to quantum random walks (and thus quantum<br\/>diffusion) are known.<br\/>Broader impacts. We will continue our successful training efforts via the REU program (we have introduced dozens of students to real research topics) and graduate research guidance. Additionally, the PIs will develop and teach a multidisciplinary course entitled ?Quantum information and computation.?<br\/>Within KAST (Kids Are Scientists Too), a 5-day program for 4th through 9th grade students, we will build new modules based on children?s fascination with technologies and computers. Additionally, the PIs will continue involvement in the DaVinci project, a program introducing high-school teachers to topics in<br\/>engineering, mathematics, and physics that can be integrated into their high school science curricula.<br\/>B-1","title":"CDI Type-I: Quantum Diffusion and Quantum Random Walks in Physical Systems","awardID":"0835735","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7750","name":"CDI TYPE I"}}],"PIcoPI":["477856","486281"],"PO":["565157"]},"143074":{"abstract":"Biological systems are the product of an evolutionary process of random tinkering and selection that resulted in unexpected and non-intuitive ?engineering? solutions to dynamically varying conditions. Thus, biological systems are robust, adaptive and evolvable information processing systems that operate asynchronously and in parallel on multiple scales. The examination and characterization of the design principles of biological circuits has the potential to revolutionize biology, medicine and the way computing and communication systems are built. This project is pioneering important advances at the interface between biology and computation by pursuing two complementary goals: (1) to develop a modular, parallel-ready simulator to replicate the multi-scalar architecture of complex biological systems; (2) to discover key design principles relevant to information processing systems in general by reproducing biological design in silico. <br\/><br\/>Information processing by cells encompasses multiple scales connecting molecular events to phenotypes. Current simulation techniques have limited multi-scale and modular capabilities, resulting in models that describe only a single feature of a given system and miss the relationships between architecture, function and behavior. This research effort addresses these limitations by representing biological systems as a hierarchy of functional executable modules. The design of the platform obeys four basic principles: 1) components are objects; 2) objects are governed by rules; 3) rules include some degree of stochasticity; and 4) objects and rules are organized in functional and spatial modules that compose a hierarchy. The development of the new platform is driven by the construction of simulations of key biological model systems with an unprecedented scope and precision, such as bacterial chemotaxis, epidermal growth factor receptor signaling, the acute inflammatory response, and parallel processing by bacterial colonies. The reproduction of these biological systems in silico is providing insights into their design principles, which in turn advances the future design and implementation of distributed technological systems.","title":"EMT\/BSSE: Hierarchical Representation and Simulation of Modular Cellular Systems","awardID":"0829929","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[380937,"490037"],"PO":["565223"]},"144053":{"abstract":"Each year thousands of patient injuries and near-misses are caused by improper or unsafe medical device interactions, because the consequences of device-device and device-human interactions are sometimes poorly understood, leading to hazardous situations. <br\/><br\/>To provide a framework for addressing these challenges, a consortium of medical, industrial, academic, and government bodies have been collaborating through the ?MD PnP? (Medical Device Plug-and-Play) program to identify the broad requirements for the integration of medical devices in high-acuity settings. A subset of these requirements formed the framework for a proposed ASTM Committee F29 standard for the functional requirements of a safe Integrated Clinical Environment (ICE). The committee for this proposed standard is chaired by Co-PI Dr. Julian Goldman. <br\/><br\/>However, there are many open research questions in extending the current standard to achieve the safe and dynamic composition of medical devices. The purpose of this project is to develop formal and medical procedure context sensitive system composition architecture for safe medical device plug-and-play. This project will do more than just ensure interoperability; it will provide decision support and make sure that the composition of medical devices and information systems with different safety levels will result in a verifiably safe MD PnP technologies. We expected the new technologies will be embedded into future standards, leading to a greatly improved medical device system.","title":"CSR-EHCS(CPS),TM: Architecture for the Safe Composition of Complex Medical Systems","awardID":"0834709","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["550264","561355","553686","456699"],"PO":["561889"]},"144174":{"abstract":"This project seeks to develop a new understanding of how the brain represents and manipulates meaning, by bringing together the perspectives of brain imaging, machine learning and computational modeling, using converging approaches from behavioral psychology, linguistics, computer science and neuroscience. In particular, the brain activity that encodes the meanings of words, phrases and sentences is studied, along with how the brain encodes the meaning of individual words in terms of their component semantic features, how it modifies its encoding of an individual word when it occurs within a phrase or clause, and how it constructs the encoding of a phrase or clause from the encodings of its component words. This work builds on recent research showing (1) that repeatable patterns of fMRI activation are associated with viewing nouns describing concrete objects such as \"hammer\" or \"toe,\" (2) that the neural patterns that encode the meanings of these words are similar across different people, and (3) that these encodings are similar whether the person views a word or a picture of the object. Whereas previous work has focused on the neural representation of single words in isolation, this project studies multiple word phrases and sentences, which comprise larger units of knowledge; for example how the neural encoding of a noun is influenced by its adjective (e.g., \"fast rabbit\" vs. \"cuddly rabbit\") and how the neural encoding of a proposition is related to the encodings of its component words (how \"cut\" and \"surgeons\" combine in the proposition \"surgeons cut\"). To address these questions, computational models are developed using a diverse set of training data including fMRI data, data from a trillion-word corpus of text that represents typical language use, and behavioral data from language comprehension and judgment tasks, as well as online linguistic knowledge bases such as VerbNet, and theoretical proposals from the cognitive neuroscience literature regarding how and where the brain encodes meaning. These perspectives are integrated into a theory in the form of a computational model trained from diverse data and prior knowledge, and capable of making experimentally testable predictions about the neural encodings and behavioral responses associated with tens of thousands of words, and hundreds of thousands of phrases and sentences.<br\/><br\/>This project potentially constitutes a significant scientific advance in understanding the relation between brain and mind, impacting a variety of scientific disciplines involved in the study of semantics, including linguistics, psychology, philosophy and cognitive science. A second impact comes from use of the methods and results to understand brain pathologies that involve language disturbances, such as aphasia, dyslexia, and autism. A third impact comes from the development of new statistical machine learning algorithms for analyzing and modeling cross-domain data sets to aid in scientific discovery. Finally, the emerging results and methods will have an educational impact through courses on Brain Imaging, Machine Learning, and Psychology taught by the Principal Investigators, and through a new course to be developed specifically on the topic of \"Neural representations of meaning,\" with materials to be made available on the web.","title":"CDI-TYPE II: From Language to Neural Representations of Meaning","awardID":"0835797","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7751","name":"CDI TYPE II"}}],"PIcoPI":["502390",384335,"533288"],"PO":["564318"]},"143085":{"abstract":"Distributed Coordination for Signal Detection in Sensor Networks<br\/><br\/>Hypothesis testing, often called signal detection, is essential for Engineering and Science while being one of the most important applications of signal processing. Though signal detection has been studied for many years, only recently has energy efficient signal detection been considered<br\/>for sensor networking applications. Energy efficiency appears to be the most critical barrier for sensor networking. Sensor networks hold great promise for solving many important problems including improved monitoring, control and repair of the human body, buildings, bridges, energy production facilities, forests and other critical infrastructure, while also providing important contributions to homeland security, law enforcement, disaster prediction\/avoidance and defense related problems.<br\/><br\/>The investigators intend to demonstrate it is possible to coordinate the operation of multiple dispersed sensors in a distributed manner, without inter-sensor communication, such that significant energy is saved without any loss in detection performance. The key is to jointly consider the signal detection and the communications. This can be achieved by having the sensors with the<br\/>most informative observations transmit first, while either the sensors or the fusion center listen to keep track of the total likelihood of the possible hypotheses. This allows the sensor transmissions to be halted when the evidence for one hypothesis becomes overwhelming. The approach generalizes previous order statistic and sequential testing approaches while optimizing performance in a specific way that models a highly distributed mode of operation for the signal processing, communication, and networking. The goal of this project is to fully understand the theory of energy efficient signal detection for sensor networks employing distributed processing. New approaches<br\/>will be developed and tested with an emphasis on performance and complexity estimation.","title":"Distributed Coordination for Signal Detection in Sensor Networks","awardID":"0829958","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":[380971],"PO":["564898"]},"144064":{"abstract":"This project seeks to develop incremental processing abstractions and<br\/>technologies to address the approaching bottleneck in processing<br\/>unstructured web-scale data. Government, medical, financial, and web-based<br\/>services increasingly depend on the ability to rapidly sift through huge,<br\/>evolving data sets. These data-intensive applications perform complex<br\/>multi-step computations over successive generations of data inflows (e.g.,<br\/>weekly web crawls or nightly telescope dumps). Current approaches to<br\/>processing unstructured data have driven the development of massively<br\/>parallel \"ad-hoc\" data processing systems, such as MapReduce. However,<br\/>they process data in a snap-shot fashion, forcing massive re-computations<br\/>when even a small amount of new data arrives.<br\/><br\/>The core of the project consists of a cluster-based incremental data<br\/>processing system that overcomes these limitations. A key component is a<br\/>dataflow programming model that combines massive parallelism and flexible,<br\/>incremental computations. An incremental processing controller<br\/>orchestrates multiple backend data processing tasks, ensuring reliable,<br\/>consistent operation in the event of node failures. The project seeks to<br\/>shed light on the fundamental challenges and benefits of incremental<br\/>processing for ad-hoc data by using both industrial and e-science<br\/>applications. For example, through cooperation with Yahoo! Research, the<br\/>project will vet existing prototypes on real-world web-indexing dataflows<br\/>and large data sets. While in the short term the project provides a<br\/>platform for such highly skilled operators, the long-term goal is to<br\/>significantly advance the methods and abstractions that the scientific<br\/>community and commercial world use to tackle processing vast, dynamic data<br\/>sets.","title":"CSR-DMSS, SM: Incremental Web-scale Data Processing","awardID":"0834784","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["518657","518658"],"PO":["535244"]},"145164":{"abstract":"Answer set programming is an important, practical declarative programming paradigm widely applied to knowledge-intensive applications. However, the requirement that variables be eliminated through grounding limits modeling and reasoning capabilities, and can yield large, incomprehensible propositional logic programs. This project aims beyond the limitations of answer set programming by merging the traditional stable model semantics with classical logic. The project includes, for example, (i) the study of safety conditions, under which first-order reasoning can be reduced to propositional reasoning, which justifies the use of answer set solvers for grounding-independent reasoning; (ii) the study of loop formulas with variables, which will allow stable models to be computed by first-order theorem provers. The success of this project will have a significant impact on a wide range of domains that requires grounding-independent reasoning, such as Question & Answer systems that use background knowledge, as well as planning and description logics.","title":"SGER: Grounding-Independent Reasoning in Answer Set Programming","awardID":"0839821","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550578"],"PO":[387198]},"145285":{"abstract":"The very success of the industries that produce information technology and the pervasiveness of their products and services in people?s daily lives frequently obscure the mutually reinforcing<br\/>roles that industry, government, and academia play in conducting IT research and its translation into new products, services, and industries. This project through the National Academy of Sciences will capture and present for a general audience such phenomena as how much industry builds on government-funded university research, the long incubation periods sometimes needed for research to be translated into major industry activities, the steady work and funding required to get from initial exploration to large-scale commercial deployment, the interdependencies between research advances in different subfields, and the complex nature of the IT research ecology. Because new major IT industries appear on a regular basis and because our understanding of the key industries and their antecedents continues to evolve, the regular updates that this project will provide will ensure that a current picture that includes cutting-edge products and services is available on an ongoing basis. Earlier depictions by the National Academies of these phenomena figure have been used repeatedly in presentations to congressional staff and other decision makers, have been discussed broadly in the research community, and have been used in economic and policy analyses. This project will provide a useful resource to decision makers, researchers, and the general public on the structure of IT research and the roles of industry, academia, and government in innovation and the creation of IT products, services, and industries.","title":"Depicting Innovations in Information Technology","awardID":"0840364","effectiveDate":"2008-09-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["560890"],"PO":["565136"]},"146143":{"abstract":"Next generation wireless networks will focus on the open <br\/>wireless architecture platform. In such a network, heterogeneous <br\/>wireless systems will be integrated to provide reliable, <br\/>high-bandwidth, on-demand services with performance guarantees <br\/>to a variety of users with di\u00aeerent tra\u00b1c characteristics, <br\/>security requirement, and hardware capabilities. This SGER <br\/>project is exploring three key issues in the resource management <br\/>of such systems. First, utility-based model is investigated to<br\/>deal with access selection and vertical handover decisions in <br\/>which policies governing the network operations are embodied <br\/>in the design of marginal utility functions. Second, a <br\/>utility-based QoS enabling scheduling model is developed in <br\/>which the utility at a given moment can re\u00b0ect the received <br\/>productivity of the system in the long term run and scheduling <br\/>decisions to deal with multiple QoS parameters are made using <br\/>reinforcement learning. Third, an integrated model that measures<br\/>security overheads experienced by security-critical tasks and is <br\/>applicable to the security-aware admission control is developed. <br\/>The proposed research is focused on the development of solutions <br\/>that are critical for next generation wireless and mobile <br\/>communication networking systems. Written reports of research <br\/>findings will be disseminated through academic publications.","title":"SGER: Resource Management in Secure Open Wireless Networks","awardID":"0844247","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["531651"],"PO":["564777"]},"147232":{"abstract":"Node compromise occurs when an attacker through some subvert <br\/>means, gains the control of a node in the network after deployment. <br\/>Once in control of that node, the attacker can alter the node to <br\/>listen to information in the network, input malicious data, cause <br\/>black hole, or any one of a myriad of attacks on the network. <br\/>Generally a node compromise occurs in two ways. The first way is <br\/>called physical\/external attack, when the attacker physically <br\/>accesses or captures a node, and then directly connects the node <br\/>to his\/her computer via a wired connection of some sort. Once <br\/>connected the attacker controls the node by extracting the data <br\/>and\/or putting new data, etc. Most researchers in this area have <br\/>only considered this kind of node compromise attack and many <br\/>methods have been discovered to detect such attacks. The second <br\/>way is called soft\/internal attack, when the attacker compromises <br\/>a node in a network via software\/viruses which can spread over <br\/>the physical communication channels or over the air interfaces. <br\/>The soft attack method is hard to be detected and is the main or <br\/>the only reason that leads to the emergence of compromised nodes <br\/>in some practical applications. <br\/><br\/>This proposed research aims at developing node compromise <br\/>distribution models and their defending considerations for <br\/>Mobile Ad-hoc Sensor Networks. Specifically, the research <br\/>involves an exploratory investigation of modeling attack <br\/>distribution that can help system to estimate the probability <br\/>of a node being compromised and thus defend against it <br\/>effectively and efficiently. We develop several models for a <br\/>node compromise based on different application environments. <br\/>Furthermore, we develop an analytical framework to theoretically <br\/>evaluate our node compromise distribution models. The analytical <br\/>results will be evaluated by extensive simulations and <br\/>experimental measurements.","title":"SGER: Node Compromise Models and Their Defending Considerations in Mobile Ad-hoc Sensor Networks","awardID":"0848468","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["394642"],"PO":["564993"]},"146154":{"abstract":"The University of Central Florida is taking responsibility for the operation of a unique long-path secure laser range facility on Air Force property on Merritt Island, just south of the Kennedy Space Flight Center. This facility, the Innovative Science & Technology Experimentation Facility, (ISTEF), provides unsurpassed options for laser-based experiments in rocket and satellite hyper-spectral imaging, precision laser illumination and ranging, atmospheric propagation across water, swamp and dry land, and other applications. This project plans to locate at this facility several laser-based experimental programs that are long-range extension studies of programs currently ongoing on the UCF campus. They are programs pursuing (i) the propagation of self-channeled femtosecond laser light through the atmosphere for unique propagation and interaction effects, (ii) the development of stand-off optical technologies for trace element explosives detection(iii) high-power eye-safe, (2\u00ecm) laser propagation, ranging and imaging of rockets and satellite instrumentation. The intellectual merit of the proposal is rooted in the new avenues of fundamental science of long range laser propagation and effects that will be enabled by this proposal. This science will have direct benefit to technologies needed by the nation?s intelligence community for national security needs. Providing access to the NCMR university partners, to UCF and other universities will open access to this facility for field station student training in advanced concepts of laser propagation, interaction and effects, spectroscopy, sensing and imaging.","title":"Long Range Laser Measurements and Signatures Intelligence","awardID":"0844277","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"H186","name":"Defense Intelligence Agency"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"H188","name":"DEFENSE INTELLIGENCE AGENCY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"I435","name":"Defense Intelligence Agency"}}],"PIcoPI":["490900"],"PO":["565136"]},"145065":{"abstract":"This project develops and explores the use of mobile technologies to support intergenerational literacy experiences as a way to educate disadvantaged children. The project explores shared reading experiences on mobile phone platforms with local disadvantaged youths in order to develop principles and guidelines for a worldwide effort in distributing reading experiences on inexpensive and pervasive mobile technologies. This research has the potential to transform the use of mobile phones in the direction of literacy development as a complement to their current use for communication. The intellectual merit of this research comprises: an understanding of the design of image, text, and audio content on small screens to support literacy development in children; an understanding of effective collaborative literacy experiences when usability factors of mobile phones are considered in impoverished contexts; and an exploration of the opportunities provided by intergenerational learning when traditional classroom instruction is either not possible or has failed to reach learners with given resources. The broader impact of this project is the potential to focus researchers in other disciplines towards developing literacy, and the availability of the designs and content developed in this project to disadvantaged youths in other cities and countries.","title":"SGER: Designing and Understanding Intergenerational Mobile Learning Communities","awardID":"0839222","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["426438","518152"],"PO":["565227"]},"139620":{"abstract":"With widespread attempts to apply data mining methods to real life<br\/>problems, there is an increasing realization that real life data is<br\/>often multi-relational, involving observations connecting multiple<br\/>entities through a set of relations. A central problem in several<br\/>applications involving multi-relational data is to simultaneously find<br\/>clusters of objects across related entities, e.g., customer clusters<br\/>and related product clusters in e-commerce, movie clusters and related<br\/>user clusters in recommendation systems, communities and shared<br\/>content in social networks, etc. The key novel aspect is that the<br\/>clustering of objects in an entity, such as the set of movies or<br\/>users, depends on its relationships with objects in other entities,<br\/>e.g., users are similar if they like similar movies, and vice versa.<br\/>The primary goal of of this project is to develop a unified<br\/>statistical approach to multi-relational clustering and related<br\/>problems in multi-relational data analysis. Towards this end, the<br\/>project investigates a family of novel statistical multi-relational<br\/>mixture models, with focus on additive and multiplicative models for<br\/>multi-relational clustering. Due to modularity of design, both<br\/>additive and multiplicative models can incorporate domain specific<br\/>semantics as well as automatic model selection using appropriate<br\/>Bayesian priors. Further, the project investigates efficient<br\/>variational inference methods appropriate for discovering latent<br\/>multi-relational clusters.<br\/><br\/>The project significantly empowers the knowledge discovery component<br\/>of data mining. Crucial clues to understanding observed data in<br\/>several disciplines, including social, biological, and information<br\/>sciences, are often spread across multiple related observations. The<br\/>project enables a statistical approach to detecting latent structure<br\/>in such multi-relational data, which is an important step towards<br\/>knowledge discovery from multiple related data sources. The project<br\/>plays an important role in developing closer collaboration across<br\/>disciplines and broaden participation in computer science. Building<br\/>on the increasing awareness regarding the ubiquity of multi-relational<br\/>data, the project contributes to the development of appropriate<br\/>educational material for the next generation work-force. Further<br\/>information on the project may be found at the project web site:<br\/>http:\/\/www.cs.umn.edu\/~banerjee\/multi-relational.","title":"III-COR-Small: Multi-Relational Data Clustering with Probabilistic Mixture Models","awardID":"0812183","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["557452"],"PO":["560586"]},"139631":{"abstract":"This project focuses on enhancing existing document representations with representations of semantic content at a subdocument granularity; representing documents from multiple perspectives; and leveraging the attention and domain knowledge of end-users to improve document representations. This will lead to improvements in the retrievability of documents in future searches. The research will provide one of the first systematic studies of the potential added value of interpretation and labeling efforts contributed by a community of end users. In addition to evaluating the results of this particular approach to improving information retrieval, the work will contribute needed tools for evaluating incrementally-added indexing. An outcome of the research will be evaluation techniques that will be applicable to other research on incrementally-added indexing. The project builds a successful, interdisciplinary collaboration with the national, public, healthcare portal in Denmark, with strong participation from the government partner as well as physicians in Denmark who help to design the studies.","title":"III-CTX-Small: Exploiting domain expertise to enhance information retrieval","awardID":"0812260","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["533018",371153],"PO":["563751"]},"137695":{"abstract":"This research project will incorporate haptics and devices into \"virtual humans\" to investigate interpersonal interactions with medical students and these simulated patients. Prior virtual human work has identified that the lowered engagement level of students and the lack of haptics hampered the fidelity and applicability of virtual human experiences. To enhance human-virtual-human experiences, this project will develop and evaluate mixed reality humans. Mixed reality humans (MRH) in this project will integrate haptics, physical objects, and physiological monitoring to form simulated conversational partners. MRHs will be evaluated in a series of studies with students in the health profession. As part of coursework, students will experience a series of Human-MRH interactions. The impact of MRHs on interpersonal patient scenarios will be evaluated with behavioral, physiological, and self-report measures.<br\/><br\/>The developed MRHs will be integrated into course work of medical, nursing, and physician assistant students. These students will be provided new opportunities to practice interpersonal scenarios associated with intimate exams. Intimate exams tend to be high-anxiety interactions, and include exams of the breast, pelvic, and prostate. Improving student communication skills in intimate exams has the potential to directly impact patient care quality and patient outcomes. It is difficult to provide technology-based experience with these exams with existing technologies; thus there is good potential for impact through development and research of MRHs. This technology will also be integrated into a health exhibit at the Museum of Science and Industry in Tampa, FL. The exhibit will have thousands of visitors interacting with virtual human doctors and patients to receive important health information while interacting with and experiencing the affective capabilities of virtual humans.","title":"HCC-Medium: Mixed Reality Virtual Humans for Training","awardID":"0803652","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["383484","553729",366019],"PO":["565227"]},"139521":{"abstract":"Commodity systems are changing the face of storage and I\/O. Commodity disks<br\/>are cheap, but unreliable. Commodity file systems are widely used, but bug<br\/>prone. Despite these critical problems, people rely increasingly on commodity<br\/>computing systems to store data of critical importance, whether from an<br\/>emotional (e.g., family photos), business (e.g., a customer database), or<br\/>legal (e.g., your tax return) standpoint.<br\/><br\/>The Wisconsin Arrays in Software Project (WASP) tackles the problem of<br\/>building the next generation of robust and reliable commodity RAID<br\/>system. Such a RAID system must be reliable, detecting and recovering from a<br\/>broad range of disk faults correctly. Such a system must also integrate<br\/>properly into existing systems, thus delivering high performance and proper<br\/>recovery in the case of system crashes. Finally, such a system must be<br\/>flexible, enabling new protection strategies to be developed and deployed to<br\/>cope with new problems as they arise. WASP has two major components: the<br\/>first is a RAID design checker known as Sting, which ensures that protection<br\/>schemes correctly protect data from a given set of disk faults. The second is<br\/>a framework to allow the specification and automatic generation of<br\/>sophisticated RAID systems; known as WASP Nest, this part of the project<br\/>promises a new higher-level design methodology for commodity RAIDs.","title":"CPA-CSA: Wisconsin Arrays in Software Project (WASP)","awardID":"0811657","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["550389","550390"],"PO":["565272"]},"139642":{"abstract":"This proposal addresses novel ways of indexing large music databases programmatically. It proposes to use melody, rhythm, and to some extent lyrics to index musical databases. The PI proposes to build a search engine for music that takes sung examples as input for searching relevant songs. The search engine adapts to individual needs through feedback and ongoing personalization to improve the search results. Finding ways to automatically index, label, and access multimedia content (such as music documents) is a research question that is increasing in importance as multimedia databases proliferate and grow.","title":"III-COR-Small: Bootstrapping Adaptive Personalized Music Search with Game-based Collaborative Tagging","awardID":"0812314","effectiveDate":"2008-09-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["485732"],"PO":["563751"]},"139653":{"abstract":"Increased participation in social networking sites (e.g., MySpace, Facebook), discussion sites (e.g., Slashdot, Digg), and contributory social media sites (e.g., YouTube, Wikipedia) has raised questions about the user experience within these online communities. This research will make conceptual contributions to the emerging understanding of participation in online communities; how new users of persistent online communities become integrated, how that participation is sustained, the predictors of exit, and patterns of involvement over time.<br\/><br\/>This project will engage in an in-depth, multi-method analysis of Slashdot, a news and discussion site focused on technology issues, and Everything2, a user-generated encyclopedia with an emphasis on creative expression. Both sites are long-lived, with up to ten years of interaction history, which enables an analysis of longitudinal patterns. Special site access agreements afford an unusually comprehensive level of access to server logs and user data. In the case of Everything2, the agreement also allows interface modifications and creation of multiple experimental conditions within the site. These field experiments will complement more traditional analyses of server logs, user surveys, and interviews. Simultaneously examining two sites with the same set of tools creates opportunities for greater generalization of results and validated measures that can be applied to other online communities in the future.<br\/><br\/>Broader Impact: The growing prevalence of large-scale online communities has changed how people collaborate, generate content, share interest, discuss topics, and maintain relationships. However, little is known about how the design of sites, terms of service, incentive structures, norms or other socio-technical features affect the user experience. Given the centrality that these types of online communities increasingly have in students' online experiences, this project thoughtfully engages pedagogy at all levels with the hook of social computing research. Ultimately the impact of this research will be improved design and maintenance of online communities as robust socio-technical systems.","title":"HCC: Participation Lifecycles in Online Communities","awardID":"0812429","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["529586"],"PO":["564456"]},"139554":{"abstract":"The focus of this proposal is to evaluate the use of instantaneous, chip-wide global information for increasing performance and reducing power and cost in large-scale, chip multi-processors (CMPs). Using global information contradicts conventional thinking because it has been assumed that traversing the entire chip requires many clock periods. Recently developed circuits, however, enable single-cycle, chip-wide signaling, suggesting new possibilities of combining global and local information simultaneously. Further, similar ideas have not been previously implemented in large, traditional multi-chip systems because design flexibility, interconnect configurability, and transistor performance improvements exist only on chip. <br\/><br\/>Preliminary exploration has identified both the required interconnect circuits and opportunities for hardware and software to take advantage of up-to-date global information. The research plans to exploit new circuit techniques that will enable the proposed network-on-chip (NOC)with hybrid interconnect (NOCHI) architecture, which has both a data plane using conventional interconnect techniques and an ultra low-latency control plane with a global interconnect. Because NOCHI ignores established design patterns, understanding the circuit properties is essential for the architectural studies, which include: new algorithms that can better control and regulate power consumption within the network and across cores; improved interaction of the cores with the off-chip memory system to reduce demands on on-chip network resources; the ramifications of global information on cache coherence and management; and software controlled, ultra low-latency efficient synchronization for multicores.","title":"Collaborative Research: CPA-CSA: CMP Architectures with Global Communication","awardID":"0811820","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["502807"],"PO":["559883"]},"139675":{"abstract":"Software engineering emerged out of a need to better address the real-world development of software, transforming the practice of software development from an art, into a craft, and eventually into an engineering discipline. In contrast, the effective application of statistical machine learning algorithms and techniques currently remains an art, based primarily in expert intuition and experience. This project is an integrated program of research and education targeting researcher, student, and practitioner application of statistical machine learning algorithms and techniques as tools for software development. The research focuses on the design of human-centered computing through the investigation of the needs of people applying statistical machine learning as a tool for software development, the creation of new methods and tools supporting the development and deployment of applications that use statistical machine learning, and the evaluation of these new methods and tools in supporting the complex and exploratory task of applying statistical machine learning. There are two primary foci in a new proposed toolset: explicit support for history and experimentation in an iterative and exploratory process and new opportunities for mixed-initiative tools to aid that process. By examining the application of statistical machine learning as a craft, this research enables the potential broad impact of statistical machine learning as a tool for software development in human-centered computing research and applications.","title":"HCC-Small: Investigating and Supporting the Iterative and Exploratory Process of Applying Statistical Machine Learning","awardID":"0812590","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["560940"],"PO":["565227"]},"139323":{"abstract":"In the wake of new sequencing and genotyping technologies, <br\/>whole genome studies are now being undertaken to understand <br\/>the genetic basis of phenotypes. Many of the principles underlying <br\/>the measurement of genotype-phenotype relationships, <br\/>as well as computing related population genetic parameters, <br\/>are relatively well understood. However, the upcoming technologies <br\/>dramatically change the scale and scope of these studies, <br\/>which already encompass tens of thousands of individuals <br\/>over a genome-wide region. The analysis of this data requires novel <br\/>algorithmic and statistical techniques. <br\/><br\/>This project focuses<br\/>on a subset of the problems that could arise in a typical <br\/>whole-genome based association study. These include:<br\/><br\/>(a) Phasing of genotypes into haplotypes using overlapping sequence data, <br\/>and the application of this algorithm to phasing individual human sequences; <br\/>the availability of high coverage long sequence data will make this approach <br\/>the method of choice for phasing in the near future. <br\/><br\/>(b) Fast filtering for pairs of loci that interactively influence a <br\/>phenotype and its application to multiple-locus testing of common <br\/>disease phenotypes. The proposed work reduces the computational <br\/>bottleneck in multiple locus testing.<br\/><br\/>(c) Detection of regions under balancing selection. Available tests <br\/>are focused on detection of regions under positive selection. <br\/>The proposed research looks for evidence of balancing selection in <br\/>the genome, with specific attention on genes associated with bipolar disorder.<br\/><br\/>(d) Reconstruction of regulatory pathways using associations between genetic <br\/>variation and gene-expression.<br\/><br\/>All software from this research is freely available as source-code, <br\/>or as web-tools for academic, research and non-commercial purposes <br\/>in accordance with University policy. <br\/><br\/>Further information on the project may be found at the project web site: <br\/>http:\/\/bix.ucsd.edu\/algen","title":"III-CXT-Small: Algorithmic strategies for genotype-phenotype correlations","awardID":"0810905","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["549885"],"PO":["565136"]},"139444":{"abstract":"CCF-0811287\/CCF-0810865<br\/>Collaborative Research: Trace-Driven Verification of Multithreaded Software<br\/>Zijiang Yang and Karem Sakallah<br\/><br\/>The ever increasing use of hyper-threading and the availability of inexpensive multiprocessor hardware present tremendous opportunities as well as serious challenges for software developers. In order for software applications to benefit from the continued exponential throughput advances in multicore processors, the applications must be well-written multithreaded software programs. Unfortunately, writing multithreaded software programs that can unleash the full potential of present and future hardware systems remains as challenging today as it was thirty years ago. This research aims to develop practical tools and methodologies that can bring down the complexity of testing\/debugging multithreaded programs to a level comparable to that of testing\/debugging sequential programs. To this end, existing debugging tools have to be enhanced with powerful reasoning engines that allow them to implicitly analyze all possible thread interleavings under the specified test inputs. During the course of this project a variety of approaches to achieve this objective will be investigated, including some novel ideas that seem particularly promising from a preliminary analysis: (1) efficient symbolic encoding of multithreaded programs, (2) trace-driven abstraction and refinement of their execution, and (3) performance enhancement techniques that allow this approach to scale to realistic program sizes.","title":"CPA-SEL: Collaborative Research: Trace-Driven Verification of Multithreaded Software","awardID":"0811287","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":[370666],"PO":["564388"]},"139686":{"abstract":"In crisis response domains, emotional manifestations are very complex and extreme emotions are common. While speech technologies have shown significant progress over the years, recognizing and understanding emotional speech in noisy environments is still a big challenge. Understanding such language is daunting given fragmented and ungrammatical utterances in addition to errors<br\/>from automatic speech recognition (ASR). Furthermore, there is little<br\/>research on the analysis of the relationship between emotion detection and language understanding which have traditionally been viewed as parallel independent tasks. Even when the output of one of these tasks is used as an input feature to the other, typically, during training a \"true\" classification is used instead of the \"estimated\" classes (as would be the case if the system were to be used in a real-setting) resulting in a mismatch and degraded performance. This project attempts to overcome such a limitation of current approaches by focusing on analyzing the degree of the dependencies between emotion and intent and investigating joint classification methods via multitask learning for language understanding and emotion detection tasks.<br\/><br\/><br\/>The primary intellectual merit of the project is integrated research on <br\/>developing an end-to-end information processing system that has the potential to very significantly impact the crisis response process. For speech processing, communucation between individuals and emergency dispatch personnel <br\/>as well as communications during responders during response pose a big challenge since callers are typically very emotional and the language used reflects that. Processing such speech requires significant research, and this project can is a first step toward this genre.","title":"RI-Small: Collaborative Research: Dispatcher's Assistant for Emergency First Response","awardID":"0812693","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["515756","499298"],"PO":["565215"]},"139588":{"abstract":"Link analysis algorithms leverage hyperlinks created by authors as <br\/>semantic endorsements between pages, while social bookmarks provide a <br\/>way to leverage annotations by information consumers as a source of <br\/>information about pages. This project explores a novel approach that <br\/>is a synergy of the two: soliciting annotations from users about the <br\/>content of pages, in a way that implicitly forms networks of <br\/>relationships between and among resources and tags. These socially <br\/>generated relationships are then aggregated to build bottom-up, <br\/>global semantic similarity networks. Algorithms are developed to <br\/>construct, analyze, and mine these networks in support of search and <br\/>recommendation applications, exploratory navigation interfaces, <br\/>resource management utilities, tag spam detection, and incentive <br\/>games to accelerate the achievement of critical mass.<br\/><br\/>To extrapolate both annotations about content (tags) and semantic <br\/>relationships (similarity) from single users to the ``wisdom of the <br\/>crowd,'' the project investigates an information-theoretic model that <br\/>extracts semantic assessments from information structures that many <br\/>users are already maintaining, namely the bookmarks and tags they <br\/>manage on their browsers or online. This entails the design and <br\/>evaluation of several network-based measures and algorithms, such as <br\/>similarity, novelty, centrality, and focus. Among the aims of this <br\/>model are the exploration of the duality between resources (URLs) and <br\/>concepts (tags or categories) and the integration of social <br\/>annotation and collaborative filtering. One way to provide users with <br\/>immediate value is to integrate client-based taxonomies and server- <br\/>based folksonomies for social bookmark management. Both traditional <br\/>users of browser bookmarks and social users of online bookmarks can <br\/>take advantage of the same semantic maps while retaining the <br\/>convenience of intuitive browser interfaces and centralized storage.<br\/><br\/>Further information about the project can <br\/>be found at http:\/\/GiveALink.org.","title":"III-COR-Small: Social Integration of Semantic Annotation Networks for Web Applications","awardID":"0811994","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["478203"],"PO":["560586"]},"149389":{"abstract":"This multi-university Industry\/University Cooperative Research Center for Safety, Security and Rescue Research located at the University of South Florida and the University of Minnesota will bring together industry, academe, and public sector users together to provide integrative robotics and artificial intelligence solutions in robotics for activities conducted by the police, FBI, FEMA, firefighting, transportation safety, and emergency response to mass casuality-related activities. The need for safety, security, and rescue technologies has accelerated in the aftermath of 9\/11 and a new research community is forming, as witnessed by the first IEEE Workshop on Safety, Security and Rescue Robotics in February 2003. <br\/><br\/>The Center will be built upon the knowledge and expertise of multi-disciplinary researchers in computer science, engineering, industrial organization, psychology, public health, and marine sciences at the University of South Florida (the lead institution) and the University of Minnesota.","title":"Collaborative Research: I\/UCRC: Safety Security Rescue Research Center (SSR-RC)","awardID":"0856311","effectiveDate":"2008-09-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"2890","name":"CISE RESEARCH RESOURCES"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"T672","name":"LABOR-META-ANALYSIS OF HISTORI"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"5761","name":"INDUSTRY\/UNIV COOP RES CENTERS"}}],"PIcoPI":["522051"],"PO":["565057"]},"142690":{"abstract":"A key challenge in computational neuroscience is to understand how the central nervous system processes and stores sensory information. Sensory systems process information by means of a complex neural circuitry. One of the major problems these systems must solve is how to identify 3-dimensional objects when sensory information about these objects is 2-dimensional, such as visual information from the retina. This research project investigates a special sensory processing system that is particularly suitable for answering questions about how complex objects are interpreted by a 2-dimensional sensory processing sheet. The system of interest is the electrosensory system of mormyrid weakly electric fish, in particular the lateral line lobe (ELL) in the hindbrain. Mormyrid fish employ an alternative sensory system during their nocturnal activity phase: active electrolocation. This allows the fish to detect and recognize 3-D objects in the absence of light.<br\/><br\/>This project will develop a stimulus apparatus that will provide the means to experimentally test the predictions of models that simulate neural activity in the 2-dimensional ELL. The stimulus apparatus will enable the exploration of new experimental questions of interactions between different locations on the skin serface. The model will extend previous studies of single cell activity to include interactions across the 2-dimensional sheet of neurons in the ELL. This modeling and engineering work will improve the coupling between theory, physiological experiment, and behavioral phenomenology.<br\/><br\/>The computational aim of this project is to model the spatiotemporal activity patterns in ELL that result from electrical images on the skin. Previous models of ELL will be extended to represent a sheet of interconnected neurons. The model will incorporate presently known anatomy and physiology of the system such as spike-timing dependent plasticity. This modeling project will advance understanding of how homeostatic synaptic learning can be controlled by the changing state of the sensory system.<br\/><br\/>The engineering aim of this project is to develop and distribute to the labs of collaborating experimenters a stimulus environment that will be able to present precisely-controlled spatial-temporal electrical images to the mormyrid electric fish. This electrosensory stimulus apparatus will overcome the limitations of current experimental paradigms, and thus help verify and challenge sensory processing models. The algorithms embedded in those models are basic to many sensory systems, and modeling the electrosensory system in mormyrids will refine the understanding of their implementation and function.","title":"Learning and processing of electrosensory patterns in mormyrid electric fish","awardID":"0827722","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["562760","494324","455545"],"PO":["564318"]},"143460":{"abstract":"Despite the clear success stories of wireless networks in the public, commercial, and governmental realms, recent work has shown that poor routing\/scheduling can significantly impair their performance. We note that the unacceptable performance loss observed is not a consequence of a lack or deficiency of scheduling\/routing algorithms in the literature; rather the cause is the information structure (channel state, queue state, topology, etc) that these algorithms assume. This proposal steers down a different path, initiating a conceptual shift toward the primary importance of information structure. We consider practical scenarios where only a small fraction of the network state can be explored. The goals of this proposal are two-fold: (a) characterizing the fundamental impact of partial\/delayed network state information (NSI) on network throughput and other performance metrics such as delay and reliability, and (b) developing high-performance and distributed algorithms that can operate optimally subject to partial information. <br\/>Broader Impact: We believe this work can contribute towards furthering basic network science required to design high-performance scheduling and routing with limited NSI. We also plan to bring some of the questions both as interesting undergraduate projects, as well as develop a new graduate-level course on the mathematics of the design of communication networks.","title":"Collaborative Research: NEDG: Network Scheduling and Routing under Partial Information Structure","awardID":"0831580","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["541981"],"PO":["557315"]},"143471":{"abstract":"Most of the Internet's complexity resides in software running on Internet routers. Bugs in this software are a highly critical problem, leading to a number of recent high-profile attacks and outages, and are increasingly becoming a bottleneck in building highly reliable networks. The PIs are designing and evaluating techniques to make the Internet resilient to software bugs. Their approach consists of two key components. First, they are building a highly reliable single instance of a network router. This involves performing a characteristic study of bugs in router software, by using static and dynamic code analysis and by taxonomizing publicly disclosed vulnerabilities. They also apply and extend techniques such as rollback, reordering inputs, microreboots, and automated debugging to construct a software router resilient to implementation bugs. Second, the PIs are developing and building an architecture for highly-available bug-resistant networks. Their design leverages the principle of \"control and data diversity\", which simultaneously runs multiple functionally-equivalent instances of a piece of software or data. Each instance is changed from the others in a way that makes it unlikely multiple copies will simultaneously undergo the same bug, for example by randomizing the execution environment, having each instance be responsible for a subset of routes, or by having different programmers implement each instance. In addition to producing designs and algorithms that enable these networks, the PIs will also make available tools and implementations to enable their use. <br\/>Successful completion of this project will significantly improve the Internet's ability to avoid and recover from failures.","title":"NeTS-NECO: Collaborative Research: Fixing the Reliability Problem in Network Software From its Root","awardID":"0831646","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["531675"],"PO":["565303"]},"146991":{"abstract":"Current networking protocols have been designed with the assumption <br\/>that an end-to-end path between source and destination is almost <br\/>always available. However, this assumption does not hold for an <br\/>entire class of wireless networks. In Delay or Disruption Tolerant <br\/>Networks (DTNs), lack of continuous connectivity, network <br\/>partitioning, and very long delays are the norm, not the exception. <br\/>Such networks have recently found applications in challenged <br\/>environments, such as space communications, military operations, <br\/>and sensor networks. Routing in DTNs is very challenging as it <br\/>must handle the time-varying and unpredictable availability of <br\/>links, long delays, and a dynamic topology. <br\/><br\/>This project contributes theoretical models and algorithms for <br\/>routing in DTNs with semi-deterministic mobility. In such networks, <br\/>node trajectory is either tightly controlled, but affected by random <br\/>deviations from the environment, or it is socially driven. In both, <br\/>opportunities for communication can be predicted using knowledge on <br\/>node mobility. These networks, underrepresented by current research, <br\/>have immediate applications in mobile sensing (e.g. littoral <br\/>underwater monitoring), disaster management, or in campus environments. <br\/>This research develops prediction models with Markov processes for <br\/>node mobility and methods for contact probability estimation. <br\/>Performance limitations and optimal parameters for the routing <br\/>protocols and algorithms are also investigated.<br\/>The research will produce a set of protocols suitable for operation <br\/>in conditions of sporadic connectivity and limited resources, <br\/>emphasizing local information exchange and adaptation techniques. <br\/>The protocols will be evaluated with detailed simulations at the <br\/>Florida Atlantic University Wireless and Sensor Network Laboratory.","title":"NeTS: Efficient Routing in Semi-Deterministic Delay Tolerant Networks","awardID":"0847664","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[391670],"PO":["564777"]},"143251":{"abstract":"In a variety of applications, such as weather monitoring, smart buildings, and manufacturing systems, a network of sensors, each with a communication constraint, is remotely deployed with the goal of making collective inference. However, the governing principle, as well as the performance limit, of such sensor (information source) networks is not well understood. Consequently, most practical systems are designed conservatively, allowing significant room for future improvement and cost reduction. In this backdrop, the investigators identify certain fundamental principles underlying network (multiterminal) source coding, and study performance bounds in the spirit of Shannon. This research entails several benefits: (1) attractive economic returns on research investment due to design benchmarking; (2) unified conceptual understanding of a large class of network coding problems; (3) development of academic materials and courses for training engineers in design and maintenance of distributed systems.<br\/>The investigators give a canonical theory characterizing the asymptotic performance bound for a broad class of multiterminal source coding problems. In particular, they employ a unifying distortion-independent coding strategy, which manifests distortion only upon suitable post-processing. Importantly, the ubiquitous notion of single-letter description is connected to a certain graphical representation; existence of such description is guaranteed if the corresponding graph admits no cycles. As a consequence, the investigators find a single-letter solution to the hitherto-open single-helper problem. On the contrary, graphs with cycles, seen, for example, in the famous Berger-Tung problem, likely admit no single-letter solution. Accordingly, the Berger-Tung bound is conjectured to be loose, and work towards a conclusive answer is in progress. However, in absence of a single-letter description, the canonical theory still provides a general road map towards a computable description, and tractable computational algorithms.","title":"CCF- Unified View of Multiterminal Source Coding","awardID":"0830583","effectiveDate":"2008-09-01","expirationDate":"2010-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[381437],"PO":["564924"]},"143372":{"abstract":"Modern organizations, such as businesses, non-profits, government <br\/>agencies, and universities, collect and use personal information from <br\/>a range of sources, shared with specific expectations about how it <br\/>will be managed and used. Accordingly, they must find ways to comply <br\/>with expectations, which may be complex and varied, as well as with <br\/>relevant privacy laws and regulations, while they minimize <br\/>operational risk and carry out core functions of the organization <br\/>efficiently and effectively. Designing organizational processes to <br\/>manage personal information is one of the greatest challenges facing <br\/>organizations (see, e.g. a recent survey by Deloitte and the Ponemon <br\/>Institute [TI07]), with far-reaching implications for every <br\/>individual whose personal information is available to modern <br\/>organizations, i.e. all of us.<br\/><br\/>This project responds to these challenges by developing methods, <br\/>algorithms and prototype tools for integrating privacy, compliance, <br\/>and risk evaluation into complex organizational processes. It <br\/>explores, articulates and characterizes formally the scope and nature <br\/>of privacy-expectations of stakeholders as well as those of key <br\/>regulations, such as HIPAA, GLBA, COPPA, BASEL 2, and Sarbanes-Oxley <br\/>(SOX). It incorporates the diverse perspectives and areas of <br\/>expertise of its multidisciplinary research team, which includes <br\/>three computer scientists, one philosopher, and collaborating <br\/>researchers from IBM. This industry connection facilitates <br\/>interaction with product teams that have served complex organizations <br\/>concerned with business process integrity, information security, <br\/>privacy, and information risk management. The research builds on <br\/>\"contextual integrity\" (a philosophical account of privacy) as well <br\/>as language and risk-based methods for privacy policy specification <br\/>and enforcement. Extensive training and educational opportunities are <br\/>provided to undergraduate and graduate students and research results <br\/>integrated into courses at CMU, NYU, Stanford, and UPenn.","title":"Collaborative Research: CT-M: Privacy, Compliance and Information Risk in Complex Organizational Processes","awardID":"0831124","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["563668"],"PO":["565327"]},"143383":{"abstract":"The project aims at studying properties of hash and trapdoor functions that are motivated by practical applications and are implicitly held by the random oracles or easy to realize in the idealistic random oracle model. But, are not well-defined and\/or not known to be realizable in the standard model. In particular, the research studies non-malleable hash functions and (possibly<br\/>trapdoor) functions that hide partial information. The project investigates the new appropriate notions of security for these primitives and seeks constructions that probably meet the security definitions. The outcome of the proposed research should help understanding of the gap between the standard and the random oracle model, and give more confidence in security of the practical schemes. Studying new security properties is timely, given NIST's ongoing cryptographic hash algorithm competition. An integral part of the project is continuing quality education on all aspects of modern cryptography.","title":"CT-ISG: New Security Properties for Hash and Trapdoor Functions","awardID":"0831184","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["549957"],"PO":["565327"]},"147750":{"abstract":"This CISE Special Project award provides funding for the organization of the 2008 CISE Pathways to Revitalized Undergraduate Computing Education (CPATH) Program principle investigators? meeting. It includes funds for the meeting itself as well as for a planning meeting of the executive team overseeing the PI meeting. The 2008 meeting is to be held in the fall of 2008 in Arlington, Virginia. The workshop provides a forum for dissemination of research results, collaboration among project teams, and exchange of information to the broader community.<br\/><br\/>The intellectual merit of this project lies with the expertise of the participating computing professionals to contribute to the knowledge and research on innovation in undergraduate computing education.<br\/><br\/>The broader impacts revolve around the potential for the collected group to raise public awareness about transformation of undergraduate computing education and to collectively initiate projects that may broaden the pathways into and through undergraduate computing education thus opening doors for a more diverse student population.","title":"NSF CISE Pathways to Revitalized Undergraduate Computing Education (CPATH) PI Workshop","awardID":"0850464","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["539087"],"PO":["564181"]},"143031":{"abstract":"PETASCALE SIMULATIONS OF DNA DYNAMICS AND SELF-ASSEMBLY<br\/>Priya Vashishta?PI, Rajiv K. Kalia, Aiichiro Nakano (University of Southern California)<br\/>Ananth Grama (Purdue University)<br\/><br\/>DNA translocation through solid-state nanopores and nanofluidic channels underlie ?lab-on-a-chip? technology and solid-state nanopore ?microscopy? for molecular structure and high-speed sequencing. Highly efficient methods for directed self-assembly of DNA offer unprecedented opportunities for the synthesis of novel genes, chromosome mapping, biosensors, molecular machines, nanoelectronics and nanomechanical systems, and formulations of mesoscopic structural motifs as building blocks of emerging periodic and aperiodic nanostructures consisting of DNAs.<br\/>This project involves the study of DNA self-assembly and translocation through nanometer-scale pores in silica and silicon nitride membranes using a predictive hierarchical petascale simulation framework consisting of: (1) Highly accurate quantum mechanical (QM) simulations to describe chemical processes in DNA translocation and concatenation; (2) multibillion-atom molecular dynamics (MD) simulations for structural properties and dynamical processes of DNAs in confined fluidic environments, with interatomic interactions validated by QM calculations and key experiments; (3) hybrid MD and adaptive lattice Boltzmann (LB) simulations in which MD is embedded in translocation\/concatenation regions, and LB in the rest of the fluid; (4) accelerated dynamics approaches to reach macroscopic time scales for direct comparison with experimental data; (5) metascalable, self-tuning, multicore parallel simulation algorithms; and (6) automated model transitioning to embed higher fidelity simulations inside coarser simulations on demand with controlled error propagation. A metascalable (or ?design once, scale on new architectures?) parallel application-development framework is also being developed for first-principles simulations of directed DNA self-assembly.","title":"EMT\/BSSE: Petascale Simulations of DNA Dynamics and Self-Assembly","awardID":"0829815","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["490009","490010","490011"],"PO":["565223"]},"143273":{"abstract":"Algorithms are at the core of computer science. The goal of this research is to design and analyze algorithms for a selection of important and interesting algorithmic problems, chosen primarily from the area of graphs and networks. The ideal is to obtain algorithms that are efficient in theory and practice, and conceptually simple as well -- in short, \"elegant\" algorithms. For each problem, a systematic exploration of possible solutions will be undertaken. Specific problems to be studied will be selected from among dynamic topological ordering and related problems, planarity testing and related graph problems and data structures, maximum and minimum cost network flow problems, amortized efficiency of binary search trees and related data structures, finding dominators and related problems, and others. The broad impact of the research will be to shed new light on the design space of solutions for specific algorithmic problems and families of problems, to produce new algorithmic and analytical techniques, to provide efficient implementations of old and new algorithms, and to produce comparisions of their empirical performance. In addition, new members of the research and teaching community in the field of algorithm design will be trained, and new approaches and new materials for communicating the important concepts in the field will be developed.","title":"Efficient Graph Algorithms and Data Structures - Theory and Practice","awardID":"0830676","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["382701"],"PO":["565251"]},"143394":{"abstract":"The integrity of commodity operating system kernels is threatened by rootkits<br\/>that modify key kernel data structures to achieve a variety of malicious goals.<br\/>While rootkits have historically been known to affect control data in the<br\/>kernel, recent work demonstrates rootkits that affect system security by<br\/>modifying non-control data, such as linked lists used to manage bookkeeping<br\/>information and metadata used for memory management. Existing techniques fail<br\/>to detect such rootkits effectively.<br\/><br\/>This project is developing techniques to provide real-time protection against<br\/>rootkits by detecting anomalies in both control and non-control kernel data<br\/>behavior using automatically-generated integrity specifications. This goal is<br\/>being achieved in two steps. First, a technique to mine specifications of<br\/>kernel data structure integrity is being developed. These specifications are be<br\/>mined automatically as data structure invariants. Second, these techniques are<br\/>being extended using operating system support to provide real-time detection.<br\/><br\/>Impacts and Results: The techniques developed in this project will defend<br\/>against the next generation of rootkits, and will enable real-time detection of<br\/>such rootkits. In addition, techniques to infer kernel invariants <br\/>may also find applications in operating system reliability, fault tolerance and<br\/>software engineering. The PIs will disseminate the results by releasing the <br\/>tools developed. The results of this project will equip the workforce with an<br\/>inter-disciplinary toolkit, that combines operating systems, computer security,<br\/>and software engineering, to address the challenges posed by the next<br\/>generation of stealth malware.","title":"CT-ISG: Advanced Techniques to Detect Kernel-Level Rootkits","awardID":"0831268","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["486425","486426"],"PO":["565327"]},"144010":{"abstract":"Energy efficiency is rapidly becoming a key constraint in the design of enterprise systems. By 2011, yearly data center energy consumption in the United States is projected to grow to over 100 billion kWh at cost of $7.4 billion. As much as 40% of this energy is consumed by DRAM and disks. <br\/>Portable consumer devices, where battery life has long been a key concern, instead use faster and more energy-efficient Flash storage. Flash is non-volatile, has near-zero standby power draw, and each Flash read requires 30x less power than a DRAM read and three orders-of-magnitude less power than a disk access. Moreover, Flash provides far lower access latency and higher bandwidth than disks, approaching DRAM performance levels. Enterprise storage vendors have recently announced products that replace traditional disks with high-capacity Flash solid-state disks (SSDs). However, because they are accessed through interfaces optimized for legacy rotating disks, SSDs fail to fully-exploit the low latency and high bandwidth Flash can provide. Furthermore, replacing conventional disks with SSDs does not address the growing power consumption of severs' <br\/>DRAM. In this project, we examine further opportunities, beyond SSDs, to save energy with Flash in enterprise systems by integrating Flash throughout the servers' storage and memory hierarchies. The long term goal of our research--to improve data center energy efficiency--has the potential to drastically reduce the carbon footprint of data centers and the need for additional power generation capacity.","title":"CSR-DMSS,SM: Beyond Solid State Disks: Using FLASH to Save Energy in Enterprise Systems","awardID":"0834403","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["535404","508097"],"PO":["535244"]},"145583":{"abstract":"Multiparty communication is an emerging phenomenon on the Internet. The envisioned applications include Internet teleconferencing, telemedicine and telesurgery etc. A typical session involves multiple interactions among participants, and requires data transferring between arbitrary pairs of participants. These applications usually have stringent delay requirements, e.g., data delivery must meet individual deadlines, and the shared data must be received by participants within bounded delay variation. If the group is large, group members are scattered far from each other in the Internet, and each member needs a variety of data items, multicasting to the entire group is too costly; On the other hand, if each request is replied independently regardless of other requests, optimizing one request may delay the delivery of the next. Existing approaches that do not consider the interaction of data streams cannot address the delay requirements effectively. This project aims to establish the theoretical basis for performance analysis and seek algorithmic solutions for QoS guarantees on the delay aspect. We will formulate practical QoS problems into new online problems and try to reach some definitive conclusions on QoS issues in multiparty communication. The proposed research will advance the fields of QoS research in networks and online algorithm design\/analysis in theory.","title":"SGER: Online Algorithms for Delay Sensitive Multiparty Session Communication","awardID":"0841388","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["544621"],"PO":["557315"]},"143042":{"abstract":"PROJECT SUMMARY( Revised August 6, 2008) <br\/><br\/>The quantum entanglement and ways to entangle qubits and continues variables have been extensively developed and continue to be at the heart of quantum information processing. While entanglement of atoms and photons is relatively well understood; one still has to go long way in understanding and achieving entanglement of macroscopic systems like Bose Einstein condensates and system of quantum dots. Besides now one has to think of important applications of the new resource that we have at our disposal. We thus propose to develop and study entangled source of matter waves we propose schemes to produce entanglement of two Bose condensates. Each condensate could be either single component or multi component. This involves multi particle entanglement as we deal with collective coordinates. We further propose to use entanglement for Heisenberg limited interferometry. Note that a recent conference [http:\/\/portale.unitn.it\/events\/interf08\/] organized by Aspect, Phillips and others discussed how quantum optics of Bose condensates is a new direction of research. Another novel system that we propose to study is the system of quantum dots which act like super atoms. It is possible to produce a system with two dots where the relative separation between dots could be in the range of few nano meters leading to prominent d-d interaction. We propose to study the quantum entanglement in a double dot system which results from d-d interactions and consider its manipulation by a coherent drive. <br\/> Intellectual Merit of Research. We propose fundamental studies. Theoretical models for producing entanglement between two quantum dots or two Bose condensates would be developed. In such systems it is also possible to entangle matter and photonic degrees of freedom. Further newer aspects of such systems emerge by applying the ideas from quantum optics such as squeezing and photon-photon correlations. <br\/> Broader Impact of the Research. Here we propose to continue our earlier collaboration involving the fields of quantum optics and condensed matter physics . We propose to use ideas and techniques from different fields. For example, the master equation techniques from quantum optics should be useful for the study of photon photon correlations in emission from system of quantum dots and in studying decoherence. The idea of collective excitations and projective measurements are appropriate for the study of entanglement of independent Bose condensates. We have active collaboration with our cold atom group. All this would be supplemented by the international collaboration that we have built up during the period of the recent NSF support (CCF-0524673) which ended in June 2008. An important element of our proposal is to train graduate students in the emerging interdisciplinary area of quantum information science.","title":"Quantum Entanglement in Novel Macroscopic Systems & Applications to Quantum Sensors","awardID":"0829860","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":[380854,380855],"PO":["565157"]},"144021":{"abstract":"As CPUs move towards increasing parallelism and new memory technologies are developed, operating system abstractions of hardware have changed little in the past 30 years. However, computer architecture is moving towards heterogeneous multi-core architectures that dynamically vary the number, performance, power, and reliability of processing elements. Memory technologies such as flash, magnetoresistive and phase-change RAM promise large quantities of fast, non-volatile memory. <br\/><br\/>This project seeks to identify current OS abstractions that will be outdated on future processing platforms, invent new abstractions that more accurately capture the range of platform behavior, and implement these new abstractions in the context of existing operating systems. The research focuses on extensions: (1) abstracting hardware support for synchronization, such as transactional memory, thread queues, or full\/empty bits, (2) abstracting heterogeneous multi-core processors, such as processors that have differing instruction sets or that can be fused together to form more powerful processing elements, (3) exposing non-volatile memory to programmers, and (4) abstracting microarchitectural contention for memory bandwidth, cache space, and shared processing elements to reduce the complexity of scheduling algorithms. <br\/><br\/>The research will lead to higher-performing systems, due to both faster persistent storage and better use of execution resources, more reliable systems, due to faster persisting of data, and more programmable systems, due to general abstractions of specific hardware. In addition, the OS and architecture research communities will benefit from better software support for new hardware technologies. This work is inherently interdisciplinary and students will benefit from exposure to new research in other fields.","title":"CSR-DMSS, SM: Operating System Abstractions for Modern Hardware","awardID":"0834473","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["559530"],"PO":["565255"]},"145473":{"abstract":"This project creates and evaluates new visualization techniques of models from a concept generator to stimulate the designer to generate concepts not originally posed by designer or computer. This technology is expected to enable the development of creative solutions to design problems that would otherwise go undiscovered. The three challenges to achieving effective visualization for enhancing creative design at the concept generation phase are: 1) how to cluster the many concept variants returned from an automated concept generation algorithm into a manageable set of representative concepts that spans the design alternative space; 2) how to visually represent the option space to the designer so that it enhances creativity; and 3) how to measure the impact of the visualization schemes on designer creativity. This exploratory research, if successful, offers the opportunity to transform how we design products and systems and how we guide any designer to a creative new product.","title":"SGER Collaborative Research: VisualizeIT - Measuring the Impact of IT-Enabled Concept Generation on Designer Creativity","awardID":"0840969","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["527806","557740"],"PO":["565227"]},"143053":{"abstract":"Recent genetic evidence shows that the human species is a smaller family than previously thought: arbitrary groups of people include many pairs of hidden relatives, that unknown to them share a recent ancestor a few generations back. The investigators develop novel computational methods to reveal these remote family ties, from large scale datasets that contains billions of snippets of genetic information. This research effort will use this information to compile a genealogy of thousands of otherwise-unrelated individuals.<br\/><br\/>Individuals with a common ancestor have a chance to share one or more long fragments of DNA, that are identical-by-descent (IBD) over several megabases. High throughput genetic data from commercial arrays of 300,000-1,000,000 Single Nucleotide Polymorphisms (SNPs) can therefore detect IBD with certainty. The computational challenge in large scale data of tens of thousands of individuals typed for these array is making all the quadratic number of pairwise comparisons in search for IBD. The investigators develop a per-locus hashing algorithm, that detects identical haplotypes across all O(n2) sample pairs, but operates in linear time. They are using this methodology to map hidden relatedness across publicly available samples, creating a useful tool for population-based linkage analysis in unrelateds, as well as inferences on population genetics of recent generations.","title":"EMT: Computational methods for mapping genealogy of unrelated individuals from high throughput genetic data","awardID":"0829882","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["390619"],"PO":["565223"]},"143295":{"abstract":"Over the past 50 years, error-control coding has been employed with spectacular success by the communications and data storage industries to achieve performance trade-offs that would have been otherwise impossible. What has been recognized only recently, however, is that coding theory could be just as useful in applications other than communications and storage. In particular, this project is concerned with numerous problems that arise in the area of digital circuit design. The problems studied come from a wide spectrum of technologies, ranging from nanoscale circuits and memory chips to more conventional VLSI architectures. In each case, these problems are inherent to the physics of the underlying medium or system. In each case, the project aims to show that sophisticated coding --- based upon methods and ideas deeply rooted in algebraic and combinatorial coding theory --- offers a significant advantage, thereby enabling circuit designers to achieve system trade-offs that would have been otherwise impossible.<br\/><br\/>Specifically, the research carried out in this project can be roughly subdivided into the following four focus areas:<br\/><br\/> Development of new coding schemes for efficient addressing and<br\/> correction of manufacturing defects in next-generation memory<br\/> nano-devices, in particular the nano-wire crossbar;<br\/><br\/> Advanced coding techniques for high-density flash memories, based<br\/> upon ground-breaking recent ideas of floating codes and rank-modulation<br\/> coding;<br\/><br\/> Development of coding schemes to reduce power dissipation and to avoid<br\/> cross-talk in VLSI circiuts, with particular emphasis on both on-chip<br\/> and off-chip buses;<br\/><br\/> Applications to circuit design of the techniques developed in a range<br\/> of well-known combinatorial problems in coding theory, including<br\/> covering arrays, separating codes, intersecting codes, and qualitatively<br\/> independent set families.","title":"Collaborative Research: Coding for Nano-Devices, Flash Memories, and VLSI Circuits","awardID":"0830752","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["485936"],"PO":["564924"]},"146331":{"abstract":"This SGER project will conduct basic theoretical and empirical work in control of continuous backbone \"continuum\" robot manipulators. The key goal of the project is to demonstrate, in hardware, practical control of continuum robot manipulation. Continuum structures have the potential to transform the nature of many robot applications. However, deployment of these robots is not yet practical, in large part due to difficulties in applying conventional robot control techniques. The proposed one year exploratory research is intended to lay the foundations for wider continuum robot research and application by establishing the practical ability of new techniques to successfully control continuum robots. We will design and implement a series of novel continuum robot controllers, taking into account the effects of the compliance built into these systems. The controllers will be tested using a unique hardware test bed at Clemson University featuring the OctArm series of continuum robot arms.<br\/>We will implement all the developed controllers on the OctArm hardware, and test across a wide variety of trajectories and manipulation tasks.","title":"SGER: Real-Time Continuum Manipulation","awardID":"0844954","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["530256","563342"],"PO":["564316"]},"144032":{"abstract":"Modern medical devices are often complex distributed systems. They require substantial embedded software and exhibit feedback loops between their components that involve multiple humans, such as caregivers, doctors, and patients. Such systems need to be flexible enough to accommodate significant uncertainty in their environments, such as varying and often unpredictable reactions of patients to treatments and medications. As with many other complex systems, the increasingly prominent trend is to assemble medical devices from independently developed hardware and software components. To accommodate this trend and simultaneously maintain the high degree of confidence that is required of medical systems, research is needed in the area of active components.<br\/>The idea of an active component extends traditional components with novel capabilities that support compositional reasoning to improve interoperability. The basic concept of an active component is that the component carries a self-model that can be used by its peers, which specifies the intended and failure behaviors that peers can expect and the behavior it assumes from peers.<br\/>The project is developing active components as a compositional framework for medical device software and systems (MDSS), and applies active components to model and analyze the environmental aspects of MDSS, to anticipate expected and unexpected interactions, and to compare mental models and systems models to detect and prevent potential user errors. In particular, the active component framework will be applied to establish high confidence in MD PnP (Medical Device Plug-and-Play) interoperability, an on-going effort targeting dynamic integration of medical devices. The project also seeks to apply, evaluate, and refine the notion of active components by using them for new advanced medical device systems where compositionality and interoperability are essential, such as medical devices based on sensor networks.","title":"CSR-EHCS(CPS) TM: Robust Composition and Interoperability of CPS Components","awardID":"0834524","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}}],"PIcoPI":["553656","553657","518109","534440"],"PO":["561889"]},"145363":{"abstract":"SGER: Advances in Difficult Aspects of Reference Resolution<br\/>This exploratory project is investigating two difficult phenomena in the domain of reference resolution: (1) the resolution of plural pronouns that can corefer either with a single syntactic constituent or with a set of constituents from different places in the text: they, those two, all three, etc. and (2) the resolution of 3rd person singular and demonstrative pronouns that can corefer either with a single syntactic constituent or with a span of text: it, that, this, all of this, etc. These phenomena have been treated in limited ways to date because the methods that have been most widely deployed for reference resolution have expected the antecedent for referring expressions to be a single noun phrase in the preceding context. In the cases under study, the antecedent need not be constrained in this way. This work is aimed at achieving human-like intelligence to support, for example, sophisticated communication between intelligent agents and people. It is theoretically grounded but keeps practical concerns at the fore. The component algorithms can inform reference resolution in any domain by any system in the relatively near term, advancing known types of technology and not relying on any unexpected breakthroughs. Among the resources to be distributed to the community are system-independent reference resolution algorithms, a corpus of relevant examples and their semantic representations, a reference resolution engine, and a critical survey of accomplishments, outstanding issues, and paths for further development.","title":"SGER: Advances in Difficult Aspects of Reference Resolution","awardID":"0840579","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[387805],"PO":["565215"]},"144153":{"abstract":"Wayfinding is an essential capability for any person who wishes to have an independent life-style. It requires successful execution of several tasks including navigation and object and place recognition, all of which necessitate accurate assessment of the surrounding environment. For a visually-impaired person these tasks may be exceedingly difficult to accomplish and there are risks associated with failure in any of these. Guide dogs and white canes are widely used for the purpose of navigation and environment sensing, respectively. The former, however, has costly and often prohibitive training requirements, while the latter can only provide cues about obstacles in one?s surroundings. Human performance on visual information dependent tasks can be improved by sensing which provides information and environmental cues, such as position, orientation, local geometry, object description, via the use of appropriate sensors and sensor fusion algorithms. Most work on wayfinding aids has focused on outdoor environments and has led to the development of speech-enabled GPS-based navigation systems that provide information describing streets, addresses and points of interest. In contrast, the limited technology that is available for indoor navigation requires significant modification to the building infrastructure, whose high cost has prevented its wide use. <br\/><br\/>This proposal adopts a multi-faceted approach for solving the indoor navigation problem for people with limited vision. It leverages expertise from robotics, computer vision, and blind spatial cognition with behavioral studies on interface design to guide the discovery of information requirements and optimal delivery methods for an indoor navigation system. Designing perception and navigation algorithms, implemented on miniature-size commercially-available hardware, while explicitly considering the spatial cognition capabilities of the visually impaired, will lead to the development of indoor navigation systems that will assist blind people in their wayfinding tasks while facilitating cognitive-map development.","title":"CDI-Type II: Collaborative Research: Cyber Enhancement of Spatial Cognition for the Visually Impaired","awardID":"0835689","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7751","name":"CDI TYPE II"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["561515"],"PO":["543539"]},"143064":{"abstract":"EMT\/NANO: Comprehensive Modeling of Power Dissipation, Leakage, and Non-Equilibrium Transport in Low-Dimensional Transistors<br\/>Eric Pop ? University of Illinois at Urbana-Champaign<br\/><br\/> Power dissipation and leakage are significant concerns of modern integrated circuits, and increasingly important in the future. Leakage is a strong function of temperature, and hence of the power dissipated. However, no detailed models or data exist for nanoscale, low-dimensional devices, and little is known about the microscopic behavior and role of interfaces for heat dissipation in technologies of high interest such as carbon nanotubes or graphene. In addition, fast switching times (of order 1 ps) are comparable to electron- or phonon-scattering times, leading to non-equilibrium carriers and heating during device operation.<br\/> This research involves producing a comprehensive, microscopic understanding of power dissipation and leakage in nanometer-scale integrated circuits. More specifically it (1) extends an existing Monte Carlo approach to investigate the role of non-equilibrium phonons on power dissipation and leakage, (2) pursues a microscopic understanding of phonon coupling across device interfaces with Molecular Dynamics simulations, and (3) wraps up the advanced physical understanding into self-consistent electrical-thermal compact models to be used in circuit modeling. The research is also incorporated into classroom materials for the courses taught by the PI, including updating or creating new Wikipedia articles in the context of a graduate class. Models and computer codes are made freely available through the NSF\/NCN computational nanoHUB. A breakthrough in our understanding of nanoscale device power issues can lead to a revolutionary approach to thermally efficient circuit and system design, from a bottom-up, device and materials perspective.","title":"EMT\/NANO: Comprehensive Modeling of Power Dissipation, Leakage, and Non-Equilibrium Transport in Low-Dimensional Transistors","awardID":"0829907","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7947","name":"NANOCOMPUTING"}}],"PIcoPI":["561831"],"PO":["565157"]},"144043":{"abstract":"As multi-core\/many-core processors are on the technology roadmap of almost every major microprocessor and embedded-processor vendor today, applications can no longer count on increasing clock rate to improve their performance. They must exploit various forms of parallelism (e.g. thread-level, data-level, and\/or memory-level) to achieve improved performance. In the mean time, processors are facing the well-known power wall, instruction-level parallelism (ILP) wall and memory wall that are likely to be exacerbated in the future. Static compilers have been one of the major contributors to dealing with all those challenges. However, they lack runtime information to be used to address possible interactions among tasks competing for the same resources. In addition, operating systems and middleware often incur too much overhead for some forms of parallelism to be effectively used at runtime.<br\/><br\/>Dynamic runtime optimizers have proven to be capable of dealing with many runtime issues such as memory optimizations on single-core processors. They incur minimal amount of runtime overhead and yet are able to explore useful runtime behavior to re-optimize binary code and re-adapt its execution to achieve higher performance or more efficient power\/thermal management. This project researches and develops a dynamic runtime optimizer for multi-core\/many-core processors. Such an optimizer requires an infrastructure with several major components each with new approaches and techniques that are different from what are currently used for single-core processors. In this project, a set of benchmarks will be statically compiled and annotated to exploit various forms of parallelism, including speculative threads generated by a research parallelizing compiler that explores thread-level speculation.","title":"CSR-PSCE,TM: Dynamic Runtime Optimization and Adaptation for High Performance and Power Management on Multi-Core Processors","awardID":"0834599","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["421081",383979,383980,"421081"],"PO":["565255"]},"145374":{"abstract":"Title: SGER: Collaborative Research: Exploring the Role of Word Senses in Subjectivity Analysis<br\/><br\/>Approaches to subjectivity and sentiment analysis often rely on manually or automatically constructed lexicons. Most such lexicons are compiled as lists of words, rather than word meanings (senses). However, many words have both subjective and objective senses, which is a major source of ambiguity in subjectivity and sentiment analysis.<br\/><br\/>The goal of this exploratory research project is to address this gap, by investigating novel methods for subjectivity sense labeling, and exploiting the results in sense-aware subjectivity analysis. Specifically, the project targets two research objectives. The first objective is to develop new methods for assigning subjectivity labels to word senses in a taxonomy. The second objective is to explore contextual subjectivity disambiguation techniques that will effectively make use of the word sense subjectivity annotations. By achieving these objectives, the project is expected to contribute to the understanding of the connections among subjectivity, word senses, and contextual subjectivity analysis, which will serve as a stepping stone for continued research efforts in this area.<br\/><br\/>The resources created in this project will be made available to the research community, which will help advance the state of the art in automatic sentiment and subjectivity analysis.","title":"SGER: Collaborative Research: Exploring the Role of Word Senses in Subjectivity Analysis","awardID":"0840608","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["564432"],"PO":["565215"]},"143075":{"abstract":"Collaborative Research: EMT\/QIS:<br\/>Quantum Algorithms and Post-Quantum Cryptography<br\/>Project Summary<br\/><br\/>Cryptography is the basic infrastructural element enabling privacy and <br\/>security for electronic transactions. However, when a large-scale <br\/>quantum computer is finally built, it will force us to abandon <br\/>established methods of cryptography, such as RSA and Diffie-Hellman, <br\/>which are in common use today. The proposed research will further <br\/>this line of disruptive quantum algorithmic research; but it also aims <br\/>to erect a new framework of secure post-quantum cryptography, in order <br\/>to maintain this societally critical infrastructure.<br\/><br\/>The most attractive approach for salvaging modern cryptography would <br\/>be to develop classical cryptosystems for which we have compelling <br\/>evidence of security even in the face of quantum adversaries. Recent <br\/>work by the PIs and their collaborators has shown that certain <br\/>algebraic problems possess hardness properties relevant even for <br\/>quantum algorithms. We propose to strengthen and leverage these <br\/>results in order to develop cryptographic schemes which can be carried <br\/>out by today's computers, but which will remain secure even against <br\/>quantum attack in the future.<br\/><br\/>In tandem with this effort, we propose to develop new quantum <br\/>algorithms for breaking cryptosystems based on conjugacy in the braid <br\/>group. This is one of the few remaining classical cryptosystems which <br\/>has not already been shown to be vulnerable to quantum attack.<br\/><br\/>Our research program is also directly integrated with graduate student <br\/>training at all four institutions, undergraduate educational <br\/>innovation, educational outreach, and broad scientific dissemination.","title":"Collaborative Research: EMT\/QIS: Quantum Algorithms and Post-Quantum Cryptography","awardID":"0829931","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["531365"],"PO":["565157"]},"137630":{"abstract":"Last Modified Date: 07\/11\/08 Last Modified By: Douglas H. Fisher <br\/><br\/>Abstract <br\/>This project exercises and expands upon methods for automatic discovery of new representations at multiple temporal and spatial scales. The specific framework generalizes classical harmonic analysis, in particular wavelet-based methods, to graphs and manifolds, thereby greatly extending the scope and the desirable characteristics of this multiscale-analysis framework to domains with arbitrary geometries. This framework, termed diffusion wavelets because it is associated with a diffusion process that defines the different scales, has unique properties relevant to learning, function approximation, compression and denoising. The set of core problems that this project addresses include fast algorithms for construction of multiscale diffusion wavelets, approximation of functions on very large graphs and high-dimensional manifolds, out-of-sample extensions of functions on manifolds and graphs, compression and denoising of functions on data sets, perturbation analysis, and randomized algorithms for multiscale analysis. Challenging application domains are being investigated, including analysis of document corpora, Markov decision processes, and 3D image rendering. In each case, multiscale diffusion analysis yields interpretable and meaningful results. For example, when applied to Markov decision processes, diffusion wavelet analysis yields new optimization methods that dynamically aggregate states and actions at multiple levels of abstraction; and when applied to 3D computer graphics, it yields new compression methods that capture geometric features of objects at multiple resolutions.","title":"RI-Medium: Collaborative Research: Learning Multiscale Representations Using Harmonic Analysis on Graphs","awardID":"0803293","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550949"],"PO":["562760"]},"144054":{"abstract":"Parallel programs are typically written using a style in which independent processes are expressed sequentially and use locally-expressed message passing as a means of data exchange as well as a means of inter-process synchronization. From the network relationships that are implied by messaging among processes arise implicit data structures and implicit algorithms - that is, global parallel data structures and global parallel algorithms for which there is no explicit global representation. Limited only to a local approach for creating parallel programs, achieving correctness and performance of large-scale parallel algorithms is rapidly moving beyond our reasoning abilities. To address this shortcoming, this project develops a declarative language approach for describing new high-level communication operations, a means for composing these operations with computations, and a means for expressing transformations for optimizing the resulting compositions.<br\/><br\/>This project's implementation plan is based on aggressive evolution of the technology that is currently most effective in large-scale parallel computing - namely, explicitly managed shared data. In the distributed-memory case this is achieved with message passing, but the same discipline can be applied in the shared memory case as well. This project's approach may enable higher levels of expressiveness and abstraction for data management, communication, and coordination. Moreover, the separation of concerns that is naturally imposed between communication and computation greatly simplifies the mental model and implementation effort for programmers. In order not to sacrifice performance because of this division, this scheme's composition and transformation mechanisms will allow communication and computation to be (automatically) combined in an optimized fashion so that high performance and high scalability are achieved. To facilitate adoption by practicing programmers of the paradigms and tools that are developed, integration with education is essential. Accordingly, this project will directly train undergraduate, graduate, and post-doctoral researchers and will develop supporting curricula and materials to train both students and practicing programmers. Close collaboration with large-scale real-world scientific applications will further increase the practical relevance of this work.","title":"CSR-PSCE, TM: A Declarative Approach to Managing the Complexity of Massively Parallel Programs","awardID":"0834722","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["487796","562709"],"PO":["535244"]},"145275":{"abstract":"Proposal Title: SGER: Collaborative Research: Exploration of Distributed Creativity<br\/>in Multi-Site 3D Tele-Immersive Spaces<br\/>Institution: University of California-Berkeley<br\/>Abstract Date: 07\/29\/08<br\/>This project explores the impact of geographically distributed 3D tele-immersive<br\/>environments on dancers' creativity and their perception of themselves and each other.<br\/>More specifically, the project will study the impact of digital options, such as scale and<br\/>multiply, on the dancers' creative expression and improvisation. A formal notation,<br\/>called a creativity graph, will be derived from a Laban movement analysis of the<br\/>dancers' movement. A dancer develops a creative dance, a new sequence of phrases,<br\/>when he\/she generates a new association between two movement states (e.g. a new<br\/>association between two Laban positions) based on some feedback from the immersive<br\/>environment due to either invoking digital options or due to some unexpected<br\/>performance of the system. This project will have a fundamental impact on our<br\/>understanding of dancers' creativity within tele-immersive dancing environments and<br\/>computing and will contribute a new concept of transformational movement graphs as a<br\/>computational representation of dance creativity in distributed multi-site 3D<br\/>tele-immersive spaces.","title":"SGER: Collaborative Research: Exploration of Distributed Creativity in Multi-Site 3D Tele-Immersive Spaces","awardID":"0840323","effectiveDate":"2008-09-15","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["561784"],"PO":["424970"]},"144065":{"abstract":"This research considers a class of social problems represented by cyber bullying and creates a sociology-based network framework to consider two new questions,in an initial study: does the network framework (where network and applications are both included) allow identification of cyber bullying within networked social groups? And if it does, can the framework lead to ways of mitigating cyber bullying? What are the representations of information in the network that preserve privacy? What are the tradeoffs between the goals of the research and ensuring that eventual users have protected privacy and have enough transparency into a framework that affects them? In the mitigation area, the project will explores the insight that network interventions such as lessening the frequency of deliveries from an aggressive poster can affect social peers positively. The project will research will develop an approach called deriving structures, so the mitigations \/ interventions could be in the applications or could be in deeper structures or both together. <br\/><br\/>In the one-year exploratory research of this project, the PI is working with a number of unpaid collaborators in Japan: 1) with several sociologists in order to develop definitions and tests for cyber bullying; 2) with educators to develop a case-based classroom social net study, including approaches to mitigation that seem realistic to them; 3) with researchers from the Japanese telecommunications company, KDDI, who are sharing the research exploration and also providing experimental infrastructure in Japan. <br\/><br\/>The project has broader impacts on society: social experiences on the net loom large in young peoples' development in many countries now. Cyber bullying of various forms is a risk for many people, young and old. Innovations in understanding this phenomenon are very important to society; the sooner the better. This angle, the sociometric use of the network and the careful analysis of network based mitigations by not only the computer scientist PI, but also by sociologist, educator and network operator collaborators, will open new doors.","title":"SGER: Exploratory Research on an Internet Framework to Improve a Social Network Structure","awardID":"0834796","effectiveDate":"2008-09-15","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["409909"],"PO":["565090"]},"136211":{"abstract":"The University of Maryland College Park is awarded a grant to create tools that expand the computing power freely available to all ATOL (Assembling the Tree of Life), and other phylogenetic researchers. The central idea is to combine Grid and GPU (graphics processing unit) computing to take better advantage of a diversity of computing resources, particularly existing desktop processing capacity available through public-computing. The main product will be a Grid system, customized for phylogenetic tree search and accessed through a user-friendly web interface, that links thousands of individual computers world-wide. The likelihood-based tree search program GARLI (Genetic Algorithm for Rapid Likelihood Inference), will serve as the initial model application during development. The new system is intended to allow analyses that far transcend what one can do on a single desktop computer or moderate-sized cluster. For example, it should become possible to carry out extensive bootstrap analyses in little more time than is now required for a single tree search. The project will also exploit a second under-utilized resource inside desktop computers, namely the graphics processing unit (GPU), which can do many scientific calculations faster than the central processing unit (CPU). A free, open-source library (C\/C++) for maximum likelihood calculations using parallel CPU\/GPU processing will be developed. GARLI versions will be developed that take advantage of major GPU platforms. Subsequent development will extend to other likelihood-intensive methods (e.g., Bayesian analysis). Five participating ATOL projects will help test the new tools, and publish a collaborative evaluation of the performance of the new system across the diverse phylogenetic problems they represent. The project will be a joint effort with Bowie State University, Coppin State College, and the University of Kansas Center for Research, Inc.","title":"Grid, Public and GPU Computing for the Tree of Life","awardID":"0755048","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4089","name":"NETWORK CENTRIC MIDDLEWARE SVC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"1171","name":"PHYLOGENETIC SYSTEMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"1165","name":"ADVANCES IN BIO INFORMATICS"}}],"PIcoPI":["430955","543581",361868,361869],"PO":["561879"]},"147475":{"abstract":"This proposal extends current research in creating a query language<br\/>for provenance to a broad range of application domains by designing,<br\/>developing, and evaluating a general-purpose query language for<br\/>graph-oriented data.<br\/><br\/>PQL, the path query language, was designed to address the challenges<br\/>encountered in expressing queries on provenance or lineage data,<br\/>but it was conceived to be the foundation for a general purpose<br\/>data model and query language for manipulating any type of<br\/>graph-oriented data. Graph-oriented data arises in many disciplines<br\/>including computer networking, information retrieval, biology, web<br\/>search, social networking, genealogy, etc. A characteristic that<br\/>unites all these domains is that need for expressing queries on<br\/>paths through a graph. Most existing solutions today have either<br\/>a weak or non-existent notion of paths as first-class entities that<br\/>can be named, compared, manipulated and constructed. PQL addresses<br\/>this problem.<br\/><br\/>Derived from the semi-structured database language, Lorel, PQL<br\/>operates on semi-structured data, which can be viewed as a collection<br\/>of objects linked together. In PQL, these links are unidirectional,<br\/>although we support both forward and backward queries across these<br\/>links. Queries are expressed by selecting and filtering one or<br\/>more paths in the graph, where paths can be described by regular<br\/>expressions. Thus, in the provenance domain, one can talk about,<br\/>''all paths in the graph between invocations of a compiler and the<br\/>resulting executables.'' In biology, one might pose a query about,<br\/>''all paths from a particular combination of gene expressions to<br\/>resulting insulin production.''<br\/><br\/>The work described in this proposal extends PQL to include update<br\/>syntax and semantics and the ability to produce query results that<br\/>are graphs constructed from components of the original. The result<br\/>of this work will be captured in both a machine-checkable formal<br\/>specification of the language and an open source PQL implementation,<br\/>complete with a (replaceable) back-end implementation.<br\/><br\/>Further information on the project can be found at the project web page:<br\/>http:\/\/www.eecs.harvard.edu\/~syrah\/pass\/","title":"SGER: PQL: A Path Query Language","awardID":"0849392","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["541948"],"PO":["563727"]},"145297":{"abstract":"This award supports travel for students participating in the Doctoral Symposium at the International Conference on Software Engineering in Vancouver, Canada, in May 2009. The Symposium provides a forum for Ph.D. students to discuss their research goals before a panel of experts in order to receive guidance on their research and research careers.","title":"Workshop: International Conference on Software Engineering 2009: Student Travel Support","awardID":"0840392","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["449118"],"PO":["564388"]},"144087":{"abstract":"Last Modified Date: 08\/04\/08 Last Modified By: Amy Baylor <br\/><br\/>Abstract <br\/>The purpose of this project is to develop and evaluate user interface tools that enable middle school students to effectively teach themselves the basics of computer programming using programs written by peers. The proposed tools will enable middle school users to 1) identify interesting functionality within programs written by peers and 2) re-create that functionality for use in their own programs through automatically generated custom tutorials. As users complete these tutorials to re-create functionality selected from programs their peers have written, they will be introduced to a broad range of programming concepts and constructs. The project will include user testing to both guide and evaluate the development of 1) user interface tools that enable novice programmers to find code of interest in an unfamiliar codebase and 2) technology to automatically generate customized tutorials based on a recorded history of the sequence of user interface actions used to construct a program. <br\/><br\/>Computer programming has become a fundamental tool that enables progress across a broad range of disciplines including basic science, communications, and medicine. Yet, Computer Science is failing to attract the number of students necessary to sustain progress both within the discipline and in those disciplines supported by computer science. Opportunities to study computer science during middle school (when many students begin to opt out of math and science based careers) are rare. This project will enable middle school children to teach themselves computer programming using programs created by their peers. The project will be implemented within Storytelling Alice, an environment that enables middle school children to write programs to create 3D animated movies. Users will be able to identify parts of movies created by other users that interest them and follow automatically generated tutorials to learn how to create the selected parts of those movies. By building tools that help users to learn effectively from programs written by peers within an appealing programming environment, we enable middle school students across the country to develop skills in computer programming at a time when formal opportunities to study computer science are decreasing and the need for computer scientists is increasing. While this project targets middle school students in the context of learning computer programming, there are many other audiences (e.g. adults learning a new piece of computer software) would benefit from software technologies that enable self-teaching","title":"Collaborative Research: Enabling Independent Learning of Computer Programming Using Programs Written by Peers","awardID":"0835371","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["458384"],"PO":["564318"]},"139610":{"abstract":"Image data is of immense practical importance in medical informatics, and a subject of strong interest to researchers in industry and academia. While digital image databases are now prevalent in clinical and educational settings, and traditional means for interacting with and querying such collections can provide some level of useful functionality, there are few examples of systems that attempt to bridge the ?semantic gap.? The work proposed in this grant is a multi-institutional collaboration combining research in medical image processing, machine learning and pattern recognition, knowledge representation and querying, and evaluation by domain experts in the field, is intended to advance the state-of-the-art in this direction. The archive of 60,000 cervigram images assembled by the National Library of Medicine and National Cancer Institute is an ideal collection for this purpose. The NLM cervigram archive forms a narrow image domain that has a limited and predictable variability. In such cases, explicit representation of domain knowledge alleviates the semantic gap between the low-level sensory recordings of a scene (raw image data), and objects and processes implied from images (semantic interpretation). This research will follow an information hierarchy that proceeds from raw image data to low-level image features, recognition of objects and tissue types, knowledge-based reasoning about disease processes, and, finally, tools and visualizations to support diagnosis decisions by clinical and NLM\/NCI collaborators. The research team will employ an underlying paradigm known as Computer-Assisted Visual Interactive Recognition, or CAVIAR, which considers the domain expert an integral part of the equation and attempts to optimize the performance of the complete human-machine system.<br\/><br\/>Intellectual Merit<br\/><br\/>Image content understanding is still considered a vexing open problem at the same time databases are growing rapidly in size and complexity. It is anticipated that this work will have a positive impact in areas relating to medical image analysis, including information extraction, organization, representation, and querying, as well as in training.<br\/><br\/>Broader Impact<br\/><br\/>Through the focus on the NLM\/NCI cervigram archive, this research may help advance the role of cervicography as a more cost-effective procedure than pap smears and colposcopy in screening for cervical cancer. Results from this targeted-domain project could also illuminate gaps and help establish new priorities for research in broader domains such as multimedia content structuring, understanding, indexing, and retrieval.","title":"`III-CXT-Small: Collaborative Research: Structuring, Reasoning, and Querying in a Very Large Medical Image Database","awardID":"0812120","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[371100,371101],"PO":["565136"]},"136585":{"abstract":"Sophisticated user interfaces are now technologically possible as we have moved from mouse and joystick to the stylus for the Palm, the trackwheel of the iPod, the touchscreen of the iPhone, the gesture recognition of Wii applications and immersive environments like the CAVE. The new frontier in user interfaces and interaction is that of mixed-reality applications which allow many different modes of interaction with the real and virtual worlds, and allow novel kinds of feedback from real to virtual and virtual to real. This project develops components and tools to build software components that will enable application designers to express their creativity without getting bogged down in the nitty-gritty detail of handling a variety of different input and output devices. This will enable a transformation not only in software but the way that software will be used. The tools will be evaluated by end users of the mixed reality applications and the designers for their ability to enhance creativity. More broadly, these tools will lead to new forms of communicating ideas, user interaction and collaboration. Additional impact will be achieved by broad dissemination of the research results, the open-source distribution of the tools developed by the project, and the involvement in the research of students from a broad range of economic, cultural and ethnic backgrounds.","title":"ITR-CreativeIT: Pilot: Mixed-Reality Without Tears: Tools to Support Creativity in Mixed-Reality Applications","awardID":"0757456","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["505729",362946,"557889"],"PO":["424970"]},"139621":{"abstract":"This project is about automated, visual object recognition. It is aimed at a computational approach which has two parts. First, it learns whether a given set of previously unseen images, say supplied by a user, contains any dominant themes, namely, subimages, that occur frequently and look similar. Such themes, and the associated subimages, are called categories and objects, respectively. Second, given a set of categories automatically inferred during the aforementioned training, and a new, test image, the approach recognizes all occurrences in the image of objects belonging to any of the learned categories. It delineates each such object in the image, and labels it with its category name. Both learning and subsequent recognition do not need human supervision. The subimages defining a category can be small or large, simple or complex. It is reasonable to expect that low-complexity categories, e.g., containing small\/few\/simple subimages are more common in real-world images. For example, the simple category of elongated shapes occurs as a part of legged animals, stools and scissors. More complex categories consist of large\/many\/complicated regions and are less common. Simple categories, e.g., the ``leg'' are thus shared by more complex ones, e.g., all legged animals, and, in turn, ``leg'' is an articulated combination of the category of elongated shapes (limbs). Therefore, category representation can be made easier by expressing it as a configuration of simpler categories, instead of subimages directly, thus yielding a hierarchical, subpart model. Accordingly, the proposed approach learns and recognizes categories as image hierarchies. The use of hierarchical embedding of regions as the defining image features results in several advantages the proposed approach offers over existing other methods which mostly use local features: (1) The proposed approach requires no supervision, e.g., labeling or segmenting of training images, or other input parameters from the user. (2) It simultaneously provides category detection and high-accuracy segmentation. (3) Training is feasible with very few examples, and not all training images must contain objects from the categories. (4) The use of hierarchical models makes explicit the relationship of a specific category to other categories of similar, lower and higher complexities; it also serves as a semantic explanation of why a category is detected when detected. Expected major contributions of the work include computational formulations of: (1) Accurate extraction of image regions; (2) Image representation by connected segmentation tree; (3) Robust image matching amidst structural noise in images; (4) Unsupervised extraction of hierarchical category models; (5) Efficient recognition of a large number of categories; (6) Unsupervised estimation of the relevance weights of subcategory detections to category recognition, and (7) Generalization of the proposed approach to extraction of texture elements, as an example of how the proposed work may impact other challenging vision problems involving hierarchy.<br\/><br\/>The progress made on this project can be seen at the website: http:\/\/vision.ai.uiuc.edu\/ahuja.html","title":"RI-Small: Discovery, Modeling and Recognition of Objects in Image Sets","awardID":"0812188","effectiveDate":"2008-09-01","expirationDate":"2012-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"J167","name":"National Security Agency"}}],"PIcoPI":["499628","542093"],"PO":["564316"]},"135397":{"abstract":"Traditional 9-1-1 systems, which date back to 1970s, support only voice, while non-emergency communications now feature other media. Adding additional media for 9-1-1 presents opportunities and challenges. Text messages, images captured by cell phones, video clips, and automatic crash notification messages can dramatically enhance the 9-1-1 services by expediting emergency responses and reducing crash clearance times. The rapid increase of residential, nomadic and mobile VoIP usage requires the development of VoIP-based next generation 9-1-1 systems and services that will replace the current circuit-switched 9-1-1 systems. Beyond limitations in media and mobility support, existing systems are inefficient and cannot easily accommodate new functionality. This project is a collaboration among University of North Texas, Columbia University and Texas A&M University, where UNT will be the lead institution, to develop a testbed that will enable research on understanding and analysis of next generation 9-1-1 services. The testbed make possible research and development in reliability, security, function-appropriate privacy and other areas that already difficult in large scale VoIP, but which become daunting when the VoIP system is critical infrastructure. <br\/><br\/>The broader impacts of this project are many. A testbed for Internet-based 9-1-1 research is particularly important as both state and federal governments are in the process of planning next-generation emergency communication platforms, unfortunately often without adequate vendor-neutral testing and evaluation. Users of the testbed will investigate issues related to locating<br\/>9-1-1 callers, securing Public Safety Answering Points, ensuring continuous availability of 9-1-1 services during large-scale emergencies, predicting emergencies, providing citizen alerts (\"reverse 9-1-1\"), and improving inter-agency coordination. The PIs expect to translate results from research on this infrastructure to engineering guidelines and disseminate results across government organizations, standards bodies such as IETF and National Emergency Number Association (NENA) and 9-1-1 centers. Moreover, the findings from the experiments in this project will be useful for the residents across USA.","title":"Collaborative Research: CRI: IAD: A Testbed for Research and Development of Next Generation 9-1-1 Services","awardID":"0751094","effectiveDate":"2008-09-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["518545"],"PO":["565327"]},"139511":{"abstract":"Techniques that can cost-effectively tolerate transient faults, also known as soft errors, play an essential role in enhancing the reliability of future billion-transistor microprocessors, built using nano-scale devices and advanced integration technologies. Today's design methodologies optimize microprocessor soft-error reliability using approaches within a single-layer (e.g. device, circuit, architecture). Moreover, existing soft-error tolerant mechanisms largely ignore the impact and effect of emerging process technologies (e.g. design parameter variation, 3D vertical integration). An important and urgent research task is to explore novel, cross-layer approaches that will allow microarchitects and circuit designers to cost-effectively mitigate the deleterious impact of soft errors in light of rapidly increasing design complexity and continuously advancing process technologies. This research addresses the above challenge by (1) investigating combined circuit and microarchitecture techniques to cost-effectively mitigate microprocessor soft error susceptibility; (2) studying the impact of process variation on soft error robustness and exploring techniques that can significantly enhance microarchitecture soft error reliability under design parameter variation; and (3) characterizing soft-error behavior in 3D die stacked chips and developing methods to effectively mitigate soft error vulnerability hot-spots in 3D microarchitecture designs, while maintaining desirable reliability\/thermal tradeoffs. The outcome of this research will provide a cross-layer solution to tolerate soft erros in future microprocessors. The PI will use the concepts, tools, techniques and other results of this research to introduce graduate and undergraduate students to the nature of soft errors, their impact on software execution and methods to mitigate their impact under emerging process technologies. The developed tools will be used to expose students to quantitative research methods and introduce them to fundamental techniques in reliability evaluation. The PI will actively recruit graduate and undergraduate students from minorities and under-represented groups for this project.","title":"CPA-CSA: Exploring Synergetic and Process Technology Aware Approaches to Effectively Enhance Processor Microarchitecture Soft Error Reliability","awardID":"0811611","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["550720"],"PO":["366560"]},"139632":{"abstract":"Despite recent successes in machine learning approaches to coreference resolution, the performance of recently developed coreference systems is plateauing. Progress on the coreference task is currently limited in part by the over-reliance on morpho-syntactic knowledge sources, which has led to the induction of overly simplistic coreference heuristics. To bring learning-based coreference resolvers to the next level of performance, this work adopts a knowledge-rich approach, investigating a variety of semantic and discourse knowledge sources for coreference resolution. <br\/>Specifically, it develops corpus-based methods for inducing semantic features, leveraging publicly available lexical databases and advanced machine-learning and inference techniques. In addition, it broadens the kind of discourse knowledge exploited by existing learning-based coreference systems, generating features for capturing not only the salience of a noun phrase but also the coherence of a text, via the use of discourse segmenters and parsers that are grounded in Centering Theory, Rhetorical Structure Theory, and Grosz and Sidner's discourse theory. The main results of this work will be to demonstrate the benefits of a knowledge-rich approach to learning-based coreference resolution, and to re-introduce the linguistically motivated discourse theories developed in the late 1970s and early 1980s into statistical coreference resolution models. The data sets produced in this research will be made available to the research community, and the experimental results will be disseminated via publications.","title":"RI-Small: Improving Machine Learning Approaches to Coreference Resolution","awardID":"0812261","effectiveDate":"2008-09-01","expirationDate":"2012-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["518623"],"PO":[371156]},"146298":{"abstract":"Wireless sensor networks that have the capabilities of sensing, computation and communication make<br\/>an ideal solution for emergency warning systems. Appropriate authorities expect immediate warnings<br\/>predicted by such systems. This research designs, implements, and evaluates an energy-efficient and<br\/>real-time data delivery model which can be employed in wireless-sensor-network-based emergency<br\/>warning systems. With the new data delivery model being used in an emergency warning system,<br\/>warnings can be obtained in real time, and network lifetime can be extended.<br\/><br\/>Different from the traditional 'data collection' and 'data aggregation' methods, this research focuses on<br\/>the stringent requirement of delivering warnings in real-time. Instead of managing a large amount of<br\/>raw data, a simpler mechanism for event detection and reporting that can reduce energy consumption as<br\/>well as notification delay is investigated. In addition to the impact on practice, the theoretical analysis<br\/>on the bounds of energy consumption and notification delay has a strong impact on the theory aspect.<br\/>Moreover, this project is a starting point to integrate research and education for the purpose of<br\/>attracting both undergraduate and graduate students to the area of wireless sensor networks.","title":"SGER: A New Framework for Energy-Efficient and Realtime Data Delivery in Heterogeneous Wireless Sensor Networks","awardID":"0844829","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["469265"],"PO":["432103"]},"156099":{"abstract":"Title: \"Collaborative Research: SoD-TEAM: Values at Play - Integrating Social Factors into Design\"<br\/><br\/>The focus of this project is on the development of a software design methodology, titled \"Values at Play (VAP)\" that takes values into account when developing software systems. The methodology, or set of best practices, addresses such issues as: how values can be consistently and systematically integrated into the design of software systems, how values investigations, in the context of technical design, can be made scientifically rigorous, and, the construction of a viable set of general software design principles that could lead to the integration of values across a variety of design tasks. The VAP project integrates the humanities, the arts, and social, and ethical interests into the scientific design of technological systems by specifically focusing on values in the design of games. The project involves three research activities including: 1. Development of a rigorous methodology (or, best practices) for supporting value conscious design; a handbook of the Values at Play methodology is made available in hardcopy and on the Web. This phase explores such issues as: how to identify values that might be relevant to a given design project, how to embody or express values in system design, how to engage in appraisal of whether and to what extent designers have successfully embedded target values in a given system. 2. Development of a values-in-design toolkit which includes: a) game building components to help students create games based on a values-orientation; and b) several working examples demonstrating values-to-design-feature correlations. 3. Assessment of design training interventions: the team will design several instruments to assess the values integration in the methodology and Toolkit. In addition to the research activities, this project engages faculty and practitioners in a Workshop about Values at Play. Embedding values within technological systems, and verifying that such systems actually reflects those values, is particularly appropriate using computer games since they affect larger society. This research will also be of use to educators and systems designers in the IT field considering game structures in their approaches as well as to scholars interested in the study of technology, society, and humanity. Potentially, this project might advance this line of inquiry to a pervasive, industry changing, international level of importance which not only promotes the humanistic study of these issues but enhances real products for real people.<br\/><br\/>Program Manager: Anita J. La Salle<br\/>Date: July 12, 2006","title":"Collaborative Research: SoD-TEAM: 'Values at Play: Integrating Ethical and Political Factors into System Design'","awardID":"0924088","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7372","name":"ITR-SCIENCE OF DESIGN"}}],"PIcoPI":["496273"],"PO":["557609"]},"139313":{"abstract":"CCF-0811287\/CCF-0810865<br\/>Collaborative Research: Trace-Driven Verification of Multithreaded Software<br\/>Zijiang Yang and Karem Sakallah<br\/><br\/>The ever increasing use of hyper-threading and the availability of inexpensive multiprocessor hardware present tremendous opportunities as well as serious challenges for software developers. In order for software applications to benefit from the continued exponential throughput advances in multicore processors, the applications must be well-written multithreaded software programs. Unfortunately, writing multithreaded software programs that can unleash the full potential of present and future hardware systems remains as challenging today as it was thirty years ago. This research aims to develop practical tools and methodologies that can bring down the complexity of testing\/debugging multithreaded programs to a level comparable to that of testing\/debugging sequential programs. To this end, existing debugging tools have to be enhanced with powerful reasoning engines that allow them to implicitly analyze all possible thread interleavings under the specified test inputs. During the course of this project a variety of approaches to achieve this objective will be investigated, including some novel ideas that seem particularly promising from a preliminary analysis: (1) efficient symbolic encoding of multithreaded programs, (2) trace-driven abstraction and refinement of their execution, and (3) performance enhancement techniques that allow this approach to scale to realistic program sizes.","title":"CPA-SEL: Collaborative Research: Trace-Driven Verification of Multithreaded Software","awardID":"0810865","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":[370281],"PO":["565264"]},"139445":{"abstract":"As general-purpose computing moves into the age of pervasive parallelism, programmability becomes the key hurdle limiting the effective use of available computing resources. Transactional memory promises to simplify parallel programming for application programmers. However, research in Transactional Memory is being seriously hampered by the lack of a reusable open source infrastructure. The project will develop the key pieces necessary to overcome this situation: A transactional memory library built out of highly decomposed pieces will provide reusable and replaceable parts suitable for investigating tradeoffs in software TM implementations. Standardized interfaces will allow libraries conforming to the interfaces to be used in a variety of environments. TM-aware run-time analysis tools, particularly profilers and debuggers, will provide the necessary tool support for TM implementors and application programmers to understand and improve the performance of software using transactions. Interesting benchmarks, in a variety of high-level languages, will move forward our understanding of TM performance characteristics.","title":"A Unified Open-Source Transactional-Memory Infrastructure","awardID":"0811289","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["554181"],"PO":["523800"]},"139589":{"abstract":"Location-based continuous queries are the backbone for monitoring, alerting, and notification location-based services. The current technologies and software infrastructure behind existing location-based continuous queries are not designed to handle their formidable rise in usage nor to maintain the location privacy of their users. The goal of this project is to conduct research, develop requisite knowledge, and build software infrastructure to (1) scale up location-based continuous queries to deal with very large number of users, and (2) inject privacy-awareness in location-based continuous queries where a high quality real time answer can be provided without the explicit knowledge of user locations. The project achieves its goals using the following approaches: (1) employ a built-in approach in which location-based queries are realized as query operators inside a database engine, (2) develop adaptive location-aware query optimization techniques, (3) exploit shared execution and load shedding techniques for location-based continuous queries(4) exploit the trade-off between user privacy and quality of service, and (5) develop efficient privacy-preserving continuous query processing techniques. Applications will benefit from this project include location-based emergency response, intelligent transportation systems, traffic control, fleet management, and location-based advertising. The project supports PhD students to pursue research in the areas of location-based services and database systems. Related tutorials and workshops are organized. A new graduate-level course integrating the research results from this project is introduced. Publications, technical reports, software, and experimental data from this research are disseminated via the project web site(http:\/\/www.cs.umn.edu\/~mokbel\/LocationBasedQueries).","title":"III-COR-Small: Towards Ubiquitous Location Services: Scalability and Privacy of Location-based Continuous Queries","awardID":"0811998","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["518156"],"PO":["565136"]},"139479":{"abstract":"Many large-scale, real-world problems are readily understood, represented, and solved as constraint satisfaction problems. Organizations throughout the world exploit this approach to solve difficult problems in design and configuration, planning and scheduling, and diagnosis and testing. Nonetheless, each new, large-scale problem faces the same bottleneck: scarce human experts must select, combine, and refine the various techniques currently available for constraint satisfaction and optimization. This cognitively-oriented project increases the ability of both people and machines to address challenging new constraint satisfaction problems.<br\/><br\/>The resultant autonomous, robust system reasons from past experience, but with the ability to recognize and respond intelligently to novelty. The new approach integrates a variety of techniques to capture crucial subproblems, the most informative and conflict-ridden parts of a problem. Because crucial sub-problems often recur with only small variations, knowledge about how to solve them may be re-used. Moreover, when a problem is unsolvable, the system identifies crucial subproblems for human analysis and reformulation ? a first step toward collaborative problem solving.<br\/><br\/>This project speeds the uptake of an important technology. It generates knowledge about crucial subproblems, search, representation, and learning for constraint solving, and thereby makes constraint-programming expertise more readily available. This project analyzes the efficacy of its approach on a variety of constraint problems, particularly real-world problems.","title":"Integrating Problem-driven and Class-based Learning for Constraint Satisfaction","awardID":"0811437","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["528943"],"PO":["491702"]},"143791":{"abstract":"Until recently, most high-performance computing applications involved matrix computations. However, new application areas such as computational biology and machine learning involve constructing, computing with, and modifying large sparse graphs. <br\/><br\/>The Galois team at the University of Texas, Austin has been building a system that exploits optimistic parallelization and compiler analysis to raise the level of abstraction at which these applications can be coded without a substantial performance penalty. The programming model is sequential Java augmented with two constructs called optimistic set iterators. This project brings together Galois project team members, computational biologists and machine learning experts to extend and evaluate the Galois system on high-end shared-memory machines and large-scale distributed memory for applications that involve computations on large sparse graphs. The main thrusts of the project are (i) design and implementation of modest language extensions for supporting nested data-parallelism, (ii) porting the existing system to distributed-memory machines, (iii) implementation of adaptive feedback-driven parallel execution, and (iv) development of compiler analyses to optimize execution.","title":"Language and System Support for Petascale Irregular Applications","awardID":"0833162","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7583","name":"ITR-HECURA"}}],"PIcoPI":["556730","517321","517663"],"PO":["565272"]},"143560":{"abstract":"Significant advances have been made over the past decade to establish high performance acoustic communication links for many ocean environments. However, there has been limited development on underwater acoustic networks, the fact that is in stark contrast with the terrestrial wireless networks. In an underwater environment, network nodes<br\/>are neither small nor inexpensive, and the harsh environment renders network deployment difficult. Network design is challenged by the fundamental nature of underwater sound propagation: channel attenuation depends on the distance and signal frequency, resulting in a distance-dependent bandwidth; extensive time-varying multipath causes delay spreads of tens or hundreds of milliseconds, and low speed of sound (1500 m\/s) results in severe motion-induced Doppler distortion and extreme channel latency. These facts necessitate dedicated design of communication algorithms and protocols on all network layers, their cross-coupling, as well as careful consideration of the overall system topology and architecture. <br\/><br\/>In the design of underwater wireless networks, the challenge is to exploit, rather than avoid the peculiar effects of acoustic propagation. Towards this goal, emphasis is placed on three areas: architecture design (determining the degree of hierarchy and multi-hopping, and associated algorithms and protocols), system optimization, and resource allocation (power and bandwidth). <br\/><br\/>Currently, no analytical results exist on optimized network deployment; furthermore, system capacity is unknown. Anaytical Results that are based on physical laws of acoustic propagation, and not on radio-like models, are expected to provide useful tools in the design of future networks challenged by high latency and limited resources.","title":"Collaborative NETS-NECO: Wireless Underwater Multi-tiered Acoustic Networks (WUMAN)","awardID":"0832186","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["515828"],"PO":["557315"]},"143450":{"abstract":"The inability of programmers to write vulnerability free code is the most pressing problem in practical computer systems security. The most serious class of vulnerabilities is memory vulnerabilities, which generally allow an attacker to subvert the program's control flow. In response to this problem, generic mitigations have been widely deployed that, through changes to the operating system and processor, seek to make it impossible for attackers to exploit errors in programs.<br\/><br\/>Implementers considering deploying such mitigations must know how much (if at all) a generic mitigation improves security and what its costs are if they are to allocate R&D resources wisely.<br\/>Unfortunately, until recently, the benefits of generic mitigations were studied only superficially. Recent first steps have already shed some light, showing, e.g., that the widely deployed \"W-xor-X\"<br\/>mitigation provides no security benefit whatsoever.<br\/><br\/>This project puts imperfect, ad-hoc mitigation on a scientific footing. It provides a formal, comprehensive analysis to determine the cost-benefit equation is for generic mitigations.<br\/><br\/>The project begins by producing quantitative analyses of current mitigation techniques and of attacks; these analyses facilitate the creation of new mitigations that resist attacks that foil current mitigations; these new mitigations are implemented, evaluated, and disseminated. In addition, the project develops a sandboxed environment for experimenting with software vulnerabilities and malicious code, and a curriculum for teaching systems security.<br\/><br\/>The results will be better use of implementation resources for vendors; a more secure legacy software environment for users; and better security education for the next generation of programmers, so they will not make the mistakes earlier ones did.","title":"CT-ISG: Memory Safety for Legacy Software, A Quantitative Approach","awardID":"0831532","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["521727"],"PO":["565327"]},"143340":{"abstract":"Collaborative Research:<br\/>A Mathematical Framework for the Performance Evaluation of Large-Scale Sensor Networks<br\/><br\/>Jeffrey P. Kharoufeh and Rusty O. Baldwin<br\/><br\/><br\/>This grant provides funding for creating and analyzing mathematical models of large-scale wireless sensor networks (WSNs). A WSN is a collection of sensing devices linked via a wireless transmission medium for the purpose of sensing and conveying information about objects, their surroundings, and their interactions, without human intervention. When the information is time sensitive, the network?s performance capabilities are critical to meeting quality-of-service guarantees. However, assessing network performance is difficult because the location of the data is generally not known a priori, or it may not exist in the network at all, sensors have limited energy reserves and fail when these reserves are exhausted, sensors may fail due a harsh operating environment, and communication channel quality may vary with time, impacting the timely delivery of critical data. These characteristics distinguish sensor networks from other communication networks and significantly complicate the task of modeling and predicting their performance over time. If successful, the results of this research will lead to fundamental principles, effective algorithms and optimization schemes that will enhance the design, analysis, control and realization of WSNs. Using a queueing network-based model, specific performance parameters will be investigated including the network energy expenditure, average time required to respond to queries and the query success rate (the proportion of queries that are answered on time). The sensitivity of the network?s performance to query deadline distributions, sensor failures and the network?s environment will also be assessed. Finally, optimization of the network?s parameters will be investigated to ensure resilience in various operating environments.","title":"Collaborative Research: NECO: A Mathematical Framework for the Performance Evaluation of Large-Scale Sensor Networks","awardID":"0830919","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["528297"],"PO":["564993"]},"143472":{"abstract":"The Internet today is a global infrastructure used by one and all. It is also the life-line of many businesses. The Internet, however, still lacks the kind of reliability and robustness we expect from critical infrastructure. While the network has in-built fault-tolerance, it is typically restricted to hard failures alone; the network is equipped to neither detect nor react to soft failures such as high delays or losses that may be due to faulty implementations or hardware failures or congestion related. To support delay- and loss-sensitive applications, it is important to devise efficient mechanisms to quickly detect such failures and recover from them in an automated fashion. Network devices provide very little support for debugging performance problems?let alone automatically respond to them. Operators today inject active probes between pairs of provider edge routers to detect any forwarding problems such as high delays and losses, in their network. Operators typically rely on applying indirect inference algorithms, such as tomographic approaches, that can infer root causes by joining active probes with network topology snapshots. Given the problem is fundamentally under-constrained; inference is only approximate at best and can be quite slow. Automated response mechanisms require fast and accurate localization in order to be effective. The main research focus of this research is to create novel in-network fault management mechanisms to automatically detect, localize, report and respond to failures and other performance degradations.<br\/><br\/>The research revolves around three basic ideas: 1) Equipping routers with specialized high-speed low-complexity measurement primitives to measure delay and loss; 2) Using this feedback to allow routing protocols to automatically respond to congestion or chronic failure conditions; and 3) Integrating these mechanisms into NetFlow to provide per-flow delay and loss. Together, these mechanisms provide a rich set of tools for network operators and administrators to monitor and manage their networks efficiently and improve the overall fault-tolerance properties of these networks.<br\/><br\/>Intellectual Merit: This research will contribute to the established research area of network fault-tolerance by allowing the network to automatically detect and respond to soft failures; it will carve new areas of research on scalable router primitives for high-fidelity measurements; and; it will significantly enhance the capabilities of routers by incorporating new definitions of flows commensurate with latest innovations and advancements in the Internet. This research directions combine three disparate areas of research?scalable router primitives, fault-tolerant routing protocols, and measurement?to significantly improve the robustness of critical infrastructure, the Internet. The work has the potential to provide a powerful fault management platform the Internet requires to sustain the next generation of delay-sensitive and interactive applications.<br\/><br\/>Broad Impact: The project will contribute to the education of the next generation of networking engineers and designers. The researchers will disseminate the results through the traditional academic channels, and will actively participate in industry forums leveraging their collaborations with AT&T and Cisco. They will also design and revise courses in Networking and Router Architectures in the graduate and undergraduate curriculum, and supervise Ph.D, Master, and undergraduate projects through the Honors program.","title":"NECO: Architectural Support For Fault Management","awardID":"0831647","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["518558"],"PO":["564993"]},"143241":{"abstract":"This research involves the development of optimization techniques for<br\/>the automatic determination of the material properties of deformable<br\/>solids via modern imaging and scanning technologies (e.g. MRI, CT,<br\/>etc.) with particular focus on hyper-elastic materials under large<br\/>deformation, high strain rates and often plasticity. Such materials<br\/>are characteristic of biological soft tissues (e.g. muscle, fat, skin<br\/>etc.) and their determination is critical to creating accurate,<br\/>subject specific simulations of the function of these anatomical<br\/>structures. Simulation using modern computational techniques is an<br\/>increasingly reliable tool for answering many questions in biomedical<br\/>engineering ranging from basic functionality to post-surgical response<br\/>of complex regions of the anatomy. However, reliable results are only<br\/>possible when accurate constitutive descriptions of the simulated<br\/>materials are available. Although much work has been done to develop<br\/>and determine constitutive models for biological soft tissues, most<br\/>material parameters have been estimated from cadaveric specimens or<br\/>from a small group of individuals. This is an unacceptable<br\/>simplification given the widely established variation in material<br\/>behavior across subjects and from the material changes inherent in<br\/>cadaveric specimens. This research will establish techniques for the<br\/>near-automatic determination of subject specific behavior necessary to<br\/>continue the relevance and reliability of biomedical simulation of<br\/>soft tissues. Though motivated by biomedical simulation, the<br\/>techniques developed in this research transcend the boundaries of<br\/>biomechanics and will provide a more general framework for determining<br\/>material properties for engineering applications.<br\/><br\/>The PI and collaborators will use their combined experience in<br\/>constitutive modeling, finite element simulation of soft tissues, mesh<br\/>generation, optimization and medical imaging to formulate and develop<br\/>methods for the determination of constitutive parameters from imaged<br\/>behaviors. This task is formulated as an inverse problem of fitting a<br\/>constitutive model to observed material motion (involving imaging,<br\/>segmentation and denoising). The PI and collaborators will draw upon<br\/>their prior experience with similar techniques for estimating muscles<br\/>activation parameters and muscle material properties from material<br\/>deformation. The capstone problems to be solved involve accurately<br\/>tracking material particle trajectories with imaging technologies and<br\/>approximating the Jacobian of elastic equilibrium configurations of<br\/>soft tissues with respect to unknown material parameters in a given<br\/>constitutive model. With this functionality, the PI and collaborators<br\/>will determine the suitability of established optimization techniques<br\/>and develop novel approaches (both engendered by the successful<br\/>solution of previously mentioned capstone problems).","title":"An Optimization Framework for the Estimation of Material Properties of Deformable Materials from Volumetric Deformation Measurements","awardID":"0830554","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["465902"],"PO":["565251"]},"143362":{"abstract":"Energy infrastructure is a critical underpinning of modern society that any compromise or sabotage of its secure and reliable operation will have a prominent impact on people's daily lives and the national economy. Past failures such as the massive northeastern power blackout in August 2003 and the recent Florida blackout in February 2008 have revealed serious defects in both system-level management and device-level designs. This project is aimed at a hardware-in-the-loop reconfigurable system with embedded intelligence and resilient coordination schemes to tackle the vulnerabilities of the power grid. The research in this project will investigate a number of critical issues related to this system, including the design of a hybrid system architecture, the development of advanced power electronic devices with embedded intelligence for reconfiguration of the power grid, the development of control- theoretic algorithms to guarantee real-time adaptation to system changes caused by various attacks, and the threats to existing state estimation algorithms and their defenses.<br\/><br\/>The proposed research can help secure the grid and prevent potential outages from happening. Instead of limiting the system to manage existing devices, the developed system is adaptive to the future power grid in years to come. The power grid is a typical example of complex networks of highly interacting subsystems. Solving these fundamental problems to create a resilient power grid has a direct and immediate impact on this and other critical infrastructures.","title":"CT-M: Collaborative Research: A Resilient Real-Time System for a Secure and Reconfigurable Power Grid","awardID":"0831059","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":[381746],"PO":["497499"]},"143483":{"abstract":"Significant advances have been made over the past decade to establish high performance acoustic communication links for many ocean environments. However, there has been limited development on underwater acoustic networks, the fact that is in stark contrast with the terrestrial wireless networks. In an underwater environment, network nodes<br\/>are neither small nor inexpensive, and the harsh environment renders network deployment difficult. Network design is challenged by the fundamental nature of underwater sound propagation: channel attenuation depends on the distance and signal frequency, resulting in a distance-dependent bandwidth; extensive time-varying multipath causes delay spreads of tens or hundreds of milliseconds, and low speed of sound (1500 m\/s) results in severe motion-induced Doppler distortion and extreme channel latency. These facts necessitate dedicated design of communication algorithms and protocols on all network layers, their cross-coupling, as well as careful consideration of the overall system topology and architecture. <br\/><br\/>In the design of underwater wireless networks, the challenge is to exploit, rather than avoid the peculiar effects of acoustic propagation. Towards this goal, emphasis is placed on three areas: architecture design (determining the degree of hierarchy and multi-hopping, and associated algorithms and protocols), system optimization, and resource allocation (power and bandwidth). <br\/><br\/>Currently, no analytical results exist on optimized network deployment; furthermore, system capacity is unknown. Anaytical Results that are based on physical laws of acoustic propagation, and not on radio-like models, are expected to provide useful tools in the design of future networks challenged by high latency and limited resources.","title":"Collaborative NETS-NECO: Wireless Underwater Multi-tiered Acoustic Networks (WUMAN)","awardID":"0831728","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["515773"],"PO":["557315"]},"134540":{"abstract":"CAREER: The Inverse Shade Tree Framework for Material Acquisition<br\/>Jason Lawrence, University of Virginia<br\/><br\/><br\/>Digital models of the way materials scatter light are necessary to synthesize the visual appearance of real-world 3D objects and have applications ranging through digital arts to manufacturing, immersive virtual environments, remote sensing, and archaeology. However, capturing and authoring these models remains impractical due to the delicate and tedious measurement techniques currently available and the paucity of structured representations and editing tools. This research involves casting these problems in a new mathematical framework that will lead to the development of more useful representations and easy-to-use acquisition methods.<br\/><br\/>The project is investigating hierarchical Bayesian models and inference techniques to provide compact and structured representations for measured appearance data in computer graphics. These models have the property that the latent structure inferred from high-dimensional and large (multi-gigabyte) datasets corresponds to intuitive and editable components, thus greatly extending the utility of measured data within production settings. Existing representations become special cases within this general probabilistic setting, leading to the development of entirely new representations for a broader class of materials than previously studied. Additionally, this research investigates new measurement techniques that combine probabilistic inference with a material database to provide fast and robust acquisition in noisy environments","title":"CAREER: The Inverse Shade Tree Framework for Material Acquisition, Analysis, and Design","awardID":"0747220","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["448317"],"PO":["532791"]},"146640":{"abstract":"The intimate connection between source coding (data compression,<br\/>quantization) and simulation (the generation of a signal with prescribed distributions from a purely random mechanism such as coin flips) has been known for over three decades. Recent results suggest even deeper connections with potentially significant implications for the related problems of compression code design, modeling random processes for the analysis and design of signal processing and coding systems, and understanding the nature and structure of mathematical models of random processes capturing the important properties of signals arising in the real world. This research is concerned with developing precise results characterizing and applying these connections to obtain new insight, theory, and algorithms.<br\/><br\/>In 1977 the performance of an optimized source coding system was shown to be bounded by the quality of a constrained simulation of the source, with equality under certain conditions. In 2008 this connection was strengthened by a precise formulation and proof of an information theoretic ``folk theorem'' stating that source coding systems performing near the Shannon optimum yield bit streams that are ``nearly coin flips'' in the rigorous sense of closeness in Ornstein's d-bar process distance. Together these results imply that source coding systems --- including digital speech, audio, image, and video communication and storage systems --- and simulation systems --- comprising stationary codings or filterings of iid bits --- are mathematically approximately equivalent systems, and hence the theory and design algorithms appropriate for one yield corresponding results for the other. This project exploits these connections to develop new theory and design algorithms for codes for compression, simulation, and modeling based on known distributions and on distributions learned from data.","title":"Source Coding and Simulation","awardID":"0846199","effectiveDate":"2008-09-01","expirationDate":"2011-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":["441541"],"PO":["564898"]},"143010":{"abstract":"Behavior-based robotics was established around the idea that robots could be constructed by connecting elementary sensor and actuator modules, without the need to form any internal representation of the world in which they operate. When designed appropriately, such robots, both as individuals and as groups, exhibit complex and seemingly ?intelligent? behaviors, solving challenging tasks in real time, while reacting only to their local environment and obeying sets of local rules. In part, this approach to robotics was inspired by considering natural systems, such as self-organization and adaptability of social insects in their colonies. An analogous approach to initiate the development of the field of behavior-based molecular robotics will be performed over the next three years, leading to groups of molecules that would appear to an observer to show a variety of task-oriented behaviors or some form of purposeful and dynamic self-organization. <br\/><br\/>Individual behavior-based molecular robots are single molecules displaying multiple sensors-actuators. When exposed to artificial landscapes displaying substrates keyed to their sensors-actuators, the molecules start executing elementary steps, determined, in a stochastic sense, by their constantly changing local environments. On some landscapes, called prescriptive, individual molecular robots and their collectives will execute algorithms mimicking wound-up automata with sequence control mechanisms. In contrast, on non-deterministic landscapes, the robots and their collectives will demonstrate properties emerging from internal organization of individual sensors and actuators and through local interactions between molecules and their environments. Importantly, these new interpretations of molecular behaviors will allow radically different experiments from all previous approaches to molecular robotics, while keeping experimental designs realistic, leading to embodiment in the physical world.","title":"Collaborative Proposal: EMT\/MISC Behavior Based Molecular Robotics","awardID":"0829744","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["459219"],"PO":["565223"]},"143252":{"abstract":"Discrete approximations of a surface or volume are necessary in numerous computational applications that require models of geometric objects. These applications typically assume that the geometric domain under consideration is divided into small, simple pieces (typically triangles or quadrilaterals in two dimensions, and tetrahedra or hexahedra in three) called finite elements. The collection of finite elements is referred to as a mesh. One such important application area is medical imaging, where high-quality meshes for modeling anatomy, represented as voxel-based images, are critical for computer-assisted clinical analysis of medical data. Meshes made of quadrilateral\/hexahedral (quad\/hex) elements offer lower mesh complexity and better solution quality than their triangular\/tetrahedral counterparts. However, the generation of quad\/hex meshes for arbitrary three-dimensional geometries is a difficult problem, and algorithms to generate meshes with provable guarantees on quality are available only for some restricted types of input. This research addresses geometric, combinatorial, and algorithmic questions related to the generation of quadrilateral surface meshes and hexahedral volume meshes for three-dimensional geometries obtained from volumetric imaging data.<br\/><br\/>Tools from digital topology and graph theory will be exploited to design algorithms to generate quadrilateral meshes of guaranteed quality for surface representations of voxel-based images. Robust methods for hexahedral mesh generation for digital volumes will in turn be designed by utilizing quadrilateral surface meshing algorithms developed as part of this research. Many fundamental geometric questions related to quad\/hex mesh generation remain unanswered. A formal understanding of these questions for the special types of geometries determined by volumetric imaging data is critical for the design of implementable algorithms that provide guarantees on mesh quality. This research project also involves collaboration with medical imaging experts, who will provide evaluation and validation of all meshing algorithms via finite element methods for three-dimensional, non-rigid image registration of volumetric MR images of human organs. Most image registration methods in current practice remain two-dimensional. Improved registration accuracy made possible by guaranteed-quality volume meshes would have enormous impact on clinical studies of medical data and greatly benefit medical practitioners.","title":"CNPq\/NSF, RUI: Surface and Volume Meshes for Volumetric Imaging Data","awardID":"0830589","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[381439,381440],"PO":["565157"]},"143142":{"abstract":"Abstract<br\/>_____________________________________________________________________________________<br\/>Collaborative Research: Data Communication via Particle Velocity Channels - A Paradigm Shift in Underwater Acoustic Communication<br\/><br\/>Over 75% of the earth?s surface is covered with water, with many resources upon which human life depends. High speed wireless data communication with acoustic waves among underwater sensors, deepwater moored instruments, autonomous underwater vehicles, and surface vessels is of crucial importance in many applications of national interest. Examples include offshore oil industry, environmental and ocean monitoring to predict natural disasters such as hurricanes, and so on. However, the underwater acoustic channel is a complex and highly bandlimited environment, and the achievable data rates by current systems are much smaller than the needs.<br\/><br\/>The core novel idea of the research is to transform the foundation of underwater acoustic communication, by communicating over the unexplored degrees of freedom of the acoustic field, i.e., the acoustic particle velocity channels. Over the past few decades, only the pressure channel of the acoustic field has been used for underwater communication. The key concept in this research is to take advantage of the vector components of the acoustic field, such as the three components of acoustic particle velocity. Particle velocity channels are promising for high speed communication, due to their possibly smaller delay spreads. The small size of particle velocity transceivers is another advantage over large pressure-only arrays traditionally used for underwater communication. In this research the investigators develop a cohesive framework for high rate underwater communication via acoustic particle velocity channels. The research objectives fall into two closely-related categories: channel modeling and transceiver design. Channel modeling objectives aim at understanding and characterization of particle velocity channels, whereas transceiver design objectives address new issues encountered at the channel modeling stage.","title":"Collaborative Research: Data Communication via Particle Velocity Channels - A Paradigm Shift in Underwater Acoustic Communication","awardID":"0830190","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["558975"],"PO":["564898"]},"143263":{"abstract":"The following is a revised summary of the specific tasks that will be addressed<br\/><br\/>through this effort:<br\/><br\/><br\/><br\/>Develop automatic sparsity detection techniques for Jacobians and Hessians.<br\/><br\/>Implement the techniques in the Automatic Differentiation tool ADOL-C.<br\/><br\/><br\/><br\/>Further develop the coloring software package COLPACK and enhance it with<br\/><br\/>new bicoloring algorithms. <br\/><br\/><br\/><br\/>Apply Automatic Differentiation to a class of problems in chromatographic separation techniques <br\/><br\/>in chemical engineering. <br\/><br\/><br\/><br\/>Develop interfaces between the software packages COLPACK and ADOL-C, to enable its<br\/><br\/>wider applicability to optimization codes. <br\/><br\/><br\/><br\/>Develop efficient algorithms for the directed acyclic graph reversal problem in adjoint computation.<br\/><br\/><br\/><br\/>Contribute to education and training in combinatorial scientific computing and <br\/><br\/>Automatic Differentiation by developing <br\/><br\/>a graduate level course, organizing mini courses and tutorials in conferences, and <br\/><br\/>by writing expository articles and book chapters. <br\/><br\/><br\/><br\/><br\/><br\/>The PIs will make every attempt to meet the scope and level of effort of the revised project.","title":"Empowering Computational Science and Engineering via Automatic Differentiation","awardID":"0830645","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["518525","518526"],"PO":["565157"]},"143384":{"abstract":"Regardless of whether sensors are used just for observations or as the basis for actual responses, the trustworthiness of sensor data becomes critical when important decisions are based upon these data. Unlike traditional networked systems, sensor networks also take measurements of physical phenomena. Traditional information assurance, which focuses on the integrity of data, does not cover all of the sources of errors that might arise in sensing data. In fact, sensor data can be corrupted at the environmental level, whether through a natural loss of calibration or through a deliberate perturbation of the measurement environment by an adversary. These sources of errors, which affect the process of measurement (PoM), are unique to sensor networks and cannot be addressed through the usual network-centric methods. Hence, to complement traditional information assurance services, defense mechanisms are needed to protect the sensor network from PoM errors. Only when a trust wrapper surrounds sensor measurements should applications make decisions or take actions with important implications.<br\/> <br\/>The proposed research consists of two aspects: corruption research and assurance research. For the former, the team will conduct a thorough threat analysis by cataloging the set of PoM errors that may be introduced for a variety of different sensors. For the latter, the team will augment the existing programming stack for sensor networks by developing a suite of measurement assurance tools. <br\/> <br\/> <br\/>Developing methods that give sensor networks a natural immunity to PoM errors will allow sensor applications to have a greater chance for success and wider spread use.","title":"CT - ISG: ROME: Robust Measurement in Sensor Networks","awardID":"0831186","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["535281","564747","494759"],"PO":["497499"]},"145452":{"abstract":"This project seeks fund to organize a workshop on Cyber-Physical Systems (CPS) to explore developing and understanding commonalities and differences between the cyber, physical, and social worlds. This two-day workshop will bring together three traditionally disparate groups: engineers, computer scientists, and social scientists. It will open up this new area in a significant way, allowing people from greatly different disciplines to communicate with each other and form cross-disciplinary bridges. It will add value to the existing series of workshops in establishing a Cyber-Physical Systems community by identifying the challenges in cross-disciplinary communication and publishing the commonalities and differences so that new researchers can be brought into the Cyber-Physical Systems community.","title":"SGER: Bridging the Cyber, Physical, and Social Worlds","awardID":"0840888","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[388044],"PO":["535244"]},"143032":{"abstract":"Hybrid CMOS-Nano-CMOS Architectures and CAD Tools <br\/>for Nanoelectronic and Bio-Inspired Applications<br\/><br\/>Abstract<br\/><br\/>Emerging within the still novel field of nanoelectronics are many technologies that offer features such as reconfigurability, non-volatile memory, and low power consumption. Through this work, we intend to explore ways in which such attractive properties can be exploited in the design and development of a high-density reconfigurable architecture. This project combines two leading technological concepts for the development of future electronic systems: hybrid CMOS-nanoelectronic circuits and 3D integration. Our project will focus on studying methodologies associated with the design and development of novel 3D CMOS-nano-CMOS circuits. Specifically, we will prototype and evaluate a small hybrid CMOS-nano circuit consisting of a multistage nanoscale PLA based on metal oxide nanoelectronic switches with a direct interface to a layer of CMOS circuitry. The goal of this project is to lay the groundwork for developing more complex 3D CMOS-nano systems in the future. As part of this research, we will study methods for accurately modeling nanoelectronic devices and circuits for robust design, optimizing CMOS-nano-CMOS circuit designs using state of the art CAD techniques and fabricating and integrating these different technologies on a single die.","title":"EMT\/NANO: Hybrid CMOS-Nano-CMOS Architectures and CAD Tools for Nanoelectronic and Bio-Inspired Applications","awardID":"0829824","effectiveDate":"2008-09-15","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["550604","234536",380828],"PO":["562984"]},"144011":{"abstract":"This research focuses on developing concepts and methodologies to evaluate and design property-preserving interfaces for safety-critical systems. These are systems that require real-time sensing\/control and rely on existing networking infrastructures for coordination and information exchange. These systems, which range from power distribution networks and air (or other) traffic control systems to avionic controllers and to embedded automotive electronics, are characterized by high complexity that is associated with both discrete and continuous aspects. The resulting intricate behavior of these safety-critical interconnected systems challenges traditional notions for safety, security, and reliability. It necessitates the development of new methodologies for understanding how to obtain compositions of such modular systems and how to design interfaces that achieve not only robust operation and performance but also ensure trust and privacy among users. The research seeks a unifying and multifaceted approach to this problem. The approach is to decompose the large body of research on modular systems and interface design to address: state estimation in interacting modular systems and implications to safety; model verification and property checking for modular hybrid systems; and compositional models and interface design in switched discrete and continuous control systems. The research draws on areas as diverse as distributed algorithms, robust and fault-tolerant design, hybrid system control, performance evaluation, applied probability, graph theory, distributed estimation, and formal methods. These are applied to the problem of analyzing and understanding the tradeoffs involved in the design of modular systems with real-time sensing and control capabilities. The research is expected to have significant impact in permitting and enabling the ubiquitous use of critical network infrastructures for a variety of diverse applications.","title":"CSR-EHCS (EHS), TM: Compositional Technology for Safety-Critical Modular Systems","awardID":"0834409","effectiveDate":"2008-09-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["456451",383890,383891,"523280",383893],"PO":["561889"]},"145463":{"abstract":"This small request from Landau, to fund students and young researchers to attend the Conference on Computational Physics and the associated satellite meeting in Ouro Preto, Brazil this August, is very worth of funding by NSF. This is the major meeting in computational physics, held every year here or abroad. It contains the right mix of people for scientists interested in this interdisciplinary field to become educated on what is happening in omputational physics. I recommend that the Division proceed with the funding of the proposal at the requested level of $17,000 and without external review. The PAM states that, Proposals for conferences, symposia, and workshops, subject to the following guidelines:proposals up to $50,000 do not require external merit review, but rather are subject to the judgment and recommendation of the Program Officer. Note that all of the funds for this are coming from from CCF in CISE and DMS in MPS. The funding profile is $7,000 from DMS, code 03040100-1271 and $10,000 from CCF code 05010000-2865.","title":"Travel Support for the Round Table Satellite Workshop in Belo Horizonte and the 2008 Conference on Computational Physics (CCP2008) in Ouro Preto, Brazil","awardID":"0840920","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["460793"],"PO":["467733"]},"145584":{"abstract":"This project creates and evaluates new visualization techniques of models from a concept generator to stimulate the designer to generate concepts not originally posed by designer or computer. This technology is expected to enable the development of creative solutions to design problems that would otherwise go undiscovered. The three challenges to achieving effective visualization for enhancing creative design at the concept generation phase are: 1) how to cluster the many concept variants returned from an automated concept generation algorithm into a manageable set of representative concepts that spans the design alternative space; 2) how to visually represent the option space to the designer so that it enhances creativity; and 3) how to measure the impact of the visualization schemes on designer creativity. This exploratory research, if successful, offers the opportunity to transform how we design products and systems and how we guide any designer to a creative new product.","title":"SGER Collaborative Research: VisualizeIT - Measuring the Impact of IT-Enabled Concept Generation on Designer Creativity","awardID":"0841389","effectiveDate":"2008-09-01","expirationDate":"2010-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["526863","476585"],"PO":["565227"]},"143043":{"abstract":"This project exploits methods from statistical physics to provide fundamental advances in computing and communication systems. The intersection of computer science, information theory and statistical physics has seen a recent explosion of activity, resulting in new algorithms and new methods of analysis. Discrete computational challenges including constraint satisfaction, error correction and control of massive networks have benefited from techniques and insights offered by statistical physics. Physics, at the same time, has been significantly enriched by approaches from discrete computation, such as message-passing algorithms. <br\/><br\/>The investigators study two complementary approaches for addressing algorithmic challenges: 1) treating problem instances as members of a random ensemble that can be analyzed as a physical model, and 2) identifying specific classes of instances amenable to physical analysis. The first suggests a fundamental connection between algorithmic performance and an underlying physical phase structure, and has already led to significant new algorithms for unstructured random graphs or networks. The challenge is to generalize it to structured cases. The second uses techniques such as renormalization group and multiscale decomposition, and is proving to be a powerful new approach in probabilistic inference.","title":"EMT\/MISC: Collaborative Research: Harnessing Statistical Physics for Computing and Communication","awardID":"0829861","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["560669"],"PO":["565157"]},"143285":{"abstract":"Towards modeling wireless networks -- When connectivity meets mobility!<br\/><br\/>The search for appropriate wireless network models that capture the effects of node locations and user interferences has led to the introduction of several classes of random graphs with increasingly complex notions of adjacency (i.e., one-hop connectivity). In such one-hop connectivity graph models, the presence of an edge between two nodes captures their ability to communicate directly and reliably with each other. However, viewed as systems, networks are \"greater than the sum of their parts\" -- One-hop connectivity gives rise to \"network connectivity\" as network resources collectively enable end-to-end data transfer between participating nodes. <br\/>When the graph determined by the one-hop connectivities is static (or slowly changing at the time scales of interest), network connectivity is readily identified with the usual notion of graph connectivity in the one-hop connectivity graph. In the presence of mobility, the one-hop connectivity structure of the network changes over time, graph connectivity may no longer be suitable to capture network connectivity and other, more appropriate, notions need to be considered. With this in mind, we introduce the notions of continuous connectivity and fly-through connectivity: Continuous connectivity requires that a path exists between every pair of nodes at all times. Fly-through connectivity only demands that a multi-hop end-to-end path be provided within acceptable delays, allowing for the possibility that the network is not continuously connected. <br\/><br\/>We explore how these different notions of network connectivity shape resource allocation (e.g., energy) in the presence of node mobility. Concerning continuous connectivity, emphasis is on (i) identifying zero-one laws and the attending critical scalings, and on (ii) determining the existence and width of associated phase transitions. The approach and methods are probabilistic in nature. Major efforts will be made to find useful characterizations for fly-through connectivity. Tools from algebraic graph theory and from the spectral theory of graphs are expected to play a key role. This should result in more realistic models for wireless networks for the purpose of designing robust and efficient resource allocation algorithms in the presence of node mobility, and for evaluating their performance.","title":"Towards Modeling Mobile Wireless Networks -- When Connectivity Meets Mobility!","awardID":"0830702","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["518081",381525],"PO":["564924"]},"144022":{"abstract":"With the increasing demand on memory performance by multi-core processors, the memory subsystem has become a new thermal concern along with the processor and the hard disk drive. To address this emerging issue, this project addresses several new, system-level Dynamic Thermal Management (DTM) schemes that coordinate the DRAM thermal management with the processor performance throttling, such as dynamically adjusting the number of active processor cores or scaling the processor's frequency and voltage level based on the memory thermal status. The project also studies coordinated thermal management schemes that consider the thermal requirements from both the processor and the memory subsystem. Thermal-aware OS job scheduling is further considered to smooth memory traffic and DRAM heat generation over time by mixing jobs with different memory demands appropriately. In addition, thermal-aware page allocation is proposed to avoid unbalanced overheating from some memory chips by considering the location of each chip and the memory access demand of each application. These schemes will first be evaluated using simulation and then implemented in OS kernels and evaluated on real systems. To support the memory thermal studies, a simple and accurate thermal model is proposed to estimate the dynamic temperature changes of DRAM memory subsystems. A two-level simulator will be developed to emulate the thermal behavior of memory subsystems. Successfully addressing the thermal concern of memory subsystems will not only ensure safe system operations but also improve the overall system performance, reduce the system manufacturing cost, and improve the system power efficiency.","title":"Collaborative Research: CSR-PSCE, SM: Memory Thermal Management for Multi-Core Systems","awardID":"0834475","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["486357"],"PO":["565255"]},"143054":{"abstract":"Computation with Nanomechanical Oscillator Networks<br\/>PI: Pritiraj Mohanty<br\/>Boston University<br\/><br\/><br\/>Layman Abstract<br\/><br\/>The neural computing paradigm is inspired by a simple understanding: synchronization of oscillations in a network of individual units in the brain is somehow related to the associative memory and learning functions. This concept of storage and retrieval of complex patterns in the synchronized states of an oscillatory network extends from pattern recognition in the brain to a number of fascinating natural occurring phenomena of synchronization such as the rhythmic blinking of a congregation of fireflies and clock-like beating of pacemaker cells in a human heart. However, practical realization of neurocomputers has been elusive in spite of intense theoretical activity in bio-inspired computing. This research involves the development of a scalable architecture, based on silicon-based nanomechanical oscillator networks as building blocks, towards a hardware implementation of nueorocomputing. Graduate, undergraduate and high school students are trained on advanced concepts of new computing paradigms, nanofabrication and ultra-sensitive mechanical measurements. The students are given a multidisciplinary exposure while undergoing specialized training at the frontiers of technological and scientific advances. <br\/><br\/>Technical Abstract<br\/><br\/>This research involves an experimental approach, which uses nanomechanical oscillator networks as building blocks of a bio-inspired computation paradigm. Drawing on advances and insights from computing in neurophysiological systems, the investigators study a network of nanomechanical oscillators, capable of storing and retrieving information in the synchronized states of the network. This research involves design, modeling, fabrication, and characterization of prototypes of nanomechanical oscillator networks. The investigators study the response of such networks to complex input patterns to determine whether the network is capable of ?learning.? This research involves the education and training of graduate, undergraduate and high school students in a vertically integrated structure.<br\/><br\/><br\/>-------","title":"EMT\/NANO: Computation Using Nanomechanical Oscillator Networks","awardID":"0829885","effectiveDate":"2008-09-15","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["392468"],"PO":["565157"]},"143296":{"abstract":"NUMBER: 0830753<br\/>INSTITUTION: Texas Engineering Experiment Station<br\/>PI: Amato, Nancy & Rauchwerger, Lawrence<br\/>TITLE: Motion-Planning Based Techniques for Modeling & Simulating Molecular Motions<br\/><br\/>Molecular motions play an essential role in many biochemical processes. Since it is difficult to experimentally observe molecular motions, computational methods for studying such issues are essential. This research investigates a novel computational method for studying molecular motions that the investigators have developed and validated against experimental data in preliminary work. The research has the potential to provide insight into a number of important questions related to protein folding, stability, and solubility. For example, protein misfolding and aggregation is associated with devastating neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, prion diseases, and related diseases. In addition to publications, results generated by the research are shared with the community in a publicly available database of molecular motions. The protein folding server also allows scientists to submit their own proteins which will be analyzed for them: http:\/\/parasol.tamu.edu\/foldingserver\/.<br\/><br\/>The new computational method invested in this research represents a trade-off between methods such as molecular dynamics and Monte Carlo simulations that provide detailed individual folding trajectories and techniques such as statistical mechanical methods that provide global folding landscape statistics. This method builds a graph (roadmap) corresponding to an approximate map of the molecule's energy landscape that encodes many (typically thousands) folding pathways. Though the individual pathways produced are not as detailed as trajectories generated from a molecular dynamics simulation, they can be used to study properties such as secondary structure formation order and folding kinetics. The major research goals of this project include the development of new and\/or improved metrics and analysis techniques for conformations and roadmaps that can be applied in protein stability and kinetics studies and the development of strategies for employing high-performance computing to increase the size and complexity of the systems that can be studied. The investigators validate and apply these new techniques to folding core identification, amyloid formation, kinetics studies, and comparative analysis of proteins.","title":"Motion-Planning Based Techniques for Modeling & Simulating Molecular Motions","awardID":"0830753","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["561154","496841"],"PO":["565157"]},"147300":{"abstract":"Failure to maintain exponential increases in supercomputer performance due to power limitations could endanger U.S. competitiveness. Fundamental improvements in parallel and distributed system design are needed, but the lack of standard metrics and methodologies to evaluate energy efficiency for supercomputers provides little incentive to design energy efficient systems. Developing new metrics and methodologies is wrought with challenges since previous performance-centric benchmarks do not accurately reflect differences in energy efficiency. Additionally, for new benchmarks and methodologies to gain acceptance, results must be easily verifiable and benchmark purveyors must have credibility and traction in the domain.<br\/><br\/> <br\/><br\/>In this project, the PIs are proposing to create novel metrics and methodologies to evaluate energy efficiency for supercomputers.They will designe a distributed software test-harness that enables automated, repeatable, and verifiable measurements of both performance and energy efficiency. They will designe new energy efficiency benchmarks for supercomputers using the software test harness. The PIs, who started Green500 List of energy efficient supercomputers, are going to work with vendors and end usersto encourage them to benchmark their systems using these new methodologies.","title":"SGER: Metrics And Methodologies for High Performance System Energy Benchmarking","awardID":"0848670","effectiveDate":"2008-09-01","expirationDate":"2011-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["563791","563449"],"PO":["565272"]},"144033":{"abstract":"Holistic Approach to Reliable Pervasive Systems<br\/><br\/><br\/><br\/>This project proposes a holistic approach to developing reliable pervasive systems through three contributions: <br\/><br\/>(1) A unified view of a pervasive distributed system is proposed for debugging purposes. This view overcomes the heterogeneous requirements for various technologies. The key idea is to formulate all system artifacts, system wide or local to a given node, static or temporal, independently of their granularity, into relations. Programs are debugged by writing SQL-like queries over these relations. A query compiler is developed that translates queries into system-level or node-level replay requests, and in the meantime, instruments application code to populate relevant relations. (2) Programming language support is proposed to facilitate the tracking of dependencies among essential events in the program. To that end, a simple language built on Java is proposed, with annotations for high-level application events and their dependencies. This represents one of the cornerstones for dealing with the large number of events in dynamic pervasive applications. In addition, the integration with a programming language allows the query compiler to verify parts of queries referring to application-specific definitions. (3) Runtime and communication support through specific overlay networks is investigated to efficiently propagate events as well as causality information across nodes. At the node level, an instrumented Java virtual machine is used to log and trace local events in a runtime-adaptable manner. Such an adaptive event logging\/tracing strategy significantly reduces resource demands.<br\/><br\/>Involvement in computer science student organizations will be specifically leveraged to promote undergraduate student research.","title":"CSR-DMSS, SM: A Holistic Approach to Reliable Pervasive Systems","awardID":"0834529","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["486086","550848"],"PO":["535244"]},"137620":{"abstract":"Spurred by financial scandals and privacy concerns, governments worldwide have moved to ensure confidence in digital records by regulating their retention and deletion. The goal of this project is to develop and explore a database management system (DBMS) architecture that supports a spectrum of approaches to regulatory compliance, thereby extending the level of protection afforded by conventional file-based compliance storage servers to the vast amounts of structured data residing in databases. The key challenge of this work is to provide compliance assurances for the DBMS, even against insiders with super-user powers, while balancing the need for trustworthiness against the conflicting requirements for scalable performance guarantees and low cost. The resulting architecture provides tunable tradeoffs between security and performance, through a spectrum of techniques ranging from tamper detection to tamper prevention for data, indexes, logs, and metadata; tunable vulnerability windows; tunable granularities of protection; careful use of magnetic disk as a cache and of secure coprocessors on the DBMS platform and compliance storage server platform; and judicious retargeting of an on-disk encryption unit. <br\/><br\/>This work enables compliance laws to be applied to business, government, and personal data now stored in databases, increasing societal confidence in such data. A new web course on compliance data management will raise the computer science community's awareness of compliance issues and will help train a new generation of professionals cognizant of these challenges and solutions. The software prototypes and technical papers describing them will be disseminated through the project's web site http:\/\/web.crypto.cs.sunysb.edu\/cdb\/","title":"III-COR Medium: Collaborative Research: Achieving Compliant Databases","awardID":"0803229","effectiveDate":"2008-09-01","expirationDate":"2013-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["560601"],"PO":["565136"]},"144044":{"abstract":"The underlying paradigm shift to parallel programming brought on by the introduction of multicore computing and the resulting challenges posed by this shift are just beginning to be experienced by consumers and programmers alike. A foremost challenge is scalability, especially in increasingly widespread automated memory management techniques such as garbage collection. As architectures move to 80 cores and beyond, new algorithms are needed that require almost zero synchronization and shared state. So-called \"lock-free\" algorithms leverage atomic low-level hardware instructions which are either quite expensive or exhibit sub-linear scalability due to cache coherency and memory-model constraints.<br\/><br\/>To pave the way for exploiting the potential of future scalable symmetric multiprocessing (SMP) this project proposes three contributions that simplify the inherent complexity within parallel systems for programmers and at the same time offering protection from deadlocks, livelocks, data races, as well as from security threats such as zero-day exploits. These contributions include (1) Ribbons - a new, more flexible, shared-memory programming model, where sharing constraints can be specified in a pair-wise fashion between restricted threads. Ribbons protect against inadvertent or malicious access to data and mitigate unbounded heap corruption in unsafe runtime environments, and subsume the familiar thread model. (2) Automated memory management techniques that exhibit near linear speedups on 32 cores and beyond. These techniques primarily consist of a garbage collection scheme that exploits thread-local data to avoid costly atomic instructions in the fast path. (3). Integrated support at the OS level. The two previous goals can be implemented in isolation, but exhibit many synergies when combined into a single system at all levels. Integrated support at the OS will lead to more scalable, energy-aware task scheduling algorithms and will provide the systemic-wide support required for the new fundamental execution model of ribbons to be viable in industry and become widely adopted.<br\/><br\/>This project proposes cooperation with the community to enhance existing memory management benchmarks to suitably test scalability across 32+ cores. The evaluation stage will also involve a case study where undergraduate students implement a project using either ribbons or normal threads and processes. All software implemented in this project will be made available under open source licenses.","title":"CSR-PSCE, SM: Memory Management Innovations for Next-Generation SMP","awardID":"0834619","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["486086"],"PO":["535244"]},"143076":{"abstract":"Quantum computers offer fundamental algorithmic advantages over classical computers and building a quantum computer would fundamentally change the power of our computers. Large scale quantum computers, however, are difficult to build in large part due to the fact that quantum systems interact with their environment and quickly lose their quantum nature. The solution to this problem, at least in theory, was provided by the theory of fault-tolerant quantum computation. The theory of fault-tolerant quantum computation, while providing an in principle demonstration of viability of quantum computers, suffers from requiring protocols that have a severe complexity overhead. As a result of these difficulties, an alternative approach toward building a robust quantum computer has been pursued in which many-body quantum systems protect quantum information by encoding this information in suitable protected degrees of freedom. In this approach the natural physics of the system helps protect the quantum information. A difficulty arises, however, due in part to the complexity required of the engineered quantum system. The research being performed here seeks to overcome this obstacle and open the path for constructing the equivalent of a transistor for quantum computers.<br\/><br\/>Here the investigators study three approaches to engineering effective many-body interactions suitable for protecting quantum information. The &#64257;rst approach involves the construction of fault-tolerant perturbation gadgets. This research involves studying the robustness of the perturbation gadgets. The second approach involves the use of quantum circuits to simulate Hamiltonian dynamics. In this technique, research is performed on new methods for fault-tolerant simulation. A &#64257;nal technique involves the use of time-dependent Hamiltonians and the research involves a detailed study of the errors in this time-dependent construction.","title":"EMT\/QIS: Robust Quantum Simulation Techniques for Fault-Tolerant Quantum Computation","awardID":"0829937","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":["499728","483594"],"PO":["565157"]},"143197":{"abstract":"Efficient detection of globally optimal surfaces representing object boundaries in volumetric images is fundamental and remains challenging in modern computer-assisted medical diagnosis and treatment and many other important medical applications. This research deals with specific problems of detecting optimal single and multiple interacting surfaces in volumetric image datasets. It permits identification of optimal surfaces of terrain-like shapes, cylindrical shapes, and complex shapes. Image data sets we consider have various different image features, such as edge, texture, and shape. The essence of these problems is to solve a number of important geometric optimization problems belonging to the fundamental topics of computational geometry, such as surface identification, geometric partitioning, geometric k-means clustering, and metric labeling. The research focus of this project is on developing efficient algorithms and novel techniques for solving these crucial problems with a provably global optimality. The application of geometric and combinatorial techniques to medical problems is intellectually deep and can result in advances in both theoretical computer science and medicine. The proposed image analysis methods allow evaluating the image data objectively in a quantitative manner, promising to substantially impact image-based clinical care. An important goal of this research is dissemination of implemented algorithms in software to application domains. In doing this, it helps to bring together the computer science and the medical community.<br\/><br\/>Intellectual Merit: We expect this project to make a number of theoretical contributions: (1) providing new algorithmic techniques for solving a set of crucial computational problems confronted by current medical research and applications; (2) introducing fresh and theoretically interesting problems and algorithms to geometric and combinatorial optimization, enriching and prodding further development of the field; (3) presenting new challenging problems and new approaches to other theoretical areas such as graph algorithms and operations research, and bringing new applications to these areas.<br\/><br\/>Broader Impacts: The successful completion of this project will result in methodologies that greatly accelerate the pace of 3-D and 4-D medical image processing. The automated image segmentation software will be platform independent and will directly address the needs of a broad base of end users across a wide range of disciplines from basic research to clinical medicine. The proposed image analysis tools will provide clinicians with the means to assess diseased organs in 3-D and 4-D, rather than in 2-D as is typically done in conventional practice. In addition, infusion of the research results into the classroom provides students with a unique opportunity to study and practice in the emerging important interdisciplinary area involving computer science and modern medicine, promotes interdisciplinary learning, and enables the training of more versatile scientists.","title":"Geometric and Combinatorial Algorithms for Optimal Surface Segmentation in Medical Images","awardID":"0830402","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["550205"],"PO":["565157"]},"138841":{"abstract":"The ubiquitous phenomenon of massive data (including data streams) imposes considerable challenges in data visualization and exploratory data analysis. About 15 years ago, terabyte datasets were still considered `ridiculous.' However, modern datasets managed by Stanford Linear Acceleration Center (SLAC), NASA, NSA, etc. have reached the perabyte scale or larger. Corporations such as Amazon, Wal-Mart, Ebay, and search engine firms are also major generators and users of massive data. The general theme of data reduction and summarization has become an active and highly inter-disciplinary area of research. This project proposes to develop various approximation techniques, which generate a \"fingerprint\" or \"sketch\" of the massive data by transforming the original data. These `sketches' are reasonably small (hence easy to store) and can provide approximate answers which are usually good enough for practical purposes. <br\/><br\/>This proposal concerns the fundamental problems of processing\/transforming massive (possibly dynamic) data. In particular, it focuses on (A) developing systematic fundamental tools for effective data reduction and efficient data summarization; (B) applying these tools to improve numerical analysis, visualization, and exploratory data analysis. Two lines of theoretically sound techniques for data reduction and summarization will be developed and further improved: (1) the method of stable random projections (SRP), effective in heavy-tailed data; (2) the method of Conditional Random Sampling (CRS), mainly for sparse data. Concrete applications of SRP and CRS will be investigated. Widely-used basic numerical algorithms can be rewritten by taking advantage of SRP or CRS. Popular methods\/tools for exploratory data analysis will also benefit considerably from the development of data reduction techniques.","title":"Efficient Data Reduction and Summarization","awardID":"0808864","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7703","name":"FOUNDATIONS VISUAL ANALYTICS"}}],"PIcoPI":["565250"],"PO":["565286"]},"136300":{"abstract":"REU Site at CENS:<br\/>Sensing Applications in Earthquake and Environmental Engineering<br\/>Abstract<br\/><br\/><br\/>The Center for Embedded Networked Sensing (CENS), headquartered at UCLA, is a multidisciplinary research center focused on developing embedded networked sensing (ENS) technology and applying this revolutionary technology to crucial scientific and social applications. CENS infrastructure provides an ideal environment for undergraduate researchers to gain research experience and become part of a productive and collaborative research community. This REU project represents an effective approach to supporting undergraduate research experience, particularly focused on promoting long-term interest in science and engineering for women and underrepresented minorities. The Center?s approach to undergraduate education is based on our expertise in working with undergraduates, a comprehensive and longitudinal evaluation of our programs, the evolution of research, collaboration and mentoring within the Center, and a body of literature that examines undergraduates? experiences in science and engineering. <br\/><br\/>The CENS REU site experience takes a systems approach to undergraduate training, emphasizing Environment, Experience, and Engagement. This approach facilitates development of a tight-knit cohort of undergraduate researchers engaging in forefront research. Our mentoring community is vital to increasing student competence, developing collegial relationships, and improving collaborative interactions. A ten-week comprehensive program, housed at UCLA, intellectually focuses on civil and environmental engineering applications of earthquake and environmental monitoring research. Research projects engage participating undergraduates in ongoing research within this application area, while contextualizing this research within the scope of the Center. The program includes mechanisms for students to understand the commonalities and connections between their application area and the development of broader ENS technologies. Additionally, the program introduces students to data and data visualization tools that are common across the Center?s applications areas.","title":"REU Site at CENS: Sensing Applications in Earthquake","awardID":"0755533","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["513161",362127],"PO":["523800"]},"147443":{"abstract":"One of the fundamental problems in computer vision is to make a machine automatically recognize an object in a query image based on previously seen examples. This problem becomes particularly difficult when the machine is trying to recognize an object among several with rather similar appearances, and the object is partially occluded or disguised. This is often the case with human face recognition. This project aims to explore new mathematical tools from sparse representation in signal processing, by casting robust face recognition as a (sparse) error correction problem. Recently, sparse error correction based on minimizing the 1-norm has seen great success in signal processing. This project will investigate its potential in image-based object recognition despite occlusion or corruption, especially for human faces. Preliminary experimental results have shown good promise of this new approach. In its one-year span, this project aims to study the special geometric and statistical models and problems associated with human face recognition and hopes to develop even more robust and scalable face recognition algorithms. To verify the results, a prototype face recognition system will be developed, with an emphasis on theoretical and algorithmic progress. All the results will be available at a public website: http:\/\/perception.csl.uiuc.edu\/recognition\/Home.html","title":"SGER: Explorations of Robust Image Classification","awardID":"0849292","effectiveDate":"2008-09-15","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["311520"],"PO":["500200"]},"144066":{"abstract":"This project addresses the challenges in developing predictive and autonomic thermal-aware and energy-efficient task scheduling algorithms for heterogeneous High-Performance Computing datacenters. Energy savings of software controlled job management in large-scale datacenters has not been adequately explored. Existing scheduling and power management algorithms are mostly reactive in nature. This research will investigate the trade-off between the schedule efficacy and the runtime complexity of producing a spatio-temporal task schedule (with start times of each task and its computing node assignment or placement), with an objective to develop algorithms that can be applied in real-world situations. Novelty of this research lies in viewing datacenters as a cyber-physical entity and proactively configuring and controlling both the cooling and computing systems in a coordinated manner. Previous research results show that energy savings are very limited unless the cooling is dynamically adjusted to the computing load. Outcomes of this research will include: a) study of the trade-off between computational complexity and efficiency of the scheduling algorithm, b) proactive algorithms that produce task, power and cooling control schedules in an unified manner, c) software architecture that will encompass the power control of the computing equipment, as well as dynamic configuration of the cooling systems, and d) test results from real-world data centers that show the applicability and practicality of the developed schemes. Results will be publicly available at IMAPCT Lab's website (http:\/\/impact.asu.edu\/), and scientific findings will be included in multi-disciplinary courses with cyber-physical focus. Practical benefits of this research will be demonstrated in academic and corporate datacenters.","title":"CSR-DMSS, SM: Next-Generation Thermal-Aware, Energy-Efficient Resource Management for Data Centers","awardID":"0834797","effectiveDate":"2008-09-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["535238"],"PO":["535244"]},"137653":{"abstract":"This project is encouraging interdisciplinary machine learning research and education that integrates the use of machine learning into the application areas, solves real science problems, and serves as a catalyst for new research in both machine learning and the application domains. We are investigating four basic machine learning issues that arise when working with real-world applications: (1) optimizing over choices available when generating training data; (2) assessing and improving the quality of training data; (3) designing specific algorithms and methods for time series and feature-based data; and (4) developing methods for abstaining during classification. Our research is motivated by on-going collaborations with researchers to create solutions for: training an artificial nose (Chemistry, Tufts); land-cover mapping from remotely sensed data (Geography, Boston University); classification of sky surveys (Astronomy, Harvard); non-invasive gluclose monitoring (Biomedical Engineering, Tufts); and liquification prediction (Civil Engineering, Tufts). The successful application of machine learning to each of the five tasks will have significant impact on the lives of humans. Our education initiatives have two complementary goals: (1) to educate computer science students on how to conduct interdisciplinary machine learning research; and (2) to educate professors, graduate students and undergraduates from science, engineering and medicine on how to recognize and pose problems as machine learning and data mining problems.","title":"III-CXT-Medium: Interdisciplinary Machine Learning Research and Education","awardID":"0803409","effectiveDate":"2008-09-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[365889,"438626"],"PO":["543481"]},"139600":{"abstract":"Children with Specific Language Impairment (SLI) experience a delay in acquisition of certain language skills, with no evidence of hearing impediments or other cognitive, behavioral, or neurological problems. To diagnose monolingual children with SLI, clinicians have at hand standardized tests, such as the Test for Early Grammatical Impairment, that provide \"cut-off\" thresholds defining the normal range for children of different ages. Diagnosing bilingual children with SLI is far more complicated, however, due to a lack of standardized tests, a lack of bilingual clinicians, and most importantly a lack of a deep understanding of bilingualism and its implications on language disorders. In addition, bilingual children often exhibit code-switching patterns that make the assessment task even more challenging. The PIs' goal in this project is to contribute to the early and accurate identification of English-Spanish bilingual children with SLI, by developing an automated method for discriminating syntactic patterns indicative of SLI. Recent approaches on differential diagnosis of bilingual children are focused either on assessing the phonological systems of both languages to identify children with speech disorders, or on the analysis of error patterns on specific morphemes, such as article gender and number agreement. In contrast, the PIs' approach is not to restrict the analysis to a specific syntactic structure, but rather to focus on adapting Machine Learning (ML) and Natural Language Processing (NLP) techniques so that they can learn the patterns that distinguish an otherwise typical language development. The PIs will pursue the objectives along two core topics: automatic part-of-speech (POS) tagging of bilingual discourse (in which they will investigate the use of ML approaches, in particular domain adaptation techniques, for combining existing linguistic resources on both languages), and statistical methods for discriminating patterns of language use indicative of SLI (in which syntactic information, generated by the tagger will be used to train statistical models). The intuitive motivation for this approach is that the language patterning of bilingual children with SLI will be different from those of typically developing children both at the syntactic level and at the interaction level of the two languages, and these differences will be captured by the statistical methods.<br\/><br\/>Broader Impacts: The clinical implications for this research are far-reaching, particularly regarding the issue of both over- and under-identification of bilingual children experiencing SLI. Because the criteria for this diagnosis involve identification of disordered patterns of language form, content, and use, children who engage in code-switching are at risk of being inappropriately labeled as SLI and placed in special education services. The ability to apply objective technology to the diagnostic process will serve as another sensitive evaluation instrument, eventually allowing for more accurate differentiation of children demonstrating language differences from those experiencing language disorders. For the NLP community, this research will advance the state-of-the-art by developing approaches that can solve problems where the task involves cross-linguistic features, children?s spontaneous speech, and small amounts of data. The NLP methods developed will be generalizable to other clinical tasks and bilingual populations.","title":"HCC-Small: Collaborative Research: Statistical Methods for Learning to Diagnose Specific Language Impairment in Bilingual Children","awardID":"0812085","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[371078],"PO":["565227"]},"145298":{"abstract":"This award supports the Second IFIP Working Conference on \"Verified Software: Theories, Tools, and Experiments (VSTTE 2008)\", Oct 6-9, 2008, Toronto, Canada. The construction of reliable software poses one of the most significant scientific and engineering challenges of the 21st century. Professor Tony Hoare of Microsoft Research has proposed the creation of a program verifier as a grand challenge for computer science and outlined an international program of research combining many disciplines such as the theory and implementation of programming languages, formal methods, program analysis, and automated theorem proving. The VSTTE conference series was established by the research community in response to this challenge. The VSTTE 2008 program includes three one-day workshops focusing on the three areas of focus: theories, tools, and experiments. This award is enabled through support provided by the NITRD High Confidence Software and Systems (HCSS) interagency Coordinating Group.","title":"Second Working Conference on Verified Software: Theories, Tools, and Experiments (VSTTE)","awardID":"0840394","effectiveDate":"2008-09-01","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"H184","name":"National Security Agency"}}],"PIcoPI":["555363"],"PO":["561889"]},"139633":{"abstract":"The project is developing a framework for efficient reasoning about effects of actions in temporal action theories (TAT) with incomplete information and state constraints in the form of static causal laws. To cope with the high complexity of temporal reasoning and planning problems, the project develops and exploits an approximation theory for query answering and planning in TAT. This theory is being used as the basis for sound and efficient algorithms for hypothetical reasoning and planning.<br\/><br\/>This framework guides the development of state-of-the-art temporal conformant planners, robust in the face of a variety of contingencies, which can deal with various classes of preferences. Techniques developed will application, for example, in the composition of web services for comparative data analysis in evolutionary biology.","title":"RI-Small: Approximation Based Reasoning and Planning","awardID":"0812267","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["561141","435957"],"PO":["491702"]},"137697":{"abstract":"This project proposes to combine new and existing methods in computer vision to partially automate the tedious procedure of reconstructing ceramics from unearthed fragments. This will involve development of new computer vision research technologies that can assist this reconstruction process, thus enabling enhanced analysis, interpretation, and presentation of history evidence. The focus of the project are ceramic and other artifacts from one of the best preserved and most diverse American urban colonial archaeological sites ever excavated - the Mall at Independence National Historical Park in Philadelphia, Pennsylvania. The project proposes to develop 3D models based on surface properties of thousands of fragments found and approach this vast collection with novel theoretical and computational vision research technology. A strong collaborative methodology will be developed between the information technologists and practitioners in the domain. The project is actively support and endorsed by U.S. Department of the Interior National Park Service. A long term goal is to have a positive and profound impact on the study of historical archaeology and laboratory practice of artifact analysis.","title":"The 3D Colonial Philadelphia Project -- Digital Restoration of Thin-Shell Objects for Historical Archeological Research and Interpretation","awardID":"0803670","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["459566",366025,"562644","563384",366028],"PO":["563751"]},"139644":{"abstract":"The objective of this project is to build a design framework for robust functionality of mobile cooperative robotic router networks in realistic communication environments. Most of the existing work on mobile cooperative decision-making uses oversimplified models for communication links by assuming either a perfect link or no link between any two agents. Such models do not embrace realistic wireless communication effects such as fading, shadowing and path loss and therefore will not be suitable for robust operation of robotic router networks. We propose novel motion-planning strategies where the agents constantly reconfigure themselves to optimally route the information through the network. In addition, the theoretical results and algorithms will be implemented on actual autonomous vehicles engaged in a wide range of tasks. By considering realistic communication unreliability, our proposed framework will result in the robust flow of information in the network. Emergency response, security and surveillance are examples of applications that rely on robust and intelligent operation of autonomous networks in harsh conditions and can benefit tremendously from the proposed work. This project also has a significant educational impact on the female high school students of Albuquerque by organizing learning summer camps at the University of New Mexico.","title":"RI-Small: An Integrative Framework for Communication and Motion-planning in Robotic Networks Operating in Fading Environments","awardID":"0812338","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["493492","551173"],"PO":["562760"]},"148246":{"abstract":"This workshop is the first policy meeting to consider what new exchange mechanisms -- and especially markets -- are being experimented with in the life sciences in order to improve access to and use of the vast amounts of data, knowledge and information created in the biomedical sciences. What is new here is the attempt to understand how financial pressures in the biomedical industries are dove-tailing with public policy priorities and not-for-profit\/not-for-loss business models which deliver greater access to intellectual assets. The workshop will bring together expertise from academia and public research organizations, health and IT industries, the venture capital and institutional investor community, non-governmental organizations, as well as policymakers. They will debate whether not only knowledge markets improve innovation efficiency but also whether they are creating new business opportunities and business models. The OECD has broad expertise in a number of areas related to knowledge markets including: collaborative mechanisms for access to intellectual property, OECD Guidelines on licensing practices, new models for organizing research in health innovation, knowledge flows and intellectual asset valuation. The workshop will include international speakers and participants from the policymaking, business and research communities from the 30 OECD countries and beyond. It will increase awareness among policymakers about new trends in organizing access to data, knowledge, and information in the life sciences and provide a typology of knowledge markets. It will show how the research infrastructure is evolving and help governments understand how their policies can influence the emergence of knowledge markets, as well as an explanation of why they may want to do so. The meeting will be summarized in a Policy Report to be discussed and agreed by OECD countries through the Working Party on Biotechnology.","title":"Expert Workshop on Knowledge Markets in the Life Sciences","awardID":"0852303","effectiveDate":"2008-09-01","expirationDate":"2009-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"H396","name":"NIH"}}],"PIcoPI":[395163],"PO":["565136"]},"139677":{"abstract":"In a query-by-humming system, a user sings part of a melody and the computer identifies the songs that contain the melody. In a sign spotting system, a sign language user searches for occurrences of specific signs in a video database of sign language content. These are two example applications where users want to retrieve the best matching subsequences in a time series database given a query sequence. This project is developing methods for efficient subsequence matching in large time-series databases using the popular Dynamic Time Warping (DTW) distance measure. Embeddings are being designed that partially convert the subsequence matching problem into the much more manageable problem of similarity search in a vector space. This conversion allows leveraging the full arsenal of vector indexing and metric indexing methods for speeding up subsequence matching. The proposed methods will be applicable in a wide variety of time series domains, including, e.g., stock market modeling, seismic activity analysis, and sensor-based health monitoring. To showcase the commercial, social, and educational impact of the research, the project will produce three demonstration systems: a query-by-humming system, a handwritten document search-by-keyword system, and a sign spotting system. The results of the research are being integrated into these systems to achieve efficient retrieval in the presence of large amounts of data. The creation and dissemination of large, real-world datasets for these three systems will be an additional contribution of the project.","title":"III-COR-Small: Collaborative Research: Time Series Subsequence Matching for Content-based Access in Very Large Multimedia Databases","awardID":"0812601","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["560637","557541"],"PO":["563751"]},"139446":{"abstract":"Microprocessor verification is a critical challenge for the computing industry. A design bug in a shipped processor chip can lead to failures and data corruptions, which can be catastrophic in many applications, such as medical equipment, avionics, and automotive control. Microprocessor design bugs can also be financially disastrous; the recall of Intel's Pentium, due to its notorious division bug, cost Intel approximately 420 million dollars. Society relies upon microprocessors, and thus the possibility of a flawed<br\/>microprocessor is an enormous concern. Unfortunately, verifying a complex, modern microprocessor is extremely difficult. Due to both its difficulty and importance, verification consumes a large fraction (60-70%) of the resources--engineers, time, and money--devoted to the creation of a new microprocessor. Despite this effort, the most recent processors from Intel, AMD, and IBM have been shipped with dozens of documented design bugs.<br\/><br\/>This research project addresses both society's need for correct microprocessor designs and industry's desire to improve product quality and shorten the design cycle, both of which can be achieved through a reduction in verification effort. The project's goal is to design microprocessors such that they can be more easily verified. To achieve this goal, this project will pursue three complementary research thrusts. The first thrust will analyze existing designs to identify features that require more verification effort than is merited by their benefits. The second thrust will re-design the ways<br\/>in which microprocessor components interact, in order to reduce design complexity. The third thrust will develop sets of invariants that facilitate verification; there are often many ways to specify the correctness of a system, some of which are far easier to verify than others. The benefits of the proposed research are broader than its technical results, because of the importance of the verification problem to national infrastructure and industry. This project will also support training of undergraduate and minority students.","title":"CPA-CSA: Verification-Aware Microarchitecture","awardID":"0811290","effectiveDate":"2008-09-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["536996"],"PO":["366560"]},"139567":{"abstract":"Humans are remarkably good at perceiving shape, even in cases where the image cues alone would seem to be insufficient. For example, up to certain distortions, people can infer the structure of a scene from a single photograph. Similarly, they can often estimate the shape of surfaces that are only partially visible, such as a chair that is half-occluded. These remarkable abilities are due to the human capacity to combine visual cues with prior information about the shapes of objects and surfaces in the world.<br\/><br\/>In computer vision, modern multi-view stereo algorithms that exploit very low-level cues now produce shape models that are proving to be nearly as accurate as laser scanners and are doing so in very unconstrained settings, e.g., using photos from Internet sharing sites. However, these algorithms lack the other piece of the puzzle --- the ability of the human visual system to exploit prior information about scene shape. <br\/><br\/>In this work, the PIs focus on the particular domain of architectural scenes for which prior notions of shape are particularly applicable. Existing priors typically fall into two categories: low-level, usually a preference to reconstruct smooth surfaces, and high-level, such as model-based techniques that have parameterized templates for specific architectural features. The PIs are exploring a range of priors between these extremes, significantly increasing the expressiveness of low-level priors and defining a set of mid-level priors. The key ideas are to consider the differential properties of architectural surfaces (e.g., curvature behavior) and to exploit the symmetries that frequently occur in this setting. The PIs are studying a range of reconstruction problems and applications, from single-view reconstruction to multi-view stereo, that exploit priors and prior selection.<br\/><br\/>Finally, the PIs are evaluating the potential of these techniques to reconstruct detailed architectural models from street-level, aerial, and interior views from the Internet. As part of this evaluation, they are obtaining ground truth laser scans and registered imagery to provide a benchmark for the research community.<br\/><br\/>The outcome of this research, i.e., tools that can automatically reconstruct geometric models from large collections of images, will enable a host of important applications, ranging across 3D visualization, localization, communication, recognition, and cultural heritage, that go well beyond traditional computer vision problems and can have broad impacts for the population at large.<br\/><br\/> http:\/\/grail.cs.washington.edu\/projects\/cpc\/","title":"RI-Small: Multi-level Priors for Multi-view Stereo","awardID":"0811878","effectiveDate":"2008-09-15","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["438138","533242"],"PO":["564316"]}}