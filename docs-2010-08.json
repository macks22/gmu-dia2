{"168850":{"abstract":"Hybrid systems, which combine discrete and continuous dynamics,<br\/>provide sophisticated mathematical models for automated highway<br\/>systems, air traffic control, biological systems, and other<br\/>applications. A key feature of such systems is that they are<br\/>often deployed in safety-critical scenarios and hence designing<br\/>such systems with provable guarantees is very<br\/>important. This is usually done through analysis of such<br\/>systems with regard to a given set of safety properties that<br\/>assert that nothing 'bad' happens during the operation of the<br\/>system. As more complex hybrid systems are considered, limiting<br\/>safety properties to a set of unsafe states, as in current methods,<br\/>considerably restricts the ability of designers to adequately<br\/>express the desired safe behavior of the system. To allow<br\/>for more sophisticated properties, researchers have advocated<br\/>the use of linear temporal logic (LTL), which makes it possible<br\/>to express temporal safety properties. This proposal develops<br\/>algorithmic tools for safety analysis of embedded<br\/>and hybrid systems operating under the effect of exogenous inputs<br\/>and for LTL specifications. The problem addressed<br\/>is the following: Given a hybrid system and a safety specification<br\/>described using LTL, can a feasible trajectory be constructed<br\/>for the system that violates the specification, when such a<br\/>trajectory exists? The problem is called the falsification problem.<br\/>The broader impact of the project is implemented through<br\/>course development, involvement in research activities of<br\/>undergraduate, graduate and postdoctoral students, efforts to<br\/>mentor underrepresented groups, and dissemination of concepts<br\/>through educational software developed at Rice.","title":"SHF: Small: A Synergistic Multi-Layered Approach for Falsification of Specifications for Hybrid Systems","awardID":"1018798","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["557516","565263"],"PO":["565264"]},"165462":{"abstract":"Software tools that support creativity have traditionally been designed primarily for small teams of professionals. However, with the rise of the Internet and inexpensive computer systems, the use of these tools is leading to new creative collaborations, where creative activity is distributed not across a small team of professionals as in the past, but rather across tens of thousands of people around the globe, many of whom are non-professionals. In this research, the investigators reconstruct the history of two such massive collaborations--one involving short movies about video games and the other involving a massive online crafting community--to understand how their phenomenal success evolved. <br\/><br\/>There is considerable interest in the scientific community in such large-scale collaborations. Many research problems today involve massive data sets and computational power. Solving these problems requires the collaboration of thousands of researchers around the world, rather than collaboration among the members of a small team in a lab. The online communities the investigators are studying have found ways to distribute their productivity on a massive scale and found success with it. This research seeks to understand and model that success. Such a model could lead to the design of software tools to support massive creative collaboration, e.g., in the sciences, as well as helping to clarify the organizational and communications environments needed to support them.","title":"Major: Massively Amateur Creativity","awardID":"1002772","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":[443241,443242],"PO":["565342"]},"168630":{"abstract":"The objective of this project is to devise computer vision methods that enable a Portable Blind Navigational Device (PBND) to guide a visually impaired person in unstructured environments. The main research question of this project is to answer if the approach of employing a single perception sensor can solve blind navigation problem, including localization of the PBND and object recognition. A distinctive feature of this work is that it addresses blind navigation problem by simultaneously processing the visual and range information of a 3D imaging sensor. <br\/><br\/>The project consists of four related research endeavors. First, it investigates techniques for accurate and precise pose estimation of the PBND in a GPS-denied environment. Second, it develops an effective 3D data segmentation method to allow scene recognition for wayfinding. Third, it applies the pose estimation method to register 3D range data and devises methods to reduce registration error. Four, it addresses real-time implementation of the methods in the PBND with limited computing power. <br\/><br\/>The research will result in new algorithms that can improve the lives of the visually impaired in the near term. They will also enable the autonomy of small robots that have wider applications in military situational awareness, firefighting, and search and rescue. The discoveries will revolutionize small robot autonomy and impact the robotics research community as a whole. Broader impacts also include training of undergraduate and graduate students, and educating the public on robotics through workshops and robot exhibits in science museums and technology showcases.","title":"RI: Small: Towards Portable Navigational Devices for the Visually Impaired","awardID":"1017672","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[451375],"PO":["564069"]},"168751":{"abstract":"There have been increasing interests in deploying wireless sensors in three-dimensional (3D) space for such applications as underwater reconnaissance and atmospheric monitoring. An individual sensor is highly resource-constrained, with extremely limited computing, storage, and communication capacities. To network a large number of such sensor nodes is challenging. Particularly, compared with its 2D counterpart, the scalability problem is greatly exacerbated in a 3D sensor network due to dramatically increased sensor quantity in order to cover a 3D space. The state-of-the-art routing algorithms that offer the best scalability cannot be applied in 3D directly. As a matter of fact, it has been proven that there is no deterministic algorithm that can guarantee delivery based on local information only in a 3D network. This project centers on scalable routing protocols for wireless sensor networks deployed in a 3D space. A key strategy of this research is to preprocess the global network information via a distributed algorithm, such that a sensor only needs to store a minimum amount of information to make correct and efficient local routing decisions, thus achieving scalable routing. To this end, geometric theories and tools are exploited to develop practical routing algorithms that provide guaranteed delivery, require small bounded storage, and are based on local computation only. Three approaches, namely, routing-via-backbone, routing-in-3D-virtual-coordinates, and segment-and-route, are developed. Furthermore, a testbed will be established for experimental exploration and evaluation of 3D routing. The 3D sensor network technologies developed in this project will benefit several fields ranging from biological research to environmental monitoring.","title":"NeTS: Small: Scalable Routing in 3D Wireless Sensor Networks","awardID":"1018306","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["551058","551059"],"PO":["565303"]},"168872":{"abstract":"This award targets the study of the theory of qubit coherence protection combining passive (dynamical decoupling) and active (quantum error correction) techniques. The dynamical decoupling is very frugal as it does not require any additional qubits to work; it will be used at the level of physical qubits as the first stage of coherence protection. For several solid-state qubit implementations, this may suppress the decoherence rate to be dealt with by the quantum error correction by an order of magnitude or more, thus dramatically reducing the hardware requirements. The PI plans to analyze the effectiveness of such a combined coherence protection scheme over long time intervals, when compared to the correlation time of the environment using both analytical and numerical techniques.","title":"AF: Small: Long-time coherence protection via dynamical decoupling and encoded control","awardID":"1018935","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":[451971],"PO":["565157"]},"168641":{"abstract":"This project focuses on the development of a framework for interpreting linguistic descriptions of places and locations as well as objects in motion found in natural language texts. The resulting static and dynamic descriptions are represented in a spatiotemporal markup language called STML, which will be incorporated into a proposed international standard for spatial annotation called ISO-Space. The STML output then enables for grounding within a metric representation such as Google Earth, through an automatic conversion to KML. A Dynamic Interval Temporal Logic (DITL) is also developed, which is consistent with the STML output and which provides the semantics for STML for subsequent reasoning about the text.<br\/><br\/>In order to automatically capture locations, paths, and motion constructions in the text, spatial processing algorithms are created. These algorithms build on earlier work on temporal processing as well as new and continuing work on identifying places, analyzing the internal structure of events with the Event Structure Lexicon, and adding paths with the PathFinder algorithm. Manual STML annotation is performed on a corpus of travel descriptions in order to evaluate these algorithms.<br\/><br\/>This work is directly relevant to current efforts to standardize semantic annotations while also creating interoperable resources. One major impact of this research is the engagement of several diverse communities and their resources into a new dialogue and sharing of ideas. These include the areas of computational semantics and linguistics, qualitative reasoning, and dynamic approaches to logic and reasoning.","title":"RI: Small: Interpreting Linguistic Spatiotemporal Relations in Static and Dynamic Contexts","awardID":"1017765","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["562587"],"PO":["565215"]},"168762":{"abstract":"In the multi-core processor era, microprocessors will only continue to scale in performance in the presence of abundant thread level parallelism. Achieving this goal of continuously scaling software parallelism will clearly require the exploitation of new compilation, language, and execution paradigms. One huge barrier to the viability of many proposed execution paradigms and the introduction of new paradigms is the inability of modern processor architectures to execute short threads efficiently. Many of these new execution models can be highly effective at exposing parallelism to the hardware if they have the freedom to identify and exploit opportunities for parallelism that are 10s to 100s of instructions long. However, current machines are not designed to execute short threads well. The goal of this research is to significantly reduce the startup cost for a new thread (or thread new to a core). This in turn reduces the break-even point that determines whether a piece of code is parallelizable or not.<br\/><br\/>The term \"thread migration\" is used to indicate a large number of parallel execution operations, all of which involve moving stored or cached state from one core to another. These operations include forked threads, migrated\/moved threads for thermal management or load balancing, loop-parallel threads, task-level parallelism, helper threading, transactional execution, and speculative multithreading - all of these operations will be accelerated to some degree by this research. This research will attack all sources of the thread startup cost, including software (e.g., operating system) overheads, branch predictor state, cached data and instruction state, the commit latency, and the overhead of transferring the primary thread state between cores. In addition to reducing the parallel programming complexity, the broader imapcts of this research include graduate and undergraduate student training, availability of an open-souce simulation infrastructure.","title":"SHF: Small: Architectural Support for New Parallel Execution Paradigms Via Agile Threads","awardID":"1018356","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["542069"],"PO":["366560"]},"168883":{"abstract":"ABSTRACT<br\/><br\/>The behavior of most physical communication channels depends on the past usage of the channel -- namely, many physical communication channels exhibit \"memory\". The challenge then is to construct tools that optimally take advantage of the channel memory. In addition, usual information theoretic analysis methods are ill equipped to even compute the capacity, i.e., how much information can be reliably sent through such channels with long memory. This research attacks these problems using recent developments in the fields of source coding and statistics and augments existing computational tools specifically for channels with memory.<br\/><br\/>In the context of communication channels, an adequate model is the simplest fit to the channel input\/output observations that enables sufficiently accurate evaluation (estimation) of the channel capacity. The investigators are extending universal compression principles, particularly those developed for the undersampled regime, to information transmission problems. The practical thrust of the research includes the development of algorithms for model based capacity computations. The starting point is the efficient implementation of the Baum-Welch\/BCJR algorithm as a computational kernel in a range of approximations to the generalized Blahut-Arimoto method. Specifically, techniques that adaptively control the number of processed states are exploited, trading off the time per iteration and accuracy of computations. While the theoretical thrust reduces the state space by fitting a simpler model, the algorithm design component adds another dimension to complexity reduction by utilizing Markov chain Monte Carlo techniques for navigating complex state spaces, i.e., by transversing only high probability states.","title":"CIF: Small: Channels with Memory -- Universal-Compression-Based Modeling Principles for Computing and Optimizing Information Rates","awardID":"1018984","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7938","name":"SENSOR NETWORKS"}}],"PIcoPI":["475523","492224"],"PO":["564898"]},"168410":{"abstract":"Analysis of multiple sets of data, either of the same type as in multi-subject data, or of different type as in multi-modality data, is inherent to many problems in computer science and engineering. Biomedical image analysis figures prominently among these and is particularly challenging because of the rich nature of the data made available by different imaging modalities. Data-driven methods are particularly attractive for the analysis and fusion of such data as they can achieve useful decompositions while minimizing assumptions on the model and underlying processes, and can also incorporate reliable prior information when available. One such approach recently introduced for medical image analysis and fusion is multi-dataset canonical correlation analysis (MCCA) that has proven especially useful for the analysis and fusion of rather disparate data, owing to its high flexibility and extendibility to a wide array of problem settings.<br\/><br\/>Intellectual Merit: In this proposal, the main aim is twofold. First, a number of powerful methods are developed for multi-subject (multi-set) data analysis and multi-modal data fusion based on canonical dependence analysis by significantly extending the power and flexibility of MCCA. Then, the successful application of the methods are demonstrated on a unique problem that demands these properties, namely the study of brain function and functional associations during simulated driving, a naturalistic task where data-driven methods have proven very useful. The data used in the project are complementary in nature but of very different nature: functional magnetic resonance imaging (fMRI), electroencephalography (EEG), structural MRI (sMRI), genetic array data--single nucleotide polymorphism (SNP)--and behavioral variables. The rich characteristics of the data and the problem at hand thus provide a special challenge for the methods developed and a unique testbed for the evaluation of their performance.<br\/><br\/>Broader Impacts: The broad impact of the proposed work lies in its potential to substantially impact science and information technology as well as in its educational features. Analysis of multiple datasets of the same type as well as fusion of data from different modalities\/sensors is a key problem in many science and engineering disciplines. The new set of methods proposed thus form attractive solutions for many other problems beyond brain function analysis. The fully integrative nature of the proposed work is also an invaluable asset in the ongoing efforts in cross-training of students and researchers as well as increasing the participation of underrepresented groups in science and technology careers.<br\/><br\/>For further information, see the project web site at the URL:<br\/> http:\/\/mlsp.umbc.edu\/research_projects.html","title":"III: Small: Collaborative Research: Canonical Dependence Analysis for Multi-modal Data Fusion and Source Separation","awardID":"1016619","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["486018"],"PO":["565136"]},"168531":{"abstract":"The PI's objective in this project is to advance the field of robotic musicianship by enabling multi-modal communication between human and robotic musicians, and by embedding knowledge-based musical intelligence in real-time musical interactions. To these ends the PI will seek to understand how temporal structures in music are represented and processed by humans, and he will develop a vision system that enables a robotic musician to anticipate a human musician's gestures. Modeled after human-human musical interaction, the artificial vision system will complement the musical listening system developed in the PI's prior work, thereby allowing a robot to better synchronize its playing with human improvisers. In addition, the PI will transcribe and analyze (using statistical tools such as Hidden Markov Models) a large corpus of works by the great classical composers as well as by masters of jazz; the off-line analysis, coupled with interviews, surveys, and focus groups, will advance our knowledge regarding cognitive and mechanical aspects of group play and the role of visual and physical cues in live performance, and will enable the PI to construct a long-term cultural knowledge base that will be added to the short-term real-time improvisation techniques already developed previously. Collectively, project outcomes will lead to a comprehensive model of human and artificial musicianship, which the PI will evaluate using behavioral measures (such as time differences in synchronization), subject questionnaires, and a Turing-like test to evaluate the quality of the musical interaction.<br\/><br\/>Broader Impacts: This project will make fundamental contribution to our knowledge in areas such as musicianship, human-robotic interaction, computer assisted collaboration, and improvisation. It will help bring human-robot interaction to the general public through high visibility concerts that capture the interest and imagination of students who are not regularly drawn to math, the sciences, or engineering by creating novel musical collaborations between humans and machines. The project will serve as a testbed for future forms of musical interactions, bringing perceptual and algorithmic aspects of computer music into the physical world both visually and acoustically, which may inspire people to play and think about music in new ways. Ultimately, the research is expected to shed light on broader concepts such as human and artificial creativity and expression, and the feasibility for machines to create, or assist in creating, meritorious aesthetic and artistic products.","title":"HCC: Small: Multi Modal Music Intelligence for Robotic Musicianship","awardID":"1017169","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["561054"],"PO":["565227"]},"168542":{"abstract":"The ability to run programs deterministically, so that re-execution always yields identical results, is useful for many purposes: e.g., replay debugging, intrusion analysis, fault tolerance, byzantine accountability, and timing channel control. Running parallel programs deterministically is traditionally difficult and costly, however, especially if we wish to guarantee precise repeatability even of arbitrarily buggy or malicious software.<br\/><br\/>Determinator is a novel operating system that enforces determinism on multithreaded and multi-process computations, parallelized both across cores in one machine and across nodes in a cluster. The kernel provides only single-threaded, ``shared-nothing'' address spaces, interacting via synchronization primitives that enforce deterministic behavior on all user-level code. Nondeterministic inputs and observable notions of time - including clocks, timers, cycle counters, and timing-dependent internal communication channels - are accessible only via controlled I\/O mechanisms, giving supervisory software precise control over how and when nondeterministic information may affect a supervised computation. Atop this constrained kernel API, an untrusted runtime uses distributed computing techniques to emulate familiar abstractions such as Unix processes, file systems, and shared memory multithreading.<br\/><br\/>By building and evaluating this experimental OS architecture, we hope to discover: (1) whether OS-enforced deterministic execution can be made practical and performance-competitive with conventional OS environments, even for massively parallel applications; (2) how to emulate conventional nondeterministic APIs and run legacy software deterministically with few modifications; and (3) how to create new, \"naturally determinisic\" parallel programming APIs, offering powerful but easy-to-use abstractions for expressing parallelism, while guaranteeing predictable and precisely repeatable results that are independent of execution scheduling.","title":"CSR: Small: SHF: An Operating System and Programming Model for Deterministic Parallel Computation","awardID":"1017206","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["502304"],"PO":["565255"]},"168663":{"abstract":"Software developers are increasingly building applications in multiple languages both to reuse existing software libraries and to leverage the languages best suited to their problems. In fact, multilingual programs are already prevalent, as essentially all programs written in Java use code written in C as well. But developing multilingual programs poses more challenges and difficulties than developing single language programs. Current programming interfaces between multiple languages tend to be tedious and unsafe; and most programming tools only support a single language. As a consequence, real-world multilingual programs are full of cross-language bugs.<br\/><br\/>This research seeks to substantially improve the correctness and development of multilingual programs. The investigators build on their prior multilingual language design (the Jeannie Java\/C language), compilers (the xtc Jeannie compiler), and debuggers (the Blink Java\/C debugger). Here, the investigators are designing and implementing novel approaches for validating the safety of existing multilingual interfaces and for creating safe multilingual programs. (1) The investigators are developing a framework for concisely capturing safety rules for multilingual interfaces and then automatically synthesizing the corresponding dynamic checker. (2) The investigators are exploring novel program analysis to refactor programs automatically that use unsafe multilingual interfaces into programs that use safe multilingual interfaces. If successful, the research impact will be improved correctness, efficiency, and reliability of multilingual programs.","title":"SHF:Small:Collaborative Research: Languages and Tools for Multilingual Systems","awardID":"1017849","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["508475"],"PO":["564588"]},"168784":{"abstract":"Tabled resolution uses memoization to address the major shortcomings of Prolog-style resolution, namely, weak termination, repeated subcomputations, and weak semantics for negation. Several complex problems requiring fixed-point computation, including several model checking and program analysis problems, have been cast as query evaluation over logic programs and solved efficiently using tabled resolution.<br\/><br\/>This project aims to combine deduction, especially tabled resolution, with probabilistic inference in order to facilitate declarative modeling and reasoning of systems with probabilistic behavior. The project includes (a) fundamental research on the semantics of probabilistic tabled resolution, (b) implementation of the research results in a robust prototype, and (c) development of applications that combine logical and probabilistic reasoniong such as probabilistic model checking of finite-state and pushdown models and probabilistic program analysis. The project addresses several problems at the level of semantics, including the treatment of programs with a mixture of continuous and discrete random variables, programs that encode dynamic models over unbounded but discrete time, and the integration of scalable probabilistic inference techniques (e.g. sampling) with deduction. At the implementation level, the project develops light-weight methods for incorporating probabilistic inference and parameter learning into a declarative programming framework called Probabilistic Tabled Logic Programming (PTLP). This framework provides a firm semantic and computational basis for combining logical and probabilistic reasoning. A computing infrastructure that tightly integrates computational logic, statistics, and constraint processing will have an immediate impact on the areas of system development and verification, planning, logistics, and optimization and control, with broad application in science and engineering.","title":"Probabilistic Tabled Logic Programming","awardID":"1018459","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["518366",451745,"509519","477226"],"PO":["565264"]},"168432":{"abstract":"This project will improve the state of the art of the implementation and optimization of algorithms for exact linear algebra computation. With exact computation, solving systems of linear equations is advanced from limited accuracy to exact solutions. This greatly increases the scope of accessible applications and allows matrix invariants such as rank, determinant, characteristic and minimal polynomial, and Smith and Frobenius normal forms to be computed.<br\/><br\/>We will combine a newly developed theoretical basis for block blackbox methods in linear algebra with high performance implementation, in hardware and software, of the computational kernels from which these implementations are constructed. The resulting implementations will be made publicly available in the framework of the LinBox software library. A system for automatically tuning the underlying computer algebra kernels will be developed and distributed as part of the LinBox library. The autotuning framework will benefit other computer algebra systems as well. The resulting advances for computation in finite domains, such as modular numbers and finite algebraic field extensions, will benefit many areas including cryptography and coding theory.<br\/><br\/>The project has many practical impacts as follows:<br\/><br\/>Experimental mathematics will be enhanced. In experimental mathematics, symbolic computation provides for testing of conjectures. And, perhaps more importantly, data from symbolic computations can guide the formulation of conjectures that are then candidates for formal proof. By permitting larger exact linear algebra computations, this project will increase the usefulness of such computation in mathematics.<br\/><br\/>The broadest, and perhaps most significant, outcome of this project is an ability to solve many problems which currently have no solution method at all. This project will make it possible to efficiently solve linear systems where numerical methods fail due to ill-condition of the problem instance, yet the exact result could it be obtained is valid and meaningful despite the approximate nature of the data.","title":"AF: Small: Collaborative Research: High Performance Exact Linear Algebra Kernels","awardID":"1016728","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["555302"],"PO":["565251"]},"168553":{"abstract":"Wireless networks have had a profound impact on the way people work and live. With the advent of mesh networks and WiMax, wireless multihop networks could have a similar impact by providing the last piece of a ubiquitous networking infrastructure. However, wireless multihop networks are subject to a capacity limit, as set forth in the classic work by Gupta and Kumar. Of different technologies proposed for throughput enhancement, the use of MIMO links is especially promising. This project considers how to use MIMO resources optimally to achieve network-wide performance goals. MIMO resources can be used to increase single-link performance through array and diversity gains and spatial multiplexing of streams, or to activate otherwise conflicting links simultaneously through interference suppression. The proposed research characterizes the inherent tradeoffs among these capabilities. The PIs are addressing the problem through both formal optimization techniques and algorithm design and analysis. <br\/><br\/>Algorithms for evaluating feasibility of a set of MIMO links, for scheduling streams across a MIMO network, and for routing of flows are being developed. Our problem formulations account for important MIMO characteristics that were ignored in prior work on MIMO networks. From these more accurate models, the PIs are designing MIMO-aware algorithms that will enable major throughput gains. In addition to the formal analyses and new algorithms that will result from this research, they are integrating MIMO models and algorithms into the ns-3 network simulator. This is expected to spur additional research into the benefits of MIMO from the broader wireless networking community.","title":"NeTS: Small: Wireless Multihop Networking with MIMO","awardID":"1017248","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["562577","562753","507792"],"PO":["557315"]},"168443":{"abstract":"The standard instrumentality for the criminal acquisition and<br\/>distribution of images and video of child sexual exploitation is<br\/>peer-to-peer (p2p) networks. Over 160,000 users based in the US<br\/>are sharing child pornography (CP) using Gnutella alone. Past<br\/>studies have found that: 21% of CP possessors had images<br\/>depicting sexual violence to children such as bondage, rape, and<br\/>torture; 28% had images of children younger than 3 years old; and<br\/>that 16% of investigations of CP possession ended with discovery<br\/>of persons who directly victimized children.<br\/><br\/>The proposed work aims to make significant advances in forensics<br\/>methods of investigating criminal acts on peer-to-peer file<br\/>sharing networks. The project represents a unique<br\/>multidisciplinary collaboration between computer science and<br\/>criminology with close participation from existing law<br\/>enforcement partners.<br\/><br\/>Intellectual Merit. The project makes the following broad<br\/>contributions:<br\/><br\/>- It proposes novel methods of tagging a remote computer over the<br\/>network with information that can uniquely identify it during a<br\/>forensic examination. These serve as both identifiers and as<br\/>indicia of intent.<br\/><br\/>- It proposes to gather a foundational dataset regarding the<br\/>prevalence and rate of spread of child pornography on p2p<br\/>networks. Further, it proposes to measure and quantify the<br\/>relationships between child abuse and the trading of child<br\/>pornography on p2p networks. Finally, it proposes the<br\/>development of models to detect the trafficking of deliberately<br\/>hidden child pornography on these networks.<br\/><br\/>Broader Impact. The project aims to reduce the number of children<br\/>sexually exploited each year by thwarting the trafficking of<br\/>their images on p2p networks and via the capture of the contact<br\/>offenders that rape, torture, or otherwise brutalize them. This<br\/>work will increase cross-disciplinary collaboration between<br\/>computer science and criminology and between law enforcement and<br\/>academia; effect technology transfer to law enforcement in the<br\/>field of online child pornography investigations, which will help<br\/>reduce the number of victims of crimes in the future; and<br\/>facilitate broad educational outreach and recruitment of<br\/>under-represented minorities in undergraduate and graduate<br\/>research in digital forensics.<br\/><br\/>For further information see the project web site at the URL: <br\/> http:\/\/prisms.cs.umass.edu\/CNS-1018615","title":"TC: Small: Collaborative Research: Strengthening Forensic Science for Network Investigations","awardID":"1016788","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[450937],"PO":["564223"]},"168564":{"abstract":"Software engineers often do not get access to confidential data because of internal security rules that are put in place by organizations that own these data and because of several laws that regulate data protection and privacy. This situation complicates basic software engineering tasks such as testing. To give some access to required data, data owners typically use a commercial tool to anonymize or \"sanitize\" the data. Unfortunately, none of the existing tools takes into account basic software engineering tasks such as testing, which leads to situations where the anonymized data is of little to no value for software engineers. Currently, software engineers operate with little or no meaningful data, which is a great obstacle to creating high quality software. <br\/><br\/>This research program addresses a fundamental question of software engineering: how can a data owner protect private information so that the data subjects (e.g., persons, equipment) cannot be re-identified while the data retains their efficacy for software engineering tasks? To preserve the usefulness of data for software engineering tasks, algorithms are needed that take into account the structures of the applications. This work will lay a foundation for a new direction of research on interactions between software engineering and data privacy, and the PIs will support it with a set of tools for low-cost software development and evolution.","title":"SHF: Small: Collaborative Research: Preserving Test Coverage While Achieving Data Anonymity for Database-Centric Applications","awardID":"1017305","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["558117"],"PO":["564388"]},"168454":{"abstract":"Transactional Memory (TM) is among the most promising techniques for simplifying the development of correct parallel programs. Dozens of different techniques have been developed to implement TM, with each appearing ideally suited to some combination of application and hardware characteristics. Unfortunately, when the wrong algorithm is chosen for a workload, pathologically bad performance can result. <br\/><br\/>The problem is particularly acute in programs whose behavior varies over<br\/>time: the best TM implementation for one phase of execution may be unacceptable during another phase. This research will create new algorithms for dynamic adaptivity, so that the implementation of TM can be changed repeatedly during program execution, thereby maximizing performance during each program phase.<br\/><br\/>The research will explore both mechanism and policy. It will create new algorithms and heuristics for selecting TM implementations. It will also model the behavior of a wide array of TM algorithms and workloads on multiple architectures, in order to create a knowledge base suitable for guiding adaptivity. To achieve maximum performance, the research will consider a broad set of characteristics, to include TM implementation details, bottlenecks, and overheads; risk of pathology; properties of the hardware environment; and workload requirements such as frequency of inter-thread synchronization, I\/O, and nontransactional access to shared data. This research will also develop novel static analyses and a dynamic optimization framework to avoid any overhead while providing robust, adaptive TM. Prototypes and source code will be distributed as open-source software.","title":"CSR: Small: Adaptive Synchronization for Multicore Systems","awardID":"1016828","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["534282"],"PO":["565255"]},"168696":{"abstract":"The IEEE Information Theory Workshops (ITWs) are among the most prestigious, high-impact conferences in information theory and theoretical communications worldwide. The 2010 ITW is hosted by Claude Shannon Institute for Discrete Mathematics, Coding, Cryptography, and Information Security and the University of Dublin, Ireland. The University and the Institute assembled one of several most active research groups in Coding and Information Theory in the world, on a par with University of California at San Diego, University of Illinois at Urbana-Champaign, Israel Institute of Technology (Technion), and Nanyang University in Singapore. The ITWs enjoy technical sponsorship of the IEEE through its information theory society and access to the IEEE advertising and publication infrastructure. The 2010 Dublin ITW facilitates collaboration and the exchange of ideas between US students and researchers and experts from Europe and Asia whose participation is expected to rise because of the selection of plenary speakers and the invited sessions.","title":"CIF: Small: 2010 IEEE Information Theory Workshop","awardID":"1018012","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["518024"],"PO":["564924"]},"168498":{"abstract":"Microprocessor designers wish to predict the performance of future microprocessors, evaluate new ideas, and validate new systems. The primary technique they use is software-based simulation. Unfortunately, software-based simulators for manycore microprocessors are slow and will become even slower as the number of cores increases. Slow simulators imply that little exploration of the design space can be done -- few design alternatives are considered and those that are considered are not evaluated thoroughly. Hybrid simulators, which use reconfigurable hardware devices such as Field Programmable Gate Arrays (FPGAs), could accelerate simulators by orders of magnitude, but are themselves very difficult and time-consuming to create; this difficulty arises from the need to first partition the simulator between hardware and software and then design the hardware portions.<br\/><br\/>The goal of this project is to provide tools which allow a designer to easily and quickly create a hybrid simulator from a software-based simulator. The tools will assist the designer to partition the design and then automatically synthesize the hardware portions based upon the selected partitioning. Three different software simulation frameworks -- SystemC, Unisim, and the Liberty Simulation Environment -- will be supported. We target a 100x simulation speed improvement relative to software-based simulators. Achieving this target will require the development of new means to exploit parallel communication and execution between the hardware and software and new means to virtualize the hardware resources. The resulting tools will enable researchers and practitioners to better explore the manycore design space, evaluate new ideas, and validate designs.","title":"SHF: CSR: Small: Assisted Partitioning and Automated Synthesis of Hybrid Manycore Simulators","awardID":"1017004","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["469901"],"PO":["366560"]},"173493":{"abstract":"This project is to study knowledge transfer oriented data mining (or<br\/>KTDM). Given two data sets, the idea of KTDM is to discover models<br\/>that are common to both data sets, as well as models that are unique<br\/>in one data set. These common and unique models with respect to the<br\/>two data sets will provide a tool to leverage the already-understood<br\/>properties of one data set for the purpose of understanding the other,<br\/>probably less understood, data set. This EAGER project is to<br\/>concentrate on models in the form of a diversified set of<br\/>classification trees. The KTDM approach is useful for real-world<br\/>applications in part due to its ability to allow users to narrow down<br\/>to particular models, guided by known knowledge from another data set.<br\/>It will help towards realizing transfer of knowledge and learning in<br\/>various domains. The project will support a graduate student and will<br\/>seek collaboration with experts in the medical domain. These will<br\/>increase the impact of the project. For more information, please see<br\/>http:\/\/www.cs.wright.edu\/~gdong\/projects.html.","title":"EAGER: Knowledge Transfer Oriented Data Mining with Focus on the Decision Trees Knowledge Type","awardID":"1044634","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[465463],"PO":["543481"]},"177530":{"abstract":"CAREER: Collaborative Communication and Storage for Sensor Networks in<br\/>Challenging Environments<br\/><br\/>NSF Proposal ID: 0953067<br\/>PI: WenZhan Song<br\/><br\/>This project concerns the sustainability and reliability of sensor networks in challenging (e.g., extreme) environments. In a challenged sensor network, a predictable and stable path may never exist, the network connectivity is intermittent, and a node could suddenly appear or disappear. The rare upload opportunity and unpredictable node disruptions often result in data loss. The unpredictable network disruptions make the traditional communication protocols inefficient and require a new design paradigm to combat network disruptions and maintain reliable operations. The driven research idea of this project is collaborative communication and storage - an integrated approach which cooperatively configures resources to increase disruption resilience, data persistence and network lifetime, and capture the fluctuating connectivity for data delivery. The expected outcomes of this project are (1) the innovative architecture principles, algorithms and protocols, design and evaluation methodologies for sensor networks in challenging environments, (2) the design and implementation of an integrated collaborative communication and storage middleware, and the running prototype sensor network systems in relevant environments, and (3) the curriculum and test bed enhancement for undergraduate and graduate research and courses, and educational outreach activities to K-12 students, minority groups and local communities. This project will greatly promote the confident use of sensor networks in challenging environments and enable new applications and economics. The outcomes of this project will be broadly disseminated to academic and industrial communities through international conferences, journals, media and intra-disciplinary and inter-disciplinary collaborations.","title":"CAREER: Collaborative Communication and Storage for Sensor Networks in Challenging Environments","awardID":"1066391","effectiveDate":"2010-08-23","expirationDate":"2015-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["542417"],"PO":["565303"]},"168840":{"abstract":"In this project, the PIs and students study a probabilistic and graphical representation, called the And-or graph (AoG) for visual knowledge representation. This AoG model embodies hierarchical and contextual models for visual objects and scenes and is the key to robust object and scene recognition. More specifically, the project addresses two major technical challenges: (i) Learning the AoG for representing objects and scenes in an unsupervised way; and (ii) Developing effective inference algorithm by scheduling top-down and bottom-up processes to extract semantic contents in a parse graph under the guidance of the AoG. The extracted semantics include the hierarchical decomposition of the image from scene to objects, and parts, as well as the contextual relations. These contents are crucial for filling in the semantic gap in large scale image search and retrieval. The technologies studied in this project are key to a number of applications, such as image content extraction for security surveillance, information gathering, Internet image search, and situation awareness. One specific application studied in this project is autonomous driving assistant for designing safer vehicles and reducing car accidence. The project also supports the training of 3 graduate students over the three year period. Research results are disseminated through public publications in major computer vision conferences and journals, institutional webpages, and shared data sets and code in the Internet.","title":"RI: Small: Learning and Inference with And-Or Graphs for Image Understanding","awardID":"1018751","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["546023","456867"],"PO":["564316"]},"168730":{"abstract":"Providing restrictive and secure access to resources is a challenging and socially important problem. Security analysis helps organizations gain confidence on the control they have on resources while providing access, and helps them devise and maintain policies. There is a dire need for analysis tools to<br\/>help administrators ensure security as they make administrative changes to reflect changes in policy. Security analysis of access control is non-trivial for an administrator due to the complexity of reasoning with the beguiling number of possible future scenarios. Techniques for the analysis of security in access control is in its infancy. The goal of this project is to go beyond decidability\/undecidability issues, and go forth to build scalable and usable security analysis tools and techniques when access control is deployed via the most commonly used role-based access control (RBAC) models or its spatiotemporal extensions.<br\/><br\/>The main thesis of this project is that finding breaches of security in an access control model is very similar to finding errors in a program. Some of the innovative expected results include: accurate mapping of the security problem for policies in access control as reachability problems in transition systems, including succinct discrete systems and automata with spatio-temporal constraints; scalable techniques to search for security breaches by exploiting the model-checking techniques developed by the program verification community; usable and useful tools for administrators to express policies and<br\/>automatically find breaches of their security policies. The project helps in building technical bridges between the communities of access control security and formal methods in verification, which is expected to trigger a flurry of research, possibly unifying problems in the two fields, and initiating each<br\/>other with new ideas. Scalable and usable security analysis will also serve needs in many settings including emergency, disaster management and homeland security applications. The tools will be<br\/>included as modules in a tele-medicine system and an emergency management system. The integration of the ideas, techniques, and tools resulting from this project into the education curriculum will positively impact the quality of a newly trained workforce that is prepared to meet security challenges, making them aware of security issues in access control, and educating them on practical ways to check for breaches in security.","title":"TC: Small: Collaborative Research: Formal Security Analysis of Access Control Models and Extensions","awardID":"1018182","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["497077"],"PO":["565264"]},"168862":{"abstract":"Two principal components for providing protection in large-scale distributed systems are Byzantine fault-tolerance (BFT) and intrusion detection systems (IDS). BFT is used to implement strictly consistent replication of state in the face of arbitrary failures, including those introduced by malware and Internet pathogens. Intrusion detection relates to a broad set of services that detect events that could indicate the presence of an ongoing attack. IDSs are far from perfect -- they can both miss attacks or misinterpret events as being malicious. In addition, IDSs themselves are vulnerable to attack. These two components approach different parts of system security. Each, however, has the potential to improve the other, which is the theme of this project. The integration of these two efforts, at both the fundamental and system levels, has proven elusive. Fault-tolerant distributed algorithms have been designed to use failure detectors for some time, but only as an abstraction. Intrusion detection has been, for the most part, a service that gives some general improvement in system security. Attempting to marry these two approaches could be a large step towards making BFT a truly practical approach in multisite systems, and gives a novel way to integrate multiple IDSs to improve the security in a multisite system with nonuniform and varying trust. <br\/><br\/>Some examples of such benefit are (1) Any evidence gathered by BFT about suspicious behavior can be useful for an IDS, since it could indicate that the system has been compromised. (2) Information from an IDS can be used by BFT to influence its behavior towards the servers of the replicated service. This could, for example, allow BFT to stop using a site even though the service has not (yet) been affected, or to assume a more benign set of failures for a site that appears to be well managed. (3) The way that BFT reacts to suspicious behavior is a complex policy that could, at least in part, be moved to IDS. Doing so would allow the policy to be tuned. (4) A further detection method is to compare the internal suspicions of BFT with the external suspicions of the IDS. (5) BFT can be used to detect and cope with attacks on an IDS. (6) IDS can confirm that parties in a BFT set are behaving according to the BFT protocol which if so can improve the performance of a BFT system. This research explores this potential of a merged system by developing a version of BFT for wide-area networks that is designed with several IDSs as part of the architecture. The IDS will serve as a suspicion detector that allows BFT to define sets of sites that trust each other, and can thus use a lower latency protocol among them. The IDSs will use BFT to agree upon detection states to make more useful detections. Information collected by BFT will be used by the IDS to detect malicious behavior. And, BFT and IDS will, where possible, check each other to increase the detection power of the system. A prototype of the system will be implement and a simple synthetic application to measure performance and sensitivity to a set of simulated attacks will be built.","title":"TC: Small: Collaborative Research: Symbiosis in Byzantine Fault Tolerance and Intrusion Detection","awardID":"1018871","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["521752","564751"],"PO":["565264"]},"165474":{"abstract":"The project examines visually-complex documents?combination of textual and graphical elements, perhaps resulting from multiple interactions by several different people over a span of time. The approach that will be taken in this project is to enable human ingenuity through technology that helps people express, evaluate, and hypothesize about deconstructions. To accomplish this, the project will be conducted in three phases. In the first phase, we will gain an understanding of how visually-complex documents are created, used, and analyzed by scholars in multiple fields of study. This information will then be used to guide the development of a software prototype allowing manually-assisted deconstruction of these documents into their component parts, as well as allowing readers to posit structural relationships among the parts. The prototype will be placed into a research group and its use will be evaluated in the third phase.<br\/><br\/>The pilot work for this research will focus on scholars working with cultural heritage materials. While the need to interact with visually complex documents is commonplace, it is particularly pressing in the domain of cultural heritage. They will work specifically with the Ship Reconstruction Lab, part of the Center for Maritime Archaeology at Texas A&M University. Archaeology is a discipline that draws on research from a wide range of academic disciplines, both STEM and humanistic. This fact makes it a particularly useful area to study in order to develop models and tools that will be relevant for the needs of people in many different disciplines. Visually complex documents are a central component of almost all areas of scholarship and enterprise, including science, engineering, business, arts, and humanities?almost all researchers and practitioners deal with documents that include spatially significant elements such as text, sketches, annotations, shorthand, and personal or shared visual notations. The spatial arrangement of text and graphical elements contribute to the document?s meaning in conjunction with the actual words or images of the document. These relationships are often visually apparent to a reader, but their actual presentation does not represent them because the elements must be flattened to be displayed on a two dimensional surface (physical or digital). The ability to directly express and manipulate their previously-lost structure?in other words to decompose the document into its constituent parts?is required for understanding or to enable analysis. Such decompositions may be spatial, temporal, or may reflect other characteristics. Significantly, it is not always clear how a document is best decomposed. This task is a creative process that requires support for alternate deconstructions and re-constructions of lost intermediate forms of the document. During this analysis, understanding will emerge as the analyst iteratively expresses different potential information layers and interacts with those expressions.","title":"Pilot: Supporting Creativity in the Analysis and Understanding of Visually Complex Documents","awardID":"1002825","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["465209"],"PO":["565227"]},"168510":{"abstract":"Despite growing interest in the economic aspects of the Internet, such as network interconnection (peering), pricing, performance, and the profitability of various network types, two historical developments contribute to a persistent disconnect between economic models and actual operational practices on the Internet. First, the Internet became too complex -- in traffic dynamics, topology, and economics -- for currently available analytical tools to allow realistic modeling. Second, the data needed to parameterize more realistic models is simply not available.<br\/>The problem is fundamental, and familiar: simple models are not valid, and complex models cannot be validated.<br\/><br\/>This project aims to achieve transformative progress in studying economic aspects of the Internet -- network interconnection (peering), pricing, and the profitability of various network types -- by creating more powerful, empirically parameterized computational tools, and enabling broader validation than previously possible. This project will involve measurement of key properties that impact Internet infrastructure economics, such as interdomain traffic, topology dynamics, routing policies and peering practices. These measurements will serve as inputs to a computational model of network interconnection and dynamics. The investigators will validate the model's ability to reproduce known macroscopic properties of the Internet topology, and its ability to reproduce known historical trends in the evolution of the Internet. The investigators will then use the model to study various \"what-if\" scenarios relating to interdomain interconnection practices, the stability and dynamics of interdomain peering links, and economic properties of provisioning Internet infrastructure.<br\/><br\/>The intellectual merit of this project lies in an approach grounded in empirical measurements of macroscopic Internet topology, traffic demand, routing policies, and peering policies. The data promise to reveal important, and thus far elusive, insights into the economic implications of topology dynamics, interdomain traffic characteristics, and routing policy, but they will also inform the parameterization of a model of network interconnection incentives and dynamics. <br\/><br\/>The broader impact of this project lies in deeper, empirically grounded interpretation of available data on the most opaque sub-discipline of network research -- internetwork economics. The educational side of the project will integrate Internet economics in two Georgia Tech courses, while a PostDoc and a PhD student will graduate as experts in this nascent sub-discipline of Internet research. The data and methods developed during the course of this project will be publicly available and regularly presented to both the research community as well as operator and policy forums, e.g., NANOG, FCC.","title":"NetSE: Small: Collaborative Research: The Economics of Transit and Peering Interconnections in the Internet","awardID":"1017064","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":[451089,"521741"],"PO":["564993"]},"166211":{"abstract":"Quantum walks may be described as the natural counterparts of classical random walks, governed by the principles of quantum mechanics. In recent times, because of their potential applications to the development of a new species of super-efficient algorithms, quantum walks have been the focus of extensive research interest. However, largely due to the mathematical challenges involved, some important aspects of the theory have remained relatively unexplored. These challenges have persisted, especially in connection with attempts to treat jointly the effects of the characteristically quantum-mechanical phenomena known as \"entanglement\" and \"decoherence\".<br\/><br\/>Through this project, PI aims to elucidate the effects of decoherence on quantum walks over certain networks. His study will address two questions: 1) How is the limiting distribution of the quantum walk related to the level of decoherence to which the quantum walk is exposed? and 2) How does the measure of entanglement between the subsystems (coin and position) depend upon the level of decoherence? In this approach, the measure of entanglement is reflected by the mutual information between the subsystems, which, in turn, is defined in terms of the von Neumann entropy. <br\/><br\/>The outcomes to flow from this proposal will be of interest to a wide audience of active investigators at the forefront of research in the fields of quantum computing and quantum information. Last but not least, through this project, PI aims to introduce his students, both graduate and undergraduate, to the emerging field of Quantum Information Science.","title":"Entanglement and Decoherence in Quantum Walks","awardID":"1005564","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7928","name":"QUANTUM COMPUTING"}}],"PIcoPI":[445285],"PO":["565157"]},"168521":{"abstract":"This project is designed to help fulfill the potential of the \"Smart Grid,\" a new vision for the electrical infrastructure of the United States, whose goals include more active participation by consumers, new generation and storage options including renewable energy, and new products, services, and markets. To accomplish this, the research will develop a collection of open source components called WattBlocks, which will provide novel and useful scientific infrastructure for investigating the ways in which energy-related information can affect human behavior. The project will also develop eSpheres, a novel social networking application that provides users with access to energy-related communities at configurable levels of scale. The combination of WattBlocks and eSpheres will lower the technological efforts required for empirical, replicable studies of human energy-related behaviors. The project will use this infrastructure in a series of two case studies, one involving campus dormitory energy competitions and one involving community home energy challenges. <br\/><br\/>To reach its full potential, the Smart Grid must provide information to consumers in a way that enables positive, sustained changes to energy-related behaviors. The central question to be pursued in this research is: What kinds of information, provided in what ways and at what times, enables consumers to make positive, sustained changes to their energy consumption behaviors? Prior research indicates that such changes can potentially be motivated by an appropriate combination of personalized information, general and specific commitments, achievable goals, social reinforcement, feedback, and financial incentives.<br\/><br\/>The project will investigate a number of important research questions, including: (1) What are the requirements for consumer-facing, open source, scientific energy information infrastructure? (2) What are the strengths and weaknesses of a dedicated social network technology like eSpheres for energy behavior change? (3) What combination of behavioral change motivators, under what conditions, induce positive change? (4) What factors influence the sustainability of these changes? (5) What is the influence of energy data feedback latency (i.e. 1 minute, 15 minutes, 1 hour, 1 day) on behavioral change? <br\/><br\/>This project will serve underrepresented populations, as the University of Hawaii is in an EPSCOR state. Approximately 84% of undergraduates at the University of Hawaii are minorities. WattBlocks and eSpheres will be released as open source software, providing new and useful infrastructure for others interested in investigating energy-related human behavior. Finally, the resulting insights can inform the design of consumer-facing Smart Grid information systems, with potentially significant cost-saving or service benefits for the U.S. population.","title":"HCC:Small:Human Centered Information Integration for the Smart Grid","awardID":"1017126","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[451116],"PO":["564456"]},"168653":{"abstract":"Software reliability is critical for many applications. The pervasiveness of multi-core hardware and parallel programming makes parallel bugs become an increasingly important and urgent issue. Concurrency bugs together with the other types of bugs have significantly impacted software reliability. Although much effort has been put on detecting software bugs, existing work is still far from ideal, and many bugs, especially those in parallel or distributed programs, are still difficult to catch by existing tools. This proposal makes a major step toward improving the correctness of software, especially parallel and distribute software, by proposing a novel and widely applicable invariance, called data-flow invariance, that can be used to detect various types of software bugs, including parallel bugs and other types of bugs, and make software more reliable and secure. We strongly believe that our proposed research can effectively improve our understanding of this challenge, provide substantial tool support to software development, and greatly improve the quality of parallel and distributed software. We have also planned various educational and outreach activities for students, especially women students in computer science.","title":"SHF: Small: Software and Hardware Support for Detecting Concurrency, Sequential and Distributed Bugs via Data-Flow Invariants","awardID":"1017804","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["551097"],"PO":["565264"]},"168774":{"abstract":"It is was recently proved that there are situations in which the quantum phenomenon called entanglement can increase the capacity of a quantum channel used to transmit classical information. Such channels are called non-additive; however, no explicit examples are known. Even more recently, it was observed that the anti-symmetry associated with the Pauli exclusion principle can be used to give an example of a channel which is non-additive for a related quantity, called the minimal output Renyi entropy, when p > 2. This grant is intended to construct more examples of non-additive channels and clarify our understanding of this phenomenon. In connection with this, the PI will closely examine the role of permutational symmetry in quantum information theory. The PI will also study entropy inequalities and cones of entropy vectors for composite systems. <br\/><br\/>This highly interdisciplinary work will clarify the role of the Pauli exclusion principle, which is often suppressed in quantum information theory. It will also be of interest to quantum chemists and condensed matter physicists, particularly those who used reduced density matrices. The work should help identify situations in which quantum systems have advantages over classical ones for communication, as well as computation.","title":"Quantum Information Theory","awardID":"1018401","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7948","name":"QUANTUM COMMUNICATION"}}],"PIcoPI":[451719],"PO":["565157"]},"168301":{"abstract":"Randomness extraction is an algorithmical process that transforms objects with low-quality randomness into objects with high-quality randomness. The objects (usually called sources) can be finite probability distributions, finite binary strings, and infinite binary sequences, and the randomness quality is measured, respectively, by min-entropy, Kolmogorov complexity, and constructive Hausdorff dimension. There exists a vast amount of work on randomness extractions in all three settings. Our project will extend the field by investigating randomness extraction in situations that go beyond some of the basic assumptions for which the main current techniques and tools have been developed.<br\/><br\/>In some applications, the assumption that the involved probability distributions are independent is problematic. Therefore, an algorithm that attempts to remedy defective sources should be able to handle even sources with bounded independence. The project will study the issue of randomness extraction from two or more sources with bounded independence, in contrast with the current literature that has only considered the case of fully independent sources. Extending some partial results, the objective is to establish upper bounds on the quality of randomness that can be obtained from such sources and to design efficient extractors that perform in the vicinity of the upper bounds. Another research line of the project is dedicated to exposure-resilient extractors, which are efficient procedures that manage to extract bits that look random even to an adversary that has adaptive access to the input sources. The objective is to design such extractors with parameters suitable for applications in cryptography.<br\/><br\/>Randomness extraction is a very active field and has been an incubator of ideas with a significant impact even outside the area. The proposed research opens new directions of study that are natural and challenging. It adds new dimensions in the study of randomness extraction and has the potential of enlarging the range of applications of extractors. Randomness extraction has applications in computational complexity, randomized algorithms, constructive combinatorics, cryptography, error-correcting codes, and other areas. The project attempts to make many of these applications more practical and more robust. The project will establish new connections between computational complexity, Kolmogorov complexity, and algorithmical information theory. The project will allow undergraduate and graduate students to participate in research activities that have a strong theoretical flavor and the promise of real-world applications.","title":"AF: Small: Studies in Randomness Extraction","awardID":"1016158","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":[450591],"PO":["565157"]},"168422":{"abstract":"Software developers aim to create reliable software at low cost. Two types of tools that help in this task are static and dynamic analysis tools.<br\/>Static analysis tools reason about the program source code, without ever running the program; a widely-used example is type systems. Dynamic analysis tools observe the program as it executes; a widely-used example is testing. Dynamic and static feedback provide complementary benefits, neither one dominates the other, and at any moment only the programmer knows which one would be most useful. Unfortunately, current programming languages impose too rigid a model of the development process: they favor either static or dynamic tools, which prevents the programmer from freely using the other variety. We propose a new approach, in which the developer always has access to immediate execution feedback, and always has access to sound static feedback. The key broader impact is to permit developers to work the way they find most natural and effective, which will improve reliability and reduce cost. Developers will create software that is more reliable than that created in an environment that favors dynamic analysis.<br\/>Developers will work faster than they can in an environment that favors static analysis.<br\/><br\/>It is well-known that dynamically-typed scripting languages and statically-typed programming languages have complementary strengths. For example, scripting can permit faster and more flexible program development and modification, whereas programming languages can yield more reliable and maintainable applications. Our goal is to give programmers the benefits of both scripting languages and programming languages, so that a programmer can shift back and forth between the two paradigms depending on the task at hand. In our approach, a programmer can view and execute a program through the lens of sound static typing, or can view and execute a program through the lens of dynamic typing with no statically-imposed restrictions. Furthermore, the programmer can switch between these two views as often as desired, or can use them both simultaneously, depending on the programmer's current information needs. In our approach, a programmer may temporarily disable the type system and test the code, even if the code does not type-check. The code is executed as if no types had been written. Type errors (inconsistencies between the execution and the declared types) are logged but do not terminate execution. The logged errors can be examined if the test fails or deferred if the test succeeds. This mode is appropriate for tasks such as initial prototyping, evolving an interface, representation changes, library replacement, and exploratory refactoring.","title":"SHF: Small: Always-On Static and Dynamic Feedback","awardID":"1016701","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["501796"],"PO":["564388"]},"168543":{"abstract":"Data replication demands complex tools such as SYNCHRONIZERS and VERSION CONTROL SYSTEMS to maintain or restore consistency when changes are made to replicated data structures. Unfortunately, the burgeoning activity in this area has not been fully matched by advances in its theoretical underpinnings, resulting in systems that are complex, hard to understand, and prone to surprising behaviors. The project aims to develop mathematical foundations and a prototype implementation of a HETEROGENEOUS VERSION CONTROL SYSTEM, able to manage propagation and integration of concurrent updates to multiple replicas, where each replica may be represented differently and may include information that is not shared with the others.<br\/><br\/>Past work in these areas has had significant real-world impacts: the Unison file synchronizer is now part of the basic toolset of a large community of users, both within and outside the research community, and Redhat's Augeas tool, based directly on the experimental bidirectional language Boomerang, is becoming a popular tool for system configuration management. Better mathematical foundations will improve the functionality and robustness of future systems---for example, industrial tools for software model transformation.<br\/><br\/>The project's primary technical threads are (1) the algebraic theory of SYMMETRIC LENSES, a mechanism for propagating bidirectional updates, (2) a new THEORY OF PATCHES providing a solid foundation to modern distributed version control systems, and (3) a prototype implementation of a system embodying both sets of ideas.","title":"SHF: Small: Algebraic Foundations for Collaborative Data Sharing","awardID":"1017212","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["563479"],"PO":["564588"]},"168664":{"abstract":"This project aims to develop a substrate called SocialLite that can use online social network data to obtain reliable identity and trust information. This work involves three steps: 1) identifying the rich variety of identity and trust information embedded in online social networks; 2) designing algorithms and software to efficiently and robustly abstract this information as a set of flexible API functions without violating a user?s privacy from large online social networks; and 3) evaluating the usefulness of the API by implementing a few sample applications. The SociaLite API can facilitate the development of robust and trustworthy distributed systems. Prototypes developed from this project will be released to the public to engage a broader developer community to use this API to build trustworthy applications. The algorithms developed in this project can advance the research community's understanding on trust metric computation and the utility of social-trust based computing. For further information, see the project website at http:\/\/www.cs.duke.edu\/nds\/wiki\/sociallite.","title":"NeTS: Small: Exploiting Social Networks to Build Trustworthy Distributed Systems","awardID":"1017858","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["462629"],"PO":["562974"]},"168312":{"abstract":"This research addresses the trustworthy distribution and retrieval of information across the Internet. Specifically, it concerns the distribution of metadata and requests, the matching of requests and metadata, and the retrieval of information corresponding to the metadata. In the Internet today, this functionality is provided by centralized search engines and search indexes. However, such centralized mechanisms can be easily subverted to prevent the distribution and retrieval of information.<br\/><br\/>The iTrust distribution and retrieval network aims to ensure that individuals cannot be prevented from distributing or retrieving ideas and information across the Internet. In iTrust, source nodes produce metadata that describes their information, and distribute that metadata to nodes chosen at random. Similarly, requesting nodes distribute their requests to nodes chosen at random. Nodes compare the requests with the metadata they hold. If a node finds a match, it returns the URL of the associated information to the requesting node, which then retrieves the information from the source node.<br\/><br\/>The iTrust distribution and retrieval network is the first to provide an effective fully distributed Internet search that is difficult to subvert. This project will develop infrastructure software for iTrust and a user interface that is convenient and easy-to-use. The technology and source code for iTrust will be freely available on a public Web site. The expected impact and significance of iTrust are that, even if the conventional centralized Internet search mechanisms are subverted, an alternative will exist to protect the free flow of information across the Internet.","title":"TC: Small: Trustworthy Distribution and Retrieval Network","awardID":"1016193","effectiveDate":"2010-08-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[450618,450619],"PO":["564223"]},"168675":{"abstract":"Despite a decade of research, the problem of securing the Internet's interdomain routing system is far from solved. For a long time, it seemed there was a problem of technical feasibility; research focused on designing more and more lightweight protocols, by reducing computational or communication overheads, or considering weaker security guarantees. It has now become clear that the challenge of deploying these protocols is not one of technical feasibility, but one of incentives. The Internet is a complex, distributed system that is not controlled by any one entity, but instead has thousands of players with different economic incentives. The real challenge is to create incentives for these players to band together and deploy a secure routing protocol that will significantly improve the reliability and security of the Internet.<br\/><br\/>To address this challenge, this project develops: (a) Metrics for measuring the utility of different network security protocols, (b) Algorithms that identify a target set of Autonomous Systems (ASes) in the Internet that should be initially convinced or incentivized to adopt the new security protocols, so that the rest of the Internet has the incentive to follow suit, (c) Guidelines for using secure routing protocols when they are partially deployed in the Internet, (d) Models for evaluating improvements in security as ASes gradually deploy the protocol. The approaches in this project combine domain knowledge and validation on real network data, with problem formulations inspired by emerging ideas from cryptography, game theory, and the literature on social networks.","title":"TC: Small: Deployment Incentives for Secure Internet Routing","awardID":"1017907","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["562066"],"PO":["565327"]},"167465":{"abstract":"This project addresses a key problem in advancing the state of the art in cognitive assistant systems that can interact naturally with humans in order to help them perform everyday tasks more effectively. Such a system would help not only people with cognitive disabilities but all individuals as they perform complex tasks they are unfamiliar with. The research focuses on structured activities of daily living that lend themselves to practical experimentation, such as meal preparation and other kitchen activities.<br\/><br\/>Specifically, the core focus of the research is activity recognition, i.e., systems that can identify the goals and individual actions a person is performing as they work on a task. Key innovations of this work are 1) that the activity models are learned from the user via intuitive natural demonstration, and 2) that the system is able to reason over activity models to generalize and adapt them. In contrast, current practice requires specialized training supervised by the researchers and supports no reasoning over the models. This advance is accomplished by integrating capabilities that are typically studied separately, including activity recognition, knowledge representation and reasoning, natural language understanding and machine learning. The work addresses a significant step towards the goal of building practical and flexible in-home automated assistants.","title":"RI-Large: Activity Learning and Recognition for a Cognitive Assistant","awardID":"1012017","effectiveDate":"2010-08-15","expirationDate":"2014-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550377"],"PO":["565215"]},"168444":{"abstract":"Embedded computers interacting with physical systems are increasingly common. Examples include medical devices, factory equipment, automobiles, and satellites. Testing embedded software during development is hard. Interacting with real physical systems, as with humans, may be dangerous or cost-prohibitive. Connecting the embedded computer to a physical mockup, like an electromechanical heart, is limited by the mockup's behavioral range. Simulating the entire system is inaccurate and slow, and does not test the actual embedded computer. Connecting the embedded computer to a digital mockup of the physical system, wherein the embedded computer's sensors\/actuators are bypassed and interact instead with a physical system computer model, still suffers from the modeled part being inaccurate and slow.<br\/><br\/>This project creates digital mockups that are accurate and fast by using modern field-programmable gate array (FPGA) chips. It is the first to develop automated synthesis techniques for converting numerous differential equations, forming the core of physical system models, into circuits on FPGAs. The project evaluates various differential equation solution techniques for FPGA suitability, and develops an interconnected processing element target architecture. The project supports real-time execution and time-controllable execution via lightweight kernel definition on those processing elements. It develops a system synthesis approach to explore the solution space for a given physical model and FPGA device. The project includes expansion of existing embedded systems educational material, and trains numerous graduate and undergraduate students. Ultimately, the project will catalyze use of digital mockups and hence lead to better embedded computers.","title":"CSR: Small: Collaborative Research: Synthesis of Time-Controllable Digital Mockups of Physical Systems","awardID":"1016789","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["495392"],"PO":["565255"]},"168565":{"abstract":"Over the last several years, Voice over IP (VoIP) has enjoyed a marked increase in popularity, particularly as a replacement of traditional telephony for international calls. Indeed, several large network providers already boast millions of subscribers. At the same time, the security and privacy implications of conducting everyday voice communications over the Internet are not yet well understood. For the most part, the current focus on VoIP security has centered around hardening the signaling protocol. Of late, the increased attention surrounding eavesdropping and call hijacking threats to VoIP communications has heightened the need for encrypting VoIP data before transmitting it over the Internet. Unfortunately, the current recommendations for encrypting VoIP traffic are tuned for efficiency rather than privacy.<br\/><br\/>In this project, we explore the risks that arise in VoIP under the common practice of using variable bit rate encoders (to save bandwidth) and stream ciphers (for confidentiality). Our ultimate goals are to better understand a wide spectrum of privacy threats to VoIP usage, and to explore countermeasures to hinder these breaches. We also explore the trade-offs associated with better ways of balancing privacy and efficiency, with an eye towards minimizing the impact on the perceptual quality of VoIP calls. The broader significance of the work is to provide sound designs that benefit ongoing efforts in the greater networking community to secure VoIP.","title":"TC: Small: Exploring Privacy Breaches in Encrypted VoIP Communications","awardID":"1017318","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["565326","554765"],"PO":["565264"]},"168686":{"abstract":"With technology scaling coupled with increasing power densities, modern processors suffer from potential soft errors and hard errors. The reliability analysis of such multi-threaded processors, e.g. Simultaneous Multithreading (SMT) and Chip-Multiprocessors (CMP), where inter-thread resource contention exists, is a relatively unexplored area. Furthermore, the modeling complexity is exacerbated by two additional factors: (1) increasing number of cores in a chip; and (2) heterogeneity brought by manufacturing process variation. Software wise, traditional compiler designs are aimed at providing high performance and recently low power when generating object codes. With increasing hardware vulnerabilities, however, high performance computing programs suffer from unexpected errors and exceptions, which might be mitigated by using fault-tolerance techniques such as error detections and check pointing, but still eventually hurt their performance. Apart from a reliable hardware platform, software designers can further improve system reliability by generating error resilient codes. Moreover, analysis of software's architectural vulnerability is still in an ad hoc stage. Therefore, this project proposes a predictive framework to handle the above challenges by employing modern statistical and machine learning methods. The outcomes of this project include a predictive framework which guides for reliable software and hardware optimization and its applications to high performance computing.<br\/><br\/>The broader impact plans include outreach activities and undergraduate and graduate training. The interdisciplinary nature of the proposed work allows students to learn cutting-edge knowledge from different areas to broaden their scope of training as well as to enhance their productivity. Students from the under-represented groups will be encouraged and given priorities for joining the project.","title":"SHF: Small: Exploring Statistical Models to Optimize Hardware and Software under Processor Reliability Constraints","awardID":"1017961","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["446394","520020"],"PO":["366560"]},"168334":{"abstract":"A strong case can be made that tensor computation is the ``next big thing'' in numerical analysis. High-dimensional modeling is becoming commonplace and it requires the manipulation and analysis of huge multidimensional arrays. The investigator and his colleagues will enrich the interplay between matrix computations and tensor computations by pursuing four basic directions of research. They will<br\/>(1) develop tensor approximation techniques based on matrices that have low Kronecker product rank, (2) implement a pair of basic tensor algebra subprograms, one that showcases a new contraction-level generalization of Strassen multiplication and one that demonstrates how to compute effectively contractions between tensors that have symmetry, (3) analyze the data sparse representation of huge vectors through tensor networks, and (4) develop a unifying framework for SVD-like tensor decompositions through an embedding idea that involves symmetric tensors.<br\/><br\/><br\/>A table of data is 2-dimensional and many matrix computation techniques exist for extracting information from the numbers that appear in the rows and columns. A tensor can be thought of as a table whose entries are other tables. For example, a table having 10 rows and 8 columns has 80 ``cells''. If each of those cells is itself a 5-by-4 table, then the entire data set can be thought of as a 10-by-8-by-5-by-4 tensor. Data sets of this variety are increasingly prominent in engineering and the sciences because it is the natural way to structure the information associated with a model that depends upon many factors.<br\/>The research plan is to help build an infrastructure for the scientific community that makes tensor-based computation as natural and easy as matrix-based computation. The successful problem-solving and problem-analysis tools provided by the matrix computation field will be broadened and generalized. The outreach agenda includes the production of educational materials that will help ensure the development of a tensor-savvy scientific community. <br\/>These materials include an online, ten-lecture short course on tensor computation, participation in a Visiting Lecturer program that targets 4-year colleges, and the addition of a tensor computation chapter in the upcoming fourth edition of the highly-cited textbook on matrix computations by Golub and Van Loan.","title":"Closing the Gap Between Matrix and Tensor Computation","awardID":"1016284","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[450672],"PO":["565027"]},"168576":{"abstract":"The modern computer has two processors, the \"central processing unit\"(CPU) and the \"graphics processing unit\" (GPU). Historically, the CPU has been used for general-purpose computing and the GPU for graphics, but as GPUs have become more flexible and programmable, they are increasingly used for computationally-intense general-purpose tasks. However, programming a GPU is difficult, because GPU programmers must divide their programs into many parallel parts. What makes this process easier is using common parallel building blocks developed by expert programmers.<br\/><br\/>The research group, together with colleagues from NVIDIA, is building a library of these parallel primitives. In this work, the PIs are concentrating on three major tasks: the addition of new primitives to this library; techniques for optimizing the primitives that we are including; and an extension of the library to supporting many GPUs in a system.","title":"SHF: Small: Software Fundamentals for Manycore Systems","awardID":"1017399","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["459062"],"PO":["565272"]},"168697":{"abstract":"Resonance-Based Signal Analysis: Algorithms and Applications <br\/><br\/>Ivan Selesnick<br\/><br\/>Many signals arising from physiological and physical processes are not only non-stationary but also posses a mixture of sustained oscillations and non-oscillatory transients that are difficult to disentangle by linear methods. Examples of such signals include speech, biomedical, and geophysical signals. For example, EEG signals contain rhythmic oscillations (alpha waves, etc) but they also contain transients due to measurement artifacts and non-rhythmic brain activity. This research program involves the development and application of new algorithms designed to decompose such signals into 'resonance' components - a high-resonance component being a signal consisting of multiple simultaneous sustained oscillations; a low-resonance component being a signal consisting of non-oscillatory transients of unspecified shape and duration. While frequency components are straightforwardly defined and can be obtained by linear filtering, resonance components are more difficult to define and procedures to obtain resonance components are necessarily nonlinear. <br\/> It is envisioned that the decomposition of a non-stationary multi-resonance signal into resonance components will enable the more effective utilization of existing processing methods specialized to each component. For example, sinusoidal modeling of speech is most efficient and effective for signals consisting primarily of sustained oscillations (high-resonance signals). On the other hand, time-domain and wavelet-domain methods are most effective for piecewise smooth signals that are defined primarily by their transients or singularities (low-resonance signals).<br\/> This research utilizes recent developments in signal processing, including sparse signal representations, morphological component analysis, constant-Q (wavelet) transforms with varying Q-factors, fast algorithms for L1-norm regularized linear inverse problems, and related algorithms. The research consists of developing algorithms for resonance-based signal decomposition and generalizations, and assessing their effectiveness for the processing of signals arising from several physical and physiological processes.","title":"CIF: Small: Resonance-Based Signal Analysis: Algorithms and Applications","awardID":"1018020","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[451533],"PO":["564898"]},"177057":{"abstract":"Current medical robotics systems use non-intelligent surgical manipulators that place the entire burden on surgeons for safeguarding against damage to the anatomy. The emergence of new surgical paradigms, such as Natural Orifice Endoscopic Trans-luminal Surgery (NOTES), requires surgical robots that are capable of supporting safe interaction with the anatomy while accessing deep surgical sites through often long natural access pathways. This requires new types of robots capable of safeguarding against damage to the anatomy by acting as intelligent intervention and information gathering tools for assisting surgeons during increasingly complex procedures.<br\/><br\/>The objective of this research is to provide the theoretical foundation for modeling and control of flexible robots for intelligent and safe interaction with the anatomy. Intelligence refers to the ability of these robots to gauge their force interaction with the anatomy, gather information about the anatomy, and act based on this information. Screw theory and stochastic estimation methods are used for modeling the ability of these robots to estimate their wrench interaction with the anatomy by using intrinsic and extrinsic sources of information. These performance measures are used in hybrid force control algorithms that allow characterizing shape, stiffness, and anatomical constraints governing safe maneuvering of suspended organs.<br\/><br\/>The outcomes of this research will allow the development of radically new technologies for newly emerging surgical paradigms (e.g. NOTES). <br\/>This research will also advance the field robotics by addressing control and resolution of multi-point contact problems along flexible robots for compliant insertion control and bracing against soft environments.","title":"CAREER: Intelligent Flexible Robots for Safe Manipulation of Anatomy","awardID":"1063750","effectiveDate":"2010-08-15","expirationDate":"2014-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["553211"],"PO":["565136"]},"168235":{"abstract":"Over the past decade, the World-Wide Web has developed a unique ecosystem of third-party services, including advertisements, active content for web analytics, and tightly integrated application programming interfaces. These third-party tools collect large amounts of information, the scale and implications of which are largely unknown to the public and difficult to gauge even for even experts. The implication of users' web-surfing on their privacy has not previously been studied from the most natural perspective, namely that of the users themselves. This research aims to close this gap by focusing on users' actual web behavior, any existing countermeasures they employ to safeguard their privacy, the amount of information leaked, and the implications this has for the profiling capabilities of third-party services.<br\/><br\/>To measure information leakage, real-world web-surfing traffic will be examined through an anonymized picture of each discernible user, including browser details, any countermeasures taken to safeguard his or her privacy, and the cookies, referrals, and web visits as observed by the third parties involved. Next, profiling strategies will be examined that reflect different vantage points and intent, mirroring the capabilities of third parties with access to information about the users' web surfing. Finally, a browser extension will be developed that not only informs users about the parties their current web-surfing is providing information to, but also actively minimizes privacy leaks via fine grained policy enforcement that allows third-party services to function correctly.","title":"TC: Small: Understanding and Taming the Web's Privacy Footprint","awardID":"1015835","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["525668","534318"],"PO":["565264"]},"168378":{"abstract":"Randomization in the context of linear-algebraic algorithms is an exciting and innovative idea. In recent years, a large body of work has focused on provably accurate randomized algorithms for regression problems, with a particular emphasis on least-squares regression. Fast algorithms for such problems are of continuous interest due to their broad applicability in scientific computing and statistical data analysis, where increasingly larger input matrices appear. The PI seeks to theoretically and numerically investigate provably accurate and practically useful randomized algorithms for such problems when (i) the constraint matrix of the regression problem is Laplacian, or (ii) the regression problem is under- or over-constrained and sparse. Thus, the PI seeks to address the alarming gap between recent breakthrough theoretical results of Spielman, Teng, and collaborators and their practical applicability, as well as the lack of efficient algorithms dealing with over- or under-constrained regression problems with sparse input matrices. In order to bridge the gap between theory and applications in this line of research, a number of novel theoretical results are necessary and will be investigated. The practical usefulness of the proposed research will be numerically evaluated using data matrices from scientific applications.<br\/><br\/><br\/>Efficiently solving large systems of linear equations is perhaps the most fundamental question in numerical analysis and linear algebra, mainly because such systems are ubiquitous in scientific computing applications. The proposed work seeks to bring the theoretical breakthroughs of the recent work of Spielman, Teng, and collaborators on solving systems of linear equations with Laplacian input matrices closer to practice. Towards that end, both theoretical as well as numerical results will be derived. This research paradigm can subsequently be used as a starting point in order to spark further research efforts on broader classes of massive systems of linear equations. A second aspect of the impact of the proposed work has to do with the considerable overlap between Theoretical Computer Science and Numerical Linear Algebra approaches that will be explored. As randomization becomes increasingly useful in the context of linear algebra, the PI expects that the next generation of researchers in this domain will need solid training in both areas, which is exactly what the proposed work will provide to graduate students. Finally, a third aspect of the impact of the proposed work will emerge from the dissemination of our results via workshops, tutorials, and mini-symposia in high-profile relevant conferences.","title":"AF: Small: Fast and Efficient Randomized Algorithms for Solving Laplacian Systems of Linear Equations and Sparse Least Squares Problems","awardID":"1016501","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["550329","489824"],"PO":["551712"]},"168499":{"abstract":"Traditional robotics research has adopted a sense-plan-act paradigm in which it is assumed that the sensors are capable of providing enough information in order to decide the next course of action. Humans and animals, however, frequently adopt a different approach, such as shuffling through a pile of unknown objects in order to identify an item of interest hidden beneath the pile. This project explores the concept of interactive perception (or manipulated-guided sensing), in which successive manipulations of objects in an environment are used to increase vision-based understanding of that environment, and vice versa. In particular, the project involves developing appropriate low-order models of highly non-rigid structures such as fabrics and textiles; constructing algorithms to perform real-time vision-based sensing of such objects in cluttered, unstructured environments; and building prototype robotic hardware for testing the resulting models and algorithms. The research forms an integral part of next-generation household service robots performing everyday tasks such as sorting and folding laundry.","title":"RI: Small: Interactive Perception for Manipulating Non-Rigid Objects","awardID":"1017007","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["563342","563342","543525"],"PO":["564316"]},"161690":{"abstract":"In recent years, wireless sensor networks have emerged as one key technology with the advantages of being low-cost, low-profile, and easy to deploy. These attractive advantages, however, imply that resources available to individual nodes are severely limited. In particular, it is highly likely that energy will continue to be a system bottleneck, as more transistors indicate more power consumption. On the other hand, there is a growing need for long-term, sustainable deployments of sensors to reduce operational costs and ensure service continuity. This research presents an operating system based approach and its underlying foundations for this problem, by providing energy virtualization in long-term unattended sensor network applications. The foundation of this work is the LiteOS operating system: a Unix-like operating system for wireless sensor networks. The centerpiece of this project is operating system based energy virtualization, which represents a comprehensive framework for accounting, reservation, and isolation of energy resource for applications sharing the same platform. The deliveries of this project are (i) an improved, highly reliable LiteOS operating system with built-in support for energy virtualization, (ii) architecture principles, design methodologies, programming APIs, and adaptive communication protocols for software development based on energy virtualization, (iii) results from theoretical and algorithmic investigations of energy virtualization, and (iv) educational testbeds, curriculum and laboratory designs for undergraduate and graduate courses. Our findings will have pervasive impact on applications that are in need of long-term and remote sensing, such as infrastructure protection, assisted living, smart buildings, factory monitoring, and numerous military applications.","title":"CAREER: Operating System based Energy Virtualization for Sensor Networks","awardID":"0953238","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["527046"],"PO":["565303"]},"168830":{"abstract":"Developing complex software systems that play critical roles in organizations and societies is currently labour intensive and costly. There is a need for research on the means to significantly reduce the cost and effort of developing complex software systems. Model driven engineering (MDE) research is primarily concerned with reducing software development costs through the use of technologies that support rigorous analysis of software models and automated transformation of verified models to dependable implementations. Complexity is tackled through (1) the use of models that describe complex systems at multiple levels of abstraction and from a variety of perspectives, and (2) automated support for transforming and analyzing models. Software developers use multi-modeling notations such as the Unified Modeling Language (UML) to manage design complexity. A challenge in using multi-models is ensuring that structural and behavioral properties are consistently described across the different models. The highly iterative nature of software design makes manual consistency checking of multi-models tedious and error prone. Furthermore, iterative, incremental development of models requires support for analyzing incomplete models. While heavyweight formal analysis techniques are useful for analyzing detailed, complete design models of highly-critical systems, lightweight analysis techniques that allow developers to do ``just enough'' analysis with available information are needed to support iterative, incremental software development. The primary aim of this project is to develop a lightweight consistency analysis method that supports iterative, incremental development of software design multi-models.<br\/><br\/>The research aims to produce an analysis method that provides modelers with meaningful feedback on the consistency of UML design multi-models as they evolve in an iterative and incremental process. The scenario-based UML design analysis method that will be developed extends the applicability of existing UML static analysis tools such as USE and OCLE to the analysis of behavior. The approach will allow developers to automatically check a design multi-model against a set of scenarios describing desirable and undesirable behaviors. What will make this analysis method different from other rigorous analysis methods is its tolerance for incompleteness. Specifically, the research aims to produce a method that will provide useful feedback on consistency when only partial descriptions of behavior are provided in multi-models. In addition to the above, the static analysis method will be integrated with a dynamic analysis tool called UMLAnT that allows developers to animate scenarios captured by UML class models. The support for both static and dynamic model analysis and the ability to analyze incomplete models can significantly reduce the cost and effort currently associated with producing dependable implementations from models in iterative, incremental design processes.","title":"SHF: Small: Scenario-Based Validation of Design Models","awardID":"1018711","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["557996"],"PO":["564388"]},"173065":{"abstract":"This project explores the implications of embodied cognition for learning and teaching at the college level. College classes and college classrooms are generally designed for the purely \"abstract\" thinker: a thinker who does not benefit from tangible, multi-sensory approaches to learning that are part of early childhood education but considered unnecessary and inefficient for adult learners. The term \"educational technology\" generally refers to desktop or laptop computers with standard GUI interfaces, and to data projectors, networks, and other familiar \"smart classroom\" technology. But a variety of developments in computing, computing education, and education research point the way to making \"smart students\" more effectively by reframing educational technology. Tangible User Interfaces offer the opportunity to combine computational power with the cognitive power of manipulatives; CS1 education with manipulatives has been promoted by some faculty; and research on learning in higher education shows that multi-sensory learning is far more effective than lecture.<br\/><br\/>The intellectual merit of the project stems from its grounding in theory and evidence, and from the importance of its challenge to prevailing wisdom. In addition to drawing on the academic literature, the project analyzes field data from students and professionals, and conducts a laboratory study of a key proposition. The broader impacts of this work can appear in a rethinking of higher education, educational technology, and adult learning environments, making them more engaging and effective.","title":"EAGER: Thinking With Things: Remaking Learning in Higher Education and Beyond","awardID":"1042580","effectiveDate":"2010-08-15","expirationDate":"2013-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[464191],"PO":["562669"]},"173076":{"abstract":"This is a multi-method empirical study of user death in Social Networking Sites (SNS), such as Facebook and MySpace. The mass adoption of SNS includes the growing presence of individuals who are no longer alive. However, the physical death of a user does not result in the elimination of his or her account nor the profile's place inside a network of digital peers. Indeed, friends' use of a user's profile postmortem to say last goodbyes, share memories, and coordinate funereal arrangements is well known, if not frequently discussed. Through a focus on collaborative representations embodied through data across time, this exploratory research will reveal ways to broaden our current technical architectures for access, authorization, and privacy, and generate empirically grounded design principles for the Human-Centered Computing community. This research effort has the potential to open new areas of exploration and transform our thinking about SNS and other online services. This work is in its very early stages, and likely to develop into a more developed research area in the future.<br\/><br\/>Focusing on death highlights three important themes for social networks and representation: embodiment, representation, and temporality. Embodiment concerns how data objects and digital representations \"stand for\" human bodies. It encapsulates issues of access, issues of ownership, issues of management, issues of presence, issues of personhood, and issues of participatory status, both at the technical level and at the social. Representation invokes the traditional considerations of online identity, the presentation of self, and the crafting of acceptable personas as well as consideration of the ways in which records are created with specific purposes and representations in mind. Representation relates to embodiment in that it speaks to the relationship that holds between the data object and the human body, but it incorporates too the active, purposive, strategic practices of representing with particular ends in mind. Temporality concerns the notion of \"lifecycles\" as it has been applied in system development - the circumstances under which digital systems come into being, are put to use, and are taken out of service. The life of a user and the life of that user?s data are frequently not the same, an issue particularly acute when considering the continuation of dead user profiles in SNS.<br\/><br\/>This work will advance theoretical understandings of death in light of new technological paradigms. This project is the first research effort to focus on the technological infrastructure of SNS - and how it shapes, enables, and at times interferes with - persistence of identities after death. Issues of preparing for, experiencing, and mourning death are fundamental to the human experience. The results of this work have the potential to influence the policy decisions of services like SNS that have the potential to inflict substantial emotional trauma on users if such sensitive issues as death are mistreated as well as the technical architectures underlying identity management for a host of systems.","title":"EAGER HCC: The Persistence of Digital Identity","awardID":"1042678","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[464219,"519772"],"PO":["564456"]},"168500":{"abstract":"Concurrent programs are widely used, in real-world applications with safety-critical requirements, so it is vital to ensure their correctness. Such programs are difficult to get right and hard to analyze, because of the huge number of ways in which concurrent threads may interact dynamically. We need to guarantee that program behavior is free of race conditions, such as concurrent attempts to update the same piece of state, since racy programs may behave erratically. Further, programs that operate on mutable data structures are prone to safety faults, such as attempts to access a previously deallocated pointer, and this is a leading cause of crashes in operating system code.<br\/><br\/>This project addresses these concerns by building a theory of concurrency based on resource separation principles. <br\/>This theory will offer resource-sensitive logics for program correctness, with solid semantic foundations.<br\/>The project will significantly expand the scope of the author's work on concurrent separation logic, to encompass <br\/>a wider range of program properties and concurrency paradigms, and combine concurrency with procedures. <br\/>The project will introduce semantic models and logics for networks of communicating processes, based on a<br\/>principle of channel separation. The intellectual merits of this proposal include the development of a unifying framework of semantic models and methodologies, with rigorous mathematical and logical underpinnings, embodying practically useful principles. In the broader setting this project aims to improve the state-of-the-art in programming methodology, facilitate the writing of reliable concurrent code, and enable formal reasoning about a wider range of problems. The project will <br\/>contribute to general understanding, by informing the design of new logics, and the discovery of proof techniques, that cross paradigm barriers. The project will foster the development of improved semantically-based analysis tools for concurrent programs, to be made available for widespread use and experimentation, and to be used for real-world safety-critical applications in which concurrency is both a feature and a problem.","title":"SHF: Small: Separation Principles for Concurrent Programs: Semantics, Logics, and Methodology","awardID":"1017011","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[451067],"PO":["565264"]},"167653":{"abstract":"The aim of this project is to advance the understanding of various aspects of probability in relation to algorithm design and analysis. In general this means the study of randomized algorithms, the average case performance of algorithms and related, seemingly random, structures such as social networks. Randomized algorithms are important because they are often the most efficient and some times the only efficient way to solve computational problems. The study of the average case sheds light on why problems which in the worst-case seem computationally difficult or even intractable, can be routinely solved in practise, by simple algorithms. The research on random models of large real world networks is important for answering algorithmic questions about them and for understanding their evolution.<br\/><br\/>The project will study several important problems from the point of view of average case analysis: (i) Cuckoo Hashing is a relatively new hashing algorithm and some of the basic questions about its performance remain unanswered, even though there has been significant progress of late. (ii) The matching problem for graphs is the quintissential polynomial time solvable problem in Combinatorial Optimiztion. Its polynomial time solution is one of the great achievements of the area. Its worst-case complexity, while polynomial still leaves room for improvement and one of the aims of the project is to settle the average case completely. (iii) The hamilton cycle problem for graphs is one of the canonical NP-hard problems. The average-case complexity was reduced to polynomial time some time ago and one of the aims of the project is to reduce this to as close to expected linear time as possible. The project will also several other problems involving average case complexity. The methodology employed will involve the tools and techniques from the field of Random Graphs. The two main tools being concentration of measure and concentration on events that happen with probability close to one.<br\/><br\/>The project will also consider the use of Rapidly Mixing Markov Chains to generate random colorings of graphs and hypergraphs. This topic has close ties to Statistical Physics and has benefited a great deal from the cross-fertilization of ideas. There are still many gaps, particularly in the case of hypergraphs, and the project aims to close them.<br\/><br\/>While graph theory is at least a hundred years old, it is only in recent years that the ubiquitousness of graphs or networks has been so widely recognized. The study of Random Graphs is about fifty years old and techniques from this area are needed to study real world networks. Simply because they evolve in a seemingly random manner. The project will involve several analyses from this area. For example, it is not known what is the component structure of a random graph, evolving under preferential attachment but subject to deletions.","title":"AF: Small: Probabilistic Considerations in the Analysis of Algorithms","awardID":"1013110","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["481434"],"PO":["565251"]},"165486":{"abstract":"The research objective of this CreativeIT award is to study the major blocks to creativity that arise from the manner in which design problems are formulated. It will investigate why some engineering designers generate creative solutions while others converge on mundane ones, specifically examining the critical role played by formulation and representation in early conceptual design. A central idea of this research is that one can usefully cast problem formulation as a process of answering questions that reveal key characteristics of the design task. These include questions about design goals and objectives, relative priority of technical issues, relations among design parameters, conflicts and gaps in the problem statement, and fictitious constraints. This investigation will result in a dynamic representation, the Problem Map, which aims to characterize a designer's understanding of a problem at a given time. An interactive computational aid that lets users construct a Problem Map for a given design problem will be developed. The project differs from earlier efforts in that its focus is on problem formulation (that is pre-ideation), not just idea generation, and its emphasis on an interactive system that keeps humans in the loop rather than replacing them with automated systems. <br\/><br\/>If successful, this research will advance understanding of creativity in engineering design, and should also generalize to other design tasks in science and engineering. Problem definition is perhaps the most critical part of design, since it determines the boundaries and topology of the search space and hence determines the originality and quality of designs that can be generated. Hence, this research is critical and timely, with potentially broad impacts. The interactive system should also serve as a useful educational tool that increases creativity in novices. The tool will demonstrate the power of Information Technology is aiding human creativity. The tool will be used in design courses at Arizona State, and will be distributed widely to other universities over the Internet. If successful, this system and the principles that underlie it will change the way that apprentice designers approach their tasks and, in the longer term, lead to more creative engineering practices. Thus, our work will also lead to innovative educational approaches in engineering design that reward creativity. The results will be disseminated through normal academic channels that involve publishing papers and presenting talks at conferences.","title":"Major: Understanding and Aiding Problem Formulation in Creative Conceptual Design","awardID":"1002910","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1464","name":"ENGINEERING DESIGN AND INNOVAT"}}],"PIcoPI":["502431",443311,"559662"],"PO":["564651"]},"174077":{"abstract":"This project focuses on developing the infrastructure for a self-sustaining organization that can manage, grow, and evangelize olympiads that involve young students (middle and junior high) in computational thinking. A large component is the creation of pilot olympiads in a select few cities in the United States. Specific goals of this project include: (1) identifying a set of foundational skills that underlie computational thinking that can be taught before college and high school; (2) identifying a style of problems and scenarios that engage a wide variety of students; and (3) implementing a curriculum of training sessions and contest questions that exemplify those foundational skills. <br\/><br\/>There are two broad reasons for creating a Computational Thinking Olympiad. First, to expose the fundamentals of computational thinking to a broad audience of potential researchers and practitioners in the field, thus increasing participation and diversity in computing. Second, to ensure long-lasting impact beyond of this project. <br\/><br\/>The success of the Computational Thinking Olympiad will have a significant impact on our society by introducing middle school students to computational thinking in its breadth and depth: (1) encouraging students to have fun with the computational thinking in an arena that is both cooperative and competitive; (2) encouraging students to pursue education in computing; (3) introducing the unplugged parts of computing to those who have not had access to the plugged-in parts; and (4) showing that computational thinking is not ``just'' programming.","title":"Collaborative Research: EAGER: Computational Thinking Olympiad","awardID":"1048437","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[466946],"PO":["565215"]},"168522":{"abstract":"Data centers have become the main networking infrastructure to<br\/>support online services, ranging from end-user applications (e.g.,<br\/>Web search and IM) to distributed system operations (e.g., GFS and<br\/>MapReduce). In this project, we explore a different design paradigm<br\/>to data center networks (DCNs). We take a new server-centric<br\/>solution approach, in contrast to the switch-centered paradigm. We<br\/>push intelligence into servers, which generally have more open and<br\/>standardized hardware architecture and software platforms. Multiple<br\/>layers of low-cost, commodity off-the-shelf, mini-switches are used<br\/>to connect servers. We leverage the large number of servers in a<br\/>data center. Even when each server just adds one more link to the<br\/>rest of servers, we obtain many links in the DCN system given the<br\/>large server population. These added links, without using high-end<br\/>switches but using only mini-switches, provide the foundation for<br\/>enhancing scalability, inter-server capacity, and fault tolerance.<br\/>Taking the approach, we can design DCNs with high scalability,<br\/>enhanced fault-tolerance, and high inter-server capacity, and better<br\/>support data center applications that exhibit various traffic<br\/>patterns of one-to-one, one-to-several, one-to-all, and all-to-all<br\/>communications. Our design also opens more space for stimulating<br\/>innovations in the DCN software through the more open programming<br\/>platform on servers, while abandoning expensive, proprietary core<br\/>switches. The results in scalable and modular data centers and<br\/>associated management tools are expected to show that, the added<br\/>complexity into servers will incur only minor systems overhead but<br\/>produce large performance gains.","title":"CSR: Small: A Server-Centric Approach to Data Center Networks","awardID":"1017127","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[451118],"PO":["565255"]},"168412":{"abstract":"In real-time voice and high-speed data applications, limited delay is a key design constraint; indeed, packet sizes as short as a few hundred bits are common in wireless systems. The objective of this research is to go beyond traditional refinements to the fundamental asymptotic information theoretic limits and investigate the back-off from capacity (in channel coding) and the overhead over entropy (in lossless compression) and the rate-distortion function (in lossy source coding) incurred by coding at a given blocklength. We plan to revisit the major design principles stemming from the analysis of capacity, rate-distortion function and minimum source coding rate and see which of them still apply in the non-asymptotic regime, and for those that do not, assess the penalty incurred by abiding by them for short blocklengths. <br\/><br\/>Our study of the non-asymptotic behavior of the optimum rate achievable as a function of both blocklength and error probability involves two complementary goals:<br\/>a) computable upper and lower bounds tight enough to reduce the uncertainty on the non-asymptotic operational fundamental limit to a level that is negligible compared to the gap to the long-blocklength asymptotics; b) analytical approximations to the bounds that are accurate even for short blocklengths, so as to offer insights into good coding strategies and enable practically relevant optimization problems. Those approximations typically involve a parameter we refer to as dispersion, which quantifies the stochastic variability of sources and channels.","title":"CIF: Small: Non-Asymptotic Information Theory","awardID":"1016625","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["550344"],"PO":["564924"]},"168533":{"abstract":"While storage capacity and CPU processing power have experienced rapid growth in the past, improvement in data bandwidth and access times of hard disk drives (HDD) has not kept pace. As a result, the speed gap between CPUs and disk I\/O is widening. Disk arrays can improve overall I\/O throughput but random access latency is still very large because of mechanical operations involved. Large buffers and deep cache hierarchy can improve latency, but the access time reduction has been very limited so far because of poor data locality at the disk storage level.<br\/><br\/>This proposal aims at rethinking the fundamental architecture of storage systems and makes an attempt at a paradigm shift of disk based storage architectures. The approach is to build a new storage architecture that exploits the two emerging semi-conductor technologies: flash memory SSD (solid state disks) and GPU (graphic processing unit). The new disk I\/O architecture is referred to as I-CASH: Intelligently Coupled Array of SSDs and HDDs. The SSD is used to store mostly read \"reference data blocks\" to make best use of its high-speed random read performance. The HDD is used to store compressed delta between a current I\/O block and its corresponding reference block in the SSD so that random writes are not performed on SSD during online I\/O operations. The SSD and HDD are controlled by a high speed GPU that performs similarity detection, delta derivations, combining delta with reference blocks, and other necessary functions for interfacing the storage to the host OS. The idea is to leverage fast read performance of SSDs and the high speed computation of GPUs to replace and substitute, to a great extent, the mechanical operations of HDD to achieve I\/O performance that is orders of magnitude better than traditional disk storage systems. Instead of working on HDD to catch up with processors' performance, which has been proven difficult if not impossible, the proposed approach lets storage systems ride the wave of the rapid advancement of multicore processors and be part of such success by trading high speed computation for low access latency. <br\/><br\/>It is anticipated that the proposed project will have significantly broad and transformative impact. 1) Servers at data centers run tens and hundreds of virtual machines that generate large amount of I\/Os that can take full advantage of our new storage architecture with potentially orders of magnitude performance improvement. 2) The research will engage both graduate and undergraduate students so that they are ready for the real world need. 3) The success of this research will help the economic development of the state of Rhode Island and the nation.","title":"Introducing I-CASH, A New Disk IO Architecture","awardID":"1017177","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["414327"],"PO":["366560"]},"168654":{"abstract":"The fastest growing areas of IT application development are for mobile devices and browser-based applications. Smartphones are becoming more capable and widespread, which attracts both users and application developers. Browser-based applications are replacing traditional native applications. <br\/><br\/>This project investigates new programming abstractions, libraries, and tools that can increase programmer and maintainer productivity for mobile and browser-based applications. Among the programming concerns that distinguish these applications from traditional applications, the project focusses on application-level checkpointing and replay. This feature enables the application to save a compact description of its state for restoring at a future time. This is one aspect that is required for usability of mobile and browser-based applications, yet is error-prone, lacks programming support and, if solved properly, can serve as foundation for powerful testing and debugging tools.<br\/><br\/>This project addresses the checkpointing and replay problem from a language-level and application-level perspective. A system of program annotations will be developed to assist programmers in developing the checkpointing and replay aspects of their programs. The annotations can be used as a basis for synthesis of the checkpointing code, and also as an input for static and dynamic analysis tools that can ensure the correctness of the checkpointing and replay aspects. <br\/><br\/>This project also explores ways in which an effective checkpointing and replay mechanism for mobile and web-based applications can improve the software maintenance lifecycle, through automated test generation and improved debugging support of deployed software.","title":"SHF: Small: Programming Support for Checkpointing and Replay","awardID":"1017810","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["451874"],"PO":["565264"]},"168544":{"abstract":"Many of the functions of interest in mathematics and physics are defined by difference or differential equations. Understanding their algebraic properties is key to using these functions to describe physical and mathematical phenomena. The investigator will develop algorithms that will reveal these properties. In particular, he will develop algorithms to determine the algebraic and differential relations that occur among solutions of a given set of linear difference equations. In addition, he will develop theory and give algorithms to measure the algebraic behavior of solutions of parameterized linear differential equations as one varies the parameter. Finally, he proposes to attack the problem of factoring underdetermined systems of partial differential equations.<br\/><br\/>Although disparate in appearance, these problems will be attacked using techniques based on studying the underlying symmetries of the defining equations. In the past, the investigator has contributed to the development of theory and algorithms to solve differential and difference equations, in particular to the Galois theories of these equations and algorithms to solve them in closed form. The present project in part refines and extends this work but goes beyond to attack the broader problems mentioned above.<br\/><br\/>This research addresses foundational and computational issues concerning the algebraic behavior of systems of linear difference and differential equations. It allows researchers in many scientific fields to understand aspects of the qualitative behavior of solutions of these equations. The researcher will develop algorithms that that will be useful to number theorists, combinatorists and analysts. In addition, the investigator anticipates that these algorithms will form the foundation on which Maple and Mathematica code are based and so have an impact on the education and day-to-day work of engineers and other scientists.<br\/><br\/>Key components of this project are the development of human resources, the fostering of interactions with other scientific fields and the advancement of international collaborations. The investigator will continue to not only train his Ph.D. students but to further develop with his colleagues a program at NC State University to train students in a broad range of topics in Symbolic Computation. He will sponsor a postdoctoral scholar, involving this scholar in the research proposed here as well as develop the scholar's teaching skills and integrate the scholar into the scientific community. He will continue and expand his work on revising the undergraduate Abstract Algebra curriculum to include Symbolic Computation as a core topic. In addition he will continue to organize workshops aimed at students and colleagues in diverse fields to disseminate to a broad scientific community the ideas of Symbolic Computation in general and Symbolic Analysis in particular. He will continue his recent collaborations with researchers in Germany, France and China and will involve graduate students from NC State University in these projects, allowing them to integrate themselves in the international research community.","title":"AF: Small: Symbolic Computation and Difference and Differential Equations","awardID":"1017217","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["520532"],"PO":["565251"]},"168434":{"abstract":"Spatial data collections with an incomplete coverage yield regions<br\/>with holes and separations that often cannot be filled by<br\/>interpolation. Geosensor networks typically generate such<br\/>configurations, and with the proliferation of sensor colonies,<br\/>there is now an urgent need to provide users with better information<br\/>technologies of cognitively plausible methods to search for or<br\/>compare available spatial data sets that may be incomplete. The<br\/>objective of the investigations is to advance knowledge about<br\/>qualitative spatial relations for spatial regions with holes<br\/>and\/or separations.<br\/><br\/>The core activity is the study of the interplay between topological<br\/>spatial relations with holed regions and topological spatial relations<br\/>with separated regions to address the potentially complex<br\/>configurations that feature both holes and separations. Three<br\/>characteristics of such a set of topological relations are addressed:<br\/>the formalization of a sound set of relations at a granularity that<br\/>allows for the distinction of the salient features of holed and<br\/>separated regions, while offering the opportunity to generalize to<br\/>coarser relations in a meaningful and consistent way; the relaxation of<br\/>such relations so that the determination of the most similar relations<br\/>follows immediately from the applied methodology; and the qualitative<br\/>inference of new information from the composition of such relations to<br\/>identify inconsistencies and to drawn information that is not<br\/>immediately available from individual relations.<br\/><br\/>The hypothesis is that combining the relation formalization with sound<br\/>similarity and composition reasoning yields critical insights for a<br\/>sufficiently expressive, common approach to modeling topological<br\/>relations for holed regions and regions with separations. The resulting<br\/>theory of topological spatial relations highlights a parallelism<br\/>between relations with holed regions and regions with separations,<br\/>which is most apparent when these regions are embedded on the<br\/>surface of the sphere, while some parts of these regularities are often<br\/>hidden in the usual planar embedding.<br\/><br\/>Since topological relations are qualitative spatial descriptions, they<br\/>come close to people's own reasoning, so that a better understanding of<br\/>the relations for compound spatial objects will have ramifications for<br\/>qualitative spatial reasoning, without a need for drawing graphical<br\/>depictions to make inferences. It also lays the foundation for<br\/>linguistic constructs to communicate in natural language spatial<br\/>configurations, ultimately leading to talking maps. An immediate impact<br\/>of this theory of topological relations between holed and separated<br\/>regions is on the querying and reasoning about dataset that are<br\/>gathered by geosensor networks.<br\/><br\/>For further information see the project web page:<br\/>URL: http:\/\/www.spatial.maine.edu\/~max\/holesAndParts.html","title":"III: Small: A Theory of Topological Relations for Compound Spatial Objects","awardID":"1016740","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[450918],"PO":["563751"]},"168555":{"abstract":"Software systems are poised to keep growing in complexity and permeate deeper into the critical infrastructures of society. The complexity of these systems is exceeding the limits of existing modularization mechanisms and reliability requirements are becoming stringent. Development of new separation of concerns (SoC) techniques is thus vital to make software more reliable and maintainable. Implicit invocation (II) and aspect-oriented (AO) programming languages provide related but distinct mechanisms for separation of concerns. The proposed work encompasses fundamental and practical efforts to improve modularization and reasoning mechanisms for II and AO languages, which is a long standing challenge for both kinds of languages. Addressing these challenges has the potential to significantly improve the quality of software by easing the adoption of new separation of concerns techniques.<br\/><br\/>The project will proceed using the experimental language, Ptolemy, which blends both II and AO ideas. Ptolemy has explicitly announced events, which are defined in interfaces called \"event types\". Event types help separate concerns and decouple advice from the code it advises. Event type declarations also offer a place to specify advice. The explicit announcement of events allows the possibility of careful reasoning about correctness of Ptolemy programs, since it is possible to reason about parts of the program where there are no events in a conventional manner. The project aims to investigate reasoning by developing a formal specification language and verification technique. The approach is based on the idea of greybox (\"model program'') specifications, as found in JML and the refinement calculus. There are known techniques for reasoning about uses of abstractions that have model program specifications, and the project will apply these to Ptolemy. The intellectual merit is in the treatment of expressions in Ptolemy that announce events and those that cause an advice to proceed. A straightforward adaptation of existing reasoning techniques to these cases appears to require a whole program analysis, which is generally not desirable for modular and scalable verification. The project also aims to investigate the utility and effectiveness of Ptolemy and its specification system. A software evolution analysis will be conducted to study the ability of competing aspect-oriented, implicit invocation, and Ptolemy implementations of open source projects to withstand change. Showing Ptolemy's benefits over II and AO languages will help software designers in deciding on advanced mechanisms for separation of concerns.","title":"SHF: Small: Collaborative Research: Balancing Expressiveness and Modular Reasoning for Aspect-Oriented Programming","awardID":"1017262","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["521544"],"PO":["564588"]},"168676":{"abstract":"Internet fraud costs consumers and businesses billions of dollars each year. Through creative combinations of spam and social engineering, attackers regularly lure end users into visiting phishing sites, malware-hosting sites, and scam sites. One popular defense mechanism against Web-based attacks is blacklisting, but today's blacklists suffer from three fundamental deciencies. First, most of them employ a combination of Web crawling and human intervention to infer malicious sites. This adds an inherent delay in adding entries and causes many malicious sites to be missed. Second, blacklists are mostly based on exact URL strings, and hence unable to adapt to simple changes to the URLs that attackers are using today to evade detection. Third, as blacklist entries grow, matching them against URLs in real-time could create performance bottlenecks. To overcome these deficiencies, this project is developing novel mechanisms to aid in the construction, maintenance, and matching of blacklists in real time. Specifically, it is developing a scalable architecture that can discover new malicious websites by passively observing the onset of new techniques exploited by the miscreants, such as redirects and fast flux in network traffic. The architecture also leverages common attacker tendencies to find novel and automated ways of discovering new malicious URLs from existing blacklisted URLs. The final thrust of the project is on developing high-speed approximate matching algorithms for effective in-network blacklisting to match URLs embedded in packets against potentially millions of blacklist entries. If successful, this project will make the Web safer for millions of Internet users.","title":"TC: Small: Collaborative Research: Predictive Blacklisting for Detecting Phishing Attacks","awardID":"1017915","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["518558"],"PO":["565136"]},"168445":{"abstract":"The vast majority of commercial embedded systems are designed with simulation-based tools such as MathWork's Simulink and Stateflow. While simulations are computationally efficient, they are not complete---they cannot be naively used to design systems with provable guarantees. This project aims to build tools and techniques to verify such models. To this end, the project overcomes two key technical hurdles. First, it has been well known that Simulink-Stateflow (SLSF) models do not have any well-defined meaning. The mathematical description of a building block can be different from the simulated behavior that is generated numerically. This problem is addressed by defining semantics of SLSF models in terms of (possibly probabilistic) hybrid automata. Secondly, the class of Simulink models (translated to hybrid automata) that can be verified automatically by currently available techniques is rather restrictive. This second problem is addressed in this project by abstracting SLSF models into hybrid automata with simple dynamics, model checking the abstract models, and then refining the abstractions based on counterexamples generated by the model checker. Such a counterexample guided abstraction refinement framework provides semi-decision procedures to automatically analyze Simulink-Stateflow models. The developed software tools developed in this project translate SLSF models into probabilistic hybrid automata, analyze the formal automata model by abstracting, model checking, and refining, and then translate valid counterexamples back into Simulink to provide the user diagnostic information. Furthermore, the project builds a repository of benchmark SLSF models and their corresponding hybrid automaton models, based on examples from existing hybrid systems literature and drawing on industrial applications. The repository will be publicly disseminated and will be used to evaluate our tool. <br\/><br\/>A new course will be developed on the verification of hybrid systems that introduces undergraduate and graduate students in engineering at Illinois to the use of formal methods in embedded system design. Successful completion of the research tasks outlined here is likely to more broadly influence the design and verification of probabilistic hybrid systems that arise in application domains such as autonomous vehicles and mixed analog-digital circuits.","title":"CSR: Small: Verifying Simulink-Stateflow models","awardID":"1016791","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["553664","469971"],"PO":["565255"]},"168566":{"abstract":"The amount of data generated during the development of today?s software systems is staggering. It includes the source code, developer e-mails, bug information, testing results, analysis data, process information, requirements, etc. The size and complexity of this information make it impossible for developers to reason about it.<br\/><br\/> Data mining techniques are a common solution to extract what is relevant to developers and managers. The success and quality of these software projects depends on the software engineers? ability to customize generic data mining algorithms to specific software engineering data. This project will produce tools and techniques that will allow software developers and managers to easily customize and apply data mining techniques to a variety of software engineering problems. Such solution will become more practical and will help many existing approaches to migrate from the research lab into industry.<br\/>Under represented categories of students will participate in this research. The project will enhance the existing software engineering curriculum and facilitate the inclusion of data mining solution in the repertoire of future software engineering practitioners and researchers.<br\/><br\/>Specifically, the project will improve the state of the art solution to three important software engineering tasks: concept location in software, software defect prediction, and development effort estimation. The project will produce an algorithm customization methodology and a framework that will be instantiated for a variety of combinations of data mining algorithm x software engineering task x software system data. The customization problem is framed and addressed as an optimization problem. The resulting customization agent will assist the software engineering user in efficiently selecting the best configuration, which includes a set of algorithms and their parameter values, customized for a particular task and software system. All tools and methodologies will be empirically evaluated in academic and industrial settings.","title":"SHF: Small: Collaborative Research: Better Comprehension of Software Engineering Data","awardID":"1017330","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["541880"],"PO":["564388"]},"168687":{"abstract":"Modern technology has completely transformed the concept of data in the biological and information sciences. Data collections about the flow of information on the web, for instance, or about regulatory and metabolic dynamics that drive cellular functionality are extremely large and heterogeneous. These collections are often characterized as networks of websites, or proteins, where directed edges denote information flow, or chemical reactions, and with node information described in terms of web pages, or chains of amino acids. Knowledge discovery and management is key. The goal of this proposal is to create novel computational and statistical approaches to store, search, and quantify patterns in large networks efficiently, and to explore the extent to which these new tools help address a number of important open problems and computational issues. The research plan includes theoretical, methodological, data analysis, and dissemination aspects.<br\/><br\/>The approach is to develop new models, methods and algorithms for analyzing large biological and information networks with rich node information. New tools will be developed: to assess the complexity of networks; to compare the fit of alternative network models; to store information about both connectivity and nodes in a network efficiently; to calibrate informative priors for networks that reflect the reality of signaling both in metabolic networks and in the spread of news on the web for empirical Bayesian analyses; to estimate the effects of node information on the local connectivity in a network; and to infer influence potentials and diffusion channels in online information networks. The proposed research is focused on three specific technical tasks: (1) establishing a new representation of valued, multivariate networks based on a statistical models; (2) developing a flexible family or probabilistic graphical models to link local connectivity in the network to high-dimensional node attributes; and (3) developing scalable algorithms to infer a non-observable network structure from multiple trails of informational artifacts on the network itself. In addition, two in-depth case studies will be developed to illustrate the potential of the proposed methodology. The first is an analysis of the effects of local influence patterns among online newspapers, news collectors and blogs on the diffusion of news and information items. The second is an analysis of the effects of local perturbations of signaling in regulatory networks on global cellular responses, for many known functions, from bacteria to human. Insights gained in tackling the case studies will in turn generalize and foster the development of the next wave of core methodology and theory in machine learning.<br\/><br\/>The proposed work meets an urgent need for the development of new and principled methods for analyzing massive amounts of network data, as well as the creation of large-scale data sets for testing and benchmarking, to the benefit of the community at large. The research plan is tightly integrated with an interdisciplinary educational program and with the development of a statistical machine learning curriculum, which will attract many undergraduates to research at the intersection of machine learning and the sciences, and will provide opportunities to actively encourage students from underrepresented groups to pursue careers in computer science and statistics. The team will distribute open source software and set-up websites to enable the community to use and build upon the tools.","title":"III: Small: Representation, Modeling and Inference for Large Biological and Information Networks","awardID":"1017967","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["302533","525637"],"PO":["565136"]},"168335":{"abstract":"Computer designers use techniques to accelerate the simulation of their new computer designs while ignoring the errors of the acceleration methods. This is common because the entire simulation of modern computer programs is intractable. Basing the performance of a design only on fragments of program execution can yield results that are misleading, and result in far from optimal computer hardware designs. Without bounds on confidence, the relative performance of two architectures is impossible to compare with any statistical validity. For computer design to advance to an engineering science with reproducible results, the status quo must change. This research remedies the situation by introducing confidence bounding to fast manycore simulation.<br\/><br\/>The Georgia Tech research team has made significant and lasting contributions to adding confidence to single-threaded processor simulation, and is adapting and discovering new techniques for the manycore simulation. The team is investigating several approaches to thread slip problem (where the relative execution order of threads is unknown at the start of the simulation of a cluster of events) and are developing models that can be used by designers. Another class of problems is state reconstruction problems unique to manycore simulation. For example, global ordering of thread executions and their sharing patterns greatly impact the coherence state of cache blocks. Coherence information, in addition to directory contents, indicate the current owner of a line, which must be reconstructed for measured cache latencies and interconnect network flow to be representative of unsampled execution. Other specific problems involve recovering the directory state, the state of the interconnection network (e.g., flit buffers, conflicts for routes, etc), and cache state.","title":"SHF: Small: Confidence in Manycore\/Multi-Core Modeling and Simulation","awardID":"1016285","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["517778"],"PO":["565272"]},"168456":{"abstract":"The scaling of on-chip memory is tremendously challenged by the excessive amount of process variations and reliability degradation at the 22nm node and below. In current practice, full custom design and extensive experimentation on test silicon are often necessary to achieve the desired performance under all process, voltage, and temperature conditions. Although such an expensive approach is acceptable in today?s chip design, it drastically reduces design productivity and predictability. The situation becomes even more severe when the ever-increasing nature of variations narrows the design window and exacerbates memory design complexity. This proposal aims to develop innovative methodologies that will enable fast sign-off of on-chip memory at the end of the silicon roadmap and beyond, through the seamless integration of predictive variability models, statistical sampling schemes, robust optimization algorithms, and efficient silicon characterization techniques. Furthermore, these new outcomes will be integrated into an online framework to statistically benchmark post-Si memory design, helping illustrate the diverse opportunities of memory design beyond the 10nm node.<br\/><br\/> This research effort will facilitate fundamental research on reliable design with unreliable components, enhance design productivity for a wide range of applications, and expedite statistical design solution for emerging nanoelectronic devices. In addition, through novel education curricula and web-based dissemination tools, this project will transfer the newly developed design knowledge to a diverse population of students, who will lead the creation of future nanoscale integrated systems of all types, from computation, communication, to consumer electronics.","title":"SHF: Small: Collaborative Research: Fast Sign-Off of Nanoscale Memory: From Predictive Device Modeling to Statistical Circuit Synthesis","awardID":"1016831","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":["543501"],"PO":["562984"]},"167246":{"abstract":"Simple contagion processes underlie various phenomena on complex networks, such as the spread of diseases on social-contact networks and information in communication networks; understanding their dynamics and developing control mechanisms are key issues in numerous applications. The goals of this proposal are: (i) Developing methods to construct synthetic relational networks using partial and noisy data; (ii) Understanding the structure of these networks and the contagion processes, and especially important network properties and typical patterns that have an impact on the dynamics of contagion; (iii) Developing techniques to control the spread of contagion processes, and to detect, prevent and arrest cascading failures in coupled socio-technical networks; and (iv) Understanding the co-evolution between the networks and dynamics, and using this to refine their models, and the strategies to control them.","title":"NetSE: Large: Collaborative Research:Contagion in Large Socio-Communication Networks","awardID":"1010921","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":["507065"],"PO":["565090"]},"168577":{"abstract":"Dynamical systems are a principal tool in the modeling and control of physical phenomena as diverse as signal propagation in the neural\/nervous system, circuit simulation, weather forecasting, and fluid dynamics. Direct numerical simulation has been one of very few available means for studying the rich complexity of these phenomena, and in many areas of engineering numerical simulation has become essential to the design process. However, the ever increasing demand for improved model fidelity leads inevitably to dynamical systems of extremely large scale and complexity. Simulations based on such systems often impose unmanageable burdens on both human and computational resources, and thus provide the principal motivation for model reduction - creating smaller, cheaper models that closely mimic the behaviors of the original system. Model reduction can thus result in tractable low dimensional systems that are suitable for analysis, simulation, optimization, and computer-aided system design.<br\/><br\/>The primary theme of this research is an empirical data approach combined with interpolation to overcome limitations of standard projection methods for linear problems. The empirical data may be provided by physical experimentation or by direct numerical simulation. Interpolation conditions enter in several ways to greatly decrease the computational complexity of the reduced models. A three orders of magnitude reduction in computation time can be achieved while retaining excellent accuracy. The proposed research will strive to put these techniques on firm mathematical foundations in order to assure accuracy and also to greatly extend the areas of application in order to establish broad applicability of these new approaches.<br\/><br\/>The proposed approaches represent a significant departure from existing methodology. Developing the proposed methods to a greater level of maturity and applicability will be a significant advance in model reduction.","title":"AF: Small: Interpolatory Methods for Dimension Reduction of Parametric and Nonlinear Dynamical Systems","awardID":"1017401","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["551029","551030"],"PO":["565157"]},"167136":{"abstract":"The human brain remains one of the great mysteries to modern science. A further understanding of the function of the brain has great implications both for how we understand ourselves as well as how we might design modern computational devices. Perhaps the greatest obstacle towards understanding the brain is the vast number and complexity of its component unit cells, or neurons. Current cellular and imaging tools have now advanced to the point where it might now be possible to build complete maps of relatively simple brains, such as that of the fruit fly. This project will support the training and efforts of a team of students who will help assemble the first complete neuron-scale set of structures for a part of the fly brain. Using high-resolution images of individual neurons, the structures of each cell will be determined, and using sophisticated computational tools a complete digital map will be assembled. This map can be used to develop simulations of regional brain function as well as to develop ideas for how complex brains are structured. Experience from this project will pave the way for developing methods to automate the mapping of brains. Finally, a team of students, from high school to graduate level will receive training in modern neuroscience.","title":"CRCNS Data Sharing: The Drosophila Larval Abdominal Neurome","awardID":"1010333","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7712","name":"ORGANIZATION"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7713","name":"ACTIVATION"}}],"PIcoPI":["473947"],"PO":["563309"]},"168588":{"abstract":"We are exploring computer system archiectures that will enable extremely high quality interactive computer graphics. Current commercial graphics processing units (GPUs) are capable of rendering a large number of triangles at interactive rates, but consume a tremendous amout of power in order to do so. The goal of the proposed research is to develop new architectures for high-performance graphics processing that will significantly enhance the ability to render visually realistic scenes, and to do so in a manner that consumes less power than current GPU power growth trends. This involves the development of novel chip microarchitecture, memory systems, and graphics algorithms. We are primarily targeting ray tracing as a rendering algorithm because of its ability to support high-quality composite lighting effects such as shadows, transparency, reflections, refractions, and indirect illumination.<br\/><br\/>The human computer interface has been significantly enhanced by improved graphics systems. We believe that increasing visual realism will enhance commodity system support of virtual realistic interfaces. Energy efficiency is a national priority and reduced energy consumption is critical to reducing the cost of computing systems. Cheap, yet high quality graphics will enable a broader user spectrum to efficiently interact with the information infrastructure.","title":"CSR: Small: Flexible Architectures for Future Graphics Processing Systems","awardID":"1017457","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["542053",451272],"PO":["565255"]},"168478":{"abstract":"Well-designed visualizations leverage the human visual system to help people understand large data sets. Yet, producing effective visualizations is a challenging design task. Designers must carefully choose how to map the data to visual variables such as position, size, shape and color. In this process they make hundreds of nuanced judgments while balancing perceptual and cognitive tradeoffs. In response, researchers in psychology, cartography, statistics, and computer science have investigated the effects of different visual variables on graphical perception: the ability of viewers to interpret visual encodings and thereby decode information in graphs. Despite great progress in developing design guidelines based on laboratory experiments, comprehensive evaluation of the visualization design space and real-world validation of the resulting guidelines have remained elusive.<br\/><br\/>The research is advancing our understanding of graphical perception and formulate new guidelines for visualization design. The research involves new experiments to address unresolved issues in graphical perception, including large-scale web-based studies using crowdsourcing techniques and controlled laboratory studies using sensitive measurements, namely eye-tracking. The investigators are applying the results of these studies to (a) develop guidelines for effective visualization design, (b) instantiate these guidelines in automated design procedures, and (c) validate the guidelines and resulting tools through case study deployments.","title":"HCC: Small: Collaborative Research: Graphical Perception Revisited: Developing and Validating Design Guidelines for Data Visualization","awardID":"1016920","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["534062"],"PO":["565227"]},"173352":{"abstract":"The PI has two hypotheses about software engineers? information needs during code reviews. The first hypothesis is that different roles in code review, such as an author and a reviewer, lead to different information needs in terms of abstraction levels; thus, existing static and dynamic program analysis that do not distinguish the role of information producer (code author) and consumer (code reviewer) may not be effective in supporting peer reviews. The second hypothesis is that existing communication, awareness, and management support features in collaborative development tools such as an instant messenger, email, and work-flow management provide high-level, yet shallow information, as these tools lack in the ability to provide code-centric information. In order to test these hypotheses, the PI will use several empirical study methods, including focus groups, semi-structured interviews, case studies, and surveys, to acquire comprehensive and systematic understanding of engineers? information needs during peer code reviews. <br\/><br\/>The outcome of this study will guide the construction of innovative software analyses that can satisfy programmers? information needs, improving the effectiveness of peer code review tasks, ultimately improving programmer productivity and software quality. Furthermore, this study will serve as a basis for identifying what types of information at which abstraction level can best support developers in examining software modification. The findings from this study will also contribute to developing necessary program delta representations, inference algorithms, and infrastructures that will enable engineers to reason about software modification at a high level.","title":"Information Needs about Software Modification during Collaborative Development Tasks","awardID":"1043810","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["527053"],"PO":["564388"]},"166950":{"abstract":"Web Science is an emerging discipline that studies the Web: how human activity is shaped by Web interactions, how the Web can benefit society, and how Web technologies can be improved. Central to Web Science is access to data that records the history of the Web, as well as data that records human activity (e.g., posed queries, tagged pages, Twitter updates). It is currently very difficult for academic researchers to obtain such Web data because it is hard to locate, it is fragmented across diverse sites, and is recorded using inconsistent formats and strategies. This project will build a Web Archive Cooperative (WAC) that will integrate existing archives (repositories of Web data), making it feasible to access large volumes of data in a simplified fashion. The WAC will be a virtual service, providing search facilities and access mechanisms to existing resources. These resources will not just be Web pages, but all types of available Web information, such as query logs, tag annotations, blogs, profiles and Twitter updates. Furthermore, resources will also include the software tools for building and managing Web archives.<br\/><br\/>The project will explore three goals for a resource discovery service: (1) the manual or automated discovery of entire existing Web related archives; (2) the selection among known archives of the ones that support a specific research question; and (3) the identification of individual resources from within the selected archives. Tools for characterizing discovered archives, especially for the case where the archive does not provide rich descriptive metadata, will also be developed. Characterization of an archive includes elements such as an estimate of the archive's coverage, particulars of the crawling parameters, like dates\/frequencies, crawl duration, depth, per-site ceiling on the number of collected pages, content statistics, and link structure. Mechanisms for integrating diverse archives will be developed, and the mechanisms will be applied to site reconstruction (from various archives) and archive views (a logical fusion of resources from multiple sources). Since integration issues are so challenging, an experimental testbed will be set up with small but diverse resources. The testbed will contain several crawls of the same target sites, each obtained with different crawlers and using different parameters. The testbed will also contain related resources. Storage trading schemes will be developed, allowing members to trade local backup space for remote space. A Web archive replication tool will be developed based on existing notions for self-preserving objects. Alternatives for replica synchronization will be studied.<br\/><br\/>Workshops to bring together key Web Science researchers will be organized to discuss available resources and impediments to sharing. These workshops will drive research and identify needed tools and protocols. With small groups of participants, challenge problems will be established, e.g., combining a set of Web archives. Reports of these results at future workshops can incentivize others to participate in the WAC. In addition, an Advisory Board of industrial, government, and academic experts has been set up to guide the project. A Summer Institute for Web Science graduate students will be held. At this Institute, students will learn to use the latest tools and will learn from each other's experiences in dealing with Web data. In addition, a one-day workshop will be developed, to be offered at Web Science conferences (WWW, SIGIR, etc.) to educate participants about WAC resources. An undergraduate Web Sciences track for computer science majors will be set up, taking advantage of WAC resources. The project will have impact in two ways. First, it will provide tools and services that facilitate access to Web resources. Any researcher, from a computer scientist studying efficient Web search, to a social scientist studying how human beliefs are changing today, to a historian studying how the early Web evolved, to a biologist understanding how disease spreads, will benefit from the work. Second, the project motivates students and young researchers to stay in academia. Currently top talent is flowing to industry because only they have comprehensive Web data, and it is so hard to do significant Web Science at universities. The WAC can provide an alternative, attracting more researchers and teachers to this important area.","title":"III: Large: Collaborative Research: Web Archive Cooperative","awardID":"1009392","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["527258"],"PO":["565136"]},"174100":{"abstract":"A transition is occurring from a world in which gatekeepers and editors filter content before it is published to a world full of user-generated content in which information filtering is done after publication. Today's online communities have developed a variety of community-based filtering and rating mechanisms to help maintain quality and manageability. However, it is an open question whether these filtering mechanisms represent \"the wisdom of crowds\" or \"the censoring mob.\"<br\/><br\/>This project will apply statistical machine learning and ethnographic studies to understand the mechanisms by which online communities censor content from the bottom up. This understanding will provide insight into how values are and can be embedded into these large, socially intelligent systems. Ultimately, the goal is to design socially intelligent community filtering systems in which individuals, communities, and intelligent software agents collaborate, to explain the mechanisms behind social, bottom-up filtering, and expand the range of the possible in terms of the values these systems can reflect and the communities it can serve. This project will study the mechanisms through which the social construction of gender impacts community filtering systems. This will be done via an in-depth study of two online communities that have vigorous community policed comment filtering; one whose participants are predominantly male and another whose participants are predominantly female.<br\/><br\/>Online communities are rapidly becoming the modern public square and community filtering has the potential to make the space vibrant and useful and\/or degenerate into a form of censorship. The health of our civil society and its ability to address large challenges depends on the health of its public discourse. By creating systems for socially intelligent filtering that reflect the community we facilitate diversity, in that minority positions are protected and preserved, while at the same time majority positions have the opportunity to develop and refine cogent arguments necessary for a well reasoned debate.","title":"EAGER: Investigating Diversity in Online Community Filtering","awardID":"1048515","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["561935","534324"],"PO":["564456"]},"164453":{"abstract":"Understanding how people make decisions in complex settings is crucial in many application areas, including marketing, intelligence analysis, and political decision making. Traditionally, human decision-makers are modeled as rational agents seeking to maximize some mathematical measure of utility. In fact, however, people are overwhelmed in information-rich environments, and have developed cognitive and emotional strategies to navigate such environments. One such strategy is \"motivated reasoning\", where information is first evaluated subconsciously for emotional content, with the goal of maintaining an existing emotional commitment, and cognitive processing of the information is then conditioned on this emotional evaluation. Political scientists have demonstrated motivated reasoning in evaluation of both candidates and issues. In general, decision-making and information-gathering are strongly influenced by emotion, prior knowledge, and the social communities to which a person belongs. Evidence suggests that accurate models of human decision-making must be complex enough to model not only utility, but prior knowledge and beliefs, human cognitive abilities, and social context. Building such cognitive models requires substantially extending the state-of-the-art in machine learning. <br\/><br\/>In the past, the ability of researchers in political psychology to develop such complex models of decision-making was limited by the amount of data obtainable obtain from surveys or human-subject experiments. The recent explosion of on-line political communities provides an opportunity to overcome this limitation. We will model human behavior for socially-driven information gathering and decision-making tasks - specifically for political decisions - by combining human-subject experiments with analysis of large datasets of social media and social interactions.","title":"Collaborative Research: SoCS: Analysis of Social Media Driven By Theories of Political Psychology","awardID":"0968295","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["453587"],"PO":["564456"]},"168732":{"abstract":"With the recent dominance of computers with many parallel cores, and the widespread use of large data centers, the need to supply high-level, simple and general approaches to developing parallel codes for these machines has become critical. There are many challenges to effectively developing such parallel codes, but certainly a principle difficulty is dealing with communication costs - or when looked at from the other side, taking advantage of locality. Unfortunately this challenge has only become more difficult on modern parallel machines that have many forms of locality - network latency and bandwidth, shared and distributed caches, partitioned memories, and secondary storage in a variety of organizations.<br\/><br\/>To address this problem this project is developing an approach for programmers to understand locality in their parallel code without needing to know any details of the particular parallel machine they are using. In the approach programmers express the full dynamic parallelism of their algorithm without describing how it is mapped onto processors, and are given a simple high-level model for analyzing locality. The research is based on the conjecture that locality should be viewed by the programmer as a property of the algorithm or code and not the machine. To ensure that real machines can take proper advantage of the locality analyzed in the model, the research is developing scheduling approaches that map the \"algorithm locality\" onto various forms of machine locality, including shared caches, distributed caches, trees of caches, and distributed memory machines. The results of the research include both theoretical bounds for such schedulers on specific machine organizations, and experimental validation on a set of benchmark applications.","title":"SHF: AF: Small: Locality with Dynamic Parallelism","awardID":"1018188","effectiveDate":"2010-08-01","expirationDate":"2014-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7934","name":"PARAL\/DISTRIBUTED ALGORITHMS"}}],"PIcoPI":["548200"],"PO":["565272"]},"168622":{"abstract":"Software engineers often do not get access to confidential data because of internal security rules that are put in place by organizations that own these data and because of several laws that regulate data protection and privacy. This situation complicates basic software engineering tasks such as testing. To give some access to required data, data owners typically use a commercial tool to anonymize or \"sanitize\" the data. Unfortunately, none of the existing tools takes into account basic software engineering tasks such as testing, which leads to situations where the anonymized data is of little to no value for software engineers. Currently, software engineers operate with little or no meaningful data, which is a great obstacle to creating high quality software. <br\/><br\/>This research program addresses a fundamental question of software engineering: how can a data owner protect private information so that the data subjects (e.g., persons, equipment) cannot be re-identified while the data retains their efficacy for software engineering tasks? To preserve the usefulness of data for software engineering tasks, algorithms are needed that take into account the structures of the applications. This work will lay a foundation for a new direction of research on interactions between software engineering and data privacy, and the PIs will support it with a set of tools for low-cost software development and evolution.","title":"SHF: Small: Collaborative Research: Preserving Test Coverage While Achieving Data Anonymity for Database-Centric Applications","awardID":"1017633","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["564387"],"PO":["564388"]},"168523":{"abstract":"This project focuses on tackling a critical barrier to long-term autonomy for robotic systems, namely the lack of theoretically well-founded self-calibration methods for inertial and vision-based sensors, commonly found on sophisticated robots. The project is motivated by the vision of power-up-and-go robotic systems that are able to operate autonomously for long periods without requiring tedious manual sensor calibration. The research team addresses this problem in the context of vision-based mobile manipulation and navigation. The core foci of the work are: 1. the development of a unified mathematical theory of anytime, automatic calibration for visual-inertial systems, and 2. an experimental characterization of the resulting algorithms with state-of-the-art, sophisticated robots of significant diversity (humanoids performing mobile manipulation and autonomous ground vehicles navigating outdoors). Inertial sensing is critically important for humanoid balance control, while visual sensing relates the 3D world to the robot's body coordinates thereby enabling manipulation. In the case of autonomous ground vehicles, monocular and stereo camera calibration is still commonly performed manually using a known calibration target. The project obviates the need for this requirement. The expected outcomes of the project are: 1. a theoretical foundation for humanoid robots to function autonomously in unstructured environments over significant periods of time, and 2. new navigation algorithms for ground vehicles allowing them to see further with greater acuity. The project explicitly incorporates undergraduate research in cooperation with an REU site currently operational at the USC Computer Science Department.","title":"RI: Small: Vision-Based Mobile Manipulation and Navigation","awardID":"1017134","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["492859","560335"],"PO":["564316"]},"168655":{"abstract":"Reference, the ability to single out objects in the world using<br\/>linguistic expressions, is a fundamental part of communication.<br\/>Cognitive scientists have showed that people collaborate on reference,<br\/>for example by negotiating about how to describe things. This project<br\/>lays the groundwork for computer systems that can do the same.<br\/>Eventual applications range from talking robots that can communicate<br\/>with people in real physical environments, to tutoring and<br\/>decision-support systems that can give comprehensible explanations of<br\/>specialized concepts. These applications are open domains, in the<br\/>sense that one interlocutor, or both, may have no simple descriptor<br\/>for an unfamiliar object. So the system has to have flexible<br\/>strategies for describing things in words and inference mechanisms<br\/>that acknowledge the possibility of a misunderstanding by either<br\/>system or user.<br\/><br\/>This project undertakes exploratory work on the communicative<br\/>strategies and inference mechanisms required for collaborative<br\/>reference in open domains. During the academic year, PI Stone works<br\/>with a computer science student to extend a prototype system with<br\/>additional methods for collaborating under uncertainty, drawing on the<br\/>student's ongoing research in cognitive modeling, planning and<br\/>learning. During the summer, Stone works with a three-member<br\/>interdisciplinary team doing corpus analysis, grammar development and<br\/>integration to characterize generic strategies for describing shape,<br\/>using rough descriptions and part-whole relationships. The results<br\/>are an open-source prototype with more general collaborative reference<br\/>abilities along with specific hypotheses, informed by publishable<br\/>linguistic and conceptual analyses, about how these abilities might<br\/>affect the system's success in referring to unfamiliar objects.","title":"RI: Small: Collaborative Reference in Open Domains","awardID":"1017811","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["474118"],"PO":["565215"]},"168556":{"abstract":"The amount of data generated during the development of today?s software systems is staggering. It includes the source code, developer e-mails, bug information, testing results, analysis data, process information, requirements, etc. The size and complexity of this information make it impossible for developers to reason about it.<br\/><br\/> Data mining techniques are a common solution to extract what is relevant to developers and managers. The success and quality of these software projects depends on the software engineers? ability to customize generic data mining algorithms to specific software engineering data. This project will produce tools and techniques that will allow software developers and managers to easily customize and apply data mining techniques to a variety of software engineering problems. Such solution will become more practical and will help many existing approaches to migrate from the research lab into industry.<br\/>Under represented categories of students will participate in this research. The project will enhance the existing software engineering curriculum and facilitate the inclusion of data mining solution in the repertoire of future software engineering practitioners and researchers.<br\/><br\/>Specifically, the project will improve the state of the art solution to three important software engineering tasks: concept location in software, software defect prediction, and development effort estimation. The project will produce an algorithm customization methodology and a framework that will be instantiated for a variety of combinations of data mining algorithm x software engineering task x software system data. The customization problem is framed and addressed as an optimization problem. The resulting customization agent will assist the software engineering user in efficiently selecting the best configuration, which includes a set of algorithms and their parameter values, customized for a particular task and software system. All tools and methodologies will be empirically evaluated in academic and industrial settings.","title":"SHF: Small: Collaborative Research: Better Comprehension of Software Engineering Data","awardID":"1017263","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["511418"],"PO":["564388"]},"168446":{"abstract":"Embedded computers interacting with physical systems are increasingly common. Examples include medical devices, factory equipment, automobiles, and satellites. Testing embedded software during development is hard. Interacting with real physical systems, as with humans, may be dangerous or cost-prohibitive. Connecting the embedded computer to a physical mockup, like an electromechanical heart, is limited by the mockup's behavioral range. Simulating the entire system is inaccurate and slow, and does not test the actual embedded computer. Connecting the embedded computer to a digital mockup of the physical system, wherein the embedded computer's sensors\/actuators are bypassed and interact instead with a physical system computer model, still suffers from the modeled part being inaccurate and slow.<br\/><br\/>This project creates digital mockups that are accurate and fast by using modern field-programmable gate array (FPGA) chips. It is the first to develop automated synthesis techniques for converting numerous differential equations, forming the core of physical system models, into circuits on FPGAs. The project evaluates various differential equation solution techniques for FPGA suitability, and develops an interconnected processing element target architecture. The project supports real-time execution and time-controllable execution via lightweight kernel definition on those processing elements. It develops a system synthesis approach to explore the solution space for a given physical model and FPGA device. The project includes expansion of existing embedded systems educational material, and trains numerous graduate and undergraduate students. Ultimately, the project will catalyze use of digital mockups and hence lead to better embedded computers.","title":"CSR: Small: Collaborative Research: Synthesis of Time-Controllable Digital Mockups of Physical Systems","awardID":"1016792","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[450944],"PO":["565255"]},"168567":{"abstract":"Software systems are poised to keep growing in complexity and permeate deeper into the critical infrastructures of society. The complexity of these systems is exceeding the limits of existing modularization mechanisms and reliability requirements are becoming stringent. Development of new separation of concerns (SoC) techniques is thus vital to make software more reliable and maintainable. Implicit invocation (II) and aspect-oriented (AO) programming languages provide related but distinct mechanisms for separation of concerns. The proposed work encompasses fundamental and practical efforts to improve modularization and reasoning mechanisms for II and AO languages, which is a long standing challenge for both kinds of languages. Addressing these challenges has the potential to significantly improve the quality of software by easing the adoption of new separation of concerns techniques.<br\/><br\/>The project will proceed using the experimental language, Ptolemy, which blends both II and AO ideas. Ptolemy has explicitly announced events, which are defined in interfaces called \"event types\". Event types help separate concerns and decouple advice from the code it advises. Event type declarations also offer a place to specify advice. The explicit announcement of events allows the possibility of careful reasoning about correctness of Ptolemy programs, since it is possible to reason about parts of the program where there are no events in a conventional manner. The project aims to investigate reasoning by developing a formal specification language and verification technique. The approach is based on the idea of greybox (\"model program'') specifications, as found in JML and the refinement calculus. There are known techniques for reasoning about uses of abstractions that have model program specifications, and the project will apply these to Ptolemy. The intellectual merit is in the treatment of expressions in Ptolemy that announce events and those that cause an advice to proceed. A straightforward adaptation of existing reasoning techniques to these cases appears to require a whole program analysis, which is generally not desirable for modular and scalable verification. The project also aims to investigate the utility and effectiveness of Ptolemy and its specification system. A software evolution analysis will be conducted to study the ability of competing aspect-oriented, implicit invocation, and Ptolemy implementations of open source projects to withstand change. Showing Ptolemy's benefits over II and AO languages will help software designers in deciding on advanced mechanisms for separation of concerns.","title":"SHF: Small: Collaborative Research: Balancing Expressiveness and Modular Reasoning for Aspect-oriented Programming","awardID":"1017334","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["562662"],"PO":["564588"]},"168479":{"abstract":"Data and knowledge integration are costly processes. Consequently, most existing solutions rely on a one-size-fits-all approach, where the data are integrated upfront and then the integrated data or knowledge-bases are used as is. Such snapshot-based integration solutions, however, cannot be effectively applied when the data sources are autonomous and dynamic or when, as in most scientific and decision making applications, assumptions, beliefs, and knowledge of the domain experts are indispensable to the integration process.<br\/><br\/>The proposed work tackles the computational challenges underlying a user driven integration (UDI) system, keeping in mind the human constraints and challenges that underlie the technical considerations. The key technical and intellectual impacts are in algorithms and data structures that can help bridge the semantic gap between the expert user and the system through a user-driven integration process based on individual user feedback. The team will specifically investigate (a) continuously revisable data\/metadata alignment through vector space embeddings and probabilistic and generative models and (b) algorithms for query processing and candidate enumeration to support feedback over graph-based models of data with alternative interpretations.<br\/><br\/>UDI has potential applications to many domains (such as science and business intelligence) that need user-driven integration to answer key questions over diverse data sets. In particular,UDI will be incorporated into the NSF-funded tDAR (the Digital Archaeological Record), which has the potential to transform archaeology?s scientific endeavors by enormously advancing the capacity for synthetic research. The investigation of fundamental information integration challenges will thus contribute substantially to a shared infrastructure of science and will enable crucial transdisciplinary research concerning complex systems.<br\/><br\/>Participation in this research by the computer science graduate students will prepare them to function effectively in multidisciplinary teams and enhance their appreciation of the associated challenges and opportunities. Use of UDI as a testbed will enable these students to experiment in scientific information management, thereby increasing their awareness of data integration and science-informatics issues. UDI We expect two graduate courses to leverage the data sets as well as the project software as an educational platform. Arizona State University also recruits top quality undergraduates through a nationally recognized residential Honors College and the Minority Access to Research Careers program and the project will involve undergraduate honors students to participate in the project. UDI will also serve as a testbed for undergraduate students through Capstone Projects.","title":"III: Small: One Size Does Not Fit All: Empowering the User with User-Driven Integration","awardID":"1016921","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["509597","558560"],"PO":["565136"]},"168248":{"abstract":"Hidden Markov models (HMMs) have been successfully applied to automatic <br\/>speech recognition for more than 35 years even though a key HMM <br\/>assumption - the statistical independence of frames - is obviously <br\/>violated by speech data. In fact, this data\/model mismatch has inspired <br\/>many attempts to modify or replace HMMs with alternative models that are <br\/>better able to take into account the statistical dependence of frames. <br\/>The scientific goal of this work is to discover predictable regions of <br\/>statistical dependence in speech data and quantify their effect on <br\/>HMM-based recognition accuracy. In contrast to previous studies of <br\/>statistical dependency, this research uses the HMM to explore its <br\/>departure from the data via exploratory data analysis (EDA). The <br\/>methodology is to first analyze the data and its fit to the model, <br\/>searching for regions of predictable statistical dependence - model\/data <br\/>mismatch. EDA is used again to develop simple models of the effect of <br\/>the predictable mismatch on recognition accuracy. A key piece of this <br\/>analysis is the development and use of graphical tools to visualize the <br\/>statistical dependency, the recognition errors, and their relationship. <br\/>The results of this research will provide important clues for the design <br\/>of HMM generalizations. The analysis methodology is central to the field <br\/>of statistics, but is rarely used in speech recognition research. <br\/>Graduate students working on this project will learn its utility and <br\/>how to use it on other problems. Open source versions of the software <br\/>developed will be made available for free downloading.","title":"RI: Small: Exploratory Data Analysis for Speech Recognition","awardID":"1015930","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[450457],"PO":["565215"]},"163771":{"abstract":"The aim of this research is to develop new algorithms and algorithmic<br\/>techniques for solving fundamental optimization problems on planar<br\/>networks. Many optimization problems in networks are considered<br\/>computationally difficult; some are even difficult to solve<br\/>approximately. However, problems often become easier when the input<br\/>network is restricted to be planar, i.e. when it can be drawn on the<br\/>plane so that no edges cross each other. Such planar instances of<br\/>optimization problems arise in several application areas, including<br\/>logistics and route planning in road maps, image processing and<br\/>computer vision, and VLSI chip design. <br\/><br\/>The investigators plan to develop algorithms that achieve faster<br\/>running times or better approximations by exploiting the planarity of<br\/>the input networks. In addition, in order to address the use of<br\/>optimization in the discovery of some ground truth, the investigators<br\/>will develop algorithms not just for the traditional worst-case input<br\/>model but also for models in which there is an unusually good planted<br\/>solution; for a model of this kind, the investigators expect to find<br\/>algorithms that produce even more accurate answers.<br\/><br\/>The research will likely uncover new computational techniques whose<br\/>applicability goes beyond planar networks. In the recent past, once a<br\/>technique has been developed and understood in the context of planar<br\/>networks, it has been generalized to apply to broader families of<br\/>networks.<br\/><br\/>In addition, new algorithms and techniques resulting from this<br\/>research might enable people to quickly compute better solutions to<br\/>problems arising in diverse application areas. For example, research<br\/>in this area has already had an impact in the computer vision<br\/>community. Further research has the potential to be useful, for<br\/>example, in the design of networks, the planning of routes in road<br\/>maps, the processing of images.","title":"AF: Medium: Collaborative Research: Solutions to Planar Optimization Problems","awardID":"0964037","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[438410,438411],"PO":["565251"]},"166984":{"abstract":"The PIs will develop new machine learning algorithms to explore how meaning is represented in the brain and how meaning representations shape human memory. Current neuroscientific theories of memory posit that forming a memory for a particular event involves associating the details of that event with the person's current mental context, i.e., everything else that she is thinking about at the time. When trying to remember the event, the person can access stored details by reinstating the mental context that was present when the memory was formed. This fits with the intuition that forgotten details (e.g., the location of misplaced house keys) can be retrieved by mentally \"re-tracing steps\", i.e., trying to reinstate the mindset that was present at the time of the original event. With these theories in mind, the goal of this work is to develop machine learning algorithms that make it possible to track, based on fMRI brain data and behavioral memory data, the process of \"mentally re-tracing steps\"---the proposed algorithms will be able to decode the state of a person's mental context as she forms memories and (later) as she searches for these memories.<br\/><br\/>The proposed work uses two fundamental ideas about memory and meaning: The first idea is that mental context is shaped by the meanings of recently encountered stimuli. The second idea is that semantic relationships between concepts in the brain mirror statistical relationships between words in naturally occurring language. The developed algorithms will bring together data from three sources---behavioral data from subjects performing memory recall tasks, fMRI neuroimaging data collected while subjects performed these tasks, and large collections of documents---to discover a latent meaning space that can simultaneously describe all three types of information. Each point in this space describes a mental context. Thus the core of the proposed work is to develop latent variable models and algorithms that can infer from data how the mental context moves through meaning space as a person stores and searches for memories.<br\/><br\/>The proposed work will lead to fundamental advances in machine learning (new algorithms for inferring hidden variables based on multiple, heterogeneous data types) and neuroscience (more refined theories of how memory search is accomplished in the brain). Furthermore, this work will catalyze the development of new technologies for diagnosing and remediating memory problems, by making it possible to track how the contextual reinstatement process is going awry in people experiencing memory retrieval failure.","title":"Text, Neuroimaging, and Memory: Unified Models of Corpora and Cognition","awardID":"1009542","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"5345","name":"BIOMEDICAL ENGINEERING"}}],"PIcoPI":["556728","531640"],"PO":["564318"]},"167512":{"abstract":"This project addresses a key problem in advancing the state of the art in cognitive assistant systems that can interact naturally with humans in order to help them perform everyday tasks more effectively. Such a system would help not only people with cognitive disabilities but all individuals as they perform complex tasks they are unfamiliar with. The research focuses on structured activities of daily living that lend themselves to practical experimentation, such as meal preparation and other kitchen activities.<br\/><br\/>Specifically, the core focus of the research is activity recognition, i.e., systems that can identify the goals and individual actions a person is performing as they work on a task. Key innovations of this work are 1) that the activity models are learned from the user via intuitive natural demonstration, and 2) that the system is able to reason over activity models to generalize and adapt them. In contrast, current practice requires specialized training supervised by the researchers and supports no reasoning over the models. This advance is accomplished by integrating capabilities that are typically studied separately, including activity recognition, knowledge representation and reasoning, natural language understanding and machine learning. The work addresses a significant step towards the goal of building practical and flexible in-home automated assistants.","title":"RI-Large: Activity Learning and Recognition for a Cognitive Assistant","awardID":"1012205","effectiveDate":"2010-08-15","expirationDate":"2014-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["517044"],"PO":["565215"]},"168843":{"abstract":"The number of processing cores in everyday computer systems is quickly outpacing programmers' and compilers' ability to create many-threaded applications capable of taking advantage of the cores. Instead of requiring applications to be rewritten to take advantage of the many-core architectures, this project aims to create automated parallelization approaches that utilize machine learning to improve traditional parallelization methods to create more effective (and often speculative) threads. These techniques will be applied to the arrangement of the parallel execution of tasks (whether coarse- or fine-grained), yielding a systematic learning approach that will outperform static parallelization attempts. This approach assists both programmer identified parallelism as well as that exposed by the compiler. This project is not limited to applications with regular loop-level parallelism, but targets irregular integer applications which have proven difficult to statically parallelize. <br\/><br\/>This project will result in the development of a high performance dynamic parallelization system based on critical information gathered from machine learning and related approaches. By creating intelligent parallelization approaches, we enable the multicore machines of the future to be used to their full advantage by both scientists and the general public. Although many of today's applications may not require the full computational power of a many-core machine, the effective utilization of such machines will enable new, more-powerful applications than are currently possible.","title":"CSR: Small: Learning-Assisted Parallelization","awardID":"1018771","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["538251",451895],"PO":["565255"]},"165455":{"abstract":"The intellectual history of science and computer science has placed undue weight on the propositional, symbolic and linguistic side of thought; leaving unexplained how thought can occur in non-verbal systems, based closer to our senses, and often coordinated with non-verbal processes in the socio-technical world. The goal of this study is to develop a new computational model of creativity based on an experimental and ethnographic study of choreographic invention. The core question is how the distributed system consisting of choreographer and dancers are able to be so generative. In all design and research work, creativity inevitably makes important use of non-propositional thinking. It is important, therefore, to deepen our insight in this ill-understood process. A further reason to study choreographic production is that it resembles large design and research projects in taking place over many weeks or months, it involves the collaboration of multiple parties, and a new production may easily cost a million dollars. At a group level the dynamics are similar to other large design efforts.The choreographer being studied has developed techniques for keeping the generative phase of the creative process open longer and for maintaining substantial variance among the dancers despite the urge for group think and convergent behavior. He has also developed techniques for exploiting the coding language of sensory systems, of both himself and his dancers, to create new movement ideas. <br\/><br\/>In this project shareable ethnographic and experimental data will be gathered that broadens the development of new computational theories about: 1) Distributed creativity: how the distributed components work to generate creative product - the mechanisms by which team members harness resources to interactively invent new concepts and elements, and then structure things into a coherent product; and 2) Embodied cognition: how the embodied aspect of cognition is harnessed to generate new 'thought' - the mechanisms by which designers, engineers, artists, dancers, and scientists think non-propositionally, using parts of their own sensory systems as simulation systems, and in the case of dance, using their own (and other's) bodies as active tools for physical sketching. The close study of both of these processes bears directly on the goals of developing new theoretical models of creativity and new models for research and education that will foster and reward creativity. The theory relocates creativity from a within-the-mind process to a more socio-technical process involving resources and other people; and it recognizes the important role that bodies and sensori-motor systems - both non-verbal and perhaps sub-rational elements - play in creative cognition.","title":"Pilot: Distributed Creative Cognition in Choreography","awardID":"1002736","effectiveDate":"2010-08-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":[443223],"PO":["565136"]},"168502":{"abstract":"The proposal is focused on applications of embedded meshes to simulations involving interfaces in volumes, primarily fluids, but also fracture simulations. Embedded meshes are detailed polygonal meshes contained within a finite volume where a scalar field quantized at a much coarser level is also defined. As the scalar field changes over time as a result of the simulation, the polygon mesh vertices move within the volume, driven by the flow created by the scalar field time changes. This is a hybrid implicit-parametric model which can be used to efficiently solve a number of problems of interest, since some computations can only be performed more efficiently using parametric models, and others using implicit models. The propsal presents a detailed plan for extending the idea of combining a separate Lagrangian discretization of the interface (embedded mesh) with Eulerian discretizations for fluids.","title":"HCC: Small: Embedded Meshes for Flow and Fracture","awardID":"1017014","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["456660"],"PO":["565227"]},"174068":{"abstract":"This project investigates the foundational aspects of an alternative approach to the specification, implementation and management of concurrent activities which is referred to as data-centric synchronization. Traditional approaches take an operational view of concurrency control, they burden the programmer with the need to identify sets of control flow paths in their program which must not interfere. This has been shown to be difficult to get right and, more often than not, to inhibit scalability to multicore systems as the code is over-synchronized. This EAGER project explores the foundations of data-centric synchronization with an emphasis on establishing a type theoretic foundation based on the notion of ownership types. Ownership types give programmers a simple notation for describing the shape and extent of heap-based data structures. Thus an ownership type system can be seen as the basic mechanism for specifying the groups of data items that must be manipulated synchronously.","title":"EAGER: Foundations of Data-Centric Concurrency Control","awardID":"1048398","effectiveDate":"2010-08-15","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["549800"],"PO":["565264"]},"168634":{"abstract":"Wireless sensor networks are evolving from specialized platforms to shared infrastructure for multiple applications. Shared sensor networks offer flexibility, adaptivity, and cost-effectiveness through dynamic resource allocation among different applications. Shared sensor networks face the critical need for optimizing Quality of Monitoring (QoM) subject to resource constraints. The emerging QoM optimization problems in shared sensor networks are computationally challenging due to their nonlinear, discrete, and dynamic nature. This project exploits a key property known as submodularity that many QoM attributes of physical phenomena exhibit. This project develops efficient and theoretically sound distributed approaches for QoM optimization, through a novel integration of submodular optimization in a market-based approach. It further studies online algorithms which can quickly adapt to network and application dynamics, partition-based algorithms that scale effectively for large-scale networks, and new optimization algorithms that can accommodate the optimization of energy consumption and heterogenous networks. Expected results of this project include theory, algorithms, and software for managing and optimizing a new generation of integrated networked sensing systems with high societal and environmental impact. The project also deploys an integrated sensor network for environmental monitoring in Tyson Research Center of Washington University for environmental and ecological research. This study promotes interdisciplinary collaboration with environmental and biological scientists, as well as outreach activities for high-school students.","title":"NeTS: Small: Generalized Submodular Optimization for Integrated Networked Sensing Systems","awardID":"1017701","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["560533","560534"],"PO":["565303"]},"168876":{"abstract":"We are living in a golden age of information where computing and sensing technology collect and generate tremendous amounts of data and the Internet facilities data storage and manipulation by the masses. A growing area of interest is how to provide this data back to the consumer to improve awareness of some aspect of his\/her life. This project investigates the use of always-on interactive displays (much like a wall clock) to support awareness with a particular focus on awareness of energy consumption in the home. The goal is to identify how such displays can be designed and effectively integrated into the fabric of home life to support awareness of energy consumption habits. Researchers will investigate several important aspects of integrating feedback devices into the home, including:<br\/><br\/>* The tradeoffs between aesthetics and utility <br\/>* Data privacy considerations<br\/>* The effect of such devices on home routines and vice versa<br\/>* Supporting motivation and system adoption in a multi-inhabitant home<br\/><br\/>These issues are most effectively addressed by studying real people in real households. Thus, researchers will undertake user-centered methods to inspire, inform, and evaluate designs through fieldwork and long-term deployments of designs in real homes.<br\/><br\/>The results of this project will apply to not only energy consumption awareness, but more broadly to other areas where data feedback and reflection may promote personal awareness. Examples include supporting awareness of fitness and nutrition habits or feedback of activities of everyday life (medicating, sleeping, eating, etc.) for the elderly aging in place.","title":"HCC: Small: Supporting Self-Awareness in Everyday Data Consumers Through Appropriate Interactive Visualizations","awardID":"1018963","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["554552"],"PO":["565227"]},"168524":{"abstract":"Despite growing interest in the economic aspects of the Internet, such as network interconnection (peering), pricing, performance, and the profitability of various network types, two historical developments contribute to a persistent disconnect between economic models and actual operational practices on the Internet. First, the Internet became too complex -- in traffic dynamics, topology, and economics -- for currently available analytical tools to allow realistic modeling. Second, the data needed to parameterize more realistic models is simply not available.<br\/>The problem is fundamental, and familiar: simple models are not valid, and complex models cannot be validated.<br\/><br\/>This project aims to achieve transformative progress in studying economic aspects of the Internet -- network interconnection (peering), pricing, and the profitability of various network types -- by creating more powerful, empirically parameterized computational tools, and enabling broader validation than previously possible. This project will involve measurement of key properties that impact Internet infrastructure economics, such as interdomain traffic, topology dynamics, routing policies and peering practices. These measurements will serve as inputs to a computational model of network interconnection and dynamics. The investigators will validate the model's ability to reproduce known macroscopic properties of the Internet topology, and its ability to reproduce known historical trends in the evolution of the Internet. The investigators will then use the model to study various \"what-if\" scenarios relating to interdomain interconnection practices, the stability and dynamics of interdomain peering links, and economic properties of provisioning Internet infrastructure.<br\/><br\/>The intellectual merit of this project lies in an approach grounded in empirical measurements of macroscopic Internet topology, traffic demand, routing policies, and peering policies. The data promise to reveal important, and thus far elusive, insights into the economic implications of topology dynamics, interdomain traffic characteristics, and routing policy, but they will also inform the parameterization of a model of network interconnection incentives and dynamics. <br\/><br\/>The broader impact of this project lies in deeper, empirically grounded interpretation of available data on the most opaque sub-discipline of network research -- internetwork economics. The educational side of the project will integrate Internet economics in two Georgia Tech courses, while a PostDoc and a PhD student will graduate as experts in this nascent sub-discipline of Internet research. The data and methods developed during the course of this project will be publicly available and regularly presented to both the research community as well as operator and policy forums, e.g., NANOG, FCC.","title":"NetSE: Small: Collaborative Research: The economics of transit and peering interconnections in the Internet","awardID":"1017139","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":["550447"],"PO":["564993"]},"168414":{"abstract":"Recent cancer genome sequencing projects and human genome-wide association studies (GWAS) have underscored the principle that complex phenotypes like cancer or disease susceptibility do not result from single DNA sequence variants in the same gene in all individuals. Rather, the inherited or somatic variants responsible for these phenotypes affect multiple genes in cellular signaling, regulatory, and metabolic pathways. New genome sequencing technologies are now providing measurements of these sequence variants in large numbers of samples, while other technologies are measuring whole-genome networks of interactions between genes. There is an urgent need for computational techniques to identify pathways, or groups of genes, that are associated to a phenotype.<br\/><br\/>This project will develop robust algorithmic and statistical techniques for four challenges in the analysis of DNA sequence variants in the context of known and novel gene-gene interactions. (1) Incorporating prior knowledge of gene interactions. This project develops a diffusion model to determine subnetworks of a genome-scale interaction network that are enriched for genetic variants across multiple samples. (2) Deriving robust statistical tests to overcome multiple hypothesis-testing problems in network analysis. Biological interaction networks containing tens to hundreds of thousands of nodes and edges have an enormous number of subnetworks that might be enriched for variants. This proposed work will design techniques to evaluate multiple candidate subnetworks with rigorous bounds on the false discovery rate. (3) Performing de novo identification of gene groups without an interaction network. The proposed work will examine combinatorial approaches to extract subsets of altered genes without prior knowledge of their interactions. These approaches will leverage the increasingly large number of sequenced samples that are becoming available. (4) Implementation of algorithms for evaluation on biological data from two applications: (a) somatic mutations identified in cancer genome sequencing studies, and (b) rare genetic variants in human association studies. These applications will be conducted in collaboration with two biomedical research groups. <br\/><br\/>Algorithms developed in this proposal will be implemented and released as open-source software for use by the biological and medical community. The project will partially support the training of graduate students, and undergraduates will be involved in implementing proposed algorithms. Finally, research from this project will use incorporated as pedagogical examples in multiple undergraduate and graduate courses.","title":"III: Small: Algorithmic Approaches for Pathway and Gene Group Analysis in Genetic Studies","awardID":"1016648","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["531608","531609"],"PO":["565136"]},"168777":{"abstract":"Providing restrictive and secure access to resources is a challenging and socially important problem. Security analysis helps organizations gain confidence on the control they have on resources while providing access, and helps them devise and maintain policies. There is a dire need for analysis tools to help administrators ensure security as they make administrative changes to reflect changes in policy. Security analysis of access control is non-trivial for an administrator due to the complexity of reasoning with the beguiling number of possible future scenarios. Techniques for the analysis of security in access control is in its infancy. The goal of this project is to go beyond decidability\/undecidability issues, and go forth to build scalable and usable security analysis tools and techniques when access control is deployed via the most commonly used role-based access control (RBAC) models or its spatiotemporal extensions.<br\/><br\/>The main thesis of this project is that finding breaches of security in an access control model is very similar to finding errors in a program. Some of the innovative expected results include: accurate mapping of the security problem for policies in access control as reachability problems in transition systems, including succinct discrete systems and automata with spatio-temporal constraints; scalable techniques to search for security breaches by exploiting the model-checking techniques developed by the program verification community; usable and useful tools for administrators to express policies and automatically find breaches of their security policies. The project helps in building technical bridges between the communities of access control security and formal methods in verification, which is expected to trigger a flurry of research, possibly unifying problems in the two fields, and initiating each other with new ideas. Scalable and usable security analysis will also serve needs in many settings including emergency, disaster management and homeland security applications. The tools will be included as modules in a tele-medicine system and an emergency management system. The integration of the ideas, techniques, and tools resulting from this project into the education curriculum will positively impact the quality of a newly trained workforce that is prepared to meet security challenges, making them aware of security issues in access control, and educating them on practical ways to check for breaches in security.","title":"TC: Small: Collaborative Research: Formal Security Analysis of Access Control Models and Extensions","awardID":"1018414","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["498231","543481"],"PO":["564588"]},"167215":{"abstract":"Simple contagion processes underlie various phenomena on complex networks, such as the spread of diseases on social-contact networks and information in communication networks; understanding their dynamics and developing control mechanisms are key issues in numerous applications. The goals of this proposal are: (i) Developing methods to construct synthetic relational networks using partial and noisy data; (ii) Understanding the structure of these networks and the contagion processes, and especially important network properties and typical patterns that have an impact on the dynamics of contagion; (iii) Developing techniques to control the spread of contagion processes, and to detect, prevent and arrest cascading failures in coupled socio-technical networks; and (iv) Understanding the co-evolution between the networks and dynamics, and using this to refine their models, and the strategies to control them.","title":"NetSE: Large: Collaborative Research: Contagion in Large Socio-Communication Networks","awardID":"1010789","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":[447923],"PO":["565090"]},"168788":{"abstract":"In this research the fusion of data from different sources will be used to describe an existing site. The fused data will be incorporated into a lightweight interactive design system that facilitates conceptual design. A method for making the transition from the output of the conceptual design phase to a full 3D model suitable for a CAD system used for construction documentation will also be developed. The work will introduce new methods for gathering and fusing data at an appropriate level of detail for design, rather than following a more traditional approach of creating detailed models and simplifying them for use in an interactive system. The proposed work can have a direct impact on the creative design process by offering a novel approach for creating and editing 3D form. This work is interdisciplinary and brings together research from computer graphics and architecture, computer vision, cognitive science, psychology and design and engineering.","title":"HCC: Small: Sketching Architectural Designs in Context","awardID":"1018470","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["548698"],"PO":["565227"]},"168315":{"abstract":"The objective of this research program is the development of privacy-aware design practices for information networks. The program is based on the recognition that an information network that collects personal information about its users can have both technical and moral implications, creating serious downstream consequences to the individual and to society. This program establishes the moral obligations of systems designers to protect consumers, creates rules and algorithms for technical designs that protect consumers? privacy rights, and develops educational practices for designers, consumers, and policy makers that further the use of these rules and algorithms. By developing a critical understanding of the technical and moral issues implicated in the use of data collection, the program motivates design practices that further the use of technology while supporting individual privacy and autonomy. <br\/>The privacy-aware design practices being developed constitute a design methodology that guides the practicing engineer\/computer scientist in the creation of mobile computing and communication systems that minimize the collection of data from users and the public at large. Design tools are being produced to facilitate the application of the design practices. These tools include algorithms for anonymous registration, authentication, and roaming. Case studies are being created to demonstrate the efficacy of the proposed rules and the supporting tools. These studies include designs for privacy-aware cellular telephone networks and power consumption monitoring networks for demand-response systems.","title":"TC: Small : Privacy-Aware Design Strategies for Mobile Communications and Computing","awardID":"1016203","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[450625,450626],"PO":["565264"]},"168557":{"abstract":"This project develops the ability to securely monitor host activities, which is the foundation of any host-based security application such as anti-virus software, firewalls, host-based intrusion detection systems, control flow analysis, taint tracking, and more. <br\/><br\/>The central idea is to use a hybrid approach that combines virtual machine introspection with secure in-VM monitoring to provide the necessary security, efficiency, and flexibility to be useful for a broad range of security applications. The main research activities address the foundational problems inherent to this hybrid architecture, and any other virtualization-based security architecture. Specifically, the project develops: (1) attestation and memory and data structure protection techniques to ensure the security of the trusted computing base (TCB) throughout the lifecycle of the system, (2) algorithms to locate security-critical data structures, reverse-engineer data structure semantics, and automatically generate semantic probes, and (3) algorithms and APIs for developers to divide a security application into in-VM and out-of-VM components. <br\/><br\/>This project also develops several security monitoring tools based on the hybrid approach, in particular, a system for user input monitoring that securely receives keyboard and mouse events and then determines what application will receive the events and how they will be processed. This tool is useful for classifying system activity as user-intended or automated. The automated activity can then be analyzed to identify potentially malicious host events (e.g., bot-generated traffic).","title":"TC: Small: A Foundational and Practical Platform for Host Security Applications","awardID":"1017265","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1668","name":"FED CYBER SERV: SCHLAR FOR SER"}}],"PIcoPI":["543138"],"PO":["565239"]},"168799":{"abstract":"The project will test the effectiveness of Virtual World Technology (VWT) for archaeological interpretation and dissemination. The research project is aimed at scholars and students of Roman archaeology, architecture, and culture, but it also has the potential to generate insights and results that archaeologists of other periods and cultures will find helpful as they adopt VWT. A testbed will be created which is a virtual world representation of Hadrian?s Villa (Tivoli, Italy). Hadrian's Villa is the best known and best preserved of the imperial villas built near Rome by its emperors. The project will attempt innovate use of VWT avatar observation as a form of scholarly research and teaching. Using this designed space, undergraduates will take on the role of particular avatars who will be required to maintain ways of being that are consistent with their defined role. Graduate students will observe their interactions as a means of testing and possible challenge the existing thesis about how this complex space functioned.","title":"HCC: III: Small Grant: Enabling the Use of Virtual Worlds for Research and Teaching in Archaeology","awardID":"1018512","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[451787,451788],"PO":["564456"]},"168447":{"abstract":"High-End Computing systems, such as cloud computing setups, are increasingly employing many-core compute resources and computational accelerators, e.g., GPUs and IBM Cell processors, for high performance. However, the use of such components results in a performance and communication mismatch, which in turn makes large-scale systems with heterogeneous resources difficult to design, build and program. Moreover, the increased data demand of modern advanced applications, coupled with the asymmetry between computation speed and data transmission speed, threaten the benefits of employing accelerators in such setups.<br\/><br\/>This project addresses the above problems by designing a flexible, scalable, and easy-to-use programming model, AMOCA. AMOCA supports innovative workload distribution techniques, which enables it to be used toward scaling modern scientific and enterprise applications on high-end asymmetric clouds comprising heterogeneous accelerator-type compute nodes. Moreover, AMOCA utilizes component-capability matching and adaptive inter-component data transfers for parallel programming models, automatically handles heterogeneous resources, and auto-tunes the model parameters to the specific instance of resources on which it is run.<br\/><br\/>AMOCA lays the foundation for adapting the cloud computing paradigm for HPC, creates open source and transformative technologies for scalable any-core system architectures, and is expected to improve the efficiency and performance of advanced applications in a broad range of disciplines that perform simulation-based experimentation including computational physics, biology, and chemistry. AMOCA employs an integrated research and education approach for training both undergraduate and graduate researchers, especially from underrepresented groups. The training will instill critical system development skills and increase the use of accelerator-based clouds in HPC.","title":"CSR: Small: Towards Realizing Cloud HPC: An Adaptive Programming Model for Accelerator-based Clusters","awardID":"1016793","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["468070"],"PO":["565255"]},"168568":{"abstract":"This project aims to deepen our understanding on two fundamental aspects of quantum information processing.<br\/><br\/>(1) Entanglement manipulations and classifications. Quantum entanglement plays a central role in quantum information processing. An objective of the theory of quantum entanglement is to classify different types of entanglement according to their inter-convertibility through manipulations that do not require quantum communication. While bipartite entanglement is well understood in this framework, entanglement among three or more subsystems is inherently much more difficult. The PI is investigating properties of multipartite, especially tripartite, entanglement, with an emphasis of the algorithmic\/computational complexity perspective. <br\/><br\/>(2) Communication complexity. Communication complexity studies the inherent communication cost for distributed computing. This project addresses three important and related open problems: the Log-Rank Conjecture for characterizing the deterministic complexity; finding the largest possible gaps between the quantum and classical complexities; and the question if entanglement can dramatically reduce the cost for quantum communication. The plan to attack those difficult problems is to focus on some restricted classes of functions that are simple yet on which the problems remain open and challenging.","title":"AF: CIF: Small: Theoretical Studies in Quantum Information and Computation","awardID":"1017335","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7948","name":"QUANTUM COMMUNICATION"}}],"PIcoPI":["549690"],"PO":["565157"]},"168337":{"abstract":"Rapid advancement of the wireless technologies provide new opportunities for mobile users to have easy access to real-time data, derive useful social information, and stay connected with business partners, colleagues and friends. Towards this end, mobile social networking applications have recently emerged to meet these needs. Current mobile social networking applications do not support advanced context-based services. Additionally, serious security and privacy concerns have been raised when accessing social networking applications either from fixed locations or on-the-go. This project aims to build a secure mobile information sharing system (SEMOIS) that supports secure and privacy-preserving real-time information sharing. SEMOIS has the ability to store secure data items with flexible access control at insecure storage nodes and enables users to send context-based messages with late-binding features. SEMOIS achieves data confidentiality and privacy-preserving through data encryption and encrypted search, and enables intentional name based message dissemination without apriori knowledge of recipients. Additionally, a set of smart learning methods are developed to extract short-term and long-term geo-social patterns from multimodal sensing data collected by mobile devices for social networking purposes, e.g., geo-social patterns are used to derive hidden communities. <br\/><br\/>Project results are expected to advance the state of the art techniques for supporting secure and privacy-preserving mobile social networks with a variety of innovative features. The project equips both graduate and undergraduate students with the necessary background and practical skills for survival in the emerging job market and further contributes to the development of the pervasive computing field. In addition, SEMOIS can be used by middle and high school students from the Tri-State area that participate in the CHOICES and NSF-funded STEM program organized by Lehigh University.","title":"CSR: Small: Collaborative Research: SEMOIS: Secure Mobile Information Sharing System","awardID":"1016296","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["517746"],"PO":["565255"]},"168579":{"abstract":"Modern software engineering is a highly social activity, which is in contrast aimed at producing technical artifacts. As a result, complex dependencies between the technical elements of software systems and the social structures of the developers that are tasked with their creation have a direct effect on software quality: in projects that exhibit a high degree of congruence between the design of a software system and the social communication structures of its developers, teams are more productive and systems contain fewer faults. This research project aims to create the necessary tool support so that software engineers can be made keenly aware of these socio-technical dependencies within the familiar context of their everyday development activities. By providing this information in this specific context ? at a time when it is useful and actionable ? the broader impacts of this research have the potential to dramatically transform software engineering habits and practices by bringing into sharper focus the existing and emerging socio-technical trends of a development effort, allowing developers to intervene when they diverge, and ultimately improving the quality of the software systems produced.<br\/><br\/>To achieve these objectives, this research project is grounded on the creation of an architecture-centric toolset that supports the analysis of social network patterns, designed to be an addition to the popular Eclipse development environment. This toolset will support continuous awareness of emerging socio-technical dependencies by collecting data and providing a host of displays that allow developers to visualize their project?s software architecture, dependencies between source code units, and the social network formed through analysis of developer communications. Key contributions and advances of this work include the novel integration of social aspects of development with the architecture of software systems, the provision of concrete socio-architectural congruence metrics, and the presentation of this information to developers during ongoing development efforts.","title":"SHF:Small:Collaborative Research: Supporting Continuous Awareness and Exploration of Social and Design Dependencies","awardID":"1017408","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["530348"],"PO":["564388"]},"168469":{"abstract":"The scaling of on-chip memory is tremendously challenged by the excessive amount of process variations and reliability degradation at the 22nm node and below. In current practice, full custom design and extensive experimentation on test silicon are often necessary to achieve the desired performance under all process, voltage, and temperature conditions. Although such an expensive approach is acceptable in today's chip design, it drastically reduces design productivity and predictability. The situation becomes even more severe when the ever-increasing nature of variations narrows the design window and exacerbates memory design complexity. This proposal aims to develop innovative methodologies that will enable fast sign-off of on-chip memory at the end of the silicon roadmap and beyond, through the seamless integration of predictive variability models, statistical sampling schemes, robust optimization algorithms, and efficient silicon characterization techniques. Furthermore, these new outcomes will be integrated into an online framework to statistically benchmark post-Si memory design, helping illustrate the diverse opportunities of memory design beyond the 10nm node.<br\/><br\/> This research effort will facilitate fundamental research on reliable design with unreliable components, enhance design productivity for a wide range of applications, and expedite statistical design solution for emerging nanoelectronic devices. In addition, through novel education curricula and web-based dissemination tools, this project will transfer the newly developed design knowledge to a diverse population of students, who will lead the creation of future nanoscale integrated systems of all types, from computation, communication, to consumer electronics.","title":"SHF: Small: Collaborative Research: Fast Sign-Off of Nanoscale Memory: From Predictive Device Modeling to Statistical Circuit Synthesis","awardID":"1016890","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":["531467"],"PO":["562984"]},"173760":{"abstract":"This CISE Special Projects award funds student support to participate in the first Networking Networking Women (N^2Women) Workshop. The workshop will be co-located with the 16th Annual International Conference on Mobile Computing and Networking (MobiCom 2010) and the 11th ACM International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc 2010. The N^2 Women workshop is scheduled to occur the day before the conferences allowing workshop participants to form connections that can be strengthened during the conferences. The purpose of the N^2 Women Workshop is to foster connections among the under-represented women in the communications and networking research fields. The N^2 Women workshop will be a one day event to provide networking researchers the opportunity to showcase their work and make connections with others. It will include participants from the industrial, governmental, and academic sectors and offer students an excellent opportunity to learn about emerging research topics and to get connected with others in the networking community. <br\/><br\/>The intellectual merit of the project resides in the contributions that the student participants will make to the N^2Women Workshop and the two associated conferences as well as the impact on the future scientific progress of the students' research. The exposure to the research-based technical content will inspire students to address the tough problems in their respective fields, with a potential for a long-term benefit of scientific progress for networking research.<br\/><br\/>The broader impacts of the project revolve around the development of the under-represented graduate students who are able to participate in the coordinated events through this travel support. The funds will allow the students to attend high-caliber technical presentations, be exposed to state-of-the-art research, and interact with leading researchers and fellow graduate students.","title":"N2Women 2010 Student Travel Grants","awardID":"1046300","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7640","name":"CPATH"}}],"PIcoPI":["546219"],"PO":["564181"]},"172462":{"abstract":"This project designs image analysis algorithms that extract salient image features, group images based on similarity of these features, classify groups according to a priori knowledge, and optimize algorithmic steps and parameters. The research team applies the algorithms jointly developed to the three collections of images; and reports accuracy and computational requirements over all of the image collections. The research activities address problems of individual and collective authorship via artistic, scientific and technological questions based on the datasets, and developing the corresponding image analyses leading to computationally scalable and accurate data-driven discoveries of salient and discriminating characteristics. More specifically, the project, (a) promotes the development and deployment of innovative image analyses targeting the problem of authorship and applied to large-scale data analysis; (b) fosters interdisciplinary collaboration among scholars in the humanities, computer sciences, and information sciences; (c) promotes international and domestic collaborations; and (d) leads to unique accuracy and computational scalability findings over a set of large, diverse digital collections made available over the grid to a significant body of researchers from complementary disciplines keen to learn from each other. The project is a part of international, multi-institutional and multi-disciplinary efforts that jointly explore authorship across three distinct but in some respects complementary digital dataset collections: 15th-century manuscripts, 17th- and 18th-century maps and 19th- and 20th-century quilts.","title":"EAGER: Digging into Image Data to Answer Authorship Related Questions","awardID":"1039385","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["554468","473994"],"PO":["564316"]},"173343":{"abstract":"This project funds approximately 20 graduate students at U.S. institutions to attend (1) the 16th Annual International Conference on Mobile Computing and Networking (MobiCom 2010) and (2) the 11th ACM International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc 2010). These two conferences, referred to as MobiCom\/Hoc, will be co-located this year. MobiCom\/Hoc (September 20-24, 2010 in Chicago, Illinois) are premiere conferences in their respective mobile computing fields. The intellectual merit of this project resides in the contributions that the student participants will make to (1) the MobiCom\/Hoc 2010 event and (2) the future scientific progress of the student's research. In addition to the technical paper presentations, MobiCom\/Hoc 2010 offers a wide variety of technical activities (e.g., keynote presentations, poster session, demos, and eight workshops). The exposure to the technical content of MobiCom\/Hoc 2010 will inspire students to address the tough problems in their respective field, with a long-term benefit of scientific progress for these types of networks. The broader impact of this proposal concerns the development of the graduate students that are able to attend MobiCom\/Hoc 2010 due to the provided travel support. The travel funds allows graduate students conducting research in the field to attend high-caliber technical presentations, be exposed to state-of-the-art research, and interact with leading researchers and fellow graduate students.","title":"MobiCom\/Hoc 2010 Student Travel Grants","awardID":"1043772","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["546219"],"PO":["557315"]},"173475":{"abstract":"The Association for Computing Machinery requests funding for a series of workshops for high school teachers from who are interested in placing and teaching Exploring Computer Science (ECS) in their schools. ECS is an entry-level, high-school computing course-developed for the Los Angeles Unified School District (LAUSD) that introduces students to the foundational, creative, interdisciplinary, and problem-solving nature of computer science. These workshops will expand the professional development opportunities for ECS beyond LAUSD. They will form a model for use in the next few years as ECS and the larger CS 10K Project, begin to scale for national impact. The PI was one of the developers of ECS and the workshops will be organized and run by the Computer Science Teacher's Association (CSTA).","title":"ECS Nationwide: A Professional Development Model for Exploring Computer Science Expansion","awardID":"1044540","effectiveDate":"2010-08-01","expirationDate":"2012-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7482","name":"BROADENING PARTIC IN COMPUTING"}}],"PIcoPI":[465403],"PO":["561855"]},"174344":{"abstract":"This project is developing a new programming model that simplifies the development of complex low-level system's code. In the new model, the programmer starts by providing simple diagrams and animations showing some of the high-level insight behind an implementation. After interacting with the user to clarify any potential omissions or inconsistencies, the system automatically derives a correct implementation. The interactions are designed to expose gaps in the programmer's reasoning, and to capture new insights in the form of diagrams, invariants, or simple unit tests. As a result, the tool helps the programmer achieve an improved understanding of the problem in addition to delivering an implementation.<br\/><br\/>The new programming model is supported by a new breed of software synthesis algorithms that work by framing the synthesis problem as a constraint satisfaction problem. In this approach, the different forms of input provided by the programmer are independently translated into constraints. At the high-level, the approach is fairly simple; the challenge lies in finding efficient representations for both the program and the constraints. The graphical formalisms in particular pose some interesting problems due to their many omissions, but a combination of abstract interpretation and demand-driven constraint generation make this problem tractable.<br\/><br\/>The new programming model embodies a human-centered approach to synthesis that could transform the way complex pieces of systems code are developed, and could show the way for a new generation of programming tools that combine formal methods with HCI to make programming easier and more reliable.","title":"EAGER: Human-Centered Software Synthesis","awardID":"1049406","effectiveDate":"2010-08-15","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["508201"],"PO":["565264"]},"174234":{"abstract":"The world is increasingly dominated by multimedia technology for communication, commerce, entertainment, art, education, and medicine. Since modern electronic media are rich in graphical and pictorial information, it has been hard for the population of visually impaired people to keep up. While some of this information can also be presented as speech or Braille text, the ability to directly present graphical and pictorial information in tactile form, in combination with auditory signals, would dramatically increase the amount of information that can be made available to the members of this large community, and would also drive advances in interfaces for diverse applications such as virtual reality and medicine. Although the tactile sense has to date received relatively little attention due to the lack of versatile devices, recent advances in tactile display technology provide impetus for new research. While existing tactile devices are mostly static, there is great promise for dynamic devices based on emerging technologies such as electro-active polymers, pneumatics, and MEMs. Dynamic devices would make it possible to generate and display arbitrary tactile patterns, but to estimate the capabilities of different device configurations it is important to understand and model the device characteristics and how they relate to human perception. It has been shown that the relevant characteristics (material, surface shape) can be simulated using accurate static physical models. This sets the stage for potentially transformative research to enable the presentation of graphical and pictorial information in tactile-acoustic form, by exploiting the capabilities of tactile display devices in combination with the abilities of human tactile and auditory perception. To reach that goal will require the investigation of fundamental issues in tactile perception as it relates to existing devices or the design of new ones, and the study of fundamental relationships among visual, tactile, and auditory perception. The PI's objective in this exploratory project is to conduct preliminary work along these lines in order to establish the feasibility of the approach. To this end, he will develop mathematical models for tactile devices and perception, and conduct experiments to validate them. Research subtasks will include development of algorithms for synthesizing tactile textures, development of structural similarity metrics for visual, tactile, and acoustic textures, and quantitative description of perceptual dimensions of visual, tactile, and acoustic textures. Tests with sighted (visually blocked) and visually-impaired people will measure our ability to discriminate among tactile patterns with and without acoustic feedback, identify dimensions of tactile texture perception (e.g., roughness, directionality), establish that pattern labels can be learned with and without acoustic cues, and explore the brain's ability to integrate tactile information into a scene.<br\/><br\/>Broader Impacts: This research will contribute to fundamental advances in sense substitution and the use of touch for human-computer interaction. It will address fundamental problems in visual, tactile, and acoustic texture analysis and perception, and the use of touch for communication of graphical and pictorial information. Project outcomes will contribute to a deeper understanding of the sense of touch and its relation to vision. In addition to ultimately enabling visually impaired people to access pictorial information, the research will have an impact on a number of other areas, including virtual reality, interfaces with tactile feedback, product design, and medical applications.","title":"EAGER: Visual, Tactile, and Acoustic Signal Analysis and Perception for Tactile-Acoustic Display","awardID":"1049001","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[467427],"PO":["565227"]},"185135":{"abstract":"Biological systems are the product of an evolutionary process of random tinkering and selection that resulted in unexpected and non-intuitive ?engineering? solutions to dynamically varying conditions. Thus, biological systems are robust, adaptive and evolvable information processing systems that operate asynchronously and in parallel on multiple scales. The examination and characterization of the design principles of biological circuits has the potential to revolutionize biology, medicine and the way computing and communication systems are built. This project is pioneering important advances at the interface between biology and computation by pursuing two complementary goals: (1) to develop a modular, parallel-ready simulator to replicate the multi-scalar architecture of complex biological systems; (2) to discover key design principles relevant to information processing systems in general by reproducing biological design in silico. <br\/><br\/>Information processing by cells encompasses multiple scales connecting molecular events to phenotypes. Current simulation techniques have limited multi-scale and modular capabilities, resulting in models that describe only a single feature of a given system and miss the relationships between architecture, function and behavior. This research effort addresses these limitations by representing biological systems as a hierarchy of functional executable modules. The design of the platform obeys four basic principles: 1) components are objects; 2) objects are governed by rules; 3) rules include some degree of stochasticity; and 4) objects and rules are organized in functional and spatial modules that compose a hierarchy. The development of the new platform is driven by the construction of simulations of key biological model systems with an unprecedented scope and precision, such as bacterial chemotaxis, epidermal growth factor receptor signaling, the acute inflammatory response, and parallel processing by bacterial colonies. The reproduction of these biological systems in silico is providing insights into their design principles, which in turn advances the future design and implementation of distributed technological systems.","title":"Emt\/bsse: Hierarchical Representation And Simulation Of Modular Cellular Systems","awardID":"1138292","effectiveDate":"2010-08-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[496754],"PO":["565223"]},"168822":{"abstract":"The management of energy consumption is a key driver in the design of modern computing platforms, from mobile devices (where lower consumption leads to better battery lifetimes) to desktop and servers (where lower consumption leads to significant decreases in electricity costs and heat dissipation). However, despite numerous advances in low power design technologies, the power consumption of the IT industry remains significant and worse, is slated to grow in the coming years. The prospect of growing energy demands for computing systems has implications for the economy, the environment and for society in general. The broad goal of this research is to make it easier to reduce the energy consumption of computer systems, while still maintaining their functionality.<br\/><br\/>An emerging way to reduce energy consumption is to leverage heterogeneity in the choice of individual components to tradeoff power consumption versus performance. The key idea of such \"Collaborative Heterogeneous\" systems is to use low-power low-performance helper platforms in lieu of high-performance high-power parts whenever feasible, thus keeping the high-power subsystems shutdown to save energy. To unleash the full potential of Collaborate Heterogeneous systems, this research will develop techniques that can automatically generate shrunk versions of applications that are suitable for running on low-power resource-constrained helper platforms. This research will lead to new techniques for application shrinkage, to better tools for measuring energy savings, and to an empirical evaluation of the energy savings made possible by Collaborative Heterogeneous systems. The techniques developed in this research will also be useful for generating \"shrunk\" versions of applications for use in heterogeneous multi-core Architectures and for running on portable energy-constrained devices like SmartPhones.","title":"SHF: Small: Application Shrinking for Reducing Energy Consumption","awardID":"1018632","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["521726",451845],"PO":["564588"]},"167975":{"abstract":"Supervised learning is used to infer an unknown regression function from a set of training data. Applications include time-series prediction, remote sensing, medical image analysis, computer vision, face detection, text categorization, image scene classification, speech recognition, and bioinformatics. Existing learning algorithms have several drawbacks. For example, the set of basis functions usually involves unknown parameters that need to be determined empirically. The optimization problem defined by the learning task depends on a trade-off parameter that requires careful selection. Analytical performance of the existing learning methods is not well studied and the role of the set of basis functions is not yet clear. In addition, currently there is no effective way to select the set of basis functions.<br\/><br\/>The investigator develops a new framework of sparsity-enforced learning for regression function learning to overcome the drawbacks of existing approaches. This framework includes numerical algorithms for sparse vector estimation, tight bounds for performance analysis, and optimization procedures for dictionary design. A flexible form for the inferred function is a linear combination of basis functions, which are constructed by discretizing the unknown parameters. Algorithms are designed to learn the weight vector by convex sparsity enforcing, hierarchical Bayesian modeling and normalized maximum likelihood principle. The discretization and the sparsity enforcement enable automatic selection of most relevant basis functions with appropriate parameters. The investigator analyzes the performance of the learning framework by deriving tight bounds on the mean-squares error of the learned weights, in particular, the Cramer-Rao bound and Hammersley-Chapman-Rob bins bound. The performance bounds (and their simplifications) are employed to optimally design the overcomplete dictionary. The investigator uses the developed framework to model time series and extract knowledge from images.","title":"CIF: Small: Algorithms, Performance and Design for Sparsity-Enforced Learning","awardID":"1014908","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[449854],"PO":["564898"]},"168602":{"abstract":"This award aims at developing strategies that allow mobile robots and sensor networks to perform their tasks near-optimally, or at least competitively. The tasks considered in this proposal deal mainly with exploration, patroling, guarding and searching. Typical tasks are that a group of robots should explore an unknown structure consisting of rooms which are connected by passages, or that a mobile guard should patrol a set of corridors, so that he can look down each corridor once during his patrol round. A typical problem involving sensors is to place sensors with a limited sensing range in a region of interest in such a way that any intruder can move only a small distance without being discovered by a sensor. For each of these problems, a strategy needs to be developed, its worst-case performance needs to be analyzed, and it needs to be compared with the unknown optimal strategy, to show that the proposed strategy is nearly optimal. Some of the strategies developed in this project will be implemented and tested by a cooperating robotics lab.","title":"AF: Small: Strategy Problems for Robots and Sensor Networks","awardID":"1017539","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":[451303],"PO":["565157"]},"175015":{"abstract":"The research objective of this EArly-Concept Grants for Exploratory Research (EAGER) award is to create visualization refinements to an existing methodology that will support virtual protein manipulation as an experimental framework for formulating design theory. The investigators will integrate insights from known constraints on mechanisms to create more efficient protein folding algorithms so as to enable nano-machine design. In this high-risk, high-pay-off project, the process will be visualized on high performance computing platforms that have been designed for protein folding. Spline models of knots - inclusive of their polygon control structures - will be visualized. The user will be able to quickly conduct extensive 'what-if' visual experiments to understand the 3D design ramifications of differing protein manipulations. The visualization capabilities will prompt new intuition and novel conjectures by focusing attention on the design properties arising from the inherent crossings and self-intersections. The designer will be able to efficiently explore a much richer design space than can be achieved by existing algorithmic methods. Hence, visualization, engineering design, and high performance computing form the foundations for this experiment to improve protein folding algorithms. <br\/><br\/>If successful and the performance improvements that have already been shown within small scale prototypes can be transferred to realistically complex models, then this work would have significant impact across numerous communities, with substantial impact in industry. These algorithms are currently the subject of extensive study throughout the bio-molecular, medical and pharmaceutical communities. There will also be a broader impact in educating students, as the students involved will have the opportunity to work with realistically complex and voluminous data sets from the industrial partner, IBM. Students will also be exposed to the complexity of the state of the art high performance computing platforms, such as IBM is providing.","title":"EAGER: Visualization of Protein Folding for Nano-Machine Design","awardID":"1053077","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1464","name":"ENGINEERING DESIGN AND INNOVAT"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"1504","name":"GRANT OPP FOR ACAD LIA W\/INDUS"}}],"PIcoPI":["508607","482465","513290"],"PO":["562900"]},"164488":{"abstract":"Understanding how people make decisions in complex settings is crucial in many application areas, including marketing, intelligence analysis, and political decision making. Traditionally, human decision-makers are modeled as rational agents seeking to maximize some mathematical measure of utility. In fact, however, people are overwhelmed in information-rich environments, and have developed cognitive and emotional strategies to navigate such environments. One such strategy is \"motivated reasoning\", where information is first evaluated subconsciously for emotional content, with the goal of maintaining an existing emotional commitment, and cognitive processing of the information is then conditioned on this emotional evaluation. Political scientists have demonstrated motivated reasoning in evaluation of both candidates and issues. In general, decision-making and information-gathering are strongly influenced by emotion, prior knowledge, and the social communities to which a person belongs. Evidence suggests that accurate models of human decision-making must be complex enough to model not only utility, but prior knowledge and beliefs, human cognitive abilities, and social context. Building such cognitive models requires substantially extending the state-of-the-art in machine learning.<br\/><br\/>In the past, the ability of researchers in political psychology to develop such complex models of decision-making was limited by the amount of data obtainable obtain from surveys or human-subject experiments. The recent explosion of on-line political communities provides an opportunity to overcome this limitation. We will model human behavior for socially-driven information gathering and decision-making tasks - specifically for political decisions - by combining human-subject experiments with analysis of large datasets of social media and social interactions.","title":"Collaborative Research: SoCS: Analysis of Social Media Driven By Theories of Political Psychology","awardID":"0968481","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["533936"],"PO":["564456"]},"168613":{"abstract":"This project falls within the discipline of computational complexity, which studies the power and limitations of efficient computation. The area develops models that represent the various capabilities of digital computing devices. It aims to determine which transformations can be realized in such a way that the amount of time, memory space, and other resources scale moderately with the input size.<br\/><br\/>Of central importance is the class of so-called NP-complete problems. The latter contains thousands of computational problems from all branches of science and engineering that have been shown equivalent in the sense that an efficient algorithm for one implies such an algorithm for all. The P vs NP question asks whether efficient algorithms exist for these problems. It constitutes the main open question in theory of computing and is one of the seven millennium prize problems proposed by the Clay Mathematics Institute as grand challenges for the 21st century. A positive answer would open up tremendous possibilities that would affect most human endeavors. On the other hand, it would also yield a way to break the cryptographic systems that are currently in use and, in fact, imply the impossibility of secure communication over the internet.<br\/><br\/>This project fits into the quest to settle that fundamental and important problem. In particular, it establishes a tight connection between that question and the amount by which instances of NP-complete problems can be efficiently compressed without affecting their solvability.<br\/><br\/>If P=NP, then NP-complete decision problems can be efficiently compressed to a single bit. On the other hand, under a hypothesis that is somewhat stronger than P<>NP, the PI has established that NP-complete problems like satisfiability and vertex cover do not allow any nontrivial amount of compression. The approach hinges on the existence of high-density subsets of the integers without arithmetic progressions of length 3. This project further develops that approach and investigates its implications for other computational parameters of interest. The project also involves a systematic study of the use of high-density subsets of the integers without arithmetic progressions of certain lengths in computational complexity, and the development of new applications.<br\/><br\/>The above construction handles deterministic compression schemes. For cryptographic and other reasons the extension to the randomized setting is of interest. One possible approach involves derandomization. In this context the project investigates the potential of typically-correct derandomization, where one aims for efficient deterministic simulations that behave correctly on most but not necessarily all inputs of any given length.","title":"AF:Small: Applications of AP-free sets and derandomization","awardID":"1017597","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":["565082"],"PO":["565157"]},"167645":{"abstract":"Reading comprehension deficits are pervasive for a disproportionate number of post-secondary students. These students have cognitive impairments that impact high level text processing skills and result in diverse reading profiles with difficulties in skills such as discerning between relevant and irrelevant information, drawing inferences, connecting background knowledge to new learning, and retaining and applying what was learned at a later date. Typically such deficits are managed by teaching the use of study-skills strategies. While there is strong face validity for these practices there is a lack of evidence-based practice, and virtually no information on candidacy or what types of deficits respond best to what types of strategies and supports. On the technology side, the popularity of electronic reading tablets offers a platform to deliver supports to improve reading comprehension and retention that could be adopted by college students. This project seeks to bridge the gap by developing the technology to support a diverse set of reading strategies in a highly adoptable form for college students with high-level reading impairments, by doing the science necessary to define a process that can assess each individual student, and by prescribing a set of strategies that eventually will be delivered in a hardware-software package. By using an iterative design process and a participatory action research model, this research will make the following contributions: a dynamic assessment process that matches reading profiles\/impairments to strategy supports; a mapping of reading impairments to reading strategies; translation of reading strategies to delivery on electronic reading tablets; a demonstration that personalization and adaptation are possible using our software engineering models; and a dissemination package that uses open source software and hardware to deliver a research tool that could be used by companies designing commercialized reading tablets. To achieve these goals, the PI will partner with three institutions that have large populations of struggling readers in post-secondary educational settings: two VA facilities that support and train veterans returning to educational settings, and a student disability services program at a large urban state university. These groups have experience with the pervasive, high level reading challenges preventing educational success, and provide the natural contexts to evaluate the PI's models and shape the tools generated from this research. Pilot studies, laboratory experiments and longitudinal studies will be employed to develop and evaluate the technology for a dynamic reading assessment and support tool.<br\/><br\/>Broader Impacts: A growing population not able to meet the reading demands of college and community college courses is the large number of veterans returning form Iraq and Afghanistan seeking education and training benefits. It is estimated that 15-20% of these veterans have suffered mild brain injury sufficient to affect academic ability. Another large group of post-secondary students that is challenged by difficulties with reading comprehension are those with developmental conditions including adult attention deficit and hyperactivity disorder (ADHD) and attention deficit disorder (ADD). Estimates regarding the number of students enrolled in colleges who report clinically significant ADHD or ADD symptoms vary between 2% to 8%; approximately 25% of students receiving disability support services are receiving those for ADHD. The PI expects two important outcomes from this work: the science missing from the literature that links reading impairments with reading strategies; and a demonstration tool, built on the science, that supports an assessment process and a delivery mechanism. With this latter outcome in mind, the PI intends to devote a large part of Year 5 of the project to making his tool highly attractive to companies who have the infrastructure to deliver products to the target populations.","title":"HCC: Large: Collaborative Research: Delivery of Personalized Reading Strategies for People with Cognitive Impairments in Post-Secondary Settings","awardID":"1013054","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[449118,449119],"PO":["565227"]},"168503":{"abstract":"This project studies methods for describing motion in video. All visible points in the world are tagged by their identity, and trajectories of their projections on the image plane are tracked through space and time. This computation is performed globally, both in space and time, and motion discontinuities are explicitly delineated in the output. In contrast with previous techniques, which estimate motion primarily from the bottom up, starting with two frames at a time, the box of data from a video camera is carved up into tube-like regions whose shapes capture information about the motion and deformation of the objects visible in the scene. Novel methods include the projection of all visual motion onto a sparse basis of point trajectories through low-rank matrix data imputation; the use of L1 regularization in a function space that preserves boundaries; the generalization of robust estimation methods from variational calculus and quadratic programming for the efficient computation of tubes and occlusions in the multi-frame case; and several domain-specific techniques for initializing general but local optimization methods close to the global solution. The resulting descriptors enable video retrieval, medical diagnosis of heart rhythm anomalies, assessment of performance in sports, sign language recognition, traffic monitoring, surveillance, and more. The project also forms the basis for a new class on experimental methods for computer vision, the materials of which are made available online.","title":"RI: Small: The Shape of Visual Motion","awardID":"1017017","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["513187"],"PO":["564316"]},"168745":{"abstract":"Software developers are increasingly building applications in multiple languages both to reuse existing software libraries and to leverage the languages best suited to their problems. In fact, multilingual programs are already prevalent, as essentially all programs written in Java use code written in C as well. But developing multilingual programs poses more challenges and difficulties than developing single language programs. Current programming interfaces between multiple languages tend to be tedious and unsafe; and most programming tools only support a single language. As a consequence, real-world multilingual programs are full of cross-language bugs.<br\/><br\/>This research seeks to substantially improve the correctness and development of multilingual programs. The investigators build on their prior multilingual language design (the Jeannie Java\/C language), compilers (the xtc Jeannie compiler), and debuggers (the Blink Java\/C debugger). Here, the investigators are designing and implementing novel approaches for validating the safety of existing multilingual interfaces and for creating safe multilingual programs. (1) The investigators are developing a framework for concisely capturing safety rules for multilingual interfaces and then automatically synthesizing the corresponding dynamic checker. (2) The investigators are exploring novel program analysis to refactor programs automatically that use unsafe multilingual interfaces into programs that use safe multilingual interfaces. If successful, the research impact will be improved correctness, efficiency, and reliability of multilingual programs.","title":"SHF: Small: Collaborative Research: Languages and Tools for Multilingual Systems","awardID":"1018271","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["519591"],"PO":["564588"]},"168866":{"abstract":"Two principal components for providing protection in large-scale distributed systems are Byzantine fault-tolerance (BFT) and intrusion detection systems (IDS). BFT is used to implement strictly consistent replication of state in the face of arbitrary failures, including those introduced by malware and Internet pathogens. Intrusion detection relates to a broad set of services that detect events that could indicate the presence of an ongoing attack. IDSs are far from perfect -- they can both miss attacks or misinterpret events as being malicious. In addition, IDSs themselves are vulnerable to attack. These two components approach different parts of system security. Each, however, has the potential to improve the other, which is the theme of this project. The integration of these two efforts, at both the fundamental and system levels, has proven elusive. Fault-tolerant distributed algorithms have been designed to use failure detectors for some time, but only as an abstraction. Intrusion detection has been, for the most part, a service that gives some general improvement in system security. Attempting to marry these two approaches could be a large step towards making BFT a truly practical approach in multisite systems, and gives a novel way to integrate multiple IDSs to improve the security in a multisite system with nonuniform and varying trust. <br\/><br\/>Some examples of such benefit are (1) Any evidence gathered by BFT about suspicious behavior can be useful for an IDS, since it could indicate that the system has been compromised. (2) Information from an IDS can be used by BFT to influence its behavior towards the servers of the replicated service. This could, for example, allow BFT to stop using a site even though the service has not (yet) been affected, or to assume a more benign set of failures for a site that appears to be well managed. (3) The way that BFT reacts to suspicious behavior is a complex policy that could, at least in part, be moved to IDS. Doing so would allow the policy to be tuned. (4) A further detection method is to compare the internal suspicions of BFT with the external suspicions of the IDS. (5) BFT can be used to detect and cope with attacks on an IDS. (6) IDS can confirm that parties in a BFT set are behaving according to the BFT protocol which if so can improve the performance of a BFT system. This research explores this potential of a merged system by developing a version of BFT for wide-area networks that is designed with several IDSs as part of the architecture. The IDS will serve as a suspicion detector that allows BFT to define sets of sites that trust each other, and can thus use a lower latency protocol among them. The IDSs will use BFT to agree upon detection states to make more useful detections. Information collected by BFT will be used by the IDS to detect malicious behavior. And, BFT and IDS will, where possible, check each other to increase the detection power of the system. A prototype of the system will be implement and a simple synthetic application to measure performance and sensitivity to a set of simulated attacks will be built.","title":"TC: Small: Collaborative Research: Symbiosis in Byzantine Fault Tolerance and Intrusion Detection","awardID":"1018910","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[451956,"525660"],"PO":["565264"]},"165478":{"abstract":"The goal of this project is to engineer the Creative Artificially-Intuitive and Reasoning Agent CAIRA, an intelligent computer system designed for making decisions and producing creative work based on both reasoning and intuition ? the latter being the process of making a spontaneous decision when there is not sufficient time for reasoning. A network of humans and several CAIRA agents will be formed and studied to provide new understanding of learning and social interaction in dynamic environments where boundary conditions change over time and decisions have to be made within a given time interval. The project will use improvised avant-garde music to study the performance of the intelligent agent, but the outcome will be applicable to other time-based communication scenarios that require creative solutions and intelligent agents that can communicate naturally, as humans do. The machine listening abilities of CAIRA will be based on a music recognition system that simulates the human auditory periphery to perform an Auditory Scene Analysis (ASA), the process by which the auditory system extracts features such as pitch, timbre, and rhythm to organize sound into perceptually meaningful units. The simulation of cognitive processes will include a comprehensive cognitive calculus for reasoning and decision-making, and the system will also possess metareasoning, that is, an entity's ability to reflect upon its own reasoning. CAIRA's capacity for metareasoning will provide groundbreaking insight into the human willingness to systematically break rules and weigh the consequences in order to achieve a given goal, for example, to produce creative work. A machine correlate to human intuition will be another key component of CAIRA, and a major goal is to find out how different balances of reasoning and metareasoning on the one hand, and intuitive decisions, on the other hand, will lead to novel and captivating forms of creativity. <br\/><br\/>Broader Impact: Based on a unique interdisciplinary approach between scientists and artists, and also engaging students from engineering and other fields in project-related courses, the project will help to better understand the process of artistic creation as a product of balancing different sources of creativity (e.g., rule-based reasoning approaches vs. intuitive decision-making). The general architecture of CAIRA will be useful for various musical applications, including automated composition systems and internet-based music performances with co-located ensembles. CAIRA will also provide an enhanced interaction with creative machines that will allow non-specialists and ability-impaired users to engage in expressive musical creation. Two examples include CAIRA's ability to automatically add musical texture or an accompaniment to a simple melody or to serve as a duet partner. Beyond this, the model will further our understanding of the human mind's ability to balance creative decisions based on spontaneity and immediate perception of an environment, to factor in environmental and social context, and to consider past training and learned stylistic rules in the creative process. This understanding will be applicable to similar complex problems, such as economic systems and semantic web services, as well as other problems where creative and timely solutions and\/or natural human\/computer communication are essential.","title":"Major: CAIRA - A Creative Artificially-Intuitive and Reasoning Agent in the Context of Ensemble Music Improvisation","awardID":"1002851","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["550695",443287,443288],"PO":["565227"]},"174069":{"abstract":"This project focuses on developing the infrastructure for a self-sustaining organization that can manage, grow, and evangelize olympiads that involve young students (middle and junior high) in computational thinking. A large component is the creation of pilot olympiads in a select few cities in the United States. Specific goals of this project include: (1) identifying a set of foundational skills that underlie computational thinking that can be taught before college and high school; (2) identifying a style of problems and scenarios that engage a wide variety of students; and (3) implementing a curriculum of training sessions and contest questions that exemplify those foundational skills. <br\/><br\/>There are two broad reasons for creating a Computational Thinking Olympiad. First, to expose the fundamentals of computational thinking to a broad audience of potential researchers and practitioners in the field, thus increasing participation and diversity in computing. Second, to ensure long-lasting impact beyond of this project. <br\/><br\/>The success of the Computational Thinking Olympiad will have a significant impact on our society by introducing middle school students to computational thinking in its breadth and depth: (1) encouraging students to have fun with the computational thinking in an arena that is both cooperative and competitive; (2) encouraging students to pursue education in computing; (3) introducing the unplugged parts of computing to those who have not had access to the plugged-in parts; and (4) showing that computational thinking is not ``just'' programming.","title":"Collaborative Research: EAGER: Computational Thinking Olympiad","awardID":"1048401","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["550878"],"PO":["565215"]},"168514":{"abstract":"Verifying temporal properties of non-linear hybrid systems, such as embedded control and mixed-signal systems, is currently one of the hardest challenges in verification research. In practice, these systems are \"verified\" using simulations. Nevertheless, extensive simulation is well known to be inadequate for guaranteeing safety. Harmful defects often remain undetected. These defects may manifest themselves during deployment as rare events with a tiny, but non-zero chance of occurrence.<br\/><br\/>This project investigates stochastic verification techniques for embedded and mixed-signal systems based on rare event simulations. Rare event simulations have been used successfully in areas such as mathematical finance, reliability theory and queuing theory for analyzing events in stochastic models with vanishing probabilities. This work adapts ideas from rare event simulations and extreme value theory to detect property violations in non-linear hybrid systems. Furthermore, our research revisits fundamental concepts such as the Boolean semantics of temporal logics. It investigates real-valued metric semantics of temporal logics, which generalize the standard true-false interpretation over simulation traces.<br\/><br\/>The research program is expected to yield useful tools for verifying embedded and mixed-signal systems. These tools can be readily integrated inside model-based development environments. As a result, the techniques that are being investigated will be directly applicable to embedded control and mixed-signal systems to improve the reliability of the designs. Additionally, the research outcomes are being included in course curricula centered on the application of semi-formal testing techniques to various software\/hardware engineering applications for undergraduate as well as graduate students.","title":"SHF: Small: Collaborative Research: Statistical Techniques for Verifying Temporal Properties of Embedded and Mixed-Signal Systems","awardID":"1017074","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["562871"],"PO":["565255"]},"168635":{"abstract":"Analysis of multiple sets of data, either of the same type as in multi-subject data, or of different type as in multi-modality data, is inherent to many problems in computer science and engineering. Biomedical image analysis figures prominently among these and is particularly challenging because of the rich nature of the data made available by different imaging modalities. Data-driven methods are particularly attractive for the analysis and fusion of such data as they can achieve useful decompositions while minimizing assumptions on the model and underlying processes, and can also incorporate reliable prior information when available. One such approach recently introduced for medical image analysis and fusion is multi-dataset canonical correlation analysis (MCCA) that has proven especially useful for the analysis and fusion of rather disparate data, owing to its high flexibility and extendibility to a wide array of problem settings.<br\/><br\/>Intellectual Merit: In this proposal, the main aim is twofold. First, a number of powerful methods are developed for multi-subject (multi-set) data analysis and multi-modal data fusion based on canonical dependence analysis by significantly extending the power and flexibility of MCCA. Then, the successful application of the methods are demonstrated on a unique problem that demands these properties, namely the study of brain function and functional associations during simulated driving, a naturalistic task where data-driven methods have proven very useful. The data used in the project are complementary in nature but of very different nature: functional magnetic resonance imaging (fMRI), electroencephalography (EEG), structural MRI (sMRI), genetic array data--single nucleotide polymorphism (SNP)--and behavioral variables. The rich characteristics of the data and the problem at hand thus provide a special challenge for the methods developed and a unique testbed for the evaluation of their performance.<br\/><br\/>Broader Impacts: The broad impact of the proposed work lies in its potential to substantially impact science and information technology as well as in its educational features. Analysis of multiple datasets of the same type as well as fusion of data from different modalities\/sensors is a key problem in many science and engineering disciplines. The new set of methods proposed thus form attractive solutions for many other problems beyond brain function analysis. The fully integrative nature of the proposed work is also an invaluable asset in the ongoing efforts in cross-training of students and researchers as well as increasing the participation of underrepresented groups in science and technology careers.<br\/><br\/>For further information, see the project web site at the URL:<br\/> http:\/\/mlsp.umbc.edu\/research_projects.html","title":"III: Small: Collaborative Research: Canonical Dependence Analysis for Multi-modal Data Fusion and Source Separation","awardID":"1017718","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["486076"],"PO":["565136"]},"168756":{"abstract":"Eigenvalue problems arise in many diverse areas of science and engineering, including, but by no means limited to nuclear magnetic resonance, signal processing, speech processing, and control theory. This project will build on previous algorithmic innovations to further accelerate the numerical solution of one class of these important problems. This research is typical of the field of Numerical Linear Algebra in that it is a blending of mathematics and computer science. Specifically, new deflation strategies, which should allow large eigenvalue problems to be broken into smaller sub-problems, will be implemented and tested. If successful, a production quality code will be submitted to netlib.org for inclusion in the Linear Algebra Package (LAPACK), an open-source repository of reliable scientific computational software.","title":"AF: Small: RUI: Eigenvalue Computation via the QR algorithm: Advanced deflation techniques","awardID":"1018322","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[451677],"PO":["565157"]},"168877":{"abstract":"Authentication is a quintessential problem in computer security. However, most commonly used authentication mechanisms suffer from a variety of shortcomings. Passwords, the most common authentication mechanism, are vulnerable to replay attacks. Physical authentication tokens overcome some of these problems; however, they face deployment and compatibility obstacles.<br\/><br\/>Recently, advanced mobile devices have become widely available. These devices provide new hardware capabilities, such as MIMO (multiple-input and multiple-output) radio, a variety of sensors, and hardware authentication modules.<br\/><br\/>We investigate how to take advantage of hardware capabilities in advanced mobile devices to design better authentication mechanisms. We are focusing on three types of hardware capabilities. First, certain mobile devices authenticate to their networks via built-in hardware modules. We investigate how to leverage such existing authenticating infrastructure for other authentication tasks, such as authenticating users to websites. Second, mobile devices have MIMO radio transceivers. We investigate how to use these transceivers to pair nearby mobile devices without requiring the user to enter shared secrets into the devices. Finally, many advanced mobile devices have sensors, such as accelerometers. We investigate how to design gesture-based user authentication using accelerometers.<br\/><br\/>The impact of this project will be highly visible. Most current authentication mechanisms have various security, usability, and deployment problems. Our new mechanisms overcome many of these problems. Since these mechanisms are based on hardware capabilities that are increasingly common in mobile devices, they can be deployed to billions of mobile users to make their authentication tasks easier and securer.","title":"TC: Small: Designing New Authentication Mechanisms using Hardware Capabilities in Advanced Mobile Devices","awardID":"1018964","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["502032"],"PO":["565327"]},"168437":{"abstract":"This project studies methods for extracting accurate knowledge bases from the <br\/>Web. Fully-automated Web information extraction techniques are massively <br\/>scalable, but have accuracy and coverage limitations. This proposal <br\/>investigates how to improve automated extraction techniques by introducing <br\/>carefully-selected human guidance. The proposed system continually extracts <br\/>knowledge from the Web, along the way dynamically synthesizing and issuing <br\/>queries to humans to increase the accuracy of the system's knowledge base and <br\/>extractors.<br\/><br\/>The approach extends the PI's previous work utilizing statistical language <br\/>models (SLMs) for information extraction. Novel SLMs are investigated for <br\/>unifying the extraction of relational data expressed in Web tables with <br\/>extraction from free text. New active learning techniques utilize the models <br\/>to identify \"high-leverage\" queries -- requesting, for example, textual <br\/>extraction patterns that when retrieved from the Web yield thousands of novel <br\/>extractions. The queries investigated are mostly amenable to non-experts, <br\/>meaning that much of the human input can be acquired at scale via online <br\/>mass-collaboration.<br\/><br\/>The broader impact of this project lies in the potential for accurate Web <br\/>extraction to radically improve Web search, allowing users to answer <br\/>complicated questions by synthesizing information across multiple Web pages. <br\/>In domains like medicine and biology, mining extracted knowledge bases could <br\/>lead to important discoveries and novel therapies.<br\/><br\/>Further information may be found at the project web page:<br\/>http:\/\/wail.eecs.northwestern.edu\/projects\/activelms\/index.html","title":"III: Small: Active Learning of Language Models for Information Extraction","awardID":"1016754","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["475349"],"PO":["560586"]},"168338":{"abstract":"This multi-disciplinary research is at the intersection of control engineering and computer science. Specifically, methods from control and dynamical systems theory are being used to address challenges in real-time embedded systems. The central objective of the research is to develop next generation verification algorithms for real-time embedded systems. The modeling and analysis framework for determining robustness of embedded systems is based on the stochastic robustness framework, where the binary notion of robustness is discarded and the notion of a risk-adjusted robustness margin tradeoff is adopted. Key elements in this verification-based research include modeling of parametric uncertainty in embedded systems and development of computational tools for accurate prediction of uncertainty in real-time systems; robust performance analysis of various scheduling algorithms; and impact of various models of computations (anytime, imprecise, interlaced) on system level robustness. Uncertainty propagation tools will use methods based on Monte-Carlo techniques, polynomial chaos, and transfer operators (such as Perron-Frobenius and Koopman operators). Tools developed in the area of multi-rate robust control will be used to analyze the effect of various models of computation on robustness of the system. It is expected that successful completion of this project will enable accurate assessment of reliability of embedded systems across a wide array of engineering disciplines. This research project will also train a new generation of researchers with a mixed background in dynamical systems, control theory, and computer science. This is aligned with current and future research and industrial needs.","title":"CSR: Small: Uncertainty Management in Real-Time Embedded Control Systems","awardID":"1016299","effectiveDate":"2010-08-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["562656"],"PO":["565255"]},"173970":{"abstract":"This Rapid Response Research (RAPID) project is developing technology for ubiquitous event reporting and data gathering on the 2010 oil spill in the Gulf of Mexico and its ecological impacts. Traditional applications for monitoring disasters have relied on specialized, tightly-coupled, and expensive hardware and software platforms to capture, aggregate, and disseminate information on affected areas. We lack science and technology for rapid and dependable integration of computing and communication technology into natural and engineered physical systems, cyber-physical systems (CPS). The tragic Gulf oil spill of 2010 presents both a compelling need to fill this gap in research and a critical opportunity to help in relief efforts by deploying cutting-edge CPS research in the field. In particular, this CPS research is developing a cloud-supported mobile CPS application enabling community members to contribute as citizen scientists through sensor deployments and direct recording of events and ecological impacts of the Gulf oil spill, such as fish and bird kills. <br\/><br\/>The project exploits the availability of smartphones (with sophisticated sensor packages, high-level programming APIs, and multiple network connectivity options) and cloud computing infrastructures that enable collecting and aggregating data from mobile applications. The goal is to develop a scientific basis for managing the quality-of-service (QoS), user coordination, sensor data dissemination, and validation issues that arise in mobile CPS disaster monitoring applications. <br\/><br\/>The research will have many important broader impacts related to the Gulf oil spill disaster relief efforts, including providing help for the affected Gulf communities as they field and evaluate next-generation CPS research and build a sustained capability for capturing large snapshots of the ecological impact of the Gulf oil spill. The resulting environmental data will have lasting value for evaluating the consequences of the spill in multiple research fields, but especially in Marine Biology. The project is collaborating with Gulf area K-12 schools to integrate disaster and ecology monitoring activities into their curricula. The technologies developed (resource optimization techniques, data reporting protocol trade-off analysis, and empirical evaluation of social network coordination strategies for an open data environment) will provide a resource for the CPS research community. It is expected that project results will enable future efforts to create and validate CPS disaster response systems that can scale to hundreds of thousands of users and operate effectively in life-critical situations with scarce network and computing resources.","title":"RAPID: Collaborative Research: Cloud Environmental Analysis and Relief","awardID":"1047753","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["536823"],"PO":["561889"]},"173981":{"abstract":"This Rapid Response Research (RAPID) project is developing technology for ubiquitous event reporting and data gathering on the 2010 oil spill in the Gulf of Mexico and its ecological impacts. Traditional applications for monitoring disasters have relied on specialized, tightly-coupled, and expensive hardware and software platforms to capture, aggregate, and disseminate information on affected areas. We lack science and technology for rapid and dependable integration of computing and communication technology into natural and engineered physical systems, cyber-physical systems (CPS). The tragic Gulf oil spill of 2010 presents both a compelling need to fill this gap in research and a critical opportunity to help in relief efforts by deploying cutting-edge CPS research in the field. In particular, this CPS research is developing a cloud-supported mobile CPS application enabling community members to contribute as citizen scientists through sensor deployments and direct recording of events and ecological impacts of the Gulf oil spill, such as fish and bird kills. <br\/><br\/>The project exploits the availability of smartphones (with sophisticated sensor packages, high-level programming APIs, and multiple network connectivity options) and cloud computing infrastructures that enable collecting and aggregating data from mobile applications. The goal is to develop a scientific basis for managing the quality-of-service (QoS), user coordination, sensor data dissemination, and validation issues that arise in mobile CPS disaster monitoring applications. <br\/><br\/>The research will have many important broader impacts related to the Gulf oil spill disaster relief efforts, including providing help for the affected Gulf communities as they field and evaluate next-generation CPS research and build a sustained capability for capturing large snapshots of the ecological impact of the Gulf oil spill. The resulting environmental data will have lasting value for evaluating the consequences of the spill in multiple research fields, but especially in Marine Biology. The project is collaborating with Gulf area K-12 schools to integrate disaster and ecology monitoring activities into their curricula. The technologies developed (resource optimization techniques, data reporting protocol trade-off analysis, and empirical evaluation of social network coordination strategies for an open data environment) will provide a resource for the CPS research community. It is expected that project results will enable future efforts to create and validate CPS disaster response systems that can scale to hundreds of thousands of users and operate effectively in life-critical situations with scarce network and computing resources.","title":"RAPID: Collaborative Research: Cloud Environmental Analysis and Relief","awardID":"1047792","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["534923"],"PO":["561889"]},"150760":{"abstract":"From throwing a baseball to playing the piano to typing on keyboards, human beings are constantly learning new sensorimotor skills. During learning, synaptic connections in the brain must be modified to form a motor memory. Further, this modification seems both permanent and robust: a sensorimotor skill, once learned, tends to persist throughout the course of a lifetime regardless of its salience (recall the old adage of never forgetting how to ride a bike). Despite the importance of motor memories, their distinctive features, and their ubiquity in vertebrate behavior, little is known about the computational principles and mechanisms that subserve the acquisition of sensorimotor skills. This US-Canadian collaborative project takes an interdisciplinary approach aimed at elucidating neural mechanisms of motor memory formation and unifying -- under a common theoretical principle -- the findings of single-neuron recording studies with established behavioral results. The theory that is proposed makes the following testable prediction: as the level of behavioral expertise in a specific task increases, the neural representation for that skill becomes more selective. By selective, it is meant that a neuron significantly recruited during the performance of the skill tends, with practice, to specialize by firing only when that skill is performed (and not when related skills are performed).<br\/><br\/>Central to the theory is a geometric interpretation of \"biologically plausible\" sensorimotor neural networks, in which neurons are modeled as noisy signal processors and synaptic change is modeled as a noisy morphological process. Because of the high noise levels, it is shown that the system must be \"hyperplastic\" -- that is, the learning rate must be unusually high in order to compensate for the noise and operate at an acceptable performance level. Geometrically, the solution for a skill can be represented as a manifold in the weight space of the network. To learn multiple skills, a network configuration must be attained such that the solution manifolds intersect. To learn multiple skills without noise leading to destructive interference, the network must arrive at a point where the intersecting solution manifolds are orthogonal. With this principle of orthogonality, the neurophysiological predictions described above can be explicitly formulated. These predictions will be tested with an experimental method -- involving floating microelectrode arrays and antidromic stimulation -- that enables the identifiably same neuron to be recorded from for multiple days\/weeks, while a behaving animal learns a task. Finally, psychophysical predictions of the theory will also be tested.<br\/><br\/>This project is jointly funded by Collaborative Research in Computational Neuroscience and the OISE Americas program. A companion project is being funded by the Canadian Institutes of Health Research.","title":"Computational Mechanisms for Storing Motor Memories in Noisy Neural Circuits: How Activity Patterns Evolve during Learning","awardID":"0904594","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["429548","553588",401883],"PO":["564318"]},"163861":{"abstract":"This project deals with models of wireless multi-terminal networks incorporating practical constraints such as individual links that experience fading, applications that are delay-sensitive, network communication that is subject to broadcast and interference constraints and nodes that are constrained to operate in half-duplex mode. The network is assumed to be static for the duration of the message, but can change from one message to the next and channel-state information is assumed to be present only at the receiver. In such settings, cooperative communication in which intermediate nodes facilitate communication between a particular source-sink pair, is key to efficient operation of the network.<br\/><br\/>A key goal of any communication system, is one of achieving an optimal rate-reliability tradeoff. The diversity-multiplexing gain tradeoff (DMT) determines the tradeoff between relevant first-order approximations to the rate and reliability of communication. The DMT of point-to-point communication links has been extensively studied and signal sets are available that are optimal under any statistical distribution of the fading channel. There now exist protocols and codes for two-hop relay networks that come close to achieving the corresponding min-cut upper bound on DMT. Goals of this project include: 1) determining the DMT of various classes of multiterminal networks ranging from broadcast, cooperative-broadcast and multiple-access channel networks to layered multi-hop networks; 2) identifying the classes of networks for which the DMT of the network is given by the DMT of the min-cut; 3) assessing the impact of asynchronous operation of the network, as well as of the presence of feedback along one or more links in the network; 4) the construction of codes with lesser decoding complexity.","title":"CIF: Medium: Collaborative Research: Explicit Codes for Efficient Operation of Wireless Networks","awardID":"0964495","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[438651,438652,438653],"PO":["564924"]},"174410":{"abstract":"Midwest Verification Day (MVD) is an informal regional workshop, started by the PIs in 2009, with the goal of cultivating a regional research network in verification and formal methods. A chief concern is to provide a forum for students to present their formal-methods research in a supportive setting, and network with colleagues at other regional institutions. The program for MVD, spanning one and a half days, will consist mostly of student talks, but also include two invited talks by senior researchers. A social program is also planned, to encourage interaction among students and faculty. MVD is colocating with a local-chapter IEEE meeting on industrial formal methods, presented by industrial researchers from nearby Rockwell Collins.<br\/><br\/>MVD will foster collaboration, student development, recruiting, and exchange of ideas in formal methods and verification, in the Midwest region of the country. Experience with similar events in other fields strongly suggests they play a crucial role in creating a vibrant regional research community, and offer invaluable opportunities for students. MVD will help develop talent within the region, and attract it from outside, in the industrially important area of verification. By connecting students with faculty from other regional institutions, MVD will strengthen opportunities for undergraduates to go on to graduate study, and help prepare regional graduate students for success in the international research community. The industrial connection with the local-IEEE meeting and Rockwell Collins will contribute to the long-term development of economically important interactions between industry and academia.","title":"2010 Midwest Verification Day Workshop","awardID":"1049597","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["532995","521573"],"PO":["565264"]},"172255":{"abstract":"This workshop on Mathematical Foundations of Open Systems explores new research directions towards a logical\/mathematical foundation for modeling the behavior of dynamic open systems that evolve over time through self-organization, regulation, and adaptation to changing environments and structures. Such a framework should provide a unified approach for obtaining an advanced understanding of natural systems, the ability to fix and modify them, and to design cyber-physical systems (CPS) in principled ways using new notions of control and coordination. The workshop, held May 23-25, 2010, Philadelphia, PA, is supported by the NSF and other agency members of the interagency coordinating group on High Confidence Software and Systems.","title":"Mathematical Foundations of Open Systems","awardID":"1037877","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"J139","name":"National Security Agency"}}],"PIcoPI":[461485],"PO":["561889"]},"171166":{"abstract":"The purpose of this travel grant is to support 10-15 graduate students from the United States to participate in the 11th IEEE International Symposium on a World of Wireless, Mobile and Multimedia Net working (IEEE WoWMoM), to be held in Montreal, Canada on June 14-17, 2010.<br\/><br\/>IEEE WoWMoM is a high quality conference and is the premier technical forum dedicated to addressing state-of-the-art challenges and directing future directions in wireless mobile multimedia networking. Participation in this conference provides the students an opportunity to interact with peers and more senior researchers, thus having the opportunity to be exposed to leading edge work in the field of wireless and mobile multimedia networks. The support of this grant enables the participation of the US-based students who would otherwise be unable to attend IEEE WoWMoM 2010.<br\/><br\/>The student selection process will consider the following: 1) students from groups traditionally underrepresented; 2) students from universities that traditionally lack funding to support graduate student attendance at conferences; and 3) students who will present their papers during the conference. Priority will be given to the graduate students from underrepresented groups, such as women, African American, Hispanics, and Native American students, and from universities that lack financial resources.","title":"NeTS:Small: IEEE WoWMoM 2010 Student Travel Support","awardID":"1031096","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["492475"],"PO":["557315"]},"168812":{"abstract":"Finding and fixing bugs are crucial in the process of developing reliable and high-quality software. Software developers could base on their own experience with their programs, or effectively find bugs by consulting the similar bugs and fixes from others in the past for the same or different systems. However, the body of knowledge in software engineering is still very limited on the nature, the causes and effects, and the characteristics of such recurring bugs. The learning process from prior known bugs is still ad-hoc, manually, and un-systematically. In this project, a comprehensive approach is introduced to capture the knowledge of prior bugs and corresponding fixes, and to leverage such knowledge to build automated tools to detect potential recurring buggy code at other locations in the same or different systems. Such tools will help to detect bugs early in the development process, leading to higher-quality software and the improvement in productivity of software developers in the bug fixing<br\/>practice.<br\/><br\/>In this project, an empirical study will be conducted to collect, analyze, and understand the nature and characteristics of recurring and similar bugs within one and across multiple systems. This project is expected to advance software engineering knowledge on the theoretical foundation, concepts, practical techniques, and automated tools to (1) capture the characteristics and measure the similarity of code units involved in prior known fixed bugs, (2) identify the locations of potential buggy units and derive the guidelines to fix them by matching them to the relevant peer code units of the known bugs, and (3) support the similar bug detection and fixing process. The teaching modules and validation efforts in this project will involve students and professionals, promoting teaching and training software quality assurance.","title":"SHF: Small: Find and Fix Similar Software Bugs","awardID":"1018600","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["562663"],"PO":["565264"]},"168823":{"abstract":"Cloud computing has emerged as a powerful paradigm for hosting Internet scale applications in large computing infrastructures due to their desirable features of unlimited resources and infinite scalability, pay-per-use model requiring no up-front investment, elasticity of resources, and fault-tolerance. Since one of the primary goals of the cloud is to host data-intensive applications, large-scale data management is a crucial component. Traditional relational databases have been extremely successful but lack scalability, elasticity, fault-tolerance, and self-management features that are required in cloud settings. This has led to the emergence of a new storage model referred to as the key-value store model. Although key-value stores have the desired features, they provide minimal consistency and reduced functionality due to their single-key access guarantees thus placing unprecedented burden on application developers. This project explores two alternative scalable data stores designs in the cloud -- ElasTraS: an elastic transactional data store targeted towards enterprise applications requiring a relational storage model; and G-Store: a transactional multi-key value store targeted for applications which favor the data model of key-value stores, but require consistent and scalable access beyond single keys. This project brings forth many novel research solutions for designing and implementing scalable data management systems, and acts as a building block for developing commodity solutions dealing with the growing scale of the Internet. The project enables both graduate and undergraduate students to be trained in the design and development of software and solutions for large-scale distributed systems. The project URL is available at: http:\/\/www.cs.ucsb.edu\/~dsl\/?q=cloud-transactions.","title":"III:Small:Transactional Data Stores in the Cloud","awardID":"1018637","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["536696","500153"],"PO":["563727"]},"173179":{"abstract":"This proposal requests travel support for organizing a workshop on Exascale Computing co-located with the International Conference on High-Performance Computing, Dec. 18-22 in Goa India. The workshop will involve presentations on the challenges of Exascale computing as well as on the results from previous workshops to be disseminated to wider audience and international community. The presentations will be given by leaders in the field of high-performance computing as well as leaders and participants from the working groups developing the Exascale agenda. In addition, a panel consisting of leaders from government funding agencies including those from US, India, Europe and Asia will discuss future directions and opportunities for funding and collaboration in the Exascale computing arena.","title":"Travel Support for Workshop: Reaching Exascale in this Decade to be Co-Located with International Conference on High-Performance Computing (HiPC 2010)","awardID":"1043085","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"6892","name":"CI REUSE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7952","name":"HECURA"}}],"PIcoPI":["560392"],"PO":["565272"]},"168834":{"abstract":"Parallel multi-threaded programs are more difficult to write than their sequential counterparts because while writing parallel programs programmers must consider all possible behaviors due to thread interleavings, in addition to the algorithmic correctness of the program. A widespread belief is that the only way to make multi-threaded programming accessible to a large number of programmers is to come up with programming paradigms and associated tools that explicitly separate reasoning about functional correctness from reasoning about additional behaviors arising due to parallelism.<br\/><br\/>This project investigates strategies for separating the parallelization correctness aspect of a program from its functional correctness. First, for most parallel programs it is desired that the non-determinism introduced by the thread scheduler in a parallel program does not change the intended output of the program. This project will develop an assertion framework for specifying that regions of a parallel program behave deterministically despite non-deterministic thread interleaving.<br\/><br\/>The second strategy is based on the observation that a natural step in the development of a parallel program is to first extend the sequential algorithm with a controlled amount of non-determinism, followed by the actual parallelization, when additional non-determinism is introduced by thread interleavings. This project will investigate the use of non-deterministic sequential programs as a specification mechanism, such that for each execution of a parallel program there exists an equivalent execution of the corresponding non-deterministic sequential program. Such non-deterministic sequential programs decouple parallelization correctness from functional correctness.","title":"SHF: Small: Specifying and Verifying Essential Deterministic Behavior of Concurrent Programs","awardID":"1018730","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[451873,451874],"PO":["565264"]},"168724":{"abstract":"This project explores the behavior of value-based learning methods in multi-agent environments. Value-based methods make decisions by using experience to estimate the utility impact of alternatives and choosing those with high predicted value. Because they evaluate components of behavior instead of treating behaviors as atomic units, they are computationally and statistically efficient. While these methods have been used in computational experiments for many years, only recently have researchers begun to formally characterize their behavior. Our own preliminary work is finding that some value-based methods exhibit super-Nash behavior, making them particularly worthy of study.<br\/><br\/>More specifically, we are analyzing, mathematically and experimentally, how value-based algorithms perform in several classes of simulated games of varying complexity from the artificial intelligence community, multi-agent engineering applications drawn from the wireless networking area, and as models of human and animal decision making in collaboration with cognitive neuroscientists. Where possible, we are refining existing value-based algorithms to work more efficiently, robustly, and generally than existing algorithms. We are also designing educational outreach activities, including creating entertaining instructional videos on how to promote cooperative behavior in real-life social dilemmas.","title":"RI: Small: Understanding Value-based Multiagent Learning and Its Applications","awardID":"1018152","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550488"],"PO":["565035"]},"167635":{"abstract":"Abstract <br\/>Reading comprehension deficits are pervasive for a disproportionate number of post-secondary students. These students have cognitive impairments that impact high level text processing skills and result in diverse reading profiles with difficulties in skills such as discerning between relevant and irrelevant information, drawing inferences, connecting background knowledge to new learning, and retaining and applying what was learned at a later date. Typically such deficits are managed by teaching the use of study-skills strategies. While there is strong face validity for these practices there is a lack of evidence-based practice, and virtually no information on candidacy or what types of deficits respond best to what types of strategies and supports. On the technology side, the popularity of electronic reading tablets offers a platform to deliver supports to improve reading comprehension and retention that could be adopted by college students. This project seeks to bridge the gap by developing the technology to support a diverse set of reading strategies in a highly adoptable form for college students with high-level reading impairments, by doing the science necessary to define a process that can assess each individual student, and by prescribing a set of strategies that eventually will be delivered in a hardware-software package. By using an iterative design process and a participatory action research model, this research will make the following contributions: a dynamic assessment process that matches reading profiles\/impairments to strategy supports; a mapping of reading impairments to reading strategies; translation of reading strategies to delivery on electronic reading tablets; a demonstration that personalization and adaptation are possible using our software engineering models; and a dissemination package that uses open source software and hardware to deliver a research tool that could be used by companies designing commercialized reading tablets. To achieve these goals, the PI will partner with three institutions that have large populations of struggling readers in post-secondary educational settings: two VA facilities that support and train veterans returning to educational settings, and a student disability services program at a large urban state university. These groups have experience with the pervasive, high level reading challenges preventing educational success, and provide the natural contexts to evaluate the PI's models and shape the tools generated from this research. Pilot studies, laboratory experiments and longitudinal studies will be employed to develop and evaluate the technology for a dynamic reading assessment and support tool. <br\/><br\/>Broader Impacts: A growing population not able to meet the reading demands of college and community college courses is the large number of veterans returning form Iraq and Afghanistan seeking education and training benefits. It is estimated that 15-20% of these veterans have suffered mild brain injury sufficient to affect academic ability. Another large group of post-secondary students that is challenged by difficulties with reading comprehension are those with developmental conditions including adult attention deficit and hyperactivity disorder (ADHD) and attention deficit disorder (ADD). Estimates regarding the number of students enrolled in colleges who report clinically significant ADHD or ADD symptoms vary between 2% to 8%; approximately 25% of students receiving disability support services are receiving those for ADHD. The PI expects two important outcomes from this work: the science missing from the literature that links reading impairments with reading strategies; and a demonstration tool, built on the science, that supports an assessment process and a delivery mechanism. With this latter outcome in mind, the PI intends to devote a large part of Year 5 of the project to making his tool highly attractive to companies who have the infrastructure to deliver products to the target populations.","title":"HCC: Large: Collaborative Research: Delivery of Personalized Reading Strategies for People with Cognitive Impairments in Post-Secondary Settings","awardID":"1012947","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[449095],"PO":["565227"]},"168614":{"abstract":"Networks are created to provide reachability; yet, network reachability is not well understood due to the many sophisticated network security policies configured on network devices to limit reachability for security and privacy purposes and the various factors such as routing that may affect reachability in unexpected ways. Due to the lack of distributed network security policy management tools, network operators have been using primitive tools to manage the increasingly complex reachability issues following a ``trial and error'' approach, which leads to many policy and reachability errors. While providing more reachability than necessary opens doors to unwanted traffic, providing less reachability than necessary may disrupt normal business operations. This project employs proactive approaches to reachability management and helps operators to design, verify, analyze, troubleshoot, and optimize distributed network security policies. The new concepts, models, theorems, and algorithms developed in this project advance our knowledge and understanding of network reachability, and the comprehensive network reachability toolkits developed in this project significantly improve network security and reliability. The rigorous models and mathematical formulations of network reachability serves as the theoretical foundation of future work on this fundamental network security issue. The comprehensive toolkit for network reachability quantification, verification, query, monitoring, analysis, and optimization can be used by network operators to troubleshoot and debug reachability problems. This technology enables the seamless collaboration of distributed network security policies and ensure the right amount of reachability is enforced. To promote learning, this effort actively involves high school, undergraduate, graduate students, especially students from under-represented minorities.","title":"TC: Small: An Algorithmic Framework for Distributed Network Security Policies Management","awardID":"1017598","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["562296"],"PO":["565327"]},"167404":{"abstract":"Simple contagion processes underlie various phenomena on complex networks, such as the spread of diseases on social-contact networks and information in communication networks; understanding their dynamics and developing control mechanisms are key issues in numerous applications. The goals of this proposal are: (i) Developing methods to construct synthetic relational networks using partial and noisy data; (ii) Understanding the structure of these networks and the contagion processes, and especially important network properties and typical patterns that have an impact on the dynamics of contagion; (iii) Developing techniques to control the spread of contagion processes, and to detect, prevent and arrest cascading failures in coupled socio-technical networks; and (iv) Understanding the co-evolution between the networks and dynamics, and using this to refine their models, and the strategies to control them. The broader impacts of this work include bridging the gap between the social sciences and computer science in addressing fundamental questions in complex networks, a corresponding enhancement to course curricula, and the involvement of students at all levels.","title":"NetSE: Large: Collaborative Research: Contagion in large socio-communication networks","awardID":"1011769","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":["459010","473468",448530,"516910"],"PO":["565090"]},"168977":{"abstract":"The ScratchEd project, led by faculty at the Massachusetts Institute of Technology and professionals at the Education Development Center, is designing, developing, and studying an innovative model for professional development (PD) of teachers who use the Scratch computer programming environment to help their students learn computational thinking. The fundamental hypothesis of the project is that engagement in workshops and on-line activities of the ScratchEd professional development community will enhance teacher knowledge about computational thinking, their practice of design-based instruction, and their students' learning of key computational thinking concepts and habits of mind.<br\/><br\/>The effectiveness of the ScratchEd project is being evaluated by research addressing four specific questions: (1) What are the levels of teacher participation in the various ScratchEd PD offerings and what do teachers think of these experiences? (2) Do teachers who participate in ScratchEd PD activities change their use of Scratch in classroom instruction to create design-based learning opportunities? (3) Do the students of teachers who participate in the ScratchEd PD activities show evidence of developing an understanding of computational thinking concepts and processes? (4) When the research instruments developed for the evaluation are made available for teachers in the Scratch community to use for self-evaluation, how do teachers make use of them? Because both computational thinking and design-based instruction are complex activities, the project research is using a combination of survey, interview, and artifact analysis methods to answer the questions.<br\/><br\/>The ScratchEd professional development and research work will provide important insight into the challenge of helping teachers create productive learning environments for development of computational thinking. Those efforts will also yield a set of evaluation tools that can be integrated into the ScratchEd resources and used by others to study development of computational thinking and design-based instruction.","title":"ScratchEd: Working with Teachers to Develop Design-based Approaches to the Cultivation of Computational Thinking","awardID":"1019396","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":["497072"],"PO":["565121"]},"174059":{"abstract":"This project explores a new frontier called Persuasive Sensing that brings together advances happening in two fields, namely sensor networks and persuasive technology. Today, advances in sensor networks are making it possible to capture, detect, and analyze data. However what is missing is to present relevant information mined from sensor data to subjects about their daily life and activity rhythms and using feedback mechanisms to alter human behavior. It is now demonstrated that human beings normally follow an approximately 24-hour fluctuating rhythm known as circadian activity rhythm. This is an exploratory research project to design and engineer such a prototype system. The data obtained from environmental sensors as well as body-wearable sensors are fused together to generate meaningful feedback to persuade end-users.<br\/><br\/>This research project is novel in several ways. First, the idea to fuse environmental sensor data that detects activity in space (or location) along with body-wearable sensors that collects physiological health data and utilize both to detect circadian activity rhythms has not be done before. Second, while sensor networks have been used in healthcare applications before, our persuasive feedback based on mining the data and benchmarking it against normal activity and the ability to detect patterns that identify onset of diseases or pathologies is novel. Third, the artificial neural network data mining algorithms that can identify patterns from circadian activity rhythms and then match those against a database of known rhythms will be a significant novel contribution. Finally, people are very different when taking suggestions. In order to achieve effectiveness, the system needs to monitor and learn from human reactions from previous suggestions and adapt. Broader impact includes the use of wireless sensor networks along with persuasive technology design that will open up new possibilities for prevention and help address chronic diseases such as obesity and diabetes. Employing post docs and graduate students, this project will train and educate the next generation of workforce in Healthcare IT and integrate research findings into classroom. Findings from the research will be incorporated into a graduate level course at Claremont Graduate University.","title":"EAGER: Persuasive Sensing Networks: A New Frontier to Changing Human Behavior","awardID":"1048366","effectiveDate":"2010-08-01","expirationDate":"2012-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[466904],"PO":["565303"]},"165479":{"abstract":"In our daily lives, the artwork we experience is increasingly the output of an algorithm executed within a computer; such algorithms are known as procedural generators. From the humble screensaver on the Macintosh with its swishes and whirls, to large simulated crowds in movies, our experience is enhanced by the power of procedurally generated content. <br\/>At the same time, specialized computer applications such as Photoshop, Illustrator, and Final Cut Pro have made it much easier for human designers to manually create top-quality content, thereby unleashing the creative potential of millions. Though the cost of tools has come down, the cost of content creation remains high, limiting the volume of content each person can create. The goal of this project is to combine the speed of procedural generators with the expressive potential of manual content generation tools. We seek to create tools where the human designer and a procedural generator work together in a collaborative way, each building on the work of the other. We call such human\/computer creative systems ?mixed-initiative,? since at any point, either the human or the computer can take the next step in contributing to the final creative content. With such mixed-initiative content creation systems, the potential exists for a rapid amplification of human creative potential, with the result being increased vibrancy and intricacy across a wide range of creative works.<br\/><br\/>This project will explore the design space of mixed-initiative content creation systems by constructing and evaluating one such system, called Springboard. Our target area for content creation is levels for computer games, specifically the genre of 2D platformers (such as Super Mario World). Computer games are a representative domain for creative content authoring, since they involve visual arts, spatial layout of game elements, animation, and game mechanics (movement, rewards, win conditions, etc.). Level design hence involves elements found in many existing content creation tools. Building and evaluating Springboard will yield an enhanced understanding of the technical architecture required to create such systems and the kinds of knowledge representations needed to permit flexible interaction of human and computer created content. Also to be explored is the value of creation, storage, and access to variants of the final creative product, thereby supporting rapid exploration of alternate designs. Studies of human designers using the system will provide insights into how mixed-initiative tools influence the human creative process. We anticipate procedurally generated levels will help designers generate more varied ideas during brainstorming activities, will permit designers to explore more design alternatives, and in general lead designers to create longer and more complex final level designs than when using traditional, manual-effort level design tools.","title":"Pilot: Understanding the Design Space of Mixed-Initiative Content Creation Tools","awardID":"1002852","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":[443290],"PO":["565227"]},"168757":{"abstract":"A complex system is often defined as a system composed of interconnected parts whose properties cannot be predicted from the properties of its individual components. The investigator studies systems composed of many interacting agents that are not controlled by designated controlling agents and are self-organized. The agents are autonomous and have only local views of their system. They are endowed with the ability to learn from received signals and they share their knowledge with their neighbors by communicating it with agreed languages and rules. A typical feature of such systems is their tendency to display emergent behavior. An important instance of emergent behavior is the phenomenon of herding. Herding is a process where agents in a group ignore their own signals about the state of nature and follow the actions of their neighbors. The research is on the development of a methodology for understanding the interplay between learning and herding in complex systems.<br\/><br\/>The aim of this work is to study the emergence of herding in complex systems with distributed signal processing. Emergence of herding is defined in a mathematically precise way so that it can be detected in a meaningful way. The agents of the system are rational, i.e., they employ Bayesian learning. The main objective is to understand the emergence of herding in multi-agent systems which is due to diffusion of system knowledge through interactions of received signals, perceived actions of neighboring agents, and learning. Various models of sharing information are studied and scenarios where herding readily arises are identified. Improved methods for efficient diffusion of knowledge in multi-agent systems are developed and ways of preventing adverse herding are sought. <br\/><br\/>At the recommended level of support, the PI will make every attempt to meet the original scope and level of effort of the project.","title":"CIF: Small: Learning and herding in complex systems","awardID":"1018323","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":["563567"],"PO":["564898"]},"168405":{"abstract":"Existing approaches to exploit sparsity present in statistical or deterministic signal descriptors have mostly relied on linear models, and available compressive sampling (CS) implementations are analog. On the other hand, major application domains, such as signal compression and reduced-rank approximation of complex systems, entail bilinear models. In addition, the pertinent technology used in everyday life, for e.g., speech and image compression, is digital. Accordingly, the need arises for fundamental research to account for sparsity in bilinear models, as well as put CS on equal footing with digital compression modules, which rely on the `workhorses' for dimensionality reduction, namely principal component analysis (PCA) or canonical correlation analysis (CCA), and have their performance benchmarked by rate-distortion limits.<br\/><br\/>This research aims at developing algorithms, and corresponding performance limits for sparsity-cognizant dimensionality reduction, compression, and reconstruction tasks. Key to achieving these goals are optimal formulations for sparse PCA, sparse CCA, and associated quantization schemes, along with sparsity-aware rate-distortion metrics for comparison with sparse overcomplete basis expansions (SOBE). The vision is to have available tools and figures of merit to exploit the `right' form of sparsity for the `right' application domain. Optimization of sparse PCA, sparse CCA, and SOBE approaches draws from contemporary advances in sparsity-aware regression, basis pursuit, subspace tracking, and coordinate-descent solvers of minimization problems regularized with the l_1 norm of the unknowns. The research agenda leverages these tools to investigate a number of challenging directions. These include: (d1) sparsity-exploiting and power-aware compression over non-ideal links; (d2) sparsity-cognizant, adaptive compression for (non-)stationary processes with memory; (d3) sparsity-aware reconstruction of hidden sources, and reduced-rank identification of sparse systems; and (d4) comparisons between deterministic and statistical descriptors of sparsity.","title":"CIF: Small: Exploiting Sparsity for Dimensionality Reduction","awardID":"1016605","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":["564051"],"PO":["564898"]},"168537":{"abstract":"AF: Small: Algorithmic Foundations of Phylogenetic Tree Reconciliation<br\/><br\/><br\/>A phylogenetic tree represents the genealogy of a set of species. One of the grand challenges in computational biology is to build the Tree of Life, the phylogenetic tree describing the evolutionary history of all extant species. A major obstacle to assembling this or indeed any large tree reliably is the sparseness of the coverage provided by the available genomic data. That is, the same genetic information may not be at our disposal for all species. For example, a gene may not have been sequenced for a species of interest or the gene may simply be absent from that species. One approach to address this problem is to find a collection of overlapping subsets of the species, each of which has the property that similar genetic data is available for every species in the subset. A phylogenetic tree is built for each subset and then the trees are combined into a single supertree for the entire set of species. Building supertrees is nontrivial, as it involves reconciling conflicts among the various input trees with regard to the placement of certain species.<br\/><br\/>The proposed research will address the fundamental algorithmic problems that arise in reconciling multiple conflicting phylogenetic trees into a single supertree that represents them all. The reconciliation problem will be treated as one of finding a supertree whose total dissimilarity with the input trees is minimum. The research will study the mathematical and computational properties of tree reconciliation under a variety of measures. Some of these will be based purely on tree structure; for example, on what groupings a tree implies on its species set. The PIs will also study what are sometimes referred to as gene tree parsimony problems. Here, the dissimilarity measures attempt to identify the possible occurrence of complex evolutionary events such as gene duplication and subsequent loss, and horizontal gene transfer. <br\/><br\/>The research aims to advance the theory of tree reconciliation and to use the results as the foundation for novel algorithmic methods. The work will build on significant recent theoretical developments in the field. Techniques will include graph theory, fast local search, axiomatic characterizations of tree reconciliation methods, impossibility proofs, compact integer linear programming formulations, and fixed parameter tractability. Specific questions to be addressed include developing methods to handle non-binary trees, extending distance measures to account for missing data, understanding which properties can or cannot be satisfied by a particular method or by any method, and resolving a number of open problems in gene tree parsimony.<br\/><br\/>The research will be informed by the needs of the evolutionary biology community, with which both PIs have an extensive and longstanding collaboration. The results will be widely disseminated within that community. It is anticipated that several of the algorithmic techniques developed in this project will be of direct utility in phylogenetic tree databases.","title":"AF: Small: Algorithmic Foundations of Phylogenetic Tree Reconciliation","awardID":"1017189","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":[451154,451155],"PO":["565223"]},"168779":{"abstract":"Many modern problems across science and engineering require the use of statistical models that describe the probabilistic relationship among the underlying variable. A core component of many of these statistical models is a matrix, comprising the parameters of the model. This proposal focuses in particular on a special subclass called graphical models that use a weighted graph to represent a distribution over the underlying variables. The main use of such statistical modeling is prediction and inference: however these tasks are typically computationally intractable or expensive for general models. <br\/><br\/>An important objective is to perform these inference tasks tractably if approximately. This proposal investigates approaches that make connections with and leverage recent advances in the seemingly unrelated field of numerical linear algebra that solve linear systems in matrices by building so-called preconditioner matrices that are approximations to the linear system matrices. <br\/><br\/>Such statistical and graphical models are used across science and engineering problems: indeed even our cellphones solve graphical model inference problems to decode their received signals. Speeding up these tasks is thus of tremendous importance to all of these varied applications. The researchers are involved in interdisciplinary initiatives at the University of Texas, Austin; the Institute for Computational Engineering and Sciences, and the Division of Statistics and Scientific Computation; within which they are specially involved in disseminating such cutting edge research across disciplines and courses.","title":"RI:Small:Matrix-structured statistical inference","awardID":"1018426","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550986","551037"],"PO":["562760"]},"163840":{"abstract":"Studies in computational complexity in three directions are proposed: <br\/>holographic algorithms, Darwinian evolution, and multicore algorithms.<br\/>In the first of these areas, holographic reductions have been shown to<br\/>be a fruitful source of new efficient algorithms for certain problems,<br\/>and evidence of intractability for othrs. In this research the aim is to<br\/>arrive at a better understanding of the possibilities and limitations of<br\/>holographic algorithms, by exploring ways in which specific currently<br\/>known limitations of this class of methods can be circumvented. For<br\/>evolution the goal is to understand better what classes of mechanisms<br\/>can evolve through the Darwinian processes of variation and selection<br\/>when only feasible resources in terms of population sizes and numbers of<br\/>generations are available. In the area of multi-core algorithms, a<br\/>methodology will be developed for expressing and analyzing parallel<br\/>algorithms that are optimal for a wide range of hardware performance<br\/>parameters. Such algorithms would make possible portable software, that<br\/>is aware of the parameters of the machine on which it executes, and can<br\/>run efficiently on all such machines.<br\/><br\/>The work on multi-core algorithms aims to have the practical goal of<br\/>increasing the effective exploitation of multi-core computers as these<br\/>become more pervasive. The work on evolution will highlight the fact<br\/>that the question of how complex mechanisms could have evolved within<br\/>the resources available, is a question that is resolvable by the methods<br\/>of computational complexity, and aims to provide more precise<br\/>mathematical specifications of what the Darwinian process can achieve.<br\/>The work on holographic algorithms aims to make progress in our<br\/>understanding of what are widely regarded as the most fundamental<br\/>questions regarding the power of practical computation.","title":"AF: Medium: New Directions in Computational Complexity","awardID":"0964401","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7934","name":"PARAL\/DISTRIBUTED ALGORITHMS"}}],"PIcoPI":[438597],"PO":["565157"]},"163741":{"abstract":"The aim of this research is to develop new algorithms and algorithmic<br\/>techniques for solving fundamental optimization problems on planar<br\/>networks. Many optimization problems in networks are considered<br\/>computationally difficult; some are even difficult to solve<br\/>approximately. However, problems often become easier when the input<br\/>network is restricted to be planar, i.e. when it can be drawn on the<br\/>plane so that no edges cross each other. Such planar instances of<br\/>optimization problems arise in several application areas, including<br\/>logistics and route planning in road maps, image processing and<br\/>computer vision, and VLSI chip design.<br\/><br\/>The investigators plan to develop algorithms that achieve faster<br\/>running times or better approximations by exploiting the planarity of<br\/>the input networks. In addition, in order to address the use of<br\/>optimization in the discovery of some ground truth, the investigators<br\/>will develop algorithms not just for the traditional worst-case input<br\/>model but also for models in which there is an unusually good planted<br\/>solution; for a model of this kind, the investigators expect to find<br\/>algorithms that produce even more accurate answers.<br\/><br\/>The research will likely uncover new computational techniques whose<br\/>applicability goes beyond planar networks. In the recent past, once a<br\/>technique has been developed and understood in the context of planar<br\/>networks, it has been generalized to apply to broader families of<br\/>networks.<br\/><br\/>In addition, new algorithms and techniques resulting from this<br\/>research might enable people to quickly compute better solutions to<br\/>problems arising in diverse application areas. For example, research<br\/>in this area has already had an impact in the computer vision<br\/>community. Further research has the potential to be useful, for<br\/>example, in the design of networks, the planning of routes in road<br\/>maps, the processing of images.","title":"AF: Medium: Collaborative Research: Solutions to Planar Optimization Problems","awardID":"0963921","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["534066"],"PO":["565251"]},"163862":{"abstract":"This research will design and implement energy-harvesting active networked tags (EnHANTs): small devices that can be attached to arbitrary objects that are traditionally not networked, such as books, furniture, walls, doors, toys, keys, produce, and clothing. EnHANTs harvest energy, network in a delay-tolerant fashion, and adapt their communications and networking mechanisms to satisfy their existing energy constraints. The tags will be thin, mechanically flexible, and small - a few square cm at most. EnHANTs are a disruptive and transformational technology that enables novel object tracking applications which are unavailable with conventional technologies, such as lost item recovery, disaster recovery, emergency alerts, and spatio-temporal proximity information.<br\/><br\/>EnHANTs require a super-energy efficient architecture and design to operate effectively using only energy accumulated via harvesting. Such severe energy restrictions impose ultra-low power requirements that are much lower than traditional standards such as Bluetooth and Zigbee. Meeting these requirements gives EnHANTs the capability to remain functional in environments that are significantly more energy constrained. Given the severe energy constraints, a total rethinking of how to perform the energy harvesting and storage, communications, and networking is needed. The PIs take a comprehensive cross-layer design approach that combines expertise in networking, communications, low-power electronics, and organic energy-harvesting devices to make the efficiency improvements of several orders of magnitude that make EnHANTs possible. A test platform of networked tags will be deployed and tested demonstrating the energy harvesting, communication, and networking components developed using custom hardware.<br\/><br\/>The research leads to new applications and services based on the low power, energy harvesting tags, that society can benefit from in a significant way.","title":"NetSE: Medium: Energy Harvesting Active Networked Tags (EnHANTs)","awardID":"0964497","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":["475040","522280","545717","539912","470173"],"PO":["564924"]},"174521":{"abstract":"EAGER: Molecular-Level Stochastic Simulation to predict the dynamics of Protein Misfolding and Aggregation<br\/><br\/>Summary: Proteins are biochemical workhorses that are needed in many important biological functions. Lately one aspect of these macromolecules have gained significant attention, which is their ability to \u00a1\u00a5stick to each other\u00a1\u00a6 to from \u00a1\u00a5protein aggregates\u00a1\u00a6 or \u00a1\u00a5amyloids\u00a1\u00a6. This behavior is more common when proteins fail to adopt a \u00a1\u00a5correct\u00a1\u00a6 three dimensional shape, commonly known as a \u00a1\u00a5misfolded\u00a1\u00a6 form. These aggregates can be both beneficial and toxic for cellular processes. Although seems simple, this process is extremely complicated and no precise molecular understanding has emerged. Also, the ability of the proteins to misfold and aggregate via multiple pathways leading to various forms of aggregates has never been explored. Such molecular-level details of the process are important to know since functional aspects of these aggregates are related to the molecular size, shapes, stability and the rates of the formation. Since many of these parameters of this stochastic process are extremely difficult to analyze via conventional biochemical means, molecular-level computational simulations can be valuable. A precise understanding of the protein aggregation phenomenon would broaden the fundamental knowledge of both pathological and functional aspects of biomolecular science besides throwing newer insights. <br\/> In this proposal, we will use amyloid-?\u00d2 (A?\u00d2) peptide as a model protein that is known to form misfolded aggregates to accomplish our goals. Our main objective is to establish a fundamental framework for stochastic molecular-level simulation of the \u00a1\u00a5on-pathway\u00a1\u00a6 fibril formation process that will serve as a basis for analyzing more realistic models with competing pathways to precisely predict the dynamics and mechanisms of protein aggregation. We have initiated a collaborative effort between two PIs at University of Southern Mississippi, (USM) with expertise in computational and biophysical analysis, to achieve our truly inter-disciplinary objectives. <br\/><br\/>Intellectual Merit: Protein aggregation is a nucleation-dependent process, however, precise understanding of its kinetics is not yet known. Aggregation and fiber formation is often considered to be a stochastic process with large variations in macroscopic molecule behavior and hence, stochastic molecular-level simulations would be essential to understand their dynamics. Furthermore, it is not realistic to consider aggregation as an isolated event as there are many different factors that influence protein aggregation in a physiological environment. Broadly, these include molecules that may \u00a1\u00a5interact\u00a1\u00a6 with the protein besides others such as ionic strength, temperature etc. Hence in this proposal, we are focused on developing a fundamental framework of modeling protein aggregation and amyloid formation phenomenon via molecular-level modeling and stochastic simulation methodologies. The biophysical experiments can show the cumulative effects of the aggregates whereas the simulation will be able to predict the concentration change dynamics with respect to time for every aggregate involved in the pathway. This will allow us to study the exact nature of each aggregate and their sensitivity to the over-all pathway dynamics. <br\/><br\/>Broader Impact: USM is an excellent place to conduct this research from a scientist training perspective; MS being among the states with the highest levels of poverty besides providing a truly diverse student population. The research will provide a broader impact to the scientific community in the form of a fundamental mechanistic knowledge about protein aggregation systems. Our educational outreach mechanisms will involve giving seminars at participating undergraduate institutions in MS, recruiting economically disadvantaged students (including women and minorities) to perform summer research in the PI and Co-PI laboratories at USM. It will also enhance our graduate program through the design of new inter-disciplinary courses (e.g. \u00a1\u00a7Systems Biology\u00a1\u00a8 and \u00a1\u00a7Computational biophysics\u00a1\u00a8).","title":"EAGER: Molecular-Level Stochastic Simulation To Predict The Dynamics of Protein Misfolding and Aggregation","awardID":"1049962","effectiveDate":"2010-08-15","expirationDate":"2011-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":[468142,"563362"],"PO":["565223"]},"174577":{"abstract":"Washington State University's GridStat project is developing middleware for electric power grid monitoring and control systems incorporating new technology that precisely measures currents, voltages, and phase angles many times per second. The GridStat at Scale using GENI project will demonstrate GridStat middleware?s ability to meet the power grid?s need for high-speed, reliable communications and will establish baseline performance expectations. Using GENI as the experimental platform allows experimentation with hundreds of nodes and isolated networks to make repeatable performance measurements over wide geographic areas. The ability of redundant resources in GridStat networks to help withstand attacks that attempt to remove or overload resources will be assessed.<br\/><br\/>These experiments will establish credibility factors for wide-area communications used in coordinated control systems to enhance the reliability and efficiency of the electric power grid and will establish an approach for testing new design ideas for the smart transmission grid.","title":"EAGER: GridStat behavior at scale using GENI","awardID":"1050221","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[468288,468289],"PO":["564993"]},"174467":{"abstract":"Network testbeds have revolutionized evaluation practices in computer science.<br\/>Over the last decade many diverse testbeds have been built throughout the world, and the trend of building faster, bigger, and more diverse testbeds continues. On the other hand, there have been no studies on testbed use. Specifically, who uses testbeds, for what types of research, and what are the use patterns and practices. <br\/>Answering these question would help testbed builders and funders understand the impact of their work and direct their efforts to maximize the payoff.<br\/><br\/>This project seeks to analyze existing data about testbeds, and collect new data, to understand how testbeds are used, by whom and for what purpose. It seeks to analyze usage over time and to infer and analyze short-term and long-term trends. It further seeks to understand what motivates people to use testbeds and what hinders their use. For those factors that hinder testbed it attempts to identify their underlying reason, namely if they stem from a specific research field, a specific testbed practice or if they are inherent in the nature of network testbeds.","title":"Investigating Network Testbed Usage","awardID":"1049758","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["550312"],"PO":["565327"]},"168703":{"abstract":"The PI's goal in this project is to establish advanced design strategies for the aural navigation of complex Web information architectures, where users exclusively or primarily listen to, rather than look at, content and navigational prompts. Conventional on-screen visual displays may not work well, if at all, in many situations. The most obvious instances occur when persons who are blind or visually impaired need to use technologies designed for sighted users. A much more common situation, however, occurs with users of mobile devices. These users are often engaged in another activity (e.g., walking around a city or driving a car) where it is inconvenient, distracting or even dangerous to continuously look at the screen. On the one hand, Web accessibility guidelines have focused on ensuring that websites are readable by assistive technologies (screen readers). On the other hand, recent aural browsers enable a more intelligent visual-to-aural transformation of specific features of Web pages. These advances, however, do not fully address the larger issue of aurally navigating complex information architectures. Previous work by the PI has shown that an effective aural experience requires the elaboration of new navigation patterns in order to overcome the limits imposed by the linearity of the aural medium. The PI will build on these preliminary results in the current project, in order to provide an advanced level of usability for audio-based web interactions. He will explore conceptual design patterns for aural navigation, by iteratively creating and refining aural design strategies inspired by the structural paradigms of human dialogues for back and history navigation, and browsing in large collections. A series of evaluation studies involving both visually impaired participants using screen readers and sighted participants using mobile devices will assess the potential and limits of the aural navigation paradigms to enhance the effectiveness of Web navigation. <br\/>.<br\/>Broader Impacts: This project will directly involve blind users in the design and evaluation of new aural design strategies, in collaboration with the Indiana School for the Blind and Visually Impaired (ISBVI). Project outcomes will yield a better understanding of the design issues and solutions for aural navigation, which will provide a solid long-term intellectual basis for the creation of better applications for visually-impaired users and for audio-only navigation contexts. In addition, the PI will work proactively to expand student participation in the research enterprise on his campus, by establishing user experience undergraduate labs.","title":"HCC: Small: Navigating the Aural Web","awardID":"1018054","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["560582"],"PO":["565227"]},"168824":{"abstract":"This project focuses on convex methods for human movement understanding, the task of estimating and quantifying human motion and movement in videos. Compared with previous approaches, the convex methods explicitly model complex inter-component correlations and global constraints and are able to achieve more reliable results. The convex formulations are automatically generated using learning methods. By taking advantage of their special structures, efficient algorithms are devised. This project studies human movement understanding from different perspectives and provides convex solutions to global movement estimation, body part tracking, and local patch motion trajectory estimation. The key research components include convex articulated graph matching with complexity decoupled from the sizes of target candidate sets, convex structure learning and efficient decomposition methods for solving large-scale problems. With robust movement estimation, this project further addresses performance recognition, a new application that quantifies the style and performance of actions.<br\/><br\/>This project benefits surveillance, robotics, human computer interaction, entertainment, sports and medical applications. The convex framework developed in this project can also be extended to many other areas to construct optimal models for information retrieval, control systems, scheduling and network analysis, and to solve large-scale problems in operational research and simulation. <br\/><br\/>This project develops a new technology for understanding human movements; a benchmark dataset is also developed. The results are disseminated by peer reviewed publications, web page downloads, and by university courses.","title":"RI: Small: Convex Architecture for Human Movement Understanding","awardID":"1018641","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["462918"],"PO":["564316"]},"168835":{"abstract":"Continued advancements in semiconductor technology make it possible to integrate hundreds of processing cores into one silicon die. However, as the transistor size continues to shrink, increasing manufacturing defects have resulted in severe yield loss and made chip products? profit to drop substantially. Process variations also cause the performance and power characteristics differ significantly from chip to chip. In addition, this non-determinism greatly exacerbates the complexity of developing, validating, and maintaining software built upon these chips, especially for mission-critical real-time embedded applications. <br\/><br\/>This project explores the hypothesis that architectural virtualization is an effective strategy and will be the norm to address extensive process variations and manufacturing defect problems on many-core platforms. This collaborative research effort seeks to develop effective methods and techniques to virtualize hardware resources on many-core platform and thus isolate the underlying hardware non-determinisms without changing the operating system and application software.<br\/><br\/>This project seeks to improve the robustness and reliability of software and systems developed on many-core platforms. Techniques developed in the research may also help to address the component upgrade and obsolescence issue, which has been challenging military and avionics industry for decades. The collaborations between PIs and their international collaborators provide a unique opportunity to build an international research and educational collaboration structure, to share research expertise, to enhance the educational opportunity, and to promote cultural exchange. In addition, this project provides opportunity to nurture, encourage and attract students from under-represented groups to engage in this exciting research.","title":"CSR: Small: Collaborative Research: Application-Aware Many-Core Virtualization for Real-Time Embedded Computing","awardID":"1018731","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["460574"],"PO":["565136"]},"168846":{"abstract":"Public safety first responders have a need for increased access to radio spectrum to improve interoperability between agencies in natural and human-caused emergencies, and to accommodate bandwidth intensive applications such as mission critical video surveillance. Dynamic Spectrum Access, a new paradigm that promises improved RF spectrum access and efficiency, has been hindered in its application to mission critical networks by a lack of the detailed understanding of the spectrum utilization characteristics necessary to drive system development. This project is generating the fundamental long-term and high-resolution spectrum measurements needed to characterize the time, frequency, energy, and spatial dynamics of mission critical wireless networks in a dense urban environment (Chicago). Empirical and analytical models that characterize public safety spectrum utilization are being created from these measurements. The empirical and analytical models are being used in turn as inputs to discrete-event simulations of candidate mission critical Dynamic Spectrum Access approaches in order to assess the ability of these approaches to improve capacity, while maintaining the necessary performance as measured by access delay and interference tolerance. Results in the form of the RF measurements, empirical and analytical models of mission critical spectral utilization, and system models are being disseminated via a secure download server linked to the Illinois Institute of Technology website. This research will impact Public Safety stakeholders including government agencies, public safety standards bodies, the Federal Communications Commission, and equipment providers by assessing the feasibility of applying to mission critical public safety wireless networks and facilitating system development.","title":"NeTS: Small: Dynamic Spectrum Access for Mission Critical Wireless Networks","awardID":"1018786","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["560174"],"PO":["565303"]},"168626":{"abstract":"Power consumption has become a dominant design constraint for computer systems. The ITRS Roadmap states: \"power management is now the primary issue across most application segments.\" Worst-case power provisioning has given way to power capping, which throttles performance when necessary. Unfortunately, power-capping is analogous to brown-outs, which penalize all customers of an electrical utility, rather than those who can best tolerate loss of power. The thesis of this research is that computer systems should treat power as a precious resource to be husbanded: To direct and manage with frugality; to use or employ to good purpose and the best advantage. The research will investigate effective power husbanding via architectural techniques that appropriately shift power between different hardware components, much as good budgeting applies money where it is most effective. Power husbanding solutions include (a) low-overhead online mechanisms to estimate resource use, (b) low-cost techniques for dynamically changing a resource's operational level, and (c) effective policies for reaching power efficient operating configurations. Research will extend previous accomplishments on reconfigurable caches to enable online control at fine granularities, extend previous and develop new core architectures to enable proportional scaling of performance and power, and adapt and develop new memory systems which often accounting for 30% of system power.<br\/><br\/>Power husbanding seeks to make computation more power efficient, which has important national and global implications. The PIs will continue to advance the state-of-the-art through students, courses, talks, industrial affiliates, commercial influence, and open-source simulation infrastructure.","title":"SHF: Small: Power Husbanding via Architectural Techniques (PHAT)","awardID":"1017650","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["541916","541915"],"PO":["366560"]},"168516":{"abstract":"In programming computers, \"knowledge is power\" - the more that is known about the data on which a program is to operate, and the machine on which it is to execute, the greater the efficiency that can be obtained. However, programs are written to process all input data and run on many different machines. Run-time program generation (RTPG) is a technique in which the programmer writes a program whose purpose is to write another program at run time when the input data (or some part of it) and machine are known. This idea and its potential to produce dramatic efficiency improvements has been known for many years, but various technical problems have hampered its adoption. Recent software research and developments in computer hardware enable us to address those problems. This research develops tools and techniques for RTPG; applies them some important problems; and demonstrates the practicality of the technique.<br\/><br\/>This work explores several critical problems in the application of RTPG. Most programs of practical interest operate on large data sets, which pose special challenges for RTPG. Further, since large data sets exacerbate the well-known problem of program generation cost, the PIs address that issue in several novel ways. The PIs design an object language for program generation that allows for compile-time preprocessing of fragments to facilitate run-time optimizations. The PIs design optimizations expressly for computer-generated programs (which have different characteristics from ordinary, programmer-written codes). Above all, the PIs employ the technique of auto-tuning, in which relevant characteristics of a target computer are determined at install time, and used to guide the run-time program generation process.","title":"SHF: Small: Run-Time Program Generation and Empirical Optimization","awardID":"1017077","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["550505","550505","542046",451105],"PO":["565272"]},"168637":{"abstract":"The vehicular ad hoc network is one of the key enabling components in Intelligent Transportation Systems that has been developed for safe and smooth driving without excessive delays. A major hurdle in the development of the networks for time-dependent safety-critical services is the lack of established models and metrics. These enable one to determine the effectiveness of the network design mechanisms for predictable quality of service, and allow the evaluation of the tradeoff between network parameters.<br\/><br\/>This collaborative research project between Oral Roberts University and Duke University analyzes and suggests enhancements of safety-critical services in vehicular ad hoc networks. Several key issues are investigated in the project. First, the project develops stochastic modeling techniques to address some open problems, such as hidden terminal issues and rebroadcast coverage problems in two-dimensional broadcast vehicular networks under typical traffic scenarios. Second, new analytical models are developed for time-dependent analysis of time-critical safety services. Consequently, performance, reliability, and survivability metrics for the safety services are defined and analyzed. Third, new solutions to assure reliable delivery of emergency messages and novel analytical model based cross-layer protocols are designed and studied. Broad impact of the work is that the outcome of this research is deemed to be beneficial to the design and analysis of general mobile ad hoc networks for critical missions. This project creates new collaborative opportunities with the industry. Both graduate and under-graduate level courses are developed to integrate the scientific findings of the project with current teaching activities at the two universities.","title":"NeTS: Small: Collaborative Research: Analytic Modeling and Enhancement of Vehicular Ad Hoc Networks for Safety Related Applications","awardID":"1017722","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[451391],"PO":["557315"]},"168758":{"abstract":"This proposal on energy materiality seeks to create an entirely new framing and participatory citizen engagement with energy, not solely as a producer-consumer model, but to directly introduce and operationalize novel strategies for engaging, making, sharing, saving, and expressing personal, interactive micro-energy. This proposal will develop an alternative perspective on energy- as-materiality and propose a series of design approaches for materializing energy and introduce additional concepts, such as: energy mementos, energy attachment, energy engagement, energy attunement, local energy, seasonal energy, and energy meta-data. This proposal is specifically focused on advancing the knowledge around novel energy production, sharing, and economics by positioning citizens in the role of active participants in these operations. While the scale of energy may initially be small, the potential for new, creative thinking and citizen led grassroots energy efforts to emerge and develop into impactful everyday energy solutions is very real. <br\/><br\/>This proposal leverages citizen creativity and participation in concepts of energy production, expression, sharing, and usage. This proposal directly engages with underrepresented groups in this debate - namely non-experts across a range of communities. The potential impact can be as simple as introducing strategies and techniques for energy problem solving and ideation using citizens and novel artifacts. This proposal will encourage improved energy literacy in people and draw non-experts into the energy solution debate as active participants.","title":"HCC: Small: Energy Materiality","awardID":"1018340","effectiveDate":"2010-08-15","expirationDate":"2013-11-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["514849"],"PO":["564456"]},"168769":{"abstract":"Software quality has been an important but illusive concept for several decades, with experts different, sometimes conflicting, guidelines. Using an ecosystem of about 20,000 open source Java projects, this study will to try to discover correlations between open source component utilization and software quality metrics, in order to provide the strongest empirical evidence yet as to how the several metrics pertaining to software quality correlate with actual utilization of reusable components. This study will provide a scientific basis to some of the existing guidelines and to the dismissal of some others. If no correlations are found, this result will disrupt current conceptualizations of component quality, forcing researchers and developers to reassess their understanding of software quality and reusable components. Software being a foundation of modern society, and Open Source development being a significant movement in society at large, it is critical to gain a deeper understanding of software quality on a global scale, leading to the development of innovative tools and methods.<br\/><br\/>The metrics used in this study are those defined by the SQO-OSS Quality Model. To understand correlations, the following method will be used. First the dependency graph will be built, capturing software dependencies at the global scale. This requires overcoming technical challenges in cleaning up and clustering the data, as real world projects contain all sorts of idiosyncrasies related to the use of external components. Second, a suite of utilization metrics will be developed using this global dependency graph that capture the depth and breadth of component usage by these projects. Finally, the SQO-OSS Quality metrics for a significant subset of projects in the data set will be computed and compared with the projects' utilization metrics in order to reveal the correlations.","title":"SHF: Small: Open Source Software Components: Utilization Assessment and Automatic Retrieval","awardID":"1018374","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["518206"],"PO":["564388"]},"168428":{"abstract":"Enabling High-Concurrency and Scalability for Many-Core Processors<br\/><br\/>We are developing a novel operating system (OS) for many-core processors. For power and other reasons, microprocessor designs now involve increasing numbers of cores, with an expectation of 100s or 1000s of cores per chip in the future. The movement to high concurrency even for mainstream applications implies the need to simplify concurrent programming and the need for an OS <br\/>capable of managing and delivering high concurrency.<br\/><br\/>The first step is to leverage \"private\" memory, which the OS and the compiler can use without the need for concurrency control, thus achieving both simplicity and high performance. With OS and compiler support, this simplest form of concurrency management can be used more often and we can detect when the privacy assumption is not valid, thus preventing one class of errors. For <br\/>\"embarrassingly\" parallel applications, this model often suffices.<br\/><br\/>For more complex concurrency patterns, we combine compiler analysis with dynamic locking, which allows us to choose adaptively from a variety of synchronization methods based on the amount of contention, and also check for common synchronization errors through a combination of static and dynamic analysis.<br\/><br\/>Finally, built-in speculative execution enables latency hiding for long-running tasks, such as asynchronous I\/O operations. With a many cores we can now speculatively execute several paths in parallel. Additionally, these mechanisms can be used as a lightweight checkpoint\/restart for fault tolerance, especially for transient or non-deterministic bugs, thus simplifying high-availability applications.","title":"CSR: Small: Enabling High-Concurrency and Scalability for Many-Core Processors","awardID":"1016714","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550677"],"PO":["564778"]},"172861":{"abstract":"To stop anonymous tools designed for free speech from being abused by criminals, this project investigates practical solutions to trace back criminals while support free speech for benign users, by exploiting two unique perspectives. First, it utilizes the resource advantages of law enforcement to explore the limitations of anonymous tools. As criminals operated from remote locations usually do not have resources to build large-scale systems, they have to rely on existing anonymous tools with third-party resources to hide their traces. Second, the proposed solutions aim to capture some criminals, without a specific target at the beginning. Such assumption greatly simplifies the system design and makes it feasible, different from common traceback solutions which aim at a specific target from the start and usually require heavy costs for large-scaled deployment. <br\/><br\/>This project will examine the implementation limitations of Freenet for asynchronous communications and Tor for interactive communications, and develop tracing back solutions for law enforcement to identify data sources and parties involved in malicious transactions. Effective methods to penetrate these systems will be designed for collective traffic analysis. By focusing on known malicious data sharing to further identify malicious parties, the proposed solutions will localize data sources and communication parties. Meanwhile, effective mechanisms for protecting benign users? privacy will also be investigated. <br\/><br\/>The proposed research will provide significant insights to fight cyber crimes. The PIs will integrate research and education to recruit undergraduate and graduate via NHSEMP Program. For further information, see the project website at http:\/\/www.ee.hawaii.edu\/~dong\/traceback.","title":"EAGER: TC: Collaborative Research: Experimental Study of Accountability in Existing Anonymous Networks","awardID":"1041677","effectiveDate":"2010-08-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[463593],"PO":["543481"]},"161740":{"abstract":"A cognitive radio (CR) is a frequency-agile wireless communication device with intelligent control and a monitoring interface that enables dynamic spectrum access. The CR concept represents a paradigm change in spectrum regulation and utilization. As basic understandings gained, there is a compelling need to fully capitalize CR's high potential for supporting new applications. <br\/><br\/>This CAREER project investigates the problem of enabling rich multimedia services in emerging CR networks. Although highly rewarding, the new dimension of dynamics on channel availability, sensing, and access brings about a whole level of technical challenges. To address these challenges, a novel cross-layer optimization and control approach is employed, complemented with distributed algorithm design and development of an open source CR video testbed. The manifold design trade-offs, multifarious dynamics, scarce resources and, on the other hand, video's tight QoS constraints make the optimization and control approach highly suited for \"squeezing\" the most out of CR video networks. The three research thrusts include: cross-layer optimization of CR video networks, classical and modern control theory based analysis and design of CR video networks, and experimental research with a CR video testbed and field experiments.<br\/><br\/>This project will serve a critical need by enabling video communications in emerging CR networks for commercial and mission-critical applications. Open source software, a CR video testbed, and experimental data will be distributed in the wireless community. The research outcomes will be integrated with course development, textbook writing, involving graduate and undergraduate students in cutting-edge research, promoting diversity, and outreach to K-12 students.","title":"CAREER: Towards Rich Multimedia Experience in Emerging Cognitive Radio Networks","awardID":"0953513","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1045","name":"CAREER: FACULTY EARLY CAR DEV"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["550958"],"PO":["565303"]},"161861":{"abstract":"Successful use of computing technologies has enhanced quality of life. In fact, it is hard to point to even one aspect of societal functioning that is not impacted positively by computer systems. The tremendous advances in computer system performance while simultaneously lowering the computing cost have been the primary reason for the success of computing. However, shrinking device sizes have recently lead to new challenges in system reliability. System reliability is an uncompromising concern and is the primary focus of this proposed research. Rather than addressing one single reliability concern, such as soft errors or process variations, this proposal takes a multi-dimensional approach to improve Mean-Time-To-Failure (MTTF). Reliability degrades extremely slowly over time and hence the solutions proposed in this research are also low cost solutions. In order to develop low cost solutions, the first step is to non-intrusively monitor the health of a processor to understand its aging process before taking proactive measures for detecting errors. When the error is detected, instead of employing expensive hardware solutions, this proposal uses low cost and flexible software mechanisms to correct the errors. While any one approach will certainly extend MTTF, the true benefits of the proposed research will bear fruition when error monitoring, detection and correction are employed in a hierarchical framework based on reliability needs. <br\/><br\/><br\/>The broader impacts of this proposal are on two fronts. The proposed research is motivated by industrial concerns regarding system reliability. On the technology front, the low cost solutions developed will be transferred for industry adoption through close industry-academia interactions. Most of the proposed research ideas will be designed and implemented by the research team as research prototypes. These prototypes will be shared with industrial partners for further evaluations in an industrial setting. Woman and minority student recruitment will be one of the key driving force to encourage broader participation in the proposed research. This objective will be achieved through active participation and involvement of USC's Family of Schools in Los Angeles.","title":"CAREER: From Nonstop-Monitoring to Nano-ISA: An Adaptive Multi-Dimensional Framework for Processor Reliability","awardID":"0954211","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["518642"],"PO":["366560"]},"163841":{"abstract":"This project studies higher-level abstractions for constructing distributed<br\/>systems that integrate information and computation across administrative and<br\/>trust domains. Current practice does not offer general, principled techniques<br\/>for implementing these systems securely. To develop these techniques,<br\/>fundamental problems of security, consistency, performance, and system<br\/>evolution are being explored. Problems studied include automatic, adaptive,<br\/>secure partitioning of programs and data across the nodes of a distributed<br\/>system; new authorization logics for efficiently managing trust<br\/>relationships in a distributed system; new methods for increasing<br\/>performance of distributed systems while guaranteeing strong data<br\/>consistency; and new ways to securely and consistently evolve the structure<br\/>of persistent information.<br\/><br\/>These topics are being studied in the context of Fabric, a new platform for<br\/>secure distributed computation. Fabric is intended to support secure<br\/>integration of information systems, a valuable capability for many application<br\/>domains, including in medicine, finance, education, government, and the<br\/>military. For example, good methods for integrating distributed information<br\/>systems would support secure sharing of medical records between institutions.","title":"TC: Medium: Higher-Level Abstractions for Trustworthy Federated Systems","awardID":"0964409","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["166043","555502"],"PO":["565264"]},"163863":{"abstract":"This project deals with models of wireless multi-terminal networks incorporating practical constraints such as individual links that experience fading, applications that are delay-sensitive, network communication that is subject to broadcast and interference constraints and nodes that are constrained to operate in half-duplex mode. The network is assumed to be static for the duration of the message, but can change from one message to the next and channel-state information is assumed to be present only at the receiver. In such settings, cooperative communication in which intermediate nodes facilitate communication between a particular source-sink pair, is key to efficient operation of the network.<br\/><br\/>A key goal of any communication system, is one of achieving an optimal rate-reliability tradeoff. The diversity-multiplexing gain tradeoff (DMT) determines the tradeoff between relevant first-order approximations to the rate and reliability of communication. The DMT of point-to-point communication links has been extensively studied and signal sets are available that are optimal under any statistical distribution of the fading channel. There now exist protocols and codes for two-hop relay networks that come close to achieving the corresponding min-cut upper bound on DMT. Goals of this project include: 1) determining the DMT of various classes of multiterminal networks ranging from broadcast, cooperative-broadcast and multiple-access channel networks to layered multi-hop networks; 2) identifying the classes of networks for which the DMT of the network is given by the DMT of the min-cut; 3) assessing the impact of asynchronous operation of the network, as well as of the presence of feedback along one or more links in the network; 4) the construction of codes with lesser decoding complexity.","title":"CIF: Medium:Collaborative Research: Explicit Codes for Efficient Operation of","awardID":"0964500","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[438661],"PO":["564924"]},"163874":{"abstract":"Despite a number of recent efforts, current solutions for database administration tasks like tuning, troubleshooting, benchmarking, and capacity-planning remain far from satisfactory. Database systems have many configuration parameters to control memory distribution, I\/O optimization, costing of query plans, and other behavior. Regular users and even expert database administrators struggle to tune these parameters for good performance. The inherent complexity makes it hard for these systems to gracefully handle uncertain and dynamically-changing workloads and diverse query mixes. The .eX project addresses these challenges through a novel methodology called automated experiment-driven management which encapsulates key building blocks to automatically generate knowledge of system behavior for simplified administration and self-tuning. These building blocks include techniques to: (i) characterize good system performance models, (ii) plan experiments that generate data to learn and maintain these models efficiently under system and workload changes, and (iii) make robust decisions using these models given the inherent uncertainty in how accurately the models capture the true underlying system behavior. Apart from system administration, .eX's automated experiment-driven management can benefit applications like MapReduce computations, large-scale computational simulations, and keyword auctions in online advertising. Two new courses for graduate and undergraduate students at Duke cover principles of automated experiment-driven management. A fully-functional prototype of .eX is being developed and deployed in multiple settings to simplify database administration. The source code of .eX will be released publicly and the technology will be migrated potentially to industrial-strength system administration products. Results from .eX will be disseminated via the project Web site (http:\/\/www.cs.duke.edu\/~shivnath\/dotex.html).","title":"III: Medium:Simplifying Database Management with Automated Experimentation","awardID":"0964560","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["550820","562557"],"PO":["565136"]},"168814":{"abstract":"The vehicular ad hoc network is one of the key enabling components in Intelligent Transportation Systems that has been developed for safe and smooth driving without excessive delays. A major hurdle in the development of the networks for time-dependent safety-critical services is the lack of established models and metrics. These enable one to determine the effectiveness of the network design mechanisms for predictable quality of service, and allow the evaluation of the tradeoff between network parameters. <br\/><br\/>This collaborative research project between Oral Roberts University and Duke University analyzes and suggests enhancements of safety-critical services in vehicular ad hoc networks. Several key issues are investigated in the project. First, the project develops stochastic modeling techniques to address some open problems, such as hidden terminal issues and rebroadcast coverage problems in two-dimensional broadcast vehicular networks under typical traffic scenarios. Second, new analytical models are developed for time-dependent analysis of time-critical safety services. Consequently, performance, reliability, and survivability metrics for the safety services are defined and analyzed. Third, new solutions to assure reliable delivery of emergency messages and novel analytical model based cross-layer protocols are designed and studied. Broad impact of the work is that the outcome of this research is deemed to be beneficial to the design and analysis of general mobile ad hoc networks for critical missions. This project creates new collaborative opportunities with the industry. Both graduate and under-graduate level courses are developed to integrate the scientific findings of the project with current teaching activities at the two universities.","title":"NeTS: Small: Collaborative Research: Analytic Modeling and Enhancement of Vehicular Ad Hoc Networks for Safety Critical Applications","awardID":"1018605","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[451822],"PO":["557315"]},"168825":{"abstract":"The investigators study a new class of statistical methods for learning time series and graphical models. Their approach is based on spectral analysis and matrix decomposition methods that have enjoyed tremendous success in applications, but their use in graphical models has drawn less attention. The goal of this investigation is to extend the enormous previous successes of matrix decomposition methods to the realm of more complicated time series and certain graphical models, which will lead to new statistical machine learning algorithms with important practical applications.<br\/><br\/>In the information age, an important measure of computer intelligence is the ability to analyze huge amount of data that become available electronically, and make critical decisions under uncertain environment. Statistical machine learning is the main technique for analyzing electronic data, and graphical models are mathematical tools for understanding these complex data both by computer systems and by human operators in order to facilitate decision making. However, traditional algorithms for learning graphical models have limitations that restrict capabilities of modern computing systems. The current research attempts a new class of mathematical algorithms that can be used to design more effective graphical models, which in turn allows modern computers to analyze data more accurately and achieve higher level of intelligence.","title":"RI: Small: Spectral Methods for Learning Time Series and Graphical Models","awardID":"1018651","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["480970","477964"],"PO":["562760"]},"164469":{"abstract":"The Social Web has become an important medium for social interaction and potentially a powerful new computational tool. As people create and share information online, their collective activity shapes the structure and usefulness of the Social Web and can even be used to address a range of problems from collective decision-making to trend prediction. Understanding how the aggregate activity of many interconnected people evolves is crucial to our ability to transform the Social Web into a platform for social computing.<br\/><br\/>Mathematical modeling is a powerful tool for studying collective human activity. In previous work, the PI and collaborators developed a framework for mathematically modeling emergent behavior of groups of users on the Social Web. This framework allowed the modeler to relate aggregate behavior of a group of users to simple descriptions of their individual behavior. However, it failed to take into account key aspects of the Social Web: user diversity and the extent to which social links indicate a commonality of users' interests.<br\/><br\/>The goal of this project is to develop a methodology for modeling diverse groups of users on the Social Web and to understand how user heterogeneity affects group behavior. Mathematical modeling and analysis will lead to better, more effective Web sites by identifying productive ways to display information to users, as well as techniques for promoting collaboration and enhancing participation. Analysis will also lead to new insights into how to use human activity for computation, and eventually a programming toolkit for social computing.","title":"SoCS: A Mathematical Framework for Modeling Behavior of Diverse Groups","awardID":"0968370","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["517887"],"PO":["565342"]},"168715":{"abstract":"Continued advancements in semiconductor technology make it possible to integrate hundreds of processing cores into one silicon die. However, as the transistor size continues to shrink, increasing manufacturing defects have resulted in severe yield loss and made chip products? profit to drop substantially. Process variations also cause the performance and power characteristics differ significantly from chip to chip. In addition, this non-determinism greatly exacerbates the complexity of developing, validating, and maintaining software built upon these chips, especially for mission-critical real-time embedded applications. <br\/><br\/>This project explores the hypothesis that architectural virtualization is an effective strategy and will be the norm to address extensive process variations and manufacturing defect problems on many-core platforms. This collaborative research effort seeks to develop effective methods and techniques to virtualize hardware resources on many-core platform and thus isolate the underlying hardware non-determinisms without changing the operating system and application software.<br\/><br\/>This project seeks to improve the robustness and reliability of software and systems developed on many-core platforms. Techniques developed in the research may also help to address the component upgrade and obsolescence issue, which has been challenging military and avionics industry for decades. The collaborations between PIs and their international collaborators provide a unique opportunity to build an international research and educational collaboration structure, to share research expertise, to enhance the educational opportunity, and to promote cultural exchange. In addition, this project provides opportunity to nurture, encourage and attract students from under-represented groups to engage in this exciting research.","title":"CSR: Small: Collaborative Research: Application-Aware Many-Core Virtualization for Real-Time Embedded Computing","awardID":"1018108","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[451579],"PO":["565255"]},"168836":{"abstract":"One of the oldest problems in linguistics is to reconstruct ancient protolanguages on the basis of their modern descendants. Identifying ancestral word forms makes it possible to evaluate proposals about the nature of language change and to draw inferences about human prehistory. Currently, linguists painstakingly reconstruct protolanguages by hand, using knowledge of the relationships between languages and the plausibility of sound changes. This research project develops statistical, computational methods that automate or augment the reconstruction process. Unlike past computational approaches, these new models use detailed phonological representations to infer hidden sound changes. Moreover, they automatically infer which words are co-descendent (cognates). <br\/><br\/>These advances, combined with new algorithms for large-scale statistical inference, enable the analysis of orders of magnitude more data than prior work. The models from this project significantly expand the computational tools available to linguists; large-scale reconstructions make it possible to collect quantitative data to help answer long-standing questions about language change. Beyond word reconstruction, the models and tools from this project will be useful for other related applications, such as machine translation, where reconstructions can be used to fill gaps in the mapping between the vocabularies of different languages, and the alignment of biological sequences, which requires considering which regions in those sequences are co-descendent. In addition, the technical advances in probabilistic modeling and approximate inference methods will have cross-cutting implications for a range of modeling problems in computational linguistics, bioinformatics, statistics, machine learning, and cognitive science.","title":"RI: Small: Probabilistic Models for Reconstructing Ancient Languages","awardID":"1018733","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1311","name":"LINGUISTICS"}}],"PIcoPI":["557870","491648"],"PO":["565215"]},"168726":{"abstract":"Hybrid mechanical systems arise in many applications, including hopping, walking, and climbing robots where contact with the ground changes; skid-steering vehicles where Coulomb friction introduces stick\/slip behavior; and prosthetic devices that interact with objects. All of these applications are influenced by nonlinearity, nonsmooth transitions, and uncertainty, and these systems demand new tools in motion planning and control.<br\/><br\/>This work takes advantage of the specific structure of mechanical systems to bound the propagation of uncertainty and to develop feedback controllers that maximize robustness of execution. The work builds on state-of-the-art techniques in motion planning and estimation, including sample-based and optimization-based planning, leading to tools for uncertain hybrid mechanical systems that are analogous to control and estimation tools used for linear systems.<br\/><br\/>Example systems are used to drive algorithm development as well as to verify performance. These include 1) the Monkeybot, a robot that uses electromagnets and a single motor to locomote along a vertical wall;<br\/>2) a parkour robot that uses mechanical contact and jumping to climb narrow passages; and 3) a skid-steered vehicle that experiences discontinuous dynamics due to stick\/slip friction effects. All phases of this work include participation of undergraduates and minority students. In addition to dissemination in conferences and journals, results are disseminated on a publicly viewable wiki.","title":"RI: Small: Hierarchical Planning, Estimation, and Control for Hybrid Mechanical Systems","awardID":"1018167","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["522422","555644"],"PO":["565136"]},"168847":{"abstract":"Multi-channel systems arise whenever a signal is picked up in or sent to more than one location. They are ubiquitous in imaging and sensing, data communication and storage, and in all audio, speech, or image processing digital technologies. Their applications range from ultrasound or MRI diagnostic scanners, to radio astronomy. The rich theory of multi-channel systems addresses two scenarios: when the entire system, including the channels, can be freely designed; or when the channels may be fixed and unknown, but data at the output of each channel is fully acquired. However, in many important applications this is not the case. The characteristics of the channels are often dictated by the physics governing the sensing process, which are unknown because of interaction of the sensors with the environment or their miss-calibration. Furthermore, because of physical or cost constraints only partial (subsampled) channel data is available. The goal of this project is to develop, evaluate and demonstrate the fundamental theory and design tools to address this important class of problems.<br\/><br\/>This project aims to extend the methods of blind multi-channel deconvolution, which are currently limited to non-subsampled systems , to provide blind perfect signal reconstruction from subsampled data. The specific aims of this project are to: (1) develop the theory of blind identification of subsampled multi-channel systems; (2) propose practical methods for blind identification of such systems using perfect (or near perfect) reconstruction filter banks; (3) provide analytical tools to study and quantify various tradeoffs between conditioning (noise performance), signal distortion, robustness, and computational cost; and (4) to demonstrate performance gains on real applications, and in particular in highly-accelerated multi-channel magnetic resonance imaging (MRI), and robust image super-resolution.","title":"CIF: Small: Blind Perfect Signal Reconstruction in Subsampled Multi-Channel Systems","awardID":"1018789","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":["551068"],"PO":["564898"]},"168737":{"abstract":"The knowledge about operating system semantics is the foundation for many security applications, including virtual machine introspection, malware detection and analysis, computer forensics, etc. However, the existing techniques for extracting operating system semantics fall short. They perform static analysis on the OS source code, and thus cannot be applied to the closed-source operating systems. The source-code analysis also suffers from the WYSINWYX (i.e., What You See Is Not What You eXecute) problem. Furthermore, the obtained semantics knowledge can be easily compromised by various kernel attacks. With such an unsound foundation, the functionality and trustworthiness of these security applications become questionable.<br\/><br\/>To fortify this foundation, the PI aims to build a binary-centric and robust analysis framework for extracting operating system semantics. It is binary-centric, because it can extract semantics information from the binary code of an OS kernel. Consequently, the WYSINWYX problem can be solved and the semantics barrier of closed-source operating systems can be overcome. It is robust, because it can capture the invariants in OS-level semantics. So trustworthy semantics knowledge can be derived from these invariants, and various forgery attacks can be detected. Then with this framework, further research will be conducted to investigate how the functionality and robustness of various security applications can be strengthen. The proposed tasks will lead to the release of prototype systems and the development of education materials for undergraduate and graduate courses and for professional training sessions.","title":"TC: Small: Mining Operating System Semantics: Techniques and Applications","awardID":"1018217","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["470093"],"PO":["565327"]},"168869":{"abstract":"The world-wide web has become one of computing's great success stories,<br\/>changing the way that people around the world communicate, compute,<br\/>and conduct their business. Unfortunately, security problems on the web<br\/>are prevalent, and these problems increase costs for website operators<br\/>and for Internet users. This project aims to develop new methods for<br\/>securing the web, providing website developers and operators with new<br\/>and improved tools to protect their site and their users.<br\/><br\/>The research involves several technical directions. First, to help<br\/>protect existing websites, this project will investigate ways of hardening<br\/>legacy web application code to defend it against the most common attacks.<br\/>Second, to provide a solid foundation for web systems of the future,<br\/>this project will study how to provide robust protection for newly<br\/>developed code. The project will also study web development frameworks<br\/>that are safe by construction. Third, this project will develop tools<br\/>and techniques to incrementally migrate existing web applications to<br\/>next-generation safe-by-construction web frameworks. Fourth, this project<br\/>will devise and carry out user studies to measure rigorously the effect<br\/>of different programming languages, frameworks, and programming practices<br\/>upon the security of web applications. The broader impacts resulting from<br\/>the proposed activity are potentially significant; if it is successful,<br\/>this research could have a significant positive impact on the security<br\/>of web services and, in the longer term, on software security in general.","title":"TC: Small: Securing Web Software Systems","awardID":"1018924","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["550026"],"PO":["564388"]},"168638":{"abstract":"Modern communication systems consist of channels which have memory, are time-varying, are often poorly modeled, and incorporate feedback. Furthermore, in many of these networks, the nodes are power limited and only have modest computational resources. Thus there is a pressing need for new single-user and multi-user communication techniques that treat channels with memory and feedback. This involves determining the capacity of these more complicated channels as well as designing error-correcting codes that perform well while incurring modest computational cost.<br\/><br\/>Feedback is a very important, though poorly understood, feature of modern communication systems. Feedback has many benefits including: (1) feedback can increase the capacity of a channel; (2) feedback can increase the error exponent and hence decrease latency; (3) feedback can often lead to simpler coding schemes; (4) feedback can allow for adaptation to unknown or time-varying channels; and (5) feedback can allow for coordination in multi-user settings. This research involves developing a general theory of feedback for reliable communication that fully exploits these different benefits. The four main objectives of this research are: (1) Determining fundamental limits and tradeoffs. This aspect of the project considers the fundamental limits and tradeoffs between the quality of the feedback and the resulting Shannon capacity of the channel. (2) Development of computational tools for computing capacity. This research will include software development. (3) Algorithm and error-correcting code development. This research supports the development of efficient codes that take advantage of feedback. (4) Educational development.","title":"CIF: Small: The Role of Feedback in Reliable Communication","awardID":"1017744","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["517534"],"PO":["564924"]},"168407":{"abstract":"This research is aimed at developing constructive methods and algorithms for computational analysis of systems of partial algebraic difference-differential equations (PADDEs) and description of their solution sets. Such systems arise in a wide variety of problems in mathematics and its applications including mathematical physics, automatic control, dynamical systems, mechanics, molecular chemistry, and cellular biology.<br\/><br\/>The key research objectives of this project are: (1) development of the theoretical foundation and algorithms for difference-differential elimination, in particular for decomposing solution sets of systems of PADDEs into unions of \"simple\" sets; (2) extension of the constructive methods of difference-differential algebra to the computational analysis of systems of partial differential equations (PDEs) with group action (this is of special interest for applications, since the solutions of fundamental systems of PDEs governing physical fields must be invariant with respect to certain group actions); (3) elaboration of methods and algorithms for computation of dimension polynomials that express Einstein's strength of a system of PADDEs. Such algorithms, in particular, will allow one to choose optimal (in the sense of A. Einstein) systems of PDEs for mathematical models of physical processes.<br\/><br\/>The main methods and approaches to be used include the characteristic set technique, which will be extended to rings of difference-differential polynomials, generalized Groebner basis method for difference-differential modules, the technique of univariate and multivariate dimension polynomials, and decomposition methods for PADDEs.<br\/><br\/>Despite the over sixty-year history of algorithmic approaches in differential and difference algebra, initiated by J. Ritt, E. Kolchin, R. Cohn and recently expanded by M. Bronstein, X. Gao, P. Hendrics, and M. Singer, among many others, there are no computational methods efficient enough to allow one to determine structures of solution sets of systems of algebraic difference-differential equations in many cases of interest. The proposed activity will result in the improvement of the existing algorithmic methods for PADDEs and more general systems of partial differential equations with group action, development of the constructive theory of difference and difference-differential ideals and, as a consequence, creation of new computational techniques for analysis of partial difference and difference-differential equations and their solution sets.<br\/><br\/>The research will develop algorithms and computational techniques that will be of use to analysts, physicists, engineers, and scientists in many other fields where the theoretical description of processes involves algebraic differential, difference, or difference-differential equations. The resulting algorithms will be the basis of code appearing in symbolic computation computer packages used in education and research in mathematical physics, automatic control, mechanics, biology, and in many other areas as well. The educational component of the project also includes an interdisciplinary program that will involve mathematics, computer science, physics, and engineering majors in training and research with the active use of computer algebra methods.","title":"AF: Small: Computational Methods for Difference-Differential Equations","awardID":"1016608","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[450850],"PO":["565251"]},"168539":{"abstract":"The widespread demand for high rate multimedia wireless communications has drastically increased the power consumption of wireless networks. However, the advances in battery technology have not kept pace, resulting in a severe mismatch between the energy thirst and battery capacity of mobile units. The major objective of this research is to develop energy-efficient techniques for robust and secure wireless networks through distributed optimization of cross-layer network design.<br\/><br\/>This research performs a comprehensive investigation into energy-efficient wireless communications. In wireless networks, the channel conditions of users, at different locations, times, and frequencies are different. The quality of service and network efficiency can be significantly improved if these differences can be exploited. Since all protocol layers impact energy consumption, the investigators will develop cross-layer energy-efficient techniques to reduce redundant message transfers and the associated energy consumption. This research will focus on energy-efficient transmission and resource allocation strategies that are amenable to applications in wireless OFDM and MIMO systems, and emphasize energy efficiency over peak rates. To enhance energy efficiency for wireless networks, novel approaches and techniques based on new performance criteria for integrative network optimization will be investigated. Key energy considerations for developing cross-layer network media access protocols and resource allocation algorithms will be integrated into the new framework.","title":"CIF:Small: Collaborative Research: Distributed PHY\/MAC Optimization for Energy and Spectral Efficient Wireless Networks","awardID":"1017192","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7939","name":"WIRELESS COMM & SIGNAL PROCESS"}}],"PIcoPI":["562754"],"PO":["564924"]},"168429":{"abstract":"Many application domains, such as intelligence, counter-terrorism,<br\/>forensics, disease control, often need to cross-match multiple very<br\/>large datasets, such as watch lists. Because those datasets may <br\/>contain privacy-sensitive or confidential information, the use of efficient<br\/>privacy-preserving protocols for cross-matching different datasets is<br\/>crucial. The problem of privacy-preserving record matching has been<br\/>addressed by the use of Secure Multi-party Computation (SMC) protocols.<br\/>Under these protocols, the data are converted to series of functions<br\/>with private inputs. However a major drawback of SMC-based protocols<br\/>is that they involve extensive cryptographic primitives such as<br\/>homomorphic encryption which do not scale to the size of practical<br\/>problems. As a result, SMC-based protocols cannot be used for resource<br\/>constrained data-intensive privacy-preserving record matching approaches <br\/>directly. This project develops a novel approach based on the observation<br\/>that to apply SMC to practical applications, one needs to bridge the gap <br\/>between the size of the datasets that can efficiently be matched using <br\/>SMC protocols and the size of the datasets seen in practice. The approach <br\/>taken by the project tackles the problem from a novel angle by developing <br\/>techniques to reduce the size of practical problems by employing <br\/>privacy-preserving data sanitization methods. The project thus solves <br\/>the privacy-preserving data matching problems through the following<br\/>steps. First, to protect the privacy of data subjects, useful statistics <br\/>about data is gathered using differential privacy. Second, differentially <br\/>private statistics are shared among the parties involved in data matching. <br\/>These parties then identify potential matching pairs where fruitful matching <br\/>may occur. Such a step is referred to as data blocking. Finally, SMC <br\/>techniques are applied to these candidates to accurately cross-match<br\/>information. In addition to syntactic matching, semantic matching is supported <br\/>by which records are compared according to some semantic similarity functions.<br\/>The semantic matching protocols includes techniques for matching and<br\/>aligning ontologies, as the use of ontologies is crucial for an effective<br\/>semantic matching. This project is the first to use differential privacy for<br\/>efficient privacy-preserving record matching that also leverages semantics-based<br\/>approach and a privacy-preserving approach to ontology alignment. The techniques<br\/>developed in the project are the first to achieve efficient privacy-preserving<br\/>matching of large scale data sets using differential privacy, thus overcoming <br\/>the scalability problems of conventional SMC techniques. The approach developed <br\/>in this project expands the opportunities and contexts for data use by enabling <br\/>the cross-match of multiple data archives, possibly owned by different parties, <br\/>without violating the privacy of the data. Many applications, of interest <br\/>for our society, will benefit by such opportunities.<br\/>For further information see the project web site at the URL:<br\/>http:\/\/www.cs.purdue.edu\/homes\/bertino\/prirelink","title":"TC: Small: Collaborative: Protocols for Privacy-Preserving Scalable Record Matching and Ontology Alignment","awardID":"1016722","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["529959"],"PO":["565327"]},"173951":{"abstract":"This proposal requests funding to assist US-based graduate students and junior faculty to travel and attend the 29th IEEE Symposium on Reliable Distributed Systems (SRDS-2010), which is to be held in New Delhi, India on October 31-November 3, 2010. The support requested in this proposal will enable the participation of students and junior faculty, especially those from under-represented groups, who would otherwise be unable to attend SRDS-2010.","title":"Travel Grants for Attending the 29th IEEE Symposium on Reliable Distributed Systems (SRDS)","awardID":"1047647","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["556632"],"PO":["535244"]},"161752":{"abstract":"The standard platform architecture for real-time embedded systems has increasingly shifted away from single processor platforms to multiprocessor platforms. The recent shift towards multiprocessor platform architectures has resulted in increased consolidation and integration of multiple subsystems upon shared processing platforms, aided in part by virtualization execution environment (VEE) technologies. The potential impact of the subsystem-integration approach is a significant reduction in the size, weight, and power (SWaP) requirements of integrated systems over non-integrated systems. However, the tighter physical integration of subsystems upon a shared processing platform introduces fundamental questions on how the processor's computational resources should be effectively allocated among the contingent subsystems and how temporal isolation between real-time subsystems should be achieved. <br\/><br\/>The overall objective of this NSF CAREER project is to obtain solutions to the above questions via development of effective real-time scheduling algorithms, formal analysis, and tools for supporting tunable temporal isolation of subsystems upon a multicore VEE platform. The specific research objectives of the project are: effective system and subsystem real-time scheduling algorithms for VEE frameworks, protocols for resource sharing between subsystems, schedulability analysis, and implementation of a VEE for controlling a real-time robotic system. The educational goal of the project is to increase overall awareness and understanding of the importance of building and verifying temporally correct systems by: recruiting graduate and undergraduate students from underrepresented populations into embedded systems research; developing embedded systems curriculum and outreach training program; introducing K-12 students to general embedded systems concepts; and developing online real-time systems repository for students, researchers, and faculty.","title":"CAREER: Real-Time Platform Virtualization in Multiprocessor Systems: Temporal Isolation and Allocation","awardID":"0953585","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["511432"],"PO":["565255"]},"163831":{"abstract":"Parallel data structures and algorithms are becoming an increasingly important research area, due to the rapid advances in GPUs and other massively parallel commodity multi-core hardware along with the software needed to program these devices. In this collaborative effort involving the University of California at Davis and Harvard University, the PIs will focus on the design and implementation of parallel hash tables, one of the most fundamental of data structures, on the new platforms. Real-time parallel hashing would enable a variety of graphics applications on dynamically changing data, including spatial hashing, surface and image matching, and hashed octrees which in turn enable a host of other applications including Boolean surface operations, point-cloud nearest neighbors, ray-tracing acceleration and photon mapping. In prior work, the PIs built a baseline implementation that shows effective parallel hashing can be done on the GPU; they can construct the table as quickly as the fastest available radix sort, and can execute parallel random access on the elements much more quickly than binary search. In the current research, the PIs plan to improve upon their baseline implementation significantly, while also focusing on related structures such as multi-maps and Bloom filters. New designs and construction algorithms will be developed, implemented, and analyzed with respect to performance, and then applied to a variety of computer graphics applications. The PIs expect this work to lead to interesting theoretical results; modern hash table constructions have never been considered in the parallel context, so finding the right model for analysis is one goal of the research.<br\/><br\/>Broader Impacts: This project will contribute to the computing infrastructure, not only for computer graphics but also for general-purpose computation. The PIs will distribute their implementations freely, in part by extending and building upon their existing (and popular) library of general-purpose data structures (the CUDA Data Parallel Primitives). The PIs note that making the most of the emerging parallel GPU resources requires training the next generation of programmers to think in parallel; therefore, they plan to exploit this project as an opportunity to revive a long-untaught undergraduate parallel programming course, in addition to studying parallel algorithms with their graduate students.","title":"HCC: Medium: Collaborative Research: Data-Parallel Hash Tables: Theory, Practice and Applications","awardID":"0964357","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["538707","459062"],"PO":["565227"]},"172323":{"abstract":"This proposal requests funds for a \"Million Book Project\" partners research and coordination meeting. Begun in 2000, the Million Book Project has scanned over 1.6 million books in China, India, Egypt and Australia and made great strides in research areas relevant to large-scale, multi-lingual database storage and retrieval. Project partners intend to continue to work together on issues related to human computer interactions, usability, automatic metadata detection and correction using artificial intelligence, intellectual property, machine translation and summarization, improving and providing centralized access to metadata, long term data storage and access issues, diversity and education. Funding provided by the National Science Foundation has attracted international partners and matching funds exceeding $100 million U.S. dollars.","title":"Million Book Project Partners Meeting 2010","awardID":"1038216","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[461693],"PO":["564456"]},"163864":{"abstract":"This project deals with models of wireless multi-terminal networks incorporating practical constraints such as individual links that experience fading, applications that are delay-sensitive, network communication that is subject to broadcast and interference constraints and nodes that are constrained to operate in half-duplex mode. The network is assumed to be static for the duration of the message, but can change from one message to the next and channel-state information is assumed to be present only at the receiver. In such settings, cooperative communication in which intermediate nodes facilitate communication between a particular source-sink pair, is key to efficient operation of the network.<br\/><br\/>A key goal of any communication system, is one of achieving an optimal rate-reliability tradeoff. The diversity-multiplexing gain tradeoff (DMT) determines the tradeoff between relevant first-order approximations to the rate and reliability of communication. The DMT of point-to-point communication links has been extensively studied and signal sets are available that are optimal under any statistical distribution of the fading channel. There now exist protocols and codes for two-hop relay networks that come close to achieving the corresponding min-cut upper bound on DMT. Goals of this project include: 1) determining the DMT of various classes of multiterminal networks ranging from broadcast, cooperative-broadcast and multiple-access channel networks to layered multi-hop networks; 2) identifying the classes of networks for which the DMT of the network is given by the DMT of the min-cut; 3) assessing the impact of asynchronous operation of the network, as well as of the presence of feedback along one or more links in the network; 4) the construction of codes with lesser decoding complexity.","title":"CIF: Medium: Collaborative Research: Explicit Codes for Efficient Operation of Wireless Networks","awardID":"0964507","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["554954"],"PO":["564924"]},"174205":{"abstract":"Technologies for storing and processing vast amounts of text are mature and well-defined. In contrast, technologies for browsing or mining content from large collections of non-textual material, especially audio and video, are less well developed. Large sale data mining on text has helped transform the relevant disciplines; the disciplines dealing with spoken language will reap similar benefits from accessible, searchable, large corpora.<br\/><br\/>This project explores the difficult problem of providing rich, intelligent data mining capabilities for a substantial collection of spoken audio data in American and British English. It applies and extends state-of-the-art techniques to offer sophisticated, rapid and flexible access to a richly annotated corpus of a year of speech (about 9,000 hours, 100 million words, or 2 terabytes), derived from the Linguistic Data Consortium, the British National Corpus, and other existing resources. This is ten times more data than has previously been used by researchers in fields such as phonetics, linguistics, and psychology, and 100 to 1,000 times the amounts that are used in common practice.<br\/><br\/>Speech-to-text alignment and search tools will open a new universe of data to researchers in many fields, from linguistics and phonetics to anthropology, speech communication, oral history, and media studies. Audio-video usage on the internet is large and growing at an extraordinary rate, offering increasingly large amounts of an increasingly large range of material. Reliable automatic annotation, indexing and search of this material will allow researchers to examine the distribution of both form and content across time, space, and social structure.","title":"EAGER: Mining a Year of Speech","awardID":"1048900","effectiveDate":"2010-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1311","name":"LINGUISTICS"}}],"PIcoPI":["507484","501444",467345],"PO":["565215"]},"174568":{"abstract":"Enterprise applications are increasingly being migrated to the cloud. The central challenge in cloud-based enterprise operations is ensuring that enterprises can control key aspects of their communication such as security and performance, and service providers can offer the right primitives to meet customer requirements. The proposed CloudNet framework will optimally accommodate diverse needs of both enterprises and cloud providers.<br\/><br\/>The project leverages a large GENI-based experimentation framework. Cloud providers, WAN, enterprise servers and clients are mapped to campus OpenFlow networks, Internet2 and Supercharged PlanetLab, PlanetLab and the CMULab wireless emulator, respectively. The experiment explores the effectiveness and trade-offs of key techniques in CloudNet.<br\/><br\/>Broader impacts of the proposed research include enabling sophisticated enterprise applications that offer a rich user experience while also accounting for performance, security and privacy. The proposed work will highlight how providers can build rich cloud-based services. The research will be integrated into undergraduate and graduate curricula.","title":"EAGER: CloudNet","awardID":"1050170","effectiveDate":"2010-08-15","expirationDate":"2013-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["553786"],"PO":["564993"]},"168936":{"abstract":"Santa Clara University, in partnership with Santa Clara School Districts, proposes to extend the reach of the Exploring Computer Science (ECS) course developed in Los Angeles into high schools in northern California. ECS was designed to be an engaging introduction to computing that is contextualized to be socially relevant and meaningful for diverse student populations. It covers a variety of topics, including problem solving, programming, Web design, data modeling, human computer interaction, robotics, ethical and social implications, and career opportunities. This project will demonstrate how the course can be replicated throughout California and other states. It will develop a course implementation package that includes alternative curriculum units expressly written for students from different cultures and with different life experiences, and materials that facilitate the successful engagement of administrators, the recruitment of students, the preparation of teachers, and additional course support. This work is part of a much long-term effort to work with local schools and their teachers, as well as organizations such as NCWIT and CTSA, in providing a comprehensive approach to increasing participation in computing in Silicon Valley.","title":"Special Project: Expanding the Impact of Computer Science in Silicon Valley High Schools and Facilitating Adoption of the ECS Curriculum Elsewhere","awardID":"1019217","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7482","name":"BROADENING PARTIC IN COMPUTING"}}],"PIcoPI":["555558","465871","465872","465873"],"PO":["561855"]},"166758":{"abstract":"The PI will consider two facets of auction design that are not only of fundamental importance, but also have defied significant positive results despite extensive research in the economics literature. These facets arise from practical constraints imposed on auctions: Budget constraints on the bidders, and asymmetric and incomplete information between the bidder and auctioneer. The PI seeks to develop and employ techniques from theoretical computer science, particularly approximation algorithms, adversarial mechanism design, stochastic control, and learning theory to develop novel tools and techniques to address these questions. The intellectual merit of the proposed research will be in blending, encompassing, and extending work on related problems in theoretical computer science, economics, stochastic decision theory, and computational learning, thereby developing new theoretical models and solutions, which in turn will impact the respective disciplines. Furthermore, it is imperative to develop such tools since the facets we consider will become critical to the successful deployment of auctions in several modern internet-based settings. The proposed work will also be broadly disseminated via workshops and specialized courses.","title":"AF: Small: Auction Design in Constrained Settings","awardID":"1008065","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7932","name":"COMPUT GAME THEORY & ECON"}}],"PIcoPI":["562557"],"PO":["565251"]},"168705":{"abstract":"A symbolic framework for the analysis of logic, timing, and probabilistic properties of computer systems is developed, using decision diagrams for the storage and manipulation of large data structures. Decision diagrams have been enormously effective in verification, but their potential has not been explored much in other settings. The framework includes symbolic solutions for Markov models based on efficient classes of edge-valued decision diagrams to represent rate matrices, using under- and over-approximations to obtain bounds when an exact numerical study is infeasible, relying on aggregation or partial exploration of states at the logic level, computing probability bounds at the numerical level, and exchanging numerical values between hierarchical submodels. The framework also addresses non-Markov settings, general distributions, and nondeterministic interval ranges for the timing of events by exploring the limits and potentials of symbolic encodings, hierarchical composition, and bounds, including hybrid techniques that integrate traditional discrete-event simulation with symbolic algorithms.<br\/><br\/>The research results will positively affect several areas of computer science and engineering, by providing researchers and engineers with the ability to study the logic, timing, and probabilistic properties of much larger and more general system models than currently possible. The software packages developed during this project will be an excellent hands-on tool for students and practitioners in need to model, verify, or analyze the logic and timing behavior of computer systems.","title":"SHF: Small: A Hierarchical Symbolic Framework to Verify Logic, Timing, and Probabilistic Properties of Computing Systems","awardID":"1018057","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[451554],"PO":["565264"]},"168837":{"abstract":"----<br\/>Communication networks are, increasingly, the backbone upon which the world's financial, educational, and governmental institutions rely. Surprisingly, very little is known about how much information current communication networks can carry. The absence of tools for characterizing the \"capacities\" of large, complex networks makes it difficult to determine how to improve the capabilities of current communication networks or to design better networks for the future. This research involves the development of new systematic strategies for building computational tools for characterizing the capacities of large networks.<br\/><br\/>This research involves bounding the capacity of a complex network by first factoring the network into individual components, then replacing those components by simpler models, and finally employing computational tools to bound the capacities of the modeling networks. Researchers create a library of component models by deriving both upper and lower bounding models for each component studied. Unlike prior mechanisms for characterizing the behaviors of individual channels, these upper and lower bounding models capture the full range of behaviors of an individual component. Thus the capacity of any network is bounded from above by the capacity of another network where each component is replaced by its upper bounding model and bounded from below by the capacity of a distinct network where each component is replaced by its lower bounding model. Researchers are developing models for both individual transmission devices like broadcast, multiple access, and interference channels and sub-networks built from component models. Modeling sub-networks allows networks to be analyzed hierarchically, yielding a technique that scales to very large networks.","title":"CIF: Small: Computational Tools for Bounding Network Capacities","awardID":"1018741","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["551140"],"PO":["564924"]},"168848":{"abstract":"Over the past few decades, the problem of data integration (DI) has received significant attention. Much of the initial attention was directed at integrating business data. Such data typically requires exact integration, because anything less is clearly not usable. Today, such exact DI systems continue to play an important role. But they are ill-suited for many emerging domains, such as personal information management, building Web community portals, scientific data management, text management for business intelligence, public safety, and military intelligence analysis. First, they are typically constructed in ``one shot'' in that the system is substantially unusable until it is completed in all of its envisioned generality. Second, when presented with new data, these systems often incur long delays before making the data available to users. Third, they typically are not designed to benefit from user feedback, even though opportunities for such feedback often exist in today's Web 2.0 world. Fourth, exact DI systems provide little or no assistance in explaining answers to users. In response, this project explores a paradigm shift, from precise DI systems to best-effort ones. Instead of being constructed in one shot, these systems are constructed incrementally. Their data is always queryable some fashion. They tolerate mistakes in the data, and can leverage user feedback to improve over time. Finally, they can explain their answers to the users, thereby allowing them to understand, verify, and trust query results.<br\/><br\/>To build best-effort DI systems, researchers will pursue the following technical thrusts. (1)Increasing support for incremental development through the specification and implementation of a declarative, semantically transparent extraction\/integration language, together with an effective optimization and execution framework. (2) Leveraging the power of a user community through the design and implementation of techniques that allow users to correct errors in the extraction\/integration process as they are encountered, that consistently propagates these corrections throughout the extracted and integrated data, and that use these corrections to improve the quality of extraction\/integration modules. (3) Developing and implementing techniques to capture information that will help users reason about the system's data along with support for exploring the implications of this information. The team will combine the technology to build a prototype end-to-end best-effort DI system and evaluate the system on three real-world applications: the DBLife portal, the GLEON limnology project, and the madison.com Web portal.<br\/><br\/>This research will be integrated with ongoing efforts in educating students on techniques for extracting and integrating structured data. Inclusion of underrepresented minorities in the projects will be continued. The results from this project will be incorporated into a textbook on data integration to be published in 2010-2011. The project will facilitate the widespread deployment of data integration systems, thus resulting in more effective information management and access for society. It will play an integral part in educating next-generation professional workers and researchers. The research will also help domain scientists in limnology in the context of the GLEON project. It also has the potential to help the developers of madison.com build a system of much greater use to the greater Madison community. Finally, data and system artifacts from the project will be disseminated broadly in the research community to significantly enhance the data management infrastructure for research and education.","title":"III:Small:Enabling Technology for Best-Effort Data Integration Systems","awardID":"1018792","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[451905,"499351"],"PO":["565136"]},"168507":{"abstract":"The widespread demand for high rate multimedia wireless communications has drastically increased the power consumption of wireless networks. However, the advances in battery technology have not kept pace, resulting in a severe mismatch between the energy thirst and battery capacity of mobile units. The major objective of this research is to develop energy-efficient techniques for robust and secure wireless networks through distributed optimization of cross-layer network design. <br\/><br\/>This research performs a comprehensive investigation into energy-efficient wireless communications. In wireless networks, the channel conditions of users, at different locations, times, and frequencies are different. The quality of service and network efficiency can be significantly improved if these differences can be exploited. Since all protocol layers impact energy consumption, the investigators will develop cross-layer energy-efficient techniques to reduce redundant message transfers and the associated energy consumption. This research will focus on energy-efficient transmission and resource allocation strategies that are amenable to applications in wireless OFDM and MIMO systems, and emphasize energy efficiency over peak rates. To enhance energy efficiency for wireless networks, novel approaches and techniques based on new performance criteria for integrative network optimization will be investigated. Key energy considerations for developing cross-layer network media access protocols and resource allocation algorithms will be integrated into the new framework.","title":"CIF:Small: Collaborative Research: Distributed PHY\/MAC Optimization for Energy and Spectral Efficient Wireless Networks","awardID":"1017053","effectiveDate":"2010-08-15","expirationDate":"2013-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7939","name":"WIRELESS COMM & SIGNAL PROCESS"}}],"PIcoPI":[451082],"PO":["564924"]},"168639":{"abstract":"Well-designed visualizations leverage the human visual system to help people understand large data sets. Yet, producing effective visualizations is a challenging design task. Designers must carefully choose how to map the data to visual variables such as position, size, shape and color. In this process they make hundreds of nuanced judgments while balancing perceptual and cognitive tradeoffs. In response, researchers in psychology, cartography, statistics, and computer science have investigated the effects of different visual variables on graphical perception: the ability of viewers to interpret visual encodings and thereby decode information in graphs. Despite great progress in developing design guidelines based on laboratory experiments, comprehensive evaluation of the visualization design space and real-world validation of the resulting guidelines have remained elusive.<br\/><br\/>The research is advancing our understanding of graphical perception and formulate new guidelines for visualization design. The research involves new experiments to address unresolved issues in graphical perception, including large-scale web-based studies using crowdsourcing techniques and controlled laboratory studies using sensitive measurements, namely eye-tracking. The investigators are applying the results of these studies to (a) develop guidelines for effective visualization design, (b) instantiate these guidelines in automated design procedures, and (c) validate the guidelines and resulting tools through case study deployments.","title":"HCC: Small: Graphical Perception Revisited: Developing and Validating Design Guidelines for Data Visualization","awardID":"1017745","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["563750"],"PO":["532791"]},"172885":{"abstract":"To stop anonymous tools designed for free speech from being abused by criminals, this project investigates practical solutions to trace back criminals while support free speech for benign users, by exploiting two unique perspectives. First, it utilizes the resource advantages of law enforcement to explore the limitations of anonymous tools. As criminals operated from remote locations usually do not have resources to build large-scale systems, they have to rely on existing anonymous tools with third-party resources to hide their traces. Second, the proposed solutions aim to capture some criminals, without a specific target at the beginning. Such assumption greatly simplifies the system design and makes it feasible, different from common traceback solutions which aim at a specific target from the start and usually require heavy costs for large-scaled deployment.<br\/><br\/>This project will examine the implementation limitations of Freenet for asynchronous communications and Tor for interactive communications, and develop tracing back solutions for law enforcement to identify data sources and parties involved in malicious transactions. Effective methods to penetrate these systems will be designed for collective traffic analysis. By focusing on known malicious data sharing to further identify malicious parties, the proposed solutions will localize data sources and communication parties. Meanwhile, effective mechanisms for protecting benign users? privacy will also be investigated.<br\/><br\/>The proposed research will provide significant insights to fight cyber crimes. The PIs will integrate research and education to recruit undergraduate and graduate via NHSEMP Program. For further information, see the project website at http:\/\/www.ee.hawaii.edu\/~dong\/traceback.","title":"EAGER: TC: Collaborative Research: Experimental Study of Accountability in Existing Anonymous Networks","awardID":"1041739","effectiveDate":"2010-08-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[463658],"PO":["543481"]},"173776":{"abstract":"This exploratory project examines how techniques for automatically generating dialog contributions and managing dialog can be incorporated into the narrative structures of outdoor role-playing computer games, such as quests and mysteries. The underlying hypothesis is that the effective use of natural language and dialog technologies in interactive games will eventually lead to more compelling and engaging games, appealing to a much wider segment of the population, and usable for a much wider range of educational, assistive and entertainment applications. As a vehicle for testing the team's research ideas, a prototype novel physical-activity based social role-playing mystery game, Spy Feet is implemented, aimed at encouraging physical activity in young women and girls.<br\/><br\/>Spy Feet runs on mobile devices equipped with GPS, and the game world is mapped onto an outdoor environment consisting of paths and landmarks that are used by plot points in the game. Players must walk between landmarks where dialogs with game characters take place or where clues are located, thus covering a significant amount of a natural terrain while playing the game. Spy Feet utilizes a prototype natural language generation engine, Spy-Gen, that dynamically generates dialogue game narrative sequences whose characterizations and interaction style is targeted at young women and girls. Spy Feet uses Spy-Gen to dynamically adapt aspects of game play to the user and her environment in order to explore how such techniques lead to different game play experiences and outcomes.","title":"EAGER: Spy Feet: Natural Language Generation for Games for Girls","awardID":"1046437","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["542063","536598","522608"],"PO":["565215"]},"174502":{"abstract":"Project Summary<br\/><br\/>Self-assembly is a process by which simple objects assemble into complex structures under minimal or no external control. It is believed that self-assembly technology will ultimately permit precise and efficient fabrications of nanostructures. Self-assembly is common in nature but is not yet well understood from mathematical and programming (i.e., algorithmic) perspectives. There are many kinds of self-assembly. This project will focus on self-assembly of DNA molecules.<br\/><br\/>Small molecules consisting of multiple DNA strands have been designed to act as four-sided building blocks (which are called tiles) for DNA self-assembly. Experimental work has demonstrated that these building blocks can effectively perform computation as well as assemble crystals. Some key aspects of the self-assembly process of such building blocks have been used to formulate a preliminary mathematical model called the abstract tile assembly model. This model extends Wang's mathematical theory of two-dimensional tilling by adding a natural mechanism for growth. The model consists of a set of square tiles. The four sides of a tile are each associated with a glue (which is implemented as a DNA strand). A special tile in the tile set is designated as the seed. Self-assembly proceeds by starting with the seed and then attaching copies of tiles from the tile set one by one to the growing seed whenever the total binding strength between a tile and the seed is no less than a threshold (which is implemented as the temperature in the tube).<br\/><br\/>Intellectual Merit: This project is in the intersection of nanotechnology and computer science with a focus on exploring the potential and limitation of DNA self-assembly from the perspective of algorithms. <br\/>The project will build on the PI's prior results and insights in this emerging field to explore theories of encoding algorithms into the glues of DNA tiles to guide the self-assembly process. Specifically, the project will investigate three interconnected research directions. These directions together will explore new ways to minimize the number of tiles with distinct glues (which is called the tile complexity) used to assemble a structure, to minimize the amount of time needed to assemble a structure, and to impose desirable structural properties on the assembly process as well as on the assembled structures. A common theme across these directions is to seek ways to automate the design of DNA self-assembly systems. Our approaches in this project will be theoretical, and we will design algorithms and prove complexity bounds for these directions. <br\/><br\/>Broader Impacts: Algorithmic DNA self-assembly is both a form of nanotechnology and a model of computation. As a computational model, algorithmic DNA self-assembly first encodes a computer program for a given computational problem into the glues of DNA tiles. The tiles then bind with each other to execute the program to produce a DNA nanostructure, which in turn encodes the desired output of the computational problem. As a nanotechnology, the goal of algorithmic DNA self-assembly is to design glues to program a set of tiles to assemble into the desired nanostructure. The PI has taught a new course this past spring quarter (spring 2010) on Algorithmic DNA Self-Assembly at the level of advanced undergraduate students and first-year graduate students. The PI will continue to teach this course on a regular basis in the next few years either as a lecture-based course or as a seminar-based course. The results which the PI will obtain from this project will be incorporated into the course. This course introduces students to research opportunities in the intersection of algorithms and nanotechnology and more generally uses science-fiction-like research in DNA self-assembly to promote multidisciplinary research and thinking by students.<br\/><br\/>Key Words: DNA self-assembly; algorithms; complexity theory; models of computation; nature-inspired computing; nanotechnology.","title":"EAGER: Algorithmic DNA Self-Assembly","awardID":"1049899","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7946","name":"BIO COMPUTING"}}],"PIcoPI":["517969"],"PO":["565223"]},"170058":{"abstract":"Abstract<br\/><br\/>Data visualization forms an important aspect of analysis in the field of visual analytics. Analysts rely on visual tools to process massive data sets and discover meaningful patterns in the data. A common strategy for many visualization tools is to transform high-dimensional data to an intermediate lower-dimensional space and then project to screen space using a visualization transformation. For example, a data set with 200 dimensions can be transformed to an intermediate 4D representation and then mapped to screen space by using two-dimensions<br\/>for the location and two dimensions to determine shape and color. Therefore, the mathematical foundations of visualization are closely related to the problem of dimensionality reduction.<br\/>While dimensionality reduction is a necessary step to visualize the data, the final goal of visual analytics is data analysis, such as searching, clustering, and the detection of outliers. Therefore, there is an urgent need to study dimensionality reduction techniques that are especially useful for data analysis. <br\/><br\/>This research involves the development and implementation of linear and nonlinear dimensionality reduction algorithms for the transformation and visualization of high-dimensional data. The novel aspect of the transformation is that dimensionality reduction and clustering are performed simultaneously in a joint framework. In addition, this research involves the development and implementation of novel algorithms for multi-source data transformations based on multiple kernel learning (MKL). This addresses the question of fusing a multitude of heterogeneous independently collected data. In the past, most research on MKL has focused on supervised learning. One major contribution of this research is to extend MKL to the unsupervised case. This research presents visual analytics as a bridge between theoretical foundations in machine learning and real-world applications. This research is utilizing two testbed data bases, one consisting of printed documents as might be used by the intelligence community and one based on public health information.","title":"Multi-Source Visual Analytics","awardID":"1025177","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7703","name":"FOUNDATIONS VISUAL ANALYTICS"}}],"PIcoPI":[455160,455161,"456042"],"PO":["562984"]},"173204":{"abstract":"This grant will defray (partial) travel costs of PhD students attending the IEEE Communications Society Conference on Sensor, Mesh, and Ad Hoc Communications and Networks (SECON) 2010. SECON has established itself as one of the premier conferences in the areas of sensor, ad hoc, and mesh networking, serving as a meeting point of researchers from academia and industry, as well as practitioners in diverse fields of wireless networking. SECON 2010 will be held in Boston, Massachusetts, June 21-25, 2010. Participation in such technical forums is critical to a PhD student?s maturity and development. To facilitate such experiences, this proposal requests funding to defray partial travel support to students who might otherwise be unable to attend the conference. Today?s graduate students will play a central role in shaping the future of wireless technology. Attendance and participation in the top conference in one's field is a key part of the socialization of graduate students to the academic and research community, which will broaden their perspectives and have cascading impacts. We outline specific plans to use the travel grant to encourage the broadening of the demographics of students in attendance at SECON.","title":"IEEE Communications Society Conference on Sensor, Mesh, and Ad Hoc Communications and Networks (SECON) 2010: Student Travel Awards","awardID":"1043186","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[464564],"PO":["564993"]},"164503":{"abstract":"With the advent of powerful wireless devices over the last several years, the delineation between activities typically reserved for the home computer and the mobile device has blurred considerably. The new opportunities for \"always on\" access have significant sociological impacts with respect to communication between individuals and the communities that they form. The new network access inspires a set of related fascinating research questions. What effect does pervasive, wireless network access have on the social interactions of young adults, in particular college-aged students? Conversely, what impact would a better understanding of these social interactions have on the underlying wireless and wired networks? What is the relationship between the two systems, i.e., does the wireless network truly drive social interactions <br\/>or do social interactions drive demand on the wireless network? <br\/><br\/>The intellectual merit of the proposal will be to gather high quality, long-term social network and behavioral data and then to analyze the data from sociological and technical perspectives. The work will provide two hundred and fifty smart phones to a diverse cohort of first-year University of Notre Dame undergraduate students. The infrastructure will build on lessons from previous efforts at MIT to create a second-generation system for the purpose of tracking the social ties, communication patterns, and behaviors of the cohort over two years. The broader impact of the work will include inter-disciplinary contributions for both computer science and sociology. The work will make available a tremendous body of anonymized data to improve both social and network performance research.","title":"SoCS: Explorations on the Effects of Pervasive Networking on Social Relationships and Resource Planning","awardID":"0968529","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["510306","522297","540626","474129"],"PO":["564456"]},"167606":{"abstract":"Despite the significant advances in robotics research and development over the years, there are still no pervasive intelligent mobile robots coexisting with humans in daily environments. Among the many possible reasons as to why this is the case, this project addresses the challenge of an effective concrete interaction of mobile robots with humans, focusing on tasks which enable joint human and robot performance and require spatial interaction. The PI's vision is that project outcomes will make it possible to have multiple robots in, say, an office building available for different navigational and informational tasks, including accompanying daylong visitors through their schedule of meetings, giving tours to occasional visitors, fetching objects for and taking them to people in offices, and delivering the daily mail. To achieve this goal, she plans to transform the state of the art in robot technology for social service robotics, by introducing a novel symbiotic human-robot and robot-robot interaction paradigm that allows robots to help and be helped by humans and each other. A robot will ask humans for assistance based on self awareness of its own limitations and a utility analysis of the estimated cost and benefits of the assistance. The PI and her team will develop and evaluate a robot platform-independent and building-independent problem environment representation, along with algorithms for incremental map learning, localization and navigation, and asynchronous (multi-robot) task partitioning and planning under uncertainty with a utility analysis that includes human availability for robot helping. They will explore effective spatial interaction between mobile robots in spaces with humans, utilizing social conventions, so that people are not just obstacles from the robot's perspective. The robot science and development research will be seamlessly integrated with educational and outreach activities, as well as with principled evaluation which will include fielding a team of robots in campus buildings.<br\/><br\/>Broader Impacts: Aside from dramatically advancing the state of the art in robot technology, enabling multiple mobile robots to be part of the workspace of an office building environment will have significant educational impact relating both to robot technology and interaction with robots. Continuous, openly available robot presence in the computer science and robotics research spaces will change the nature of the relationship between researchers and their classroom research projects, by triggering synergistic collaborations and new, higher-risk experiments with lower setup cost. C Campus outreach tours will be transformed from a narrow view of the future of technology in laboratory settings to a sweeping exposure to the reality and implications of humans and robots coexisting throughout the built environment, significantly broadening inquiry and discussion about the role of interactive technology in our lives. Disseminated curricula incorporating low-cost mobile robots in the secondary school classroom will lift the robot-classroom relationship from one of build kits for very low-capability robots to one of high-level interaction design, industrial design, and discussions of human-robot relationships.","title":"HCC: Large: SSCI-MISR: Symbiotic, Spatial, Coordinated human-robot Interaction for Multiple Indoor Service Robots","awardID":"1012733","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["549680","539160","551205","518535"],"PO":["565227"]},"168706":{"abstract":"This project focuses on questions related to bounding the space requirements of computation in various settings, and the relationship between computation time and space.<br\/><br\/>Current computation tasks often involve very large data sets, for example data originating from the internet, biological or other scientific databases. The size of input data in certain computational tasks requires special considerations and solutions that work with only small portions of the input at a time: manipulating all of the input at once would be prohibitive even with the latest computer technology. Locally decodable codes and streaming algorithms are motivated by such applications.<br\/><br\/>Locally decodable codes are error correcting codes with the extra property that in order to retrieve the correct value of one position of the input with high probability, it is sufficient to read just a small number of positions of the possibly corrupted codeword. So far the known constructions of such codes with constant number of queries have very large length with respect to the input size, and there is a large gap between the known upper and lower bounds on the length of codewords, even in the case of 3-query codes. The project further examines the relationship between the length necessary for the codewords, the number of queries allowed for decoding, and the error correcting properties of locally decodable codes.<br\/><br\/>In addition, the project includes proving bounds on storage space in the cell probe model while limiting the number of positions accessed to answer questions about the data, and bounding the space requirements of streaming algorithms. Finally, the project addresses the relationship between the size and depth of Boolean circuits necessary to compute a given function. This is directly related to the relationship between the time and space of computation.","title":"AF: Small: Locally Decodable Codes and Space Bounded Computation","awardID":"1018060","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":[451556],"PO":["565157"]},"168838":{"abstract":"Hardware-level security and trust in many of society's microelectronic-based infrastructures, e.g., transportation, energy, etc., is inadequate. This project investigates chip-level hardware primitives that are designed to improve the security and trust in such systems. In particular, many security mechanisms depend on a secret, unique identifier that is associated with the chip or board in the system. An embedded digital signature inserted by the manufacturer is not secure because it can be extracted by adversaries. A physical unclonable function (PUF) is a recent approach for providing an entire set of secure, unique identifiers for each chip. PUFs are chip-level primitives that leverage the intrinsic and random manufacturing variations of the process technology. A PUF that measures the resistance variations in the chip's power grid is investigated as a hardware primitive for providing secure, unique identifiers. Similarly, integrated circuit trust relates to the degree of confidence one has that a fabricated instance of a chip implements only those functions described in the original specification -- nothing more and nothing less. <br\/>There are increasing opportunities for adversaries to secretly change a chip's function given the trend of the industry to disseminate the chip fabrication process over many organizations. \"Trust but verify\" is likely the only approach to dealing with this threat. To support this verification process, a set of hardware primitives are investigated that are designed to measure the parametric, analog characteristics of chip as a means of detecting any malicious logic that might have been inserted by an adversary. A chip built in an advanced technology is used to experimentally validate the PUF and hardware Trojan detection methods.","title":"TC: Small: Collaborative Research: Exploration and Validation of Hardware Primitives for Security and Trust","awardID":"1018748","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["486576"],"PO":["565264"]},"168728":{"abstract":"A relatively small number of universal data types are used as basic building blocks in programming languages corresponding to a few well tested mathematical abstractions. This project is about structuring equivalences between them as bijective data type transformations connecting computational objects widely used by computer scientists and mathematicians, like sequences, sets, trees, logic circuits, graphs. An implementation of these transformations will be built in the form of an open source Data Transformation Library. It is also planned to make available the software in the form of a Java applet and a Mathematica notebook, having as broader impact various teaching applications and support for experimenting with data transformations in an intuitive, visual framework.<br\/><br\/>The PI's recent work has showed that these bijections can be organized naturally as a finite groupoid with objects provided by the data types and morphisms provided by their transformations. The project plans to extend these encodings into a comprehensive data transformation framework providing a uniform view of the basic building blocks of various computational artifacts. It also plans exploring the transfer between data types of inductive definitions of their fundamental operations, opening the possibility to derive interesting new algorithms. As a proof of concept, algorithms performing arbitrary size arithmetic computations using symbolic representations like trees or graphs will be obtained. In a broader sense, the project is likely to bring significant simplifications to the ontology shared by computer science and engineering fields, resulting in better communication and synergy between them.","title":"SHF: Small: A Framework for Bijective Data Transformations","awardID":"1018172","effectiveDate":"2010-08-15","expirationDate":"2013-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["451827"],"PO":["565264"]},"168618":{"abstract":"A tandem repeat (TR) is any pattern of nucleotides which occurs as repeating, consecutive copies along a DNA molecule. Often, the pattern copies are not identical. A TR can be polymorphic, that is, it can be different across individuals in a population: 1) the number of copies may be different, 2) the arrangement of non-identical copies may be dfferent, and 3) the copies may contain different small mutations. TR variants are known to affect important biological processes, such as chromatin structure, gene plasticity, gene expression, and disease states, so their discovery is crucial for correctly understanding complex bio-molecular interactions. While a conservative estimate suggests that 100,000 human TRs may be polymorphic, until recently, genome-wide study of TR polymorphism, in humans and other organisms, has been too difficult and costly, with the result that the true extent of polymorphism and its effects are unknown. New genome sequencing technologies offer the first real opportunity to fill in the details of TR diversity. These technologies sequence millions of high quality, short DNA fragments in a single<br\/>experiment. Current sequencing projects are producing many billions of reads rich in TR variant information. Yet, current read mapping algorithms,<br\/>which attempt to assign each read to its proper location on the reference genome, are not designed to detect TR variants. <br\/><br\/>This project has three central goals: 1. Algorithm Development; 2.Genome Studies; 3. Variation Curation in a public database. Strategies will be developed to accurately and efficiently map TR-containing reads to reference genome TR loci. Anticipated algorithmic developments include: 1) Optimization of tree-based alignment, for use when millions of short, disjoint sequences must be aligned to each other. The reads and references can each be merged into separate Patricia tree data structures and alignment computed between tree nodes, thereby eliminating redundant computation in the prefixes of the two sequence sets. 2) Production of space-saving, Burrows Wheeler transforms (BWT) of the most redundant tree parts by employing approximate shortest common superstrings (SCS) for the two sequence sets. 3) Development of an efficient Four-Russians style block computation for edit distance alignment in the trees by exploiting redundancy inherent in the small alphabet and block input scores, 4) Development of a bounding computation for edit-distance based on efficient, bit-register computation of longest common subsequence (LCS) alignment, and 5) Parallelization of all algorithms for further efficiency with multi-core processors, Single Instruction, Multiple Data (SIMD) bit-register computations, and highly parallel graphics processing units (GPUs). Data from six recently published whole human genomes, two human centenarian genomes, and the 1000 genomes project will be analyzed to discover TR variants. An internet-accessible, public database and analysis platform for curation and display of TR variants will be developed.<br\/><br\/>The TR variant discovery software and all data sets produced will directly enhance the infrastructure for TR diversity research in genome biology, genome evolution, and comparative genomics. The software and data will be freely available to the research community through a high capacity website maintained in the PI's lab at Boston University. The PI will participate in a variety of activities that link research and education and support participation by members of underrepresented groups, including provision of opportunities in research for graduate and undergraduate students, participation in high school enrichment and curriculum development projects, and editorship of an international journal engaged in the dissemination of bioinformatics research.","title":"III:Small:Algorithms for Tandem Repeat Variant Discovery Using Next Generation Sequencing Data","awardID":"1017621","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[451346],"PO":["565136"]},"168739":{"abstract":"While the continued scaling of transistor dimensions enables the integration of an increasing number of processing cores on a single chip, the performance of future multicore processors will be limited by power dissipation and peak temperature constraints. High-performance computing systems will be achievable only through energy-efficient design and energy-aware programming methods. Each core will require dedicated active power and frequency management to make sure that at any given instant it does not waste any energy by operating at a speed higher than what is required by the given task that is executing. Such management requires the introduction of novel on-chip voltage regulation modules, real-time monitoring of the current usage for each voltage domain, as well as detailed awareness of the extent of parallelism achievable for each running application. <br\/><br\/>The PIs will investigate the design and fabrication of a scalable on-chip infrastructure for message-passing multicore processors that integrates support for efficient inter-core communication with programmable fine-grain control mechanisms to regulate independently the processing speed and power dissipation of each core. The proposed infrastructure will consist of a heterogeneous network-on-chip (NoC), a set of voltage and frequency control modules that are distributed on the chip, each next to the controlled core, and a new application programming interface (API). The NoC will be dynamically configured to sustain multiple traffic classes with different quality-of-service requirements. The fine-grained power management will rely on high-Q on-chip magnetic energy storage through the use of magnetic materials in a CMOS post-process fabrication step combined with high-efficiency Buck converters based on pulse-width modulation with hysteric control for fast response times. The API will expose both the inter-core message-passing communication and the voltage\/frequency control of each core to the application software programmers. <br\/><br\/>This proposal will allow the PIs to train graduate and undergraduate students in integrated circuit design employing a leading edge CMOS technology and exploiting new magnetic materials as well as in hardware\/software co-design of on-chip infrastructures for dynamic power management. Ongoing industrial interactions with leading information-technology and semiconductor companies promise continual relevance of the project and avenues for dissemination.","title":"SHF: Small: Integrated Infrastructures for On-Chip Communication and Power Management in Message-Passing Multicore Processors","awardID":"1018236","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["529662","518555"],"PO":["366560"]},"168508":{"abstract":"Dynamic multithreading has emerged as a dominant paradigm for simplifying the programming of parallel applications on shared-memory multicore processors. Concurrency platforms incorporating dynamic multithreading provide memory abstractions, such as cactus stacks and hyperobjects, which make many parallel programming chores more like ordinary serial programming. Unfortunately, the overhead of these memory abstractions limits the contexts in which they can be effectively used. <br\/><br\/>This project is exploring how thread-local memory mapping (TLMM) can be used to provide low-overhead support for memory abstractions. TLMM is a novel operating-system feature that allows different pthreads to map a region of an otherwise shared virtual-address space independently. <br\/><br\/>The researchers are building a robust TLMM-Linux and a fully functional prototype of a concurrency platform, called Cilk-M, which includes a portable compiler, an efficient implementation of a runtime scheduler, and implementations of cactus stacks and hyperobjects. They are also engaged in programming and measuring application benchmarks, such as Boolean satisfiability, to understand how the memory abstractions might be optimized and what new memory abstractions might be invented to simplify dynamic multithreaded programming. Finally, they are studying the Cilk-M technology to understand its theoretical properties and investigating how operating systems and hardware might better facilitate the implementation of memory abstractions.","title":"CSR: Small: Using Thread-Local Memory Mapping to Support Memory Abstractions for Dynamic Multithreading","awardID":"1017058","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["548187","471720"],"PO":["565255"]},"172864":{"abstract":"The objective of this proposal is to bring together faculty and students from the U.S. Southwest area through a workshop on theoretical and applied topics pertaining to cyber-physical systems (CPS). The target U.S. Southwest area, which comprises the states of Arizona, Colorado, New Mexico, Nevada, and Utah, has numerous active research projects of interest to the global CPS community. Through this single-track workshop, which will take place during the Fall of 2010 at the University of Arizona, Tucson, participants will have an opportunity to present new results and explore new venues to contribute to CPS. Additionally, invited speakers from academia and government agencies will deliver technical and informative talks on open problems, opportunities, and future directions of CPS research. The workshop will provide funds to graduate students, faculty, and invitees to attend the meeting.<br\/><br\/>Intellectual merit:<br\/>The proposed workshop will promote the exchange and discussion of creative ideas across the multidisciplinary fields bridged by CPS. This workshop is a key step in materializing the collaborative vision of CPS, regionally within the Southwest as well as nationally as a potential model activity across the U.S. <br\/><br\/>Broader impacts:<br\/>The proposed workshop will strengthen collaboration between universities in the Southwest region on topics of national interests. It will provide an ideal venue for dissemination of research results of the participants. The involvement of participants from EPSCOR states will promote new research collaborative activities enlarging their research capabilities. The workshop will provide graduate students a unique opportunity to present and discuss their research with peers and experienced researchers in a semiformal environment. It will consist of the first workshop on CPS in the region, the goal being to have it organized yearly by participants from other institutions within the region. Dissemination of workshop information will be primarily through the workshop website supplemented by e-mail.","title":"Workshop: 1st Southwest Workshop on Theory and Applications of Cyber-Physical Systems","awardID":"1041704","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["502445"],"PO":["561889"]},"172688":{"abstract":"This award is to support tutorials at the intersection of artificial intelligence and cognitive psychology at the 2010 meeting of the Cognitive Science Society. Encouraging computational sophistication in the next generation of cognitive scientists, and encouraging interdisciplinarity in researchers at all career stages, are key elements of the training mission of the Cognitive Science Society. Access to information presented at the conference will also be provided through Web broadcasts of the symposia and a special issue of the Society journal with an emphasis on the teaching mission of workshop attendees.","title":"Promoting Expertise in Computational Cognitive Science","awardID":"1040683","effectiveDate":"2010-08-15","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0404","name":"Division of BEHAVIORAL AND COGNITIVE SCI","abbr":"BCS"},"pgm":{"id":"7252","name":"PERCEPTION, ACTION & COGNITION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[463073],"PO":["563458"]},"163855":{"abstract":"Parallel data structures and algorithms are becoming an increasingly important research area, due to the rapid advances in GPUs and other massively parallel commodity multi-core hardware along with the software needed to program these devices. In this collaborative effort involving the University of California at Davis and Harvard University, the PIs will focus on the design and implementation of parallel hash tables, one of the most fundamental of data structures, on the new platforms. Real-time parallel hashing would enable a variety of graphics applications on dynamically changing data, including spatial hashing, surface and image matching, and hashed octrees which in turn enable a host of other applications including Boolean surface operations, point-cloud nearest neighbors, ray-tracing acceleration and photon mapping. In prior work, the PIs built a baseline implementation that shows effective parallel hashing can be done on the GPU; they can construct the table as quickly as the fastest available radix sort, and can execute parallel random access on the elements much more quickly than binary search. In the current research, the PIs plan to improve upon their baseline implementation significantly, while also focusing on related structures such as multi-maps and Bloom filters. New designs and construction algorithms will be developed, implemented, and analyzed with respect to performance, and then applied to a variety of computer graphics applications. The PIs expect this work to lead to interesting theoretical results; modern hash table constructions have never been considered in the parallel context, so finding the right model for analysis is one goal of the research.<br\/><br\/>Broader Impacts: This project will contribute to the computing infrastructure, not only for computer graphics but also for general-purpose computation. The PIs will distribute their implementations freely, in part by extending and building upon their existing (and popular) library of general-purpose data structures (the CUDA Data Parallel Primitives). The PIs note that making the most of the emerging parallel GPU resources requires training the next generation of programmers to think in parallel; therefore, they plan to exploit this project as an opportunity to revive a long-untaught undergraduate parallel programming course, in addition to studying parallel algorithms with their graduate students.","title":"HCC: Medium: Collaborative Research: Data-Parallel Hash Tables: Theory, Practice and Applications","awardID":"0964473","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["560092"],"PO":["565227"]},"172029":{"abstract":"The research investigates a new approach for automatically parallelizing sequential programs. In contrast to existing parallelizing compilers, which use static analysis to parallelize loop nests that use affine access functions to manipulate dense matrices, this approach applies a set of transformations similar to those that expert developers apply when manually developing parallel programs. These transformations induce a search space that the compiler will automatically explore to deliver the best parallelization it can find. The compiler evaluates the success of each transformation by running the transformed program on representative inputs to observe 1) the impact (if any) of the transformation on the performance of the parallel program and 2) if the transformed program produces an acceptably accurate result. The technique will produce a report that the developer can examine to understand the parallelization process. The research will adapt as necessary to reflect knowledge gained during the course of the research.<br\/><br\/>The significance of this research is that multicore machines are believed to be the foundation of our future computing infrastructure, and that such machines are known to be difficult to program. Given this combination, investigating new and potentially more effective techniques can help make it possible to obtain the parallel software necessary to utilize this class of machines.","title":"EAGER: Profile and Transformation Driven Automatic Parallelization with Interactive Reports","awardID":"1036241","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["497068"],"PO":["565264"]},"166837":{"abstract":"Web Science is an emerging discipline that studies the Web: how human activity is shaped by Web interactions, how the Web can benefit society, and how Web technologies can be improved. Central to Web Science is access to data that records the history of the Web, as well as data that records human activity (e.g., posed queries, tagged pages, Twitter updates). It is currently very difficult for academic researchers to obtain such Web data because it is hard to locate, it is fragmented across diverse sites, and is recorded using inconsistent formats and strategies. This project will build a Web Archive Cooperative (WAC) that will integrate existing archives (repositories of Web data), making it feasible to access large volumes of data in a simplified fashion. The WAC will be a virtual service, providing search facilities and access mechanisms to existing resources. These resources will not just be Web pages, but all types of available Web information, such as query logs, tag annotations, blogs, profiles and Twitter updates. Furthermore, resources will also include the software tools for building and managing Web archives.<br\/><br\/>The project will explore three goals for a resource discovery service: (1) the manual or automated discovery of entire existing Web related archives; (2) the selection among known archives of the ones that support a specific research question; and (3) the identification of individual resources from within the selected archives. Tools for characterizing discovered archives, especially for the case where the archive does not provide rich descriptive metadata, will also be developed. Characterization of an archive includes elements such as an estimate of the archive's coverage, particulars of the crawling parameters, like dates\/frequencies, crawl duration, depth, per-site ceiling on the number of collected pages, content statistics, and link structure. Mechanisms for integrating diverse archives will be developed, and the mechanisms will be applied to site reconstruction (from various archives) and archive views (a logical fusion of resources from multiple sources). Since integration issues are so challenging, an experimental testbed will be set up with small but diverse resources. The testbed will contain several crawls of the same target sites, each obtained with different crawlers and using different parameters. The testbed will also contain related resources. Storage trading schemes will be developed, allowing members to trade local backup space for remote space. A Web archive replication tool will be developed based on existing notions for self-preserving objects. Alternatives for replica synchronization will be studied.<br\/><br\/>Workshops to bring together key Web Science researchers will be organized to discuss available resources and impediments to sharing. These workshops will drive research and identify needed tools and protocols. With small groups of participants, challenge problems will be established, e.g., combining a set of Web archives. Reports of these results at future workshops can incentivize others to participate in the WAC. In addition, an Advisory Board of industrial, government, and academic experts has been set up to guide the project. A Summer Institute for Web Science graduate students will be held. At this Institute, students will learn to use the latest tools and will learn from each other's experiences in dealing with Web data. In addition, a one-day workshop will be developed, to be offered at Web Science conferences (WWW, SIGIR, etc.) to educate participants about WAC resources. An undergraduate Web Sciences track for computer science majors will be set up, taking advantage of WAC resources. The project will have impact in two ways. First, it will provide tools and services that facilitate access to Web resources. Any researcher, from a computer scientist studying efficient Web search, to a social scientist studying how human beliefs are changing today, to a historian studying how the early Web evolved, to a biologist understanding how disease spreads, will benefit from the work. Second, the project motivates students and young researchers to stay in academia. Currently top talent is flowing to industry because only they have comprehensive Web data, and it is so hard to do significant Web Science at universities. The WAC can provide an alternative, attracting more researchers and teachers to this important area.","title":"III: Large: Collaborative Research: Web Archive Cooperative","awardID":"1008492","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[446829],"PO":["565136"]},"164439":{"abstract":"This PIRE project forms an international consortium of leading superconductivity researchers from the U.S., Japan, Canada, UK, and China to investigate novel superconductors to clarify superconducting mechanisms and properties and develop novel superconducting materials. In conventional electrical systems heat is generated by friction as electrons collide with atoms and impurities in the wire, a property that is ideal for appliances such as toasters or irons but not for most other electrical applications. Superconductivity can be thought of as \"frictionless\" electricity whereby electrons glide unimpeded between atoms, thus vastly improving the conductor's energy efficiency. To date this has only been achieved at extremely low temperatures; the challenge is to harness this phenomenon at or near room temperature and at high electrical currents. This project will fill gaps in our current understanding of superconductivity, reconcile current theories, and advance the development of better materials for fast-performing devices and cost-saving electric motors, generators, and power transmission lines. <br\/><br\/>The project links leading materials experimentalists and eminent theorists in a study of FeAs, CuO, CeCoIn5, and URu2Si2 superconductors using powerful experimental probing techniques including neutron scattering, muon spin relaxation, X-ray scattering, Raman spectroscopy, and scanning tunneling microscopy. These advanced methods allow elucidation of the phase diagrams of these important new materials of which some significant aspects are currently unknown. The PIRE team will explore the parameters affecting the highest temperature at which a certain material is superconducting and ways of increasing that temperature so that superconductivity will not require such expensive refrigeration. Some anomalies in the superfluid density and specific heat discontinuities, inconsistent with the standard theory of superconductivity, will also be investigated both experimentally and theoretically.<br\/><br\/>International collaboration is essential for this work because it will provide U.S. scientists and students with access to critical world-class accelerator-based facilities available in the UK and Canada but not in the U.S., to high quality specimens fabricated in China and Japan, and to first-rate scientific expertise from all countries. Combining and comparing the results of multiple probes on the same high-quality specimens will significantly improve the accuracy of data. Face to face collaboration of theorists and experimentalists focused on key concepts will facilitate the translation of mathematical theory into realistic and effective models and materials. The project places great emphasis on training students and early career scientists. Students and postdoctoral researchers will undertake 3-6 month research visits to work on superconducting mechanisms at foreign sites, where they will also receive language and cultural training. The project will actively recruit minority students into the sciences via workshops for high-school students and teachers from disadvantaged schools in New York and via an outreach program on superconductivity and scanning tunneling microscopy. High school and undergraduate students will gain valuable beam-time experience through the project, and female students, who are as a group underrepresented in the physical sciences, will be provided valuable mentoring from four female leading scientists on the team. The PIRE team will also develop a contemporary, internet-based set of solid state physics lectures and a text book on introductory solid state physics that reflect current knowledge in condensed matter physics and related experimental techniques.<br\/><br\/>The project will strengthen and internationalize materials research programs at the U.S. institutions and engage more U.S. students in international research collaborations. It will place Columbia University and its students and faculty at the core of a research and education partnership with extensive research collaborations, teaching cooperation, and frequent reciprocal research visits for participating faculty and students. Impacts extend beyond the PI and his institution, including providing U.S. students with research opportunities at two Department of Energy U.S. National Laboratories (Oak Ridge and Los Alamos) and training of early career scientists at the UK's ISIS and Canada's TRIUMF facilities, both of which will build the core workforce for new probing facilities currently under construction in the U.S. and Japan. This PIRE project will build upon an existing Inter American materials science network (CIAM) and forge a foundation for long-term research and educational collaborations among scientists and institutions in the five participating nations, all advancing the state-of-the-art in superconductivity and its applications. <br\/><br\/>Participating U.S. institutions include Columbia University (NY), University of Tenn","title":"PIRE: International consortium for probing novel superconductors with neutrons, muons, photons and STM","awardID":"0968226","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"7742","name":"PIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1710","name":"CONDENSED MATTER PHYSICS"}}],"PIcoPI":["489866","480614","517329","474706","545167"],"PO":["399099"]},"168806":{"abstract":"The World Wide Web and other networked information systems provide enormous benefits by enabling access to unprecedented amounts of information. However, for many years, users have been frustrated by the fact that these systems also create significant problems. Sensitive personal data are disclosed, confidential corporate data are stolen, copyrights are infringed, and databases owned by one government organization are accessed by members of another in violation of government policy. The frequency of such incidents continues to increase, and an incident must now be truly outrageous to be considered newsworthy. This project takes the view that when security violations occur, it should be possible to punish the violators in some fashion. <br\/><br\/>Although \"accountability\" is widely agreed to be important and desirable, there has been little theoretical work on the subject; indeed, there does not even seem to be a standard definition of \"accountability,\" and researchers in different areas use it to mean different things. This project addresses these issues, the relationship between accountability and other goals (such as user privacy), and the requirements (such as identifiability of violators and violations) for accountability in real-world systems. This clarification of the important notion of accountability will help propel a next generation of network-mediated interaction and services that users understand and trust.<br\/><br\/>The project's technical approach to accountability as an essential component of trustworthiness involves two intertwined research thrusts. The first thrust focuses on definitions and foundational theory. Intuitively, accountability is present in any system in which actions are governed by well defined rules, and violations of those rules are punished. Project goals are to identify ambiguities and gaps in this intuitive notion, provide formal definitions that capture important accountability desiderata, and explicate relationships of accountability to well studied notions such as identifiability, authentication, authorization, privacy, and anonymity. The second thrust focuses on analysis, design, and abstraction. The project studies fundamental accountability and identifiability requirements in real-world systems, both technological and social. One project goal is to use the resulting better understanding of the extent to which accountability is truly at odds with privacy and other desirable system properties to design new protocols with provable accountability properties. Building on that understanding and insights gained in designing protocols, the project also addresses fundamental trade-offs and impossibility results about accountability and identifiability in various settings. The broader impacts of the work include not only engagement with students but also a new perspective on real world accountability in trustworthy systems.","title":"TC: Small: Collaborative Research:Accountability and Identifiability","awardID":"1018557","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["478168","540234"],"PO":["565136"]},"168817":{"abstract":"The standard instrumentality for the criminal acquisition and<br\/>distribution of images and video of child sexual exploitation is<br\/>peer-to-peer (p2p) networks. Over 160,000 users based in the US<br\/>are sharing child pornography (CP) using Gnutella alone. Past<br\/>studies have found that: 21% of CP possessors had images<br\/>depicting sexual violence to children such as bondage, rape, and<br\/>torture; 28% had images of children younger than 3 years old; and<br\/>that 16% of investigations of CP possession ended with discovery<br\/>of persons who directly victimized children.<br\/><br\/>The proposed work aims to make significant advances in forensics<br\/>methods of investigating criminal acts on peer-to-peer file<br\/>sharing networks. The project represents a unique<br\/>multidisciplinary collaboration between computer science and<br\/>criminology with close participation from existing law<br\/>enforcement partners.<br\/><br\/>Intellectual Merit. The project makes the following broad<br\/>contributions:<br\/><br\/>- It proposes novel methods of tagging a remote computer over the<br\/>network with information that can uniquely identify it during a<br\/>forensic examination. These serve as both identifiers and as<br\/>indicia of intent.<br\/><br\/>- It proposes to gather a foundational dataset regarding the<br\/>prevalence and rate of spread of child pornography on p2p<br\/>networks. Further, it proposes to measure and quantify the<br\/>relationships between child abuse and the trading of child<br\/>pornography on p2p networks. Finally, it proposes the<br\/>development of models to detect the trafficking of deliberately<br\/>hidden child pornography on these networks.<br\/><br\/>Broader Impact. The project aims to reduce the number of children<br\/>sexually exploited each year by thwarting the trafficking of<br\/>their images on p2p networks and via the capture of the contact<br\/>offenders that rape, torture, or otherwise brutalize them. This<br\/>work will increase cross-disciplinary collaboration between<br\/>computer science and criminology and between law enforcement and<br\/>academia; effect technology transfer to law enforcement in the<br\/>field of online child pornography investigations, which will help<br\/>reduce the number of victims of crimes in the future; and<br\/>facilitate broad educational outreach and recruitment of<br\/>under-represented minorities in undergraduate and graduate<br\/>research in digital forensics.<br\/><br\/>For further information see the project web site at the URL: <br\/> http:\/\/prisms.cs.umass.edu\/CNS-1018615","title":"TC: Small: Collaborative Research: Strengthening Forensic Science for Network Investigations","awardID":"1018615","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[451830,451831],"PO":["564223"]},"168608":{"abstract":"Due to technology and industry trends, parallelism and energy<br\/>efficiency have emerged as critical considerations in future computing<br\/>systems. The investigators observe that many convenient models for<br\/>writing parallel programs can lead to significant redundancy in<br\/>computation and data. This redundancy can be used to improve<br\/>performance and reduce energy consumption.<br\/><br\/>The investigators observe that many parallel software models<br\/>(e.g. data parallelism, Single-Program Multiple Data (SPMD),<br\/>multi-programming, and high-throughput computing) consist of multiple<br\/>threads of computation with very similar instructions streams and<br\/>working sets of data. The trick, however, is to design efficient<br\/>mechanisms that both exploit this similarity and effectively support<br\/>the differences. This project will follow two research thrusts to<br\/>eliminate redundancy across five application domains. First, the<br\/>research will explore multi-threaded processor core designs that<br\/>eliminate redundant instruction fetch and\/or execution. Second, the<br\/>research will explore content-aware caching techniques that reduce<br\/>redundant storage of identical data across parallel threads and<br\/>processes.","title":"SHF: Small: Minimal Multithreading - Exploiting Redundancy in Parallel Systems","awardID":"1017578","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7942","name":"HIGH-PERFORMANCE COMPUTING"}}],"PIcoPI":["486140","528013"],"PO":["565272"]},"168729":{"abstract":"Among all types of software bugs, concurrency bugs in multi-threaded parallel programs are especially troublesome. They widely exist and are becoming increasingly severe due to the pervasiveness of multi-core machines. Existing approaches to detecting concurrency bugs mostly struggle at the complicated cause of concurrency bugs --- non-deterministic interaction among multiple threads in concurrent programs.<br\/><br\/>This project aims to address the concurrency bug problem through an effect-oriented approach. Specifically, it will provide (1) a characteristic study and a deep understanding of the error propagation process of real-world concurrency bugs; (2) an effect-oriented bug detection and testing framework that can identify potential failures in a program and search for concurrency bugs leading to these failures through backward analysis; (3) a bug-fixing tool that leverages the error propagation information identified above and suggests patches to software developers; (4) a general effect-oriented philosophy that can guide other tools related to multi-threaded parallel programs. This research will improve our understanding of the dependability problem of concurrent software, provide substantial tool support to help lower software development and maintenance costs, and improve software users' everyday experience through faster and more reliable software on a wide spectrum of platforms.","title":"Fighting Concurrency Bugs through Effect-Oriented Approaches","awardID":"1018180","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["517878"],"PO":["565264"]},"168619":{"abstract":"Pattern matching is a fundamental research field with applications in domains such as biological sequence alignment, web search engines and network intrusion detection. Given a pattern P and a text string T, the central problem is to find occurrences of P in T. When data becomes massive, we cannot assume that text can be stored in RAM. Pattern matching problems must now be considered with more appropriate models like external memory model, cache-oblivious model, streaming models, MapReduce paradigm and multi-core models. In many cases, a blend of models or newer, more appropriate models need to be developed keeping the practical aspects of the application in sight.<br\/><br\/>The focus of this project is to develop efficient search algorithms and indexes when a data set resides on disks, on network storage, or is accessible only as an online stream. The data must be efficiently searchable even though it may be in compressed format. The project considers traditional pattern matching problem, as well as variants such as (i) approximate matching -- where the pattern may not exactly match a substring in T, (ii) online matching -- where the pattern(s) are known in advance and text comes as a stream, and (iii) string retrieval -- where instead of finding all the occurrences, the focus is on retrieving high ranking documents which contain one or more occurences of the query pattern. Issues of I\/O efficiency and space utilization are central to this project. This involves developing suitable massive data set models, deriving optimal theoretical bounds and implementing practical tools. Methodologies include combinatorial and randomized methods in pattern matching, succinct data structures, top-k query processing and I\/O efficient indexes.<br\/><br\/>The project will build new, solid theoretical foundations in pattern matching, with direct applications to fields like databases and information retrieval. It will significantly drive forward current state of the art in web search engine technology (by impacting the way inverted indexes are used) and genome sequence alignment tools (e.g., BLAST). Tools and software developed during this project will be widely distributed to the research community. Some components will be incorporated into undergraduate and graduate algorithms course curricula as implementation projects.","title":"AF: DC: Collaborative Research: Pattern Matching for Massive Data Sets","awardID":"1017623","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["518519"],"PO":["565251"]},"170940":{"abstract":"The International Society for Computational Biology is awarded a grant to support student and select presenter participation in the annual meeting of the International Society for Computational Biology on Intelligent Systems for Molecular Biology. The conference will be held in Boston, Massachusetts, July 9-13, 2010. The conference will hold thematic sessions for invited presentations, tutorial programs on related subjects, and special interest group workshops. Biological areas presented at the conference will include molecular structure, genomics, molecular sequence analysis, evolution and phylogenetics, molecular interactions, metabolic pathways, regulatory networks, developmental control, molecular biology generally and human health. Results will be disseminated as published proceedints indext in MEDLINE and Current Contents, and video recordings of selected presentations will be available online at http:\/\/www.iscb.org\/ismb2010.","title":"Conference: ISMB 2010 Conference Support for Students & Young Scientists","awardID":"1029761","effectiveDate":"2010-08-15","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"1165","name":"ADVANCES IN BIO INFORMATICS"}}],"PIcoPI":[457635],"PO":["561879"]},"171611":{"abstract":"This is funding to support a workshop of approximately 12 promising doctoral students from the United States and abroad, along with 5 distinguished research faculty. The event, the 4th annual doctoral consortium on Sociotechnical Issues in Medical Informatics, will take place in conjunction with the 2010 Annual Symposium of the American Medical Informatics Association (AMIA) in Washington DC on November 13-17. The AMIA Symposium, held every fall, is the world's most comprehensive annual meeting and premier forum on biomedical and health informatics, attracting over 2,000 researchers, professionals and students from an array of occupational settings who are interested in all aspects of health information technologies. More information about the conference may be found at http:\/\/symposium2010.amia.org.<br\/><br\/>One issue that is of particular importance within medical informatics is the need to design\/deploy systems with an understanding of how these systems fit into organizational and social contexts. The term sociotechnical denotes the importance of considering technical and organizational\/social issues together rather than in isolation. For instance, the medical informatics community has long been interested in the technical features of the Electronic Medical Record (EMR), but less attention has been paid to how the design of the EMR will be affected by who the primary users of the system are, what context it is used in, and what interactions it should support. Yet, to design effective EMR systems, we need to understand how these different issues impact each other. The growing need for integrating research across disciplines (e.g., HCI and Medical Informatics) is the primary motivation for this doctoral consortium. Therefore, student participants will span a broad range of disciplines and approaches that inform medical informatics, including computer science, information science, engineering, clinical sciences, law, management and related fields. The doctoral consortium will take place on the weekend prior to the AMIA Symposium, so the students can also participate in the conference which will introduce them to the exciting breadth of research topics within the medical informatics community. Conversely, bringing a new generation of scholars from different disciplines into the medical informatics community will broaden that community's understanding of what other research disciplines can offer medical informatics.<br\/><br\/>The consortium will provide a forum for doctoral students to share their work and also network with other doctoral students and researchers. Student participants will give short presentations during the consortium and will receive constructive remarks and feedback on their research from prominent researchers as well as through interaction with other students. The feedback is designed to help students understand and articulate how their work is positioned relative to related research, whether their topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are appropriately analyzed and presented. Thus, the consortium will help shape both these ongoing and future inter- and multi-disciplinary research projects focusing on organizational and social issues surrounding health-related technologies. The names and abstracts of the accepted participants will be printed in the AMIA program guide, and will also be made available to the public on a special website to be developed by the organizers.<br\/><br\/>Broader Impacts: The doctoral consortium will bring together people from different disciplines who might not otherwise engage with one another and will expose them to different scientific research approaches and questions. It will foster a sense of community among the young researchers by allowing them to create a supportive mentoring and social network both among themselves and with senior researchers at a critical stage in their professional development. The organizers will make a concerted and proactive effort to ensure a diverse pool of student participants including both strong international participation and members of under-represented groups in the STEM disciplines; this, in turn, will broaden the students' horizons to the future benefit of the field.","title":"American Medical Informatics Association (AMIA) Doctoral Consortium 2010","awardID":"1034081","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["559463"],"PO":["565227"]},"173932":{"abstract":"The NSF Workshop on \"Interdisciplinary Challenges beyond the Scaling Limits of Moore's Law\" is organized to explore the scientific issues and technological challenges beyond the scaling limits of Moore's Law with the goal of positioning the U.S. at the forefront of Communications and Computation technologies beyond the physical and conceptual limits of current systems. In accordance with the Moore's Law empirical observation, until recently device integration levels have continued to expand exponentially, and as a result, so has computation power. However, it is now well accepted that current approaches will reach their limits in next 10 years due to a confluence of both fundamental and practical limitations. <br\/><br\/>Continuing evolution of electronics beyond the scaling limits of Moore's Law is likely to require a broader re-thinking, ranging from novel materials and devices, to circuit and system architectures so that new insights can be employed in computation and knowledge processing technologies. Current technologies already provide examples of energy-efficient systems which have evolved for diverse tasks such as embedded approaches in cell phones or specialized task-specific technologies such as in e-readers. <br\/>The workshop will explore the overcoming of current barriers by fostering interdisciplinary debate and dialog. The workshop will bring together experts from academia, government, and industry in the fields of life sciences, chemistry, physics, mathematics, materials science, engineering, and computer science to discuss new computational devices and new approaches to computation as well as ways to extend progress in current devices and systems. Some broad questions to be addressed will include the future of terascale devices, strategies to minimize energy utilization, novel materials and devices to overcome the voltage-scaling limitation of existing device technology, and new approaches to reliability based on self-healing and programming.<br\/>Promising directions are expected to include interdisciplinary merging of architectures, algorithms, materials and devices, and signaling approaches for specific applications. The premise is that a coherent engineering where synergistic approaches draw on diverse insights from electrosciences, materials sciences, physical sciences, mathematics, and computer science, will be necessary. An objective of the workshop is to identify promising insights and directions that project solutions for electronics in the decade of the 2020's.<br\/><br\/>Invited speakers will highlight insights and challenges from their disciplinary perspectives and will attempt to answer questions posed to them beforehand. The breakout sessions will focus on defining the most important topics for future research. The discussion will also bring out opportunities and necessary changes in education as the complexity of large scale integrated electronics demands greater interdisciplinary knowledge. <br\/><br\/>The workshop will include presentations by leading practitioners in the fields of semiconductor devices and computer architectures, and by scientists working in the areas of physics, chemistry, mathematics, materials science, molecular electronics and nanoscale systems. The report of this workshop will detail important challenges, fundamental and technological, that are likely to be at the forefront of this field for many years to come. The workshop will be held at the Westin Arlington Gateway Hotel, Arlington, Virginia on August 2-4, 2010. It is expected that the workshop will identify the technological challenges and research opportunities for NSF and the scientific community. The proceedings of the workshop and the list of recommendations will be made available to all participants of the workshop, NSF, other government scientists, industry and policymakers. <br\/><br\/>The intellectual merit of the workshop is in the vigorous debate and discussion that will be fostered and the identification of fruitful and compelling directions that allow electronics and computation to advance even as electronic device dimensions reach their nanoscale limits. The broader outcome of the workshop is in the identification of the interdisciplinary directions and the related educational approaches that are likely to be most suitable in the undergraduate and graduate curriculum.","title":"Workshop: Interdisciplinary Challenges beyond the Scaling Limits of Moore's Law. To Be Held in Arlington, VA, August 2-4, 2010.","awardID":"1047541","effectiveDate":"2010-08-15","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"1775","name":"ELECTRONIC\/PHOTONIC MATERIALS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["492139"],"PO":["564900"]},"150722":{"abstract":"An understanding of how the brain processes visual stimuli is confounded by the complexity of the natural visual world, combined with the intricate neuronal processing that occurs within and across multiple cortical areas. Over the last few decades, substantial progress has been made by using simple laboratory stimuli, such as moving bars or dots, to develop simple descriptions of neuronal tuning to these elements. This approach has provided a reasonable functional description of lower cortical areas, but it is unlikely to be sufficient to characterize the regions of the extrastriate cortex whose responses are thought to represent elements particular to the natural visual world. Though a joint Canadian-US collaboration, this project couples new experimental approaches based on a set of complex stimuli approaching natural vision with appropriately complex models, in order to understand how neurons in successive stages of cortical processing are tuned to more natural visual features.<br\/><br\/>Most previous work with natural stimuli has focused on the ventral pathway in the cortex, which is concerned with computing object shape and identity. This is an extremely challenging problem, as the dimensionality of shape space is unknown. This project focuses instead on the dorsal stream of the primate visual cortex, which is primarily identified with motion processing. The advantage of this approach is that motion, particularly that seen in natural vision, can be locally decomposed into a low-dimensional optic flow space that can be sampled using naturalistic stimuli designed for this proposal. The development of such stimuli will extend both the spatial and temporal complexity of probes to areas in the dorsal stream, while providing the necessary constraints for a novel nonlinear modeling framework that will be developed. These models will then be applied to motion stimuli derived through simulation of natural three-dimensional virtual environments, allowing the complex processing uncovered to be linked to natural visual features. Furthermore, by performing this study across successive areas comprising the dorsal hierarchy (V1, MT, and MST), this project aims to expose general principles of cortical processing, namely how higher level abstractions are derived from lower-level visual features.<br\/><br\/>This project is jointly funded by Collaborative Research in Computational Neuroscience and the OISE Americas program. A companion project is being funded by the Canadian Institutes of Health Research.","title":"Characterizing Cortical Computation in the Context of Natural Vision","awardID":"0904430","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7327","name":"CRCNS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":[401790],"PO":["564318"]},"161755":{"abstract":"The PI is developing bottom-up mechanisms for securing wireless networks against a class of \"layer-violating\" attacks. In a layer-violating attack, the attacker uses protocol behavior at one layer of the network stack to compromise a secure protocol at a different layer. Such layer-violating attacks often can span from the physical layer all the way to the transport layer. In the PI's approach, each layer interlocks with the higher layer, relying on the security properties guaranteed by the lower layer to provide security properties to the next higher layer, resulting in a protocol stack that is resilient to layer-violating attacks. The PI's efforts focus on four important security properties: availability against jamming, fairness, routing, and privacy. <br\/><br\/>The outcomes of this research will be: <br\/>- A protocol stack secure against jamming at all layers, ensuring a specific level of performance despite the presence of an adversarial attacker <br\/>- A protocol stack that provides fairness regardless of attacks at any layer, and specifies the types of fairness achievable against a cross-layer adversarial attacker <br\/>- A results-oriented routing protocol that provides reliable performance assurances against attacks at any layer <br\/>- A protocol stack that provides privacy across all layers of the network stack, ensuring minimal leakage of privacy-sensitive information.<br\/><br\/>The PI is also revising the introductory programming curriculum in the ECE department; one aspect of this revision is an emphasis on safe and secure code writing. The PI is also working to develop a middle-school-level curriculum to encourage underprivileged groups to pursue engineering education.","title":"CAREER: Protecting against Layer-Violating Attacks in Wireless Networks","awardID":"0953600","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["531879"],"PO":["565327"]},"171325":{"abstract":"This proposal seeks funding for the Center for Safety, Security, and Rescue Research studies conducted by the University of Minnesota (lead) and its research partner at the University of Denver. Funding Requests for Fundamental Research are authorized by an NSF approved solicitation, NSF 10-507. The solicitation invites I\/UCRCs to submit proposals for support of industry-defined fundamental research. <br\/><br\/>The proposal is about further developing ongoing robot design activities for safety and security. The presented research focus is in the area of underwater robot design, and it describes activities regarding the waterproofing, buoyancy control, camera stabilization and sensor fusion strategies. The proposed work builds on the strong previous track record, and the team is strong. Unlike most existing approaches, the proposed effort should provide miniature size robots with high mobility. <br\/><br\/>This proposal presents a set of robotic systems (including an amphibious, tumbling robot) and algorithms that capitalize on earlier pertinent work at the Industry\/University Cooperative Research Center for Safety, Security, and Rescue Research. The proposal also offers innovative stabilization algorithms and hardware for the sensor suite in order to improve the quality of data provided to the first responder. In addition, innovative sensor fusion schemes for underwater navigation are proposed. The broader impacts of the proposed work range from a planned inclusion of undergraduates into the research plan to the possibility that life-saving commercial devices could eventually result from the work described. The connection of this work with the NSF Safety, Security, and Rescue Research Center will ensure that the proposed technologies will be exposed to first responders at the proposed field trials.","title":"I\/UCRC: Collaborative Research: Miniature Ground\/Water Robot","awardID":"1032047","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"5761","name":"INDUSTRY\/UNIV COOP RES CENTERS"}}],"PIcoPI":["522995","522995","543539"],"PO":["564474"]},"162657":{"abstract":"Portable embedded systems place ever-increasing demands on high-performance, low-power microprocessor design. This has triggered a paradigm shift in the embedded processor industry, and embedded processor architects are permanently altering their roadmap to incorporate multiple cores on the same chip to preserve exponential computational speed-up (to continue Moore's law). While the industry focus is on putting higher number of cores on a single chip, the key challenge is to optimally architect these processors to meet stringent real-time constraints in virtualized environments with different real-time operating systems (RTOS). Moreover, these processors will work on diverse application types having sporadic event-driven data. In such complex systems, the optimal performance can only be realized by co-designing the RTOS kernels, the virtualization environment, and the processor micro-architecture. <br\/><br\/>The goal of this research is to develop a cycle-accurate simulation platform for the micro-architectural exploration of future kilocore-scale heterogeneous embedded chip-multiprocessors (ECMPs), together with an interface to boot RTOS-es on the simulator, and enable virtualization on the ECMPs. The integrated framework is flexible and modular for designing a wide variety of ECMPs and RTOS-es, flexibly threaded for fast simulation of thousands of cores on a wide range of computing platforms, assertion-based and check-pointed for quickly regenerating and changing complex trigger conditions required for debugging, instantly-bootable to reduce the RTOS booting time from power-up to simulation, enabled with deep-chip-vision for better observability of silicon behavior at the architectural level, and open-sourced for non-commercial use. The RTOS integration environment provides software interfaces and libraries to the architect and the OS designer to simulate the execution of embedded applications on different exploratory ECMPs, in the presence of different virtualization scenarios and RTOS scheduling algorithms. This project promotes inter-disciplinary research between computer architects, RTOS designers, Computer-Aided Designers, and embedded application programmers at different universities and research institutions. The various education components woven into the project will foster team-based research, learning, and teaching among the university student participants.","title":"II-NEW: Implementation of an Architecture Simulation Platform with a Virtualized Real-time Operating System Interface for Kilocore-scale Heterogeneous Embedded Chip MultiProcessors","awardID":"0958363","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[434651,434652,434653],"PO":["565272"]},"174318":{"abstract":"This proposal aims to develop a new computing paradigm to build more effective cloud computing schemes for web-scale multimedia search and learning. It considers the need for algorithmic and systematic design, and aims to break down the gap between fast searching requirement and the burden of processing high dimensional multimedia features. It is well-known that loading and computing high dimensional data are both expensive procedures. The proposed paradigm employs data summary for small trunks and uses those summaries to estimate the lower bound and upper bound for searching measures. Based on these bounds, this paradigm can filter out a lot of data samples before loading them, and thus can reduce the transmission and computation overhead. The new paradigm generalizes Google?s MapReduce computing paradigm for the task of searching high dimensional data, and fits better the applications of processing multimedia data than the general-purpose computing paradigm. <br\/><br\/>The intellectual merit of this proposal is to exploit the computing resources offered by cloud computing and to develop novel algorithms to perform the multimedia data search in a distributed and efficient manner. The PI?s ambition of making cloud computing suitable for high-dimensional numerical data, if successful, will revolutionize the future of cloud computing, and have a tremendous impact on society at large. The challenges of the problems and its potential payoff and impact, if successful, make this proposal ideally suited for the EAGER program.","title":"Efficient Learning Algorithms for Search via Cloud Computing","awardID":"1049332","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550184"],"PO":["564316"]},"173009":{"abstract":"This Broadening Participation in Computing award funds the extension of the Into the Loop Alliance. The Into the Loop Alliance has a primary goal of helping to strengthen the capacity of the Los Angeles Unified School District (LAUSD), the second largest and one of the most diverse school districts in the country, to offer and support high-quality, college preparatory computer science classes, especially in high schools with high numbers of African-American and Latino students. The goal is also to pursue a strategy that creates sustainable changes in the culture and practices at the school and district level; and to develop a model and repository of best practices that can help spread and inform similar efforts in other school districts. The strategic approaches utilized range from the classroom and school level to the policy level. <br\/><br\/>This Alliance Extension provides the opportunity to expand and sustain the original alliance work locally and nationally. With its initial funding, the Into the Loop Alliance has created an innovative pre-Advanced Placement (AP) computer science curriculum, Exploring Computer Science (ECS); piloted ECS in approximately 16 LA schools with over 900 students enrolled, predominately Latino\/a and African American students; designed an ECS Professional Development program; and developed authentic higher education computer science involvement with K-12 schools and educators . In addition to deepening and extending all of the above, the alliance extension includes a commitment to three new strategic initiatives: designing assessment measures for student learning of computational thinking; launching a statewide policy campaign, the California Computing Initiative (CCI); and providing leadership for state and national expansion of these initiatives. All of these goals are aimed at institutionalization and sustainability of the alliance progress to date.<br\/><br\/>Intellectual Merit: Improving STEM education and guaranteeing equal access to quality education for all students is one of our country's most pressing challenges. Into the Loop sits at the crux of this national challenge. The underlying project research about increasing rigorous learning of computer science opportunities in schools, about assessing student learning of computational thinking, about professional development for computer science teachers, especially in schools with high numbers of students of color, will shed light on similar challenges in other STEM disciplines.<br\/><br\/>Broader Impact: Into the Loop will provide a model of what has to be done both on the school, district, state, and national levels to improve quality computer science education for all students. The models of professional development that will be designed and implemented in the second largest and one of the most diverse school district in the country will contribute to efforts underway to broaden participation in computing: specifically, to recruit and train a high school teachers who can impart to students the magic and computational thinking of computer science, to re-position Computer Science at the high school level as an academic subject, and to redesign the high school curriculum so that it is rigorous and engaging for a broad segment of our student population. Creating statewide policy change that can reinforce all of these initiatives will be critical for institutionalizing and sustaining these reforms and will be a model for other states as well.","title":"BPC-AE: Into the Loop: A University K-12 Alliance to Increase and Enhance the Computer Science Learning Opportunities for African American, Latino\/a, and Female Students","awardID":"1042302","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["528135","528136","560926","528138"],"PO":["560704"]},"168807":{"abstract":"Advanced Grid-Enabled Algorithms to Discover Conformations of Proteins<br\/>Proteins are molecular machines that need to move in order to function. Computationally one can model their motion according to physical equations of motion that are weighted according to an energy function. Discovery of the shapes or conformations of proteins is a sampling problem in a very high dimensional space, and thus difficult. Solving this problem can assist in the discovery of biologically relevant intermediates of reactions, design of new medicinal drugs, and better characterization of their biological function.<br\/>This proposal will develop new algorithms and software to discover the conformations of proteins. We exploit distributed or grid computing to pursue exploration of the conformational space. We use a dimensionality reduction approach where the search is confined to the slowest and most collective motions of the protein. This greatly accelerates sampling and is scalable to large proteins. We will apply our technique in two algorithms, replica exchange and the on-the-fly string method. Replica exchange uses multiple simulations at different temperatures and performs periodic exchanges among them to achieve annealing. The string method finds a minimum free energy path that connects two known conformations of a protein: for instance, inactive to active enzyme. Our technique automatically finds ?slow? and collective variables along which to search conformations. <br\/>Intellectual merit: Our methods hold promise to accelerate the search by 3 or more orders of magnitude, opening entirely new avenues of research related to large conformational changes that happen over long timescales for larger proteins.<br\/>Broader impacts: These algorithms will be released as open source software with a Python interface and GPU-accelerated implementations, and will be able to use large distributed systems. Validation will be done on a specific protein system for which experimental data is available. <br\/>In collaboration with the Shodor Foundation, we will involve high school students and teachers in developing web content in support of the work on this proposal. We will explore whether including these materials in courses raises interest in computational biology or biology or computation.","title":"AF: Small: CCF: CISE: Advanced Grid-Enabled Algorithms for Discovering Protein Conformations","awardID":"1018570","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":[451806,451807],"PO":["565223"]},"168708":{"abstract":"This project will improve the state of the art of the implementation and optimization of algorithms for exact linear algebra computation. With exact computation, solving systems of linear equations is advanced from limited accuracy to exact solutions. This greatly increases the scope of accessible applications and allows matrix invariants such as rank, determinant, characteristic and minimal polynomial, and Smith and Frobenius normal forms to be computed.<br\/><br\/>We will combine a newly developed theoretical basis for block blackbox methods in linear algebra with high performance implementation, in hardware and software, of the computational kernels from which these implementations are constructed. The resulting implementations will be made publicly available in the framework of the LinBox software library. A system for automatically tuning the underlying computer algebra kernels will be developed and distributed as part of the LinBox library. The autotuning framework will benefit other computer algebra systems as well. The resulting advances for computation in finite domains, such as modular numbers and finite algebraic field extensions, will benefit many areas including cryptography and coding theory.<br\/><br\/>The project has many practical impacts as follows:<br\/><br\/>Experimental mathematics will be enhanced. In experimental mathematics, symbolic computation provides for testing of conjectures. And, perhaps more importantly, data from symbolic computations can guide the formulation of conjectures that are then candidates for formal proof. By permitting larger exact linear algebra computations, this project will increase the usefulness of such computation in mathematics.<br\/><br\/>The broadest, and perhaps most significant, outcome of this project is an ability to solve many problems which currently have no solution method at all. This project will make it possible to efficiently solve linear systems where numerical methods fail due to ill-condition of the problem instance, yet the exact result, could it be obtained, is valid and meaningful despite the approximate nature of the data.","title":"AF: Small: Collaborative Research: High Performance Exact Linear Algebra Kernels","awardID":"1018063","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":["541916",451563],"PO":["565251"]},"168719":{"abstract":"Wireless access through unlicensed spectrum is a major component for future broadband initiatives. Unfortunately, unlicensed frequency bands are congested, and existing technologies do not provide quality assurance for users with different access priorities and needs. The lack of quality of service guarantee is a key roadblock to providing multimedia content and facilitating broadband communications. There is a cogent need for technologies that provide distributed hierarchical access of unlicensed spectrum through dynamic spectrum sensing, learning, and cognitive sharing of spectrum opportunities. <br\/><br\/>This research develops mathematical theory and practical methodologies for the cognitive access of wireless spectrum shared by multiple classes of users. The technical approaches of this research are based on the structural optimality of certain carrier sensing policies. The optimality and simplicity of these policies result in new practical yet optimal solutions to cognitive spectrum sharing among users from different priority classes. The overall objective is twofold. First, it aims at gaining a foundational understanding of the limits of cognitive spectrum access in hierarchical networks. In particular, it characterizes the maximum throughput and effective bandwidth regions for users for different priority classes by exploiting structures of optimal policies for cognitive sensing and access. Second, this research develops engineering practical solutions for decentralized cognitive access of shared spectrum. In particular, new multiuser sensing and access protocols are developed and tested for practical applications. Effects of fading, interference and other implementation issues are also examined. This research includes a significant education component aimed at integrating frontier research with undergraduate and graduate curricula.","title":"CIF: Small: Cognitive Spectrum Access: Fundamental Limits, Protocols, and Performance Analysis","awardID":"1018115","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["531874"],"PO":["564924"]},"167509":{"abstract":"Today's computer systems are more powerful than ever, but have become so complex that it is now difficult for programmers to produce high-performance software. Even slight changes in programs or differences in a user's system can cause dramatic slowdowns. Currently, there is no way to guarantee that a program will perform as well as it did during testing. This situation makes it extremely difficult to track down the sources of inefficiencies or repair them. The result is reduced power and computational efficiency on servers, and a degraded user experience on client platforms.<br\/><br\/>This research aims to deliver reliable performance on modern computer systems. By introducing randomness into the way a computer runs programs, a reliably performant system will significantly reduce the probability that any small change will have a large impact on performance. For instance, consider a cache miss caused by a conflict. With standard caches, repeated access to the same elements would always cause misses, degrading performance. In a randomized cache or with randomized object placement, it would be very unlikely for the same line to be repeatedly evicted. The investigators are designing and evaluating the use of both randomized algorithms in software and hardware, separately and in combination, to remedy the numerous sources of pathological behavior in modern systems. The result will enable performance-portable applications that are immune to unfortunate interactions with microprocessor components.","title":"SHF: Large: Collaborative Research: Reliable Performance for Modern Systems","awardID":"1012195","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["562740"],"PO":["565272"]},"168609":{"abstract":"Field Programmable Gate Arrays (FPGAs) become increasingly attractive alternatives to conventional microprocessors and application specific integrated circuits. Indeed, FPGAs combine the best properties of both, such as highly customized datapath and cost efficiency, while with miniaturization of complementary metal oxide semiconductor (CMOS) technology during past decades the capacity of FPGAs is now close to a million programmable logic blocks. This opens new opportunities for the use of FPGAs, in particular, in signal, image and network processing, cryptography, scientific and high performance embedded computing. <br\/><br\/>However, further improvements of FPGA circuits cannot rely on just lateral device shrinking since CMOS technology is getting already close to its fundaments scaling limits and other opportunities must be explored. One such opportunity is presented by the fact that in contemporary FPGAs, the performance overhead for reconfigurability is very high causing \"useful\"<br\/>resources to take only a small fraction of the area of a chip. In this work, PIs focus on hybrid technology FPGAs, in which all configurations bits are implemented in monolithically stacked layers of metallization as resistance switching (i.e., memristive) crosspoint devices integrated on top of a silicon substrate. Preliminary results indicate that even without any optimization and using only conventional CMOS technology, the proposed circuits efficiently hide configuration overhead, and can thus be at least ten times denser (and potentially faster) than CMOS FPGAs with the same design rules and similar power density. To substantiate these claims PIs proposed rigorous interdisciplinary research agenda with three main thrusts: <br\/>1) investigation of circuit and architectural innovations, including optimal logic and routing architectures taking into account constraints of state-of-the-art fabrication technology; <br\/>2) detailed performance estimation based on physical models of the most promising crosspoint memristive devices; <br\/>3) development of design automation tools to map arbitrary circuits onto novel fabrics, including fabric-aware logic synthesis and defect tolerant placement and routing.","title":"SHF: Small: Design, Modeling and Automation of Monolithically Stackable Hybrid CMOS\/Memristor Programmable Circuits","awardID":"1017579","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7947","name":"NANOCOMPUTING"}}],"PIcoPI":["518632","456843"],"PO":["565157"]},"161701":{"abstract":"CAREER: Sequential Monte Carlo Methods for High Dimensional Systems<br\/>Abstract<br\/><br\/>Advances in the development of models and methods that can satisfactorily describe and analyze high dimensional systems are extremely valuable for many different disciplines including biology, meteorology, economics,social sciences, and engineering. These systems are characterized by nonlinearities and are difficult to understand.Computational methods governed by simple local rules have the potential of providing insightful interpretations<br\/>and of paving the way towards quantitative and qualitative descriptions and understanding of complex systems.This project is focused on development of such methods and in particular on the development of a synergistic research and educational program in sequential Monte Carlo-based signal processing for high dimensional systems.<br\/><br\/>This research aims at laying the foundations of a sequential Monte Carlo methodology for high dimensional systems. In the literature, there are claims stating that particle filters cannot be used for complex systems because their random measures degenerate to single particles. While this is true for standard implementation of these filters, it does not hold true for alternative approaches. A new methodology based on the principle of divide and conquer is developed. In particular, the collapse of traditional particle filltering is avoided by setting an interconnected network of filters, each of them working on lower dimensional spaces. Research tasks include development of the theoretical grounds of the methods, establishment of guidelines for its use by practitioners,analysis of its accuracy, stability and scalability, and validation on a wide range of complex systems. The proposed methods are original and provide solutions to arguably the biggest open problem of particle filtering.","title":"CAREER: Sequential Monte Carlo Methods for High Dimensional Systems","awardID":"0953316","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[432413],"PO":["564898"]},"172855":{"abstract":"In this EAGER the PI will explore issues relating to the development and use of data from game playing for assessment and rehabilitation of neurological disorders. The focus is on Cerebral Palsy (CP), a disease that causes a variety of motor and other impairments. The most common symptoms of CP are a lack of muscle coordination, stiff muscles, exaggerated reflexes, and impaired gait, and treatment includes Physical Therapy (PT) and Occupational Therapy (OT), speech therapy, drugs, surgery, and orthotic devices. Initially, the PI will investigate game-based systems for the upper extremities. Traditionally, OT experts use subjective judgment in conjunction with the Manual Ability Classification System (MACS) to measure the motor skills without connection to cortical level activity. The PI's approach is to connect cortical activity with motor activity. She will explore the coupling and integration of computer games and wearable sensors for both neurological and motor assessment testing, as well as for long-term rehabilitation at home. In particular, she will design a family of computer games that correspond to OT exercises and build an initial set of feature classifiers for types of CP with motor skill assessment. She will also build computer infrastructure for remote rehabilitation and a cyclical evaluation methodology. The primary outcome of this research will be an initial setup for remote rehabilitation that uses machine reinforcement learning and fuses multimodal information collected from a variety of sensors. Using computer games in conjunction with brain imaging and traditional rehabilitation outcomes is a radical and untested but potentially transformative approach to healthcare practices for chronic conditions such as CP.<br\/><br\/>Broader Impacts: The PI's long-term goal is to develop an intelligent system called CPLAY, whose front end is a set of computer games to provide controlled stimuli to children with CP in order to facilitate a desired motor response and generate performance data for diagnosis and rehabilitation treatment. CPLAY's backend will be a set of computational engines to enable data logging (from a child playing the game), data fusion, analysis and decision support. The @lab version of CPLAY will be used for functional near infrared (fNIR) imaging, while the @home version will be used for rehabilitation with various additional sensors capturing and fusing data during game playing. The fNIR imaging is used to determine neuro-plasticity and motor recovery. The @home version tracks performance over time, considers context of the game, and can be monitored remotely if needed. Both versions will allow for game adjustments to provide personalized treatment. The CPLAY approach promises to lower costs and facilitate family engagement in the rehabilitation. Project outcomes will include a paradigm, methodology and tools with broad applicability to other neurological disorders.","title":"EAGER: An Exploratory Pilot Project to Build an Intelligent Human-Computer Interface System for Neurological Disorder Assessment and Rehabilitation","awardID":"1041637","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[463572,"229551"],"PO":["565227"]},"163901":{"abstract":"In a world where everybody and everything is electronically connected, identity is essential to support trustworthy transactions. This project investigates novel techniques to design and implement on-chip fingerprints. Such fingerprints establish the hardware identity of an electronic system. The on-chip fingerprints are based on the existing, small and random manufacturing variations of electronic chips. Using a cross-disciplinary approach that combines recent advances in the field of statistics with those in circuit design, this project develops on-chip fingerprint structures that are optimized for stability, implementation cost, and security. Stable on-chip fingerprints are maximally sensitive to random manufacturing variations, and minimally sensitive to other environmental factors such as temperature, voltage, noise, and aging. Low-cost on-chip fingerprints are obtained by using statistical, architectural, and circuit-level techniques that maximize the amount of extracted entropy. Secure on-chip fingerprints are resistant against common attacks such as reverse engineering and model building. Thanks to its cross-disciplinary character, this project establishes a much-needed link between advanced statistical analysis and deep-submicron design for the purpose of circuit identification. This leads to better PUF designs, applicable across a wider range of applications: secure passports, anti-counterfeiting schemes, and security and trust at the endpoints. The project includes strong integration of research and education. For pre-college and entering freshman students, the project offers an introduction to trusted hardware, in the context of existing on-campus programs that involve minorities in engineering. For graduate students, the project offers a team-taught course, shared between the electrical engineering department and the statistics department.","title":"TC: Medium: From Statistics to Circuits: Foundations for Future On-chip Fingerprints","awardID":"0964680","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["548205","518481","485466"],"PO":["565327"]},"161745":{"abstract":"The cloud computing model has opened up new possibilities for the realization of the long-cherished goal of utility computing. Utility computing represents the desire to have IT acquired, delivered, used, paid for, and managed in a manner similar to the way we use other commoditized utilities. The principal appeal of utility computing lies in the systematized framework it could create for the interaction between providers and consumers of IT resources. In particular, utility computing should enable consumers to participate in active and informed ways in making resource procurement decisions in a transparent ``market'' of competing providers. Consumers of current cloud-based offerings have a limited view of and control over resource procurement and control, a significant hindrance in the realization of a utility. This research will develop mechanisms and techniques that would reduce this gap, thereby helping turn cloud-based offerings of the near future into mature utilities. <br\/><br\/>This research will define, formulate, and solve fundamentally novel resource management problems---consumer-end metering, auditing, and dynamic mapping between virtual and physical resources---in cloud computing. It will result in novel utility-enabling facilities that will reduce the burden on application developers and system administrators wishing to outsource their IT needs to the cloud by easing and automating currently non-existent or inadequate decision-making related to resource procurement and modulation. Prototypes and source code will be shared with other researchers for independent use, experimentation, and deployment. The plan for integration of teaching and research will consist of (i) engaging undergraduates in research through REU supplements, (ii) design of new graduate courses on this topic with projects based on cloud-hosted teaching testbeds, and (iii) dissemination among researchers and industry via open-source software and workshops.","title":"CAREER: Turning Cloud - Based Hosting Into a Utility","awardID":"0953541","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["541886"],"PO":["565255"]},"171788":{"abstract":"The aim of this exploratory project is to conduct a study of practical programming with user-defined axiom to explore how semantic information, expressed through axioms, can be taken advantage of in structured generic programming. This work is a preliminary step to doing research on dependable, scalable generic programming, with application to scientific software. More specifically, the PI will investigate how, and which forms of user-defined axioms can be turned into property decision procedures that complement conventional type systems. This target of this work is the C++ programming language, which has a huge base of code and programmers and thus will have a broad impact. The work will address the question of how axioms can be used to make programs simpler, more flexible, and provide semantic information to code generators. The system will be released as open source software and evaluated on software from computer algebra.","title":"EAGER: Exploration in Type Systems With User-Defined Axioms","awardID":"1035058","effectiveDate":"2010-08-15","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["502347"],"PO":["564588"]},"161778":{"abstract":"Graphical models, such as Bayesian networks and influence diagrams, provide principled approaches to solving reasoning and decision making under uncertainty problems. However, the adaptability and scalability of existing methods for these graphical models are often limited. This project aims to address some of these limitations by developing new and improved approaches to explanation, decision making, and learning in graphical models. It includes the following specific objectives: (1) developing new approaches to finding explanations that only contain the most relevant variables for given observations in Bayesian networks, (2) developing heuristic search-based methods and algorithms to solve influence diagrams more efficiently, (3) developing new algorithms for learning optimal Bayesian networks guided by domain-specific heuristic information so that only a small fraction of the solution space need to be explored, and (4) applying the methods developed in this project to real-world applications including multiple-fault diagnosis, supply chain risk management, and online collaborative learning. <br\/><br\/>This project can lead to significantly better approaches to reasoning and decision making under uncertainty in many disciplines where graphical models have found successful applications, including medicine, security, planning, business, economics, education, and many others. This project can also lead to the development of new and enhanced courses and curricula, the involvement of students from underrepresented groups in the research, and a wide dissemination of the research outcomes through free software, publications, and presentations.","title":"CAREER: Explanation, Decision Making, and Learning in Graphical Models","awardID":"0953723","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[432571],"PO":["562760"]},"172789":{"abstract":"In 2003, the PI's research group at the MIT Media Lab was awarded a four-year grant from the National Science Foundation (ITR-0325828) to develop a new programming environment, called Scratch, that enables young people to create their own interactive stories, games, animations, and simulations - and share their creations with one another online. The Scratch website (http:\/\/scratch.mit.edu), launched in May 2007, has become a vibrant online community, with 500,000 registered members sharing, discussing, and remixing one another's projects.<br\/><br\/>This summer, the Scratch group is hosting a conference, called Scratch@MIT, for educators, researchers, developers, and others who support the use of Scratch, so that they can share best practices with one another (see http:\/\/scratch.mit.edu\/conference). The PI previously hosted one similar Scratch conference, in July 2008 at MIT. The conference attracted great interest: registration reached full capacity of 300 people a full month before the conference. The Scratch group is increasing the maximum number of participants for this summer's conference to 350 people, and they again expect to reach full capacity.<br\/><br\/>Intellectual Merit. As young people program and share Scratch projects, they learn to think creatively, reason systematically, and work collaboratively - essential skills for success in the 21st century. Educators attending the Scratch@MIT conference will have the opportunity to participate in hands-on workshops, share experiences and ideas, and learn about new techniques and strategies, so that they are better able to support and facilitate student learning with Scratch in their own educational contexts.<br\/><br\/>Broader Impact. Since Scratch software is available free of charge, it has attracted a broad and diverse community of users. The investigators want to make sure that participation at the Scratch@MIT conference is similarly broad and diverse. They realize that registration fees and travel costs could make it difficult for some people to participate in the conference. With this support they will waive registration fees for 50 conference participants who would not otherwise be able to attend. They expect that this will allow participation by more educators from public schools and community centers (who typically have less access to funding than researchers or teachers from private schools). By making the conference accessible to those who could not otherwise afford the registration fees, they expect an increase in the overall diversity of participants at the conference.","title":"Broadening Participation at the Scratch@MIT Conference","awardID":"1041290","effectiveDate":"2010-08-01","expirationDate":"2011-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["497072"],"PO":["564181"]},"174637":{"abstract":"This award supports a doctoral symposium at the 4th ACM Conference on Recommender Systems (RecSys 2010) to be held in Barcelona, Spain, September 26-30, 2010. RecSys is the leading international forum for the presentation and discussion of research and practice on recommender systems. The doctoral symposium will help students refine their dissertation projects so that they can make stronger contributions to the solution of critical intellectual challenges in recommender systems. <br\/><br\/>Nearly all of the leading E-Commerce sites world-wide use recommender systems, to help their customers find the products and services they will most value from the millions of alternatives available. Creating effective recommender systems is a difficult challenge. The proposed doctoral symposium will help in training a generation of scientists who will become leaders in developing this important field of research, and in applying the research results in practical applications. One particular strength of the RecSys conference, from its inception to the present day, is the rich participation of industry leaders in the conference. Through a poster session, the broader community, including the substantial contingent of industry participants in the conference, will also gain exposure to this new cohort of students. By helping doctoral students to appreciate each other's different perspectives and form a cohort across those boundaries, we can help to preserve the diversity of the field in the coming years.","title":"WORKSHOP: ACM Recommender Systems Conference 2010 Doctoral Symposium","awardID":"1050483","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["483488"],"PO":["565342"]},"175737":{"abstract":"","title":"Multi-Media Services","awardID":"1057521","effectiveDate":"2010-08-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0105","name":"Office of LEGISLATIVE & PUBLIC AFFAIRS","abbr":"LPA"},"pgm":{"id":"0608","name":"TRAINING CONTRACTS"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0105","name":"Office of LEGISLATIVE & PUBLIC AFFAIRS","abbr":"LPA"},"pgm":{"id":"0636","name":"ADMINISTRATIVE CONTRACTS"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0105","name":"Office of LEGISLATIVE & PUBLIC AFFAIRS","abbr":"LPA"},"pgm":{"id":"097P","name":"IN-HOUSE PRODUCTION SUPP SVCS"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0105","name":"Office of LEGISLATIVE & PUBLIC AFFAIRS","abbr":"LPA"},"pgm":{"id":"7957","name":"COMMUNICATING SCIENCE BROADLY"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0105","name":"Office of LEGISLATIVE & PUBLIC AFFAIRS","abbr":"LPA"},"pgm":{"id":"LX20","name":"PATENT AND TRADEMARK OFFICE, U"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0112","name":"Office of EPSCoR","abbr":"EPS"},"pgm":{"id":"9168","name":"EPSCOR OUTREACH"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1798","name":"SPECIAL PROJECTS (AST)"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0307","name":"Division of MATERIALS RESEARCH","abbr":"DMR"},"pgm":{"id":"7222","name":"OFFICE OF SPECIAL PROGRAMS-DMR"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1978","name":"PROJECTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0600","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"7273","name":"ENVIRONMENTAL RESEARCH & EDUCA"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0704","name":"Division of EMERGING FRONTIERS IN RES & IN","abbr":"EFRI"},"pgm":{"id":"7633","name":"EFRI RESEARCH PROJECTS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1340","name":"ENGINEERING EDUCATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0706","name":"Division of DESIGN & MANUFACTURING INNOV","abbr":"DMI"},"pgm":{"id":"1468","name":"Manufacturing Machines & Equip"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"1504","name":"GRANT OPP FOR ACAD LIA W\/INDUS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0709","name":"Division of BIOENGINEERING & ENVIRON SYSTE","abbr":"BES"},"pgm":{"id":"1385","name":"SPECIAL STUDIES AND ANALYSES"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0709","name":"Division of BIOENGINEERING & ENVIRON SYSTE","abbr":"BES"},"pgm":{"id":"5342","name":"Gen & Age Rel Disabilities Eng"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"1329","name":"PLANT GENOME RESEARCH PROJECT"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"8288","name":"BM Gates Foundation"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"9100","name":"UNDIST PANEL\/IPA FNDS-PLNT GEN"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7513","name":"TUES-Type 1 Project"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7261","name":"PROGRAM EVALUATION"}},{"dir":{"id":"14","name":"Office of OFFICE OF POLAR PROGRAMS                ","abbr":"OPP"},"div":{"id":"1401","name":"Division of ARCTIC SCIENCES DIVISION","abbr":"ARC"},"pgm":{"id":"1335","name":"IGERT FULL PROPOSALS"}},{"dir":{"id":"14","name":"Office of OFFICE OF POLAR PROGRAMS                ","abbr":"OPP"},"div":{"id":"1403","name":"Division of ANTARCTIC SCIENCES DIVISION","abbr":"ANT"},"pgm":{"id":"7172","name":"GRADUATE RESEARCH FELLOWSHIPS"}},{"dir":{"id":"14","name":"Office of OFFICE OF POLAR PROGRAMS                ","abbr":"OPP"},"div":{"id":"1403","name":"Division of ANTARCTIC SCIENCES DIVISION","abbr":"ANT"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":[471036],"PO":[471037]},"176958":{"abstract":"This project studies three fundamental problems to improve the performance of wireless mesh networks. <br\/><br\/>(1) Managing delay and jitter. One fundamental problem affecting the performance of current mesh networks is the hop-by-hop relaying of data, resulting in significant per-hop and per-packet delay and\/or jitter. This project designs a new MAC paradigm and a distributed method of scheduling data transmissions in a path-aware manner, to eliminate per-packet delay and jitter while minimizing per-hop delays. <br\/><br\/>(2) Capacity analysis and utility optimization. As an augmentation to the large amount of simulation studies on multi-channel<br\/>multi-radio mesh networks, this project develops a general theoretical model to analyze both unicast and broadcast capacities of mesh networks, and applies the model to optimally assign channels to maximize capacity, as well as optimizing application-specific utility functions relevant to user-perceived network performance. <br\/><br\/>(3) Channel assignment for dynamic spectrum access mesh networks. Recent advancement in cognitive radio technology and regulatory reform in spectrum policy offer dynamic spectrum access (DSA) capability to mesh networks via providing dynamically available channels. This project adopts a cross-layer and path-centric approach for assigning dynamic channels in<br\/>DSA mesh networks to achieve high spectrum utilization and traffic throughput. <br\/><br\/>Through developing innovative protocols, analysis and optimization models, this project promotes development of high-performance multi-radio, multi-channel, and DSA-capable mesh networks. The research on DSA mesh networks potentially facilitates the communication among different emergency management divisions in disaster rescues operations. This project also benefits the underrepresented minority students through education and research.","title":"NeTS-WN: Collaborative Research: Toward High-Performance Mesh Networks","awardID":"1063177","effectiveDate":"2010-08-25","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[474446],"PO":["564993"]},"174659":{"abstract":"In response to the Gulf of Mexico oil spill innovative UWB radar sensor network imaging methodologies to produce 2-D image based on 1-D signals are under development. Using these techniques beach soil reflectivity can be measured. The differences between the reflectivity of oil spilled beach soil and that of the normal beach soil shows the dielectric property changes due to oil spill. The oil spill has changed the beach soil dispersion, and oil contamination drastically reduces the bearing capacity of sand, which impacts building foundations and may cause building safety issue. In this project, models of the relations between soil reflectivity, oil content, and compression index are under development. The research crosses multiple disciplines, including sensor networks, radars, geoscience and remote sensing, civil engineering and geotechnology, signal and image processing, wireless communications, and pattern recognition; the research team includes four investigators drawn from three departments (EE, Civil Engineering, and CSE). This project promises to generate new results for radar remote sensing and radar sensor network approaches to assessing oil spill impact on beach soil.","title":"RAPID: Collaborative Research: Gulf of Mexico Oil Spill Impact on Beach Soil: Radar and Radar Sensor Network-Based Approaches","awardID":"1050618","effectiveDate":"2010-08-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["531713","364695","502401"],"PO":["565303"]},"168907":{"abstract":"Science and analysis today are increasingly tackled by systematic exploration of high-resolution captured, or simulated, data. With a more expansive sample of raw data, more detailed models and precise questions of the meaning of the data can be formed. However, massively parallel systems for processing massive data sets render traditional programming, storage and fault tolerance strategies ineffective. <br\/><br\/>Table-based or column-oriented distributed data storage systems are being developed to support such large scale data analysis, led by Google?s BigTable and including open source variations such as Apache Hbase. These new systems have the flavor of database row and column organization, but have simpler semantics, weaker isolation, and non-SQL interfaces, for example. The effectiveness of these new systems for applications other than internet search support is not well understood.<br\/><br\/>This exploratory project is developing an evaluation framework and exploring a set of these new table-based storage systems, with the goal of capturing an understanding of the state of the art, how they perform and scale, and their reliability and usability.<br\/><br\/>In addition to benchmarks focussing on key metrics, the project's evaluation framework includes real world applications drawn from machine learning approaches to understanding streams of events, such as internet blog publications, and approaches to understanding complex interrelationships, such as social networking graphs, in order to extract insight about the requirements needed to enable these emerging types of knowledge discovery applications.","title":"EAGER: Exploratory Evaluation: Scalability and Effectiveness of Data-Intensive Table-based Computing Software Systems","awardID":"1019104","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7952","name":"HECURA"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"J147","name":"National Security Agency"}}],"PIcoPI":["465808"],"PO":["565272"]},"168819":{"abstract":"Internet fraud costs consumers and businesses billions of dollars each year. Through creative combinations of spam and social engineering, attackers regularly lure end users into visiting phishing sites, malware-hosting sites, and scam sites. One popular defense mechanism against Web-based attacks is blacklisting, but today's blacklists suffer from three fundamental deciencies. First, most of them employ a combination of Web crawling and human intervention to infer malicious sites. This adds an inherent delay in adding entries and causes many malicious sites to be missed. Second, blacklists are mostly based on exact URL strings, and hence unable to adapt to simple changes to the URLs that attackers are using today to evade detection. Third, as blacklist entries grow, matching them against URLs in real-time could create performance bottlenecks. To overcome these deficiencies, this project is developing novel mechanisms to aid in the construction, maintenance, and matching of blacklists in real time. Specifically, it is developing a scalable architecture that can discover new malicious websites by passively observing the onset of new techniques exploited by the miscreants, such as redirects and fast flux in network traffic. The architecture also leverages common attacker tendencies to find novel and automated ways of discovering new malicious URLs from existing blacklisted URLs. The final thrust of the project is on developing high-speed approximate matching algorithms for effective in-network blacklisting to match URLs embedded in packets against potentially millions of blacklist entries. If successful, this project will make the Web safer for millions of Internet users.","title":"TC: Small: Collaborative Research: Predictive Blacklisting for Detecting Phishing Attacks","awardID":"1018617","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["491568"],"PO":["565136"]},"171932":{"abstract":"The objective of this research is to develop a new approach for composition of safety-critical cyber-physical systems from a small code base of verified components and a large code base of unverified commercial off-the-shelf components. The approach is novel in that it does not require generating the entire code base from formal languages, specifications, or models and does not require verification to be applied to all code. Instead, an explicit goal is to accommodate large amounts of legacy code that is typically too complex to verify. The project introduces a set of verified component wrappers around existing unverified code, such that specified global system properties hold. <br\/>The intellectual merit of the project lies in its innovative approach for managing component interactions. Unexpected interactions are the primary source of failure in cyber-physical systems. A fundamental conceptual challenge is to understand the different interaction spaces of cyber-physical system components and determine a set of trigger conditions when certain interactions must be restricted to prevent failure. The project develops analysis techniques that help understand the different interaction types and provides component wrappers to restrict them when necessary.<br\/>Broader impact lies in significantly reducing the design and composition effort for the next generation of safety-critical embedded systems. A variety of student projects are being offered to undergraduates and graduate students. The researchers especially encourage women and minorities to participate. Outreach activity, such as hosting K-12 students on school field\/science days, further strengthen the educational component. Technology transfer to John Deere is expected.","title":"CPS: Medium: The Ectokernel Approach: A Composition Paradigm for Building Evolvable Safety-critical Systems from Unsafe Components","awardID":"1035736","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["553686","553633","542023"],"PO":["564778"]},"161834":{"abstract":"Wireless Sensor Networks (WSNs) have the potential to monitor the physical world at an unprecedented spatio-temporal scale. Recently, WSNs have been deployed for many emerging performance-critical applications such as monitoring important infrastructures (power grid and bridges) and detecting natural hazards (volcanoes and earthquakes). However, deeply integrated with the physical world, WSNs often suffer from significant performance variations caused by uncertainties including environmental noise, dynamics of physical phenomena, and network deployment inaccuracy. <br\/><br\/>This project develops a principled network design and analysis approach to performance assurance of WSNs. In contrast to existing heuristics-based solutions, this approach adopts data fusion, an advanced information processing scheme, to enable sensors to efficiently collaborate in delivering predictable network performance. This project has four aims: 1) A framework for analyzing spatio-temporal sensing performance based on established data fusion models. The analysis captures fundamental relationship between spatio-temporal coverage defined by event detection and false alarm probabilities, fusion models, network density, and physical uncertainties including noise and deployment inaccuracy. 2) Data fusion schemes that exploit mobile sensors to reconfigure the capability of a network in response to physical dynamics. 3) A unified fusion and communication architecture abstraction that allows developers to implement and optimize network protocols using group-level primitives. 4) A model-driven medium access control protocol that can achieve predictable throughput and delay in collaborative data processing.<br\/><br\/>This project will have impact in numerous critical applications that require stringent sensing and communication performance. The results of this project will be integrated into outreach activities, curriculum development, and student mentoring.","title":"CAREER: Design and Analysis of Performance-Critical Wireless Sensor Networks: A Fusion-Centric Approach","awardID":"0954039","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["554514"],"PO":["565303"]},"163902":{"abstract":"This project uses machine learning to accelerate the execution of a class of computer programs relevant to AI. Given a program and a class of inputs, the new methods automatically seek execution strategies that are fast while still achieving a high level of accuracy.<br\/><br\/>The project focuses on the main inference algorithms that underlie statistical AI: dynamic programming, belief propagation, Markov chain Monte Carlo, and backtracking search. Each of these inference algorithms faces an enormous search space, iteratively extending or refining its picture of this space. Each algorithm must continually choose which computational step to take next.<br\/><br\/>The opportunity is to learn a strategy for making these choices. Some choices are on the \"critical path\" and help the system find an accurate output, while others lead mainly to wasted work. The learned strategy for evaluating choices in context may itself be computationally intensive, so the method learns to speed that up as well, within the same framework.<br\/><br\/>The project will disseminate software and will have broader impact on several fields. The targeted algorithms are central to natural language processing, speech processing, machine vision, computational biology, health informatics and music processing. Their ability to form a coherent global analysis of a set of observations is a hallmark of intelligence, and will enable artificial systems that aid human understanding and performance. Speeding them up is critical as researchers develop increasingly sophisticated statistical models.<br\/>Furthermore, the learning methodologies developed will be useful in other settings that attempt to learn computational or behavioral strategies.","title":"RI: Medium: Learned Dynamic Prioritization","awardID":"0964681","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[438772,"550878"],"PO":["565215"]},"163528":{"abstract":"Proposal Summary<br\/><br\/>The GLORIAD project will include cooperative partnerships with its longstanding partners in Russia (Kurchatov Institute), Korea (KISTI), China (Chinese Academy of Sciences), the Netherlands (SurFnet), the Nordic countries (NORDUnet and ICELINK), Canada (CANARIE), and Hong Kong (HKOEP). In addition, it develops new partnerships with Egypt (ENSTINet and TEGROUP), India (Tata and the National Knowledge Network), Singapore (SINGAREN), and Vietnam (VinAREN), and further cooperation with STARLight, TRANSPAC, National Lambda Rail (NLR), and Internet2. Numerous other research and education networks will be connected in Southeast Asia, Africa, Middle East, Central Asia, and the Caucasus region through the development of open, optical network exchanges in these locations. Expanding the US-China network capacity and providing hybrid, dynamically provisioned services enables a new range of advanced applications, It furthers US-Nordic partnership in cyberinfrastructure development, connects scientifically vital polar parts of the globe that are not yet connected, and provides a new possibility for furtherance of US-Russia science collaboration. GLORIAD will also explore and create new opportunities for network development in Africa. <br\/><br\/>GLORIAD will demonstrate proactive US leadership in cyberinfrastructure by extending US reach to global scientific communities as it builds a more open, decentralized global network architecture for science and education. This project builds on network measurement, monitoring, and security tools that focus directly on secure networking and on improving individual user performance. It furthers GLORIADs program to create a standards-based, open-source, decentralized method of network operations, the distributed virtual network operations center (dvNOC), and to enable a new approach to governance and management of cyberinfrastructure. <br\/><br\/>Intellectual Merit<br\/>The GLORIAD project advances cyberinfrastructure services and metrics, broadens the global community working on measuring and mitigating customer-based infrastructure performance, and furthers the model of distributed, decentralized management of cyberinfrastructure resources. Additional connectivity and bandwidth allows greater access to cutting edge research in the US, Egypt, South East Asia, Central Asia, Caucasus, Africa, and Muslim-majority countries. It advances research in pragmatic network deployments and operations. The extension of performance tools will have the potential to strongly impact research and education network deployments.<br\/><br\/>Broader Impact <br\/>GLORIAD?s broadest impact lies in contributions as an infrastructure for research and education. The project will foster collaborations among disciplines and institutions. The network provides unprecedented bandwidth and dedicated services for US researchers to collaborate with counterparts in India, Hong Kong, Singapore, Egypt, Vietnam, South East Asia, Africa, and Central Asia\/Caucasus region. GLORIAD has a long track record of fostering partnerships between public-private entities that address major science problems and can potentially reach tens of thousands of people.<br\/><br\/>GLORIAD is committed to diversity in all activities, and aims to support US national interests by providing network access to Muslim-majority countries and underserved regions of the world. GLORIAD?s broad dissemination of information will enhance scientific and technological understanding by offering tools and information for new users. GLORIAD will bring this information to a diverse group in various languages and levels of expertise, from expert to student to the policy community and the general public.","title":"IRNC: ProNet: GLORIAD","awardID":"0963058","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7369","name":"INTERNATIONAL RES NET CONNECT"}}],"PIcoPI":["526814","359193",437476,437477,"446168"],"PO":["564246"]},"164519":{"abstract":"The vision for the Smart Grid is to support new ways of producing, transmitting, distributing, and consuming energy to change and improve the sustainability of future energy environments. This represents a unique opportunity for synergy between research in social-computational systems and the Smart Grid application domain. This project will demonstrate that on the one hand, for the Smart Grid effort to live up to its vision and expectations, technological developments are necessary but not sufficient: fostering and supporting changes in human behavior are equally important. On the other hand, social-computational systems face interesting, specific, and unique research questions to cope with the challenges associated with Smart Grids.<br\/><br\/>The project will design and develop: (a) components of an initial theoretical framework for social-computational systems based on cultures of participation and (b) initial architecture and interaction mechanisms for HYDRA, an open, collaborative, living knowledge environment to foster and support ecologies of different levels of participation. Evaluation in naturalistic settings will provide requirements and guidelines for future developments as a major outcome.<br\/><br\/>The project will transcend existing technological research by creating prototypes of new socio-technical systems to empower human beings and provide opportunities, incentives, and rewards for changing their behaviors. Collaborations with local technology companies, local governments, and international researchers will ensure the broad impact of our research. Beyond the lessons learned from our specific developments, the theoretical grounding of our research will make our methodologies, components, architectures, requirements, and guidelines applicable to a large number of social-computational systems.","title":"SoCS: Energy Sustainability and Smart Grids: Fostering and Supporting Cultures of Participation in the Energy Landscape of the Future","awardID":"0968588","effectiveDate":"2010-08-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["483372","483373"],"PO":["565342"]},"172923":{"abstract":"The Center for Computer Games and Virtual Worlds at the Donald Bren School of Information and Computer Sciences will host an interdisciplinary workshop on advancing the agenda for research and education in computer games and immersive environments. Speakers from academia will be drawn from the computer science, film and media arts, social sciences, education and humanities; industry presenters will be drawn from the digital games and virtual world industries that span entertainment, enterprise applications, and training. The over-arching aim will be to craft research agendas and future directions for research and educational programs for computer games and immersive environments. Structured as a research summit, this event will engage both speakers and attendees in an interdisciplinary dialog on the creative, social, technical, and businesses challenges posed by the \"beyond-the-next generation\" of immersive computer games.<br\/><br\/>The focal topics to be addressed in the workshop include: (a) Multi-core technologies for computer games and virtual worlds; (b) Massively multiplayer online games and virtual worlds as objects of empirical study; (c) Computer games and virtual worlds as complex systems with high socio-economic consequence; (d) Computer games and virtual worlds as new tools for supporting scientific research in other fields; and (e) Computer games and virtual worlds for future education and learning needs. Computer games are already a major industry of great economic significance, but the scientific and educational potential of the technology has not yet been realized fully. The goal of the workshop is to bring together current researchers and scholars interested in these problem areas, and to elicit, capture, and document what this community finds are the critical research needs and grand challenges that help articulate a national research agenda going forward over the next 3-10 years.","title":"Workshop: Research Directions and Challenges in Computer Games and Virtual World Environments","awardID":"1041918","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["535599","472358"],"PO":["564456"]},"173979":{"abstract":"This Rapid Response Research (RAPID) project is developing technology for ubiquitous event reporting and data gathering on the 2010 oil spill in the Gulf of Mexico and its ecological impacts. Traditional applications for monitoring disasters have relied on specialized, tightly-coupled, and expensive hardware and software platforms to capture, aggregate, and disseminate information on affected areas. We lack science and technology for rapid and dependable integration of computing and communication technology into natural and engineered physical systems, cyber-physical systems (CPS). The tragic Gulf oil spill of 2010 presents both a compelling need to fill this gap in research and a critical opportunity to help in relief efforts by deploying cutting-edge CPS research in the field. In particular, this CPS research is developing a cloud-supported mobile CPS application enabling community members to contribute as citizen scientists through sensor deployments and direct recording of events and ecological impacts of the Gulf oil spill, such as fish and bird kills. <br\/><br\/>The project exploits the availability of smartphones (with sophisticated sensor packages, high-level programming APIs, and multiple network connectivity options) and cloud computing infrastructures that enable collecting and aggregating data from mobile applications. The goal is to develop a scientific basis for managing the quality-of-service (QoS), user coordination, sensor data dissemination, and validation issues that arise in mobile CPS disaster monitoring applications. <br\/><br\/>The research will have many important broader impacts related to the Gulf oil spill disaster relief efforts, including providing help for the affected Gulf communities as they field and evaluate next-generation CPS research and build a sustained capability for capturing large snapshots of the ecological impact of the Gulf oil spill. The resulting environmental data will have lasting value for evaluating the consequences of the spill in multiple research fields, but especially in Marine Biology. The project is collaborating with Gulf area K-12 schools to integrate disaster and ecology monitoring activities into their curricula. The technologies developed (resource optimization techniques, data reporting protocol trade-off analysis, and empirical evaluation of social network coordination strategies for an open data environment) will provide a resource for the CPS research community. It is expected that project results will enable future efforts to create and validate CPS disaster response systems that can scale to hundreds of thousands of users and operate effectively in life-critical situations with scarce network and computing resources.","title":"RAPID: Collaborative Research: Cloud Environmental Analysis and Relief","awardID":"1047780","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["555863"],"PO":["565255"]},"173958":{"abstract":"This project entails the design of a new computational framework based on music compositional process and sketching to map and identify patterns in large-scale complex scientific data sets. This research project will be undertaken at the California NanoSystems Institute AlloSphere, University of California, Santa Barbara. The Allosphere is one of the largest 3D immersive display devices in the world for scientific visualization and artistic instrumentation. Leveraging mathematical concepts and constructs for binding structure and information flow in scientific and artistic research, the computational framework will incorporate a three-dimensional hierarchical sketching system that will be the basis for structural representation. Sonic marking of patterns in large datasets and techniques in music composition will be explored as ideal methods to identify complex integrated layers of data as music carries meaning on several time-scales, from individual timbres and pitches to short melodies and rhythms all the way up to large-scale form and structure of a work, each engaging distinct perceptual and cognitive processes. Real-time, interactive representations of the data in the AlloSphere will allow researchers to rapidly prototype parametric systems for more time-consuming and resource-demanding simulations and experiments. <br\/><br\/>Representing complex scientific data through large-scale immersive 3D audiovisual data representation will facilitate understanding to a wide audience, from advanced researchers who will be able to communicate across disciplines, to the general public. The AlloSphere will motivate dissemination of engineering and science research to wide audiences in education and society, through this new software platform that will allow a broader public to comprehend science that would be out of their reach of understanding. The AlloSphere Research Facility has its own formal outreach initiative that services the CNSI's Professional Outreach Program, that coordinates research interns, undergraduates and high school students through programs such as; (1) The after-school LEAPS (Let's Explore Physical Science) program works with eighth grade students and plans to start an independent research project option for high school seniors; (2) The INSET (NSF Internships in NanoSystems Science, Engineering and Technology) program recruits students from largely underrepresented groups from California community colleges for eight-week summer internships; (3) The Apprentice Researchers program engages high school juniors in individual laboratories at UCSB.","title":"EAGER: A Computational Framework Integrating Methods from Music Composition and Sketching for Large Scale Scientific Data Visualizations in the 3D Immersive Allosphere","awardID":"1047678","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["561003",466630],"PO":["564456"]},"173606":{"abstract":"This proposal explores effective solutions to RAM scraping attacks in <br\/>the context of data management workloads. Recent studies identify <br\/>RAM scraping attacks, wherein an adversary installs malware to steal<br\/>memory resident data, as one of the major causes of data thefts in<br\/>enterprise information systems. Proposed research explores <br\/>new vulnerabilities introduced by RAM scraping and develops<br\/>practical solutions that offer the right blend of security against such<br\/>attacks while keeping performance degradation (due to cost\/overhead of<br\/>security scheme) within acceptable limits. The research team will explore <br\/>realistic attack scenarios, model adversarial capabilities and constraints <br\/>for typical DBMSs (e.g., small duration repeated attacks, effects of <br\/>buffering and data lifetime, presence of concurrent queries, bandwidth limits,<br\/> etc.), design criteria of a risk-aware query optimization that can account<br\/>for a variety of disclosure risks emanating from RAM-scraping malwares;<br\/>and develop optimization techniques that simultaneously optimize performance <br\/>and reduce disclosure risks. Proposed research also explores the threat of<br\/>RAM-scraping attacks for the emerging cloud-based data processing frameworks <br\/>like MapReduce.<br\/><br\/><br\/>Intellectual Merit: The proposed research represents the first attempt<br\/>towards a new direction of research , viz. mitigating impact of RAM<br\/>scraping attacks in the context of DBMS workloads, that has not been<br\/>addressed by the database research community at large. The proposed<br\/>approach of redesigning a query optimizer to explore tradeoffs between<br\/>performance and disclosure risks represents a significant innovation in<br\/>query processing and memory management in the database management area.<br\/>Furthermore, exploration of RAM scraping attacks in MapReduce style<br\/>cloud-based data processing frameworks represents new innovation in cloud<br\/>security.<br\/><br\/>Broader Impact: The results of this EAGER grant will help launch a new line<br\/>of research within database security community ? viz., risk aware database<br\/>management that balances data exposure risks from RAM scraping attacks with<br\/>performance degradation in a variety of adversarial settings. The proposed<br\/>research could also make a significant impact on the DBMS vendors and data<br\/>service providers in the emerging cloud computing framework.<br\/><br\/>For further information see the project web site at the URL:<br\/>http:\/\/www.ics.uci.edu\/~projects\/privacygroup\/projects.html","title":"EAGER-TC: Limiting Effect of RAM-Scraping Attacks in DBMSs","awardID":"1045296","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["515756"],"PO":["469867"]},"172837":{"abstract":"The HealthSec workshop is a forum for discussing innovative and potentially disruptive ideas on all aspects of medical and health security and privacy. A fundamental goal of the workshop is to bootstrap future innovation and collaborative, cross-disciplinary research between diverse fields, including, but not limited to, technology, medicine, and policy. This award will assist approximately 10 US-based graduate students to attend the First Workshop on Health Security and Privacy (HealthSec). Participation in HealthSec and similar conferences is a valuable and important part of the graduate school experience. It provides students with the opportunity to interact with senior researchers in the field, and be exposed to leading edge work in the field.","title":"Student Travel Support for the First Workshop on Health Security and Privacy (HealthSec '10)","awardID":"1041547","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["559051"],"PO":["497499"]},"173948":{"abstract":"Abstract<br\/><br\/>This project takes a unified spectral transformation approach to address challenges of analyzing network topology and identifying fraud patterns in large-scale dynamic networks by using data spectral transformation with network topology visualization. Large-scale social and communication networks contain rich topological information embedded inside, in addition to various structured, semi-structured, and unstructured data. There has been little to date work dedicated to exploring network topology, especially from the spectral analysis point of view. If the proposed methods, which are based on the simple adjacency matrix representation of a graph and the node representation based on k communities in the graph, can be demonstrated to work for very large data sets, it will be a significant advance. The research is being integrated with information visualization and visual analytics algorithms and has a testbed of banking data available to allow for a search for fraud. <br\/><br\/>The research is characterizing patterns of various attacks in the spectral projection space and developing spectrum based methods to identify these attacks. The approach, which exploits the spectral space of the underlying interaction structure of the network, is orthogonal to traditional approaches using content profiling. The ability to perform this spectral analysis is dependent upon the development of complex mathematical techniques. Critical issues that are being explored include the scalability of the methods to very large data sets and the determination of the dimensionality of the node representation in spectral space (which depends upon the number of clusters in the graph). Another issue is that each component of the k-dimensional representation of each node is interpreted as the 'likelihood' of a node's attachment to the k communities. However, it must be guaranteed that the components of the k-dimensional vector that represent each node will be all nonnegative or else an interpretation of the negative number as 'likelihood' must be developed that is mathematically consistent. These and other related mathematical issues are being explored.","title":"EAGER: FODAVA: Spectral Analysis for Fraud Detection in Large-scale Networks","awardID":"1047621","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7703","name":"FOUNDATIONS VISUAL ANALYTICS"}}],"PIcoPI":["492490",466605],"PO":["532791"]},"161738":{"abstract":"The objective of this project is to design algorithms that allow teams of simple mobile robots to complete a wide range of tasks reliably. The key insight is that effective planning is possible for such teams, even in spite of significant uncertainty stemming from both sensing and motion. This approach builds upon existing work on minimalism for single robots, but must also overcome substantial complications that arise from coordination and communication between the robots. A distinguishing feature of this work is that the problems are complex and nontrivial at multiple scales: Planning for the multi-robot teams cannot be fully decoupled from the planning and control issues for individual robots.<br\/><br\/>The project combines three related research endeavors. First, it investigates techniques for representing each robot's uncertainty about its own state, and about the state and knowledge of the other robots. Second, it develops energy-efficient and robust strategies for communication between robots. Third, it applies these techniques in specialized planning algorithms to allow robot teams to complete their tasks in a decentralized manner.<br\/><br\/>This research will result in a collection of new algorithms that will allow teams of simple robots to manage uncertainty, communication, and planning in order to complete broad classes of tasks. These algorithms will enable simpler, more autonomous teams of robots to be deployed with less expense. Such robots will have significant positive impact on many sectors of our society, including transportation, space exploration, and agriculture. Broader impact will include development and distribution of \"covertly educational\" game software for middle school students, and training of undergraduate and graduate student researchers.","title":"CAREER: Algorithms for Minimalist Robot Teams","awardID":"0953503","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[432491],"PO":["562760"]},"174608":{"abstract":"In response to the Gulf of Mexico oil spill innovative UWB radar sensor network imaging methodologies to produce 2-D image based on 1-D signals are under development. Using these techniques beach soil reflectivity can be measured. The differences between the reflectivity of oil spilled beach soil and that of the normal beach soil shows the dielectric property changes due to oil spill. The oil spill has changed the beach soil dispersion, and oil contamination drastically reduces the bearing capacity of sand, which impacts building foundations and may cause building safety issue. In this project, models of the relations between soil reflectivity, oil content, and compression index are under development. The research crosses multiple disciplines, including sensor networks, radars, geoscience and remote sensing, civil engineering and geotechnology, signal and image processing, wireless communications, and pattern recognition; the research team includes four investigators drawn from three departments (EE, Civil Engineering, and CSE). This project promises to generate new results for radar remote sensing and radar sensor network approaches to assessing oil spill impact on beach soil.","title":"RAPID: Collaborative Research: Gulf of Mexico Oil Spill Impact on Beach Soil: Radar and Radar Sensor Network-Based Approaches","awardID":"1050326","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["523780"],"PO":["565303"]},"173608":{"abstract":"This project funds US-based graduate students to attend the 2010 USENIX Annual Technical Conference (ATC), the premier forum for advanced professionals from academic and industrial backgrounds. ATC brings together researchers from across the networking and systems community (computer networking, security, cloud computing, and operating systems) to foster cross-disciplinary approaches and to address shared research challenges. This cross-disciplinary emphasis makes ATC well-suited for broadening student awareness and participation in the field. Participation in USENIX ATC?10 and similar conferences is a valuable and important part of the graduate school experience. It provides students with the opportunity to interact with more senior researchers, and exposes students to leading work in the field. <br\/><br\/>This project enables about 11 US-based graduate students who may otherwise be unable to attend to participate in ATC'10. Held June 22-25 in Boston, MA., ATC'10 consists of 4 days of mini-conferences, workshops and training, and a 3-day technical program. Over 500 researchers and practitioners attend annually.<br\/><br\/>Students receiving assistance serve as scribes who write summaries of conference sessions for a future issue of the ;login: magazine, participate in the Student Birds-of-a-Feather session, and are strongly encouraged to present posters and works-in-progress. Students are recruited through an open, well advertised process, and selected by a committee appointed by the ATC program chair and USENIX board.","title":"Student Travel Support for the 2010 USENIX Annual Technical Conference","awardID":"1045308","effectiveDate":"2010-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[465808],"PO":["535244"]},"162708":{"abstract":"Go to any meeting or lecture with the younger generation of researchers, business people, or government, and there is a laptop or smartphone at every seat. Each laptop and smartphone is capable not only of recording and transmitting video and audio in real time, but also of advanced analytics on the data (e.g. speech recognition, speaker identification, face detection, etc.). Yet this rich resource goes largely unexploited mostly due to lack of good training data for machine learning algorithms.<br\/><br\/>The first step in exploiting this resource is to collect a corpus of audio, video, and annotations of \"natural\" meetings using the participants' own laptops and cellphones, allowing both analysis of the meetings and training of machine learning algorithms. This one year planning project involves design of the corpus, including collection protocols, signals, formats, and annotations, collection of a small pilot corpus, and ongoing interaction with the community through mailing lists, forums, wikis, and a workshop hosted at ICSI in Berkeley, California. The annotations include the words spoken, events such as laughter, a telephone ringing, or a new participant entering the room, who is speaking, who appears on which camera, head and hand gestures, the participants' focus of attention, and summaries and topics.<br\/><br\/>Successful planning for the collection of a significant number natural meetings from a variety of settings using the participants' own laptops and cellphones allows for a new generation of analytic tools for meetings including browsing, search, collaboration tools, and teleremote aids.","title":"CI-P: The \"Poor Quality\" Meetings Corpus","awardID":"0958578","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[434804],"PO":["565215"]},"168490":{"abstract":"For decades, computer system design and operation were driven largely by high performance objectives. Yet, as the large scale integration of semi-conductor devices is approaching its physical limits, energy efficiency and robustness have been recently promoted to first-class design constraints. Energy efficiency is mandated by the emergence of small foot-print, portable, and battery-powered computers as well as ever-increasing power density that puts stringent constraints even on computers connected to the power grid. Moreover, recent research has revealed that aggressive power management techniques can significantly increase vulnerabilities of computer systems to transient faults (soft errors) that can cause incorrect operations at run-time. These problems are even more pronounced for real-time embedded systems that must perform correctly at high reliability levels, under strict timing and energy constraints.<br\/><br\/>In recent past, a number of pioneering reliability-aware power management schemes were proposed that aim at mitigating the negative effects of the popular dynamic voltage and frequency scaling. This project is addressing the conservatism of the existing solutions and developing a more general framework. Specifically, the project is devising novel solutions to achieve arbitrary reliability levels through the use of shared recovery tasks. In addition, the research is extending the framework to multiprocessor and emerging multicore platforms. The project has two major broader impact dimensions: First, energy-awareness has a direct impact on environment, economy, and society at large. Second, by promoting reliability to a first-order objective, the project will help to prevent malfunctions in safety-critical computer systems and protect property and human lives.","title":"CSR: Small: Collaborative Research: Generalized Reliability-Aware Power Management for Real-Time Embedded Systems","awardID":"1016974","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[451043],"PO":["565255"]},"168380":{"abstract":"In theoretical computer science, algorithms are usually evaluated with respect to their worst-case performance, whereas in other areas, average-case analysis is often used. Both of these approaches have drawbacks: worst-case analysis is overly pessimistic and average-case analysis often rests on unrealistic assumptions. To address these issues, a number of other analysis frameworks have been proposed including self-improving algorithms, smoothed analysis, instance-optimality, and algorithmic design based on a variety of data models. The objective of the project is to continue this line of research and develop techniques that go beyond worst-case analysis in the areas of approximation algorithms, algorithmic mechanism design and online algorithms. In the area of approximation algorithms for NP-hard problems, the project focuses on the development of approximation algorithms that achieve a kind of instance optimality. In the area of algorithmic mechanism design, the PI will continue to study the design and analysis of profit maximizing auctions in single-parameter environments and beyond. In the area of online algorithms, the PI will work to develop effective online algorithms for a fundamental and practical self-organizing data structure problem.<br\/><br\/>Through the development of more effective and practical algorithms and a deeper understanding of the performance of these algorithms in practice, this research has the potential to impact a variety of subfields of computer science including artificial intelligence, systems and networking, data mining, and electronic commerce.","title":"AF: Small: Beyond Worst-Case Analysis in Approximation Algorithms, Algorithmic Mechanism Design and Online Algorithms","awardID":"1016509","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7932","name":"COMPUT GAME THEORY & ECON"}}],"PIcoPI":[450784],"PO":["565251"]},"168370":{"abstract":"A fundamental challenge for Artificial Intelligence is sequential decision making under uncertainty, a task where automated algorithms lag far behind human-level intelligence. The primary reason for the disparity is curse of dimensionality - the number of states is exponential in the problem features. Recent advances that restrict decision-theoretic computation to a reachable subset of state space have scaled to moderately-sized problems, but proven ineffective in scaling to real problems. On the other hand, probabilistic planners based on deterministic planning might scale up, but with a massive loss in solution quality.<br\/><br\/>This project is investigating several methods to scale probabilistic planning to real-sized problems. We combine decision-theoretic analysis, basis function approximation and the classical AI planning techniques, to develop a series of highly scalable planners. A common theme in our techniques is the use of deterministic plans to automatically obtain domain abstractions in the form of 'good' or 'bad' properties, or intermediate subgoals. The project introduces and exploits a principled collaboration between decision theory and classical planning techniques, thus retaining the benefits of both - high quality as well as high performance. Experiments show that our new planner solves difficult planning competition problems using orders of magnitude less memory outputting high quality policies.<br\/><br\/>Our research also proposes effective solutions to long-standing problems of generating a set of basis functions and computing a hierarchical problem decomposition. Both basis function approximation and hierarchical decomposition are popular in existing literature for speeding up planning, but they are not fully automated - a human is required to specify the basis functions and the hierarchy. We provide novel, domain-independent solutions that remove this additional human effort. <br\/><br\/>Our research addresses several long standing challenges in AI, like scaling stochastic planning, and automatically generating basis functions and subgoal hierarchies. We expect to produce state-of-the-art planners that will be effective in large and complex real world scenarios, e.g., planetary exploration, military operations planning, and robotic decision making.","title":"RI: Small: Integrating Paradigms for Approximate Stochastic Planning","awardID":"1016465","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["450900","450899"],"PO":["562760"]},"168690":{"abstract":"This project uses humanoid robots to study how the human perceptual process drives the creation of spatial representations suitable for memory and for spatial reasoning. The primary goal is to take extended visual streams of information and to parse them into individual events. Each of these events in turn can be represented by a generic spatial data structure capable of supporting both memory for the event and reasoning about it. Because the representations are generic, future events should also be able to evoke them allowing them to be used predictively. To do this the research team models certain aspects of the human visual system, namely that it is split into two parts. The spatial part of the visual system is called the ?dorsal? system. The dorsal system is highly sensitive to change in the visual field that therefore serves as the critical component in our parsing algorithm. Moments of great visual change trigger the creation of a generic visual representation. When these individual representations are linked they form a kind of episodic memory of an event. <br\/><br\/>A significant aspect of the project is that it involves training undergraduates to do research. The application is RoboCup, a global competition featuring autonomous soccer-playing robots. The Bowdoin College RoboCup team was formed to get undergraduates involved in research early in their academic careers. The students on the team not only attend competitions worldwide, but also get to share their work with their peers internationally. The team?s codebase is publicly available and is monitored and used by numerous universities worldwide.","title":"RI: Small: RUI: Spatial Prototypes","awardID":"1017983","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[451518],"PO":["564316"]},"168470":{"abstract":"The project has three main themes: <br\/><br\/>1. Markov chain Monte Carlo algorithms: The study of techniques such as lifting to speed up algorithms based on Markov chains, as well as the analysis of Markov chains---for lattice triangulations and the random cluster model---that are beyond the range of current techniques.<br\/><br\/>2. Statistical physics and computation: A continuing study of the Glauber dynamics for spin systems, focusing especially on the key open question of the influence of boundary conditions on the mixing time; the harnessing of an emerging understanding of spatial mixing to derive new algorithms and complexity results for counting and sampling problems; the application of new techniques for the analysis of the Boltzmann equation in physics to a computational study of nonlinear models in population genetics and genetic algorithms.<br\/><br\/>3. Mobile geometric graphs: A mathematically rigorous investigation of the effects -- in terms of both increased power and novel algorithmic challenges -- of introducing mobile nodes into models of wireless networks.<br\/><br\/>Numerous connections among the themes provide intellectual coherence. For example, Markov chains play a central role in statistical physics through the Glauber dynamics; phase transitions and threshold phenomena appear in all three themes, as does the pervasive notion of dynamical evolution of a system over time; and the understanding of mobile geometric graphs is intimately connected with continuum percolation in physics.<br\/><br\/>In addition, all three themes are examples of outreach from theoretical computer science to other disciplines, notably probability theory, statistical physics and wireless networking, and the project is expected to contribute to cross-fertilization between computer science and these fields. Throughout the project, the choice of research questions is driven not only by their intrinsic significance but also by the challenges that they present to existing techniques and the extent to which they illuminate connections with these other fields.","title":"AF: Small: Markov Chains, Statistical Physics, and Mobile Geometric Graphs","awardID":"1016896","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":[450995],"PO":["565157"]},"168591":{"abstract":"This project develops a transformative mechanism for cooperative object caching in Social Wireless Networks (or SWNETs). The key idea is to reduce wireless bandwidth strain in carriers' 3G\/4G networks by intelligently storing user objects within SWNETs formed across mobile devices through short range WiFi style wireless links. The applicable user objects would include phone Apps, mp3 music, video clips, news items, book chapters, and research papers. The key research objectives of the project is to develop a generalized caching framework for managing various forms of costs and rewards in today's Mobile Content Ecosystems (MCEs) comprising of the network operators, content providers, and the end-users. The research involves: 1) general formulation of cost-reward flow among the MCE stakeholders, 2) developing optimal distributed cooperative caching algorithms, 3) characterizing the impacts of network, user and object dynamics, 4) investigating the impacts, and developing mechanism for controlling non-cooperation in the ecosystems, and finally 5) developing a prototype Social Wireless Network for tracking human mobility in the context of cooperative network caching. Impacts of this research include a potential transformation of the area of object life-cycle management in Social Wireless Networks with an end-goal of network capacity and cost-revenue management in today's Mobile Content Ecosystems. This research can be considered as a key enabler for the emerging social network applications and how they can be symbiotically supported by the cellular and short range wireless network infrastructure.","title":"NeTS: Small: Cooperative Networked Caching for Cost and Reward Management in Mobile Content Ecosystems","awardID":"1017477","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[451278],"PO":["565303"]},"168481":{"abstract":"This project forms an interdisciplinary research team with integrated <br\/>expertise of computer scientists and biomedical scientists to tackle<br\/>the challenging issues in analyzing protein interaction data. Specifically,<br\/>the project develops a novel approach to detecting overlapping clusters on<br\/>emerging large volume of protein-protein interaction data and validates<br\/>the computational approaches in yeast. The vast amount of protein-protein<br\/>interaction data provides us with a good opportunity to systematically<br\/>analyze the structure of a large living system and also allows us to<br\/>understand essential principles like essentiality, genetic interactions,<br\/>functions, functional modules, protein complexes, and cellular pathways.<br\/><br\/>The identification of functional modules in protein interaction networks is<br\/>of great interest because they often reveal unknown functional ties between<br\/>proteins and hence predict functions for unknown proteins. A protein may<br\/>be included in one or more functional groups. Therefore, overlapping clusters<br\/>need to be identified in protein interaction data. <br\/><br\/>This project develops a <br\/>unique method to integrate domain knowledge with the protein interaction<br\/>data so that the data will be more reliable. It also develops<br\/>a unique method to support overlapping modularity analysis for<br\/>protein interaction data that intelligently integrates biological<br\/>information into the modularity analysis process. Another unique<br\/>aspect of this project is the tight integration of computational methods <br\/>with biological verification. By associating unknown proteins with the <br\/>known proteins within each functional module, we can suggest that <br\/>those proteins positively work for the corresponding functions that <br\/>are assigned to the modules. <br\/><br\/>This project can also find broad applications in other areas which <br\/>handle data with the modular network property, such as web network, <br\/>social networks, and technological networks.<br\/><br\/>For further information see the project web page:<br\/>http:\/\/www.cse.buffalo.edu\/DBGROUP\/PPI-networks\/index.html","title":"III:Small: Overlapping Clustering Analysis of Biological Networks","awardID":"1016929","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[451021,"518277"],"PO":["565136"]},"168570":{"abstract":"This project revolves around tumbling which is an exciting area of robotic locomotion that takes advantage of ground-body interactions to achieve high mobility on smaller scales when compared to conventional methods. Additionally, the required hardware complexity to produce such locomotion is very low. In this respect, tumbling can be viewed as a minimalistic approach to producing miniature mobile robots capable of traversing complex and dynamic terrains. Due to the nature of tumbling however, the added mobility comes at the price of increased control complexity. The minimalistic nature of tumbling robots generally results in underactuated systems that exhibit nonholonomic constraints which greatly complicate the motion planning problem. Additionally, tumbling often involves time-varying supports and sliding contacts with the ground. Ultimately, this research views tumbling as a largely unexplored yet promising area of research. This work addresses the intricacies of tumbling locomotion. Specifically, we are developing general planning algorithms for tumbling robots and identify important design characteristics of tumbling robots that lead to simplified control. <br\/><br\/>Seminars and workshops to bring together practitioners, end-users, researchers, and policy makers will be organized to have the maximal impact. Web-based dissemination of the algorithms and rapid prototyping\/simulation tools ensure that the results of this project reach all communities. Students trained in this project participate in the US FIRST competitions, summer mentoring programs for high school students, summer schools in robotics, and other outreach programs.","title":"RI: Small: Theory and Experiments with Tumbling Robots","awardID":"1017344","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["557449"],"PO":["564316"]},"168460":{"abstract":"For decades, computer system design and operation were driven largely by high performance objectives. Yet, as the large scale integration of semi-conductor devices is approaching its physical limits, energy efficiency and robustness have been recently promoted to first-class design constraints. Energy efficiency is mandated by the emergence of small foot-print, portable, and battery-powered computers as well as ever-increasing power density that puts stringent constraints even on computers connected to the power grid. Moreover, recent research has revealed that aggressive power management techniques can significantly increase vulnerabilities of computer systems to transient faults (soft errors) that can cause incorrect operations at run-time. These problems are even more pronounced for real-time embedded systems that must perform correctly at high reliability levels, under strict timing and energy constraints.<br\/><br\/>In recent past, a number of pioneering reliability-aware power management schemes were proposed that aim at mitigating the negative effects of the popular dynamic voltage and frequency scaling. This project is addressing the conservatism of the existing solutions and developing a more general framework. Specifically, the project is devising novel solutions to achieve arbitrary reliability levels through the use of shared recovery tasks. In addition, the research is extending the framework to multiprocessor and emerging multicore platforms. The project has two major broader impact dimensions: First, energy-awareness has a direct impact on environment, economy, and society at large. Second, by promoting reliability to a first-order objective, the project will help to prevent malfunctions in safety-critical computer systems and protect property and human lives.","title":"CSR: Small: Collaborative Research: Generalized Reliability-Aware Power Management for Real-Time Embedded Systems","awardID":"1016855","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["485622"],"PO":["565255"]},"168581":{"abstract":"People construct routines as they repeatedly perform the same sequence of actions. Routines provide a huge benefit by freeing people?s attention, allowing them to carry out their daily tasks without constantly thinking about every little thing they must do. Problems begin to arise when people must deviate from their routines. Families rely heavily on their routines to address the complex logistics and conflicting agendas of work, school, family, and enrichment activities. However, families often deviate from their routines, and when breakdowns in the plans occur, they feel their lives are out of control.<br\/><br\/>This research will develop a system that learns the routine movements of family members, and a planning system that leverages this model in order to generate a speculative plan for future days. The system will also predict conflicts with scheduled deviations and detect when plans begin to breakdown, such as when someone forgets to deviate from a routine. A calendar interface that displays the routine movements of family members along with their scheduled deviations and a small set of reminder applications that help people enact their plans and that support them when plans breakdown will form the basis for evaluating the underlying systems. This research is transformative in the novel integration of machine learning and planning techniques, and its application to a real-world and complex problem. Finally, this research provides insights on how intelligent, ubiquitous computing technology influences families? feelings of control and their quality of life. <br\/><br\/>The proposed work has the potential to significantly improve the quality of life for millions of families by reducing stress caused from breakdowns in plans and routines. Lowering stress can improve the quality of marriages, the quality of parenting, and the physical and mental health of children. We will involve undergraduate and graduate students in our research and will incorporate our findings into our courses on ubiquitous computing, interaction design, and on smart homes. We expect that our focus on a social problem will attract non-science-focused students to science and expose science-focused students to design methods of inquiry.","title":"HCC: Small: Learning Routines to Support People's Activities","awardID":"1017429","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["532662",451255],"PO":["564456"]},"168482":{"abstract":"In every business, engineering endeavour and scientific discipline, workers are digitizing their knowledge with the hope of using computational methods to categorize, query, filter, search, diagnose, and visualize their data. While this effort is leading to remarkable industrial and scientific advances, it is also generating enormous amounts of ad hoc data (i.e., that data for which standard data processing tools such as query engines, statistical packages, graphing tools, or other software is not readily available). Ad hoc data poses tremendous challenges to its users because it is often highly varied, poorly documented, filled with errors, and continuously evolving --- yet ad hoc data also contains much valuable information. The goal of this research is to develop general-purpose software tools and techniques capable of managing ad hoc data efficiently. This research has the potential for a broad impact on society by dramatically improving the productivity of industrial data analysts, computer systems administrators and academics who must deal with ad hoc data on a day-to-day basis.<br\/><br\/>The central technical challenge of the research involves designing, implementing and evaluating a new domain-specific programming language that facilitates the management of ad hoc data sets. This new programming language will allow data analysts to specify the structure of ad hoc data files, how those files are arranged in a file system and what meta-data is associated with them. Once a specification is complete, it will be possible to use it as documentation for the data set or for generating data-processing tools. The research will also involve developing new methods for enabling users to generate specifications quickly and accurately, without actually having to write down all of the details by hand. Finally, the research will develop new algorithms for implementing the generated data-processing tools efficiently.","title":"SHF:Small:Language Support for Ad Hoc Data Processing","awardID":"1016937","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["483683"],"PO":["564588"]},"168493":{"abstract":"The widespread prevalence of embedded devices, and networked collections of them, makes ensuring their reliability a social imperative. However, formal verification of such embedded systems is challenging as they are concurrent ``hybrid'' systems, i.e., coupled digital programs and physical entities that interact with an analog environment while meeting real-time constraints. Lured by the importance and scientific depth of the problem, the analysis of such hybrid systems for correctness has received widespread attention in the last couple of decades. Considerable progress has been made in defining languages and models for designing such systems, understanding the theoretical bounds to automated verification of such systems, and developing tools to simulate and verify formal hybrid models. However, key challenges remain. Current state of the art in formal methods allows for the automated analysis of single, closed hybrid systems with simple continuous dynamics. This is in contrast to the fact that, in practice, hybrid systems tend to have complex continuous dynamics, and usually consist of multiple modules interacting concurrently.<br\/><br\/>This work addresses these challenges so as to enable the automated analysis of systems that are open, concurrent, and hybrid. Specifically, the following research tasks will be carried out: (a) Develop techniques to tightly approximate hybrid systems with complex continuous dynamics by hybrid systems with simple dynamics; (b) Develop decision procedures for the composition of hybrid systems; (c) Develop assume-guarantee reasoning for hybrid systems in the presence of approximate abstractions; (d) Apply the proposed methods to analyze software and algorithms deployed in HoTDeC, an environment for distributed control of hovercrafts.","title":"SHF: Small: Verifying Open Concurrent Real Time Systems","awardID":"1016989","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["553664","553663"],"PO":["565264"]},"168394":{"abstract":"This research involves the study of some specific cases of the Correlation Problem, the instances of which are carefully chosen so that advances in any of these cases would constitute dramatic advances, and improve our understanding of the whole Correlation Problem. These cases are also chosen with an eye to practical applications. We would employ algebraic methods to generate block-Hankel weighing matrices, multi dimensional Hadamard matrices, almost difference sets and perfect sequences. Our motivation stems from their usefulness in several areas of communication engineering: quantum computing, MC-CDMA systems , quasi-synchronous CDMA , multiple antenna wireless communication systems , FHSS which are widely used in military radios, CDMA and GSM networks, radars and sonars, and Bluetooth communications - to name a few.<br\/>Discrete mathematical structures that can be developed using modern algebra, number theory and finite geometry and other combinatorial structures are useful in constructing sequences and arrays with desirable correlation properties. They are systematically studied via their algebraic counterparts. The results obtained will lead to new mathematical theories that are of interest to combinatorial design theorists and communication engineers. We thus investigate sequence design problems, which have a variety of applications in communication engineering. Our methods will be very algebraic and would employ tools from algebra, finite fields, and algebraic number theory.","title":"CIF: Small: Algebraic Methods in the Study of Some Problems in Communication Engineering","awardID":"1016576","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7938","name":"SENSOR NETWORKS"}}],"PIcoPI":[450820],"PO":["564898"]},"168295":{"abstract":"Modern software engineering is a highly social activity, which is in contrast aimed at producing technical artifacts. As a result, complex dependencies between the technical elements of software systems and the social structures of the developers that are tasked with their creation have a direct effect on software quality: in projects that exhibit a high degree of congruence between the design of a software system and the social communication structures of its developers, teams are more productive and systems contain fewer faults. This research project aims to create the necessary tool support so that software engineers can be made keenly aware of these socio-technical dependencies within the familiar context of their everyday development activities. By providing this information in this specific context ? at a time when it is useful and actionable ? the broader impacts of this research have the potential to dramatically transform software engineering habits and practices by bringing into sharper focus the existing and emerging socio-technical trends of a development effort, allowing developers to intervene when they diverge, and ultimately improving the quality of the software systems produced.<br\/><br\/>To achieve these objectives, this research project is grounded on the creation of an architecture-centric toolset that supports the analysis of social network patterns, designed to be an addition to the popular Eclipse development environment. This toolset will support continuous awareness of emerging socio-technical dependencies by collecting data and providing a host of displays that allow developers to visualize their project?s software architecture, dependencies between source code units, and the social network formed through analysis of developer communications. Key contributions and advances of this work include the novel integration of social aspects of development with the architecture of software systems, the provision of concrete socio-architectural congruence metrics, and the presentation of this information to developers during ongoing development efforts.","title":"SHF: Small: Collaborative Research: Supporting Continuous Awareness and Exploration of Social and Design Dependencies","awardID":"1016134","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["548131"],"PO":["564388"]},"168670":{"abstract":"This project explores the interplay between the topology\/geometry of networks and their traffic load pattern with the ultimate objective of deriving new load-balancing algorithms based on curvature control. The phenomenon that is observed in reality is the strong concentration of the traffic on some small subsets of links\/nodes. This can be seen in the Internet (?backbone?), in the power grid (?line overload?), in vehicular traffic, in metabolic exchange in living organisms, etc. This phenomenon?the emergence of the centroid?cannot be completely accounted for by the local heavy-tailed paradigm, but strong evidence is provided here that this is a general feature dictated by the large-scale hyperbolic structure of the underlying network. Here, hyperbolic is a metaphor to refer to the fact that such networks as the Internet Service Provider (ISP) behave like negatively curved Riemannian manifolds, of which the saddle is the most intuitive visualization. Behavior refers to the geodesic flow, which carries the traffic and controls its stability or instability (e.g., fluttering) under such perturbation as outage or power depletion. <br\/><br\/>The first part of the proposed research will be devoted to refining criteria for real networks to be identifiable with negatively curved Riemannian manifolds. After developing intuitive criteria based on angle deficit\/excess and clustering coefficient, the Gromov Thin Triangle Condition (TTC) and Four-Point Condition (FPC) will be scaled by the size of the graph to become relevant to real networks, which, no matter how awesome their sizes, are nevertheless finite. This leads to the new concept of scale-specific Gromov hyperbolic graphs, of which the Rocketfuel data base already provides an example. Such real-life networks as those provided by Bell Labs, sensor networks, air traffic control, even metabolic and nervous system networks will be used as testbeds. Next, the first step towards congestion analysis is the development of a network-specific concept of centroid or center of mass, already known in the mathematical community in its Riemannian manifold version. While in simulation the centroid has appeared to coincide with the point of maximum traffic, an important research milestone will be the theoretical justification of this fact. At the other end of the curvature spectrum, there is strong evidence that traffic on uniformly positively curved networks is balanced, provided the Dijkstra routing algorithm incorporates a randomization of the equal cost paths. <br\/><br\/>The preceding leads to the culmination of the research: by reassigning link weights so that the resulting network is positively curved, the routing based on the modified network would balance the load. Provided that the Euler characteristic of the network reveals no obstructions, the reassignment is carried over by the so-called Yamabe flow algorithm, which has a decentralized structure and hence would mesh with such network algorithms as flooding. Finally, the algorithm will be given an adaptive control structure, meaning that once it reaches positive curvature, it will continuously update the link weights as necessitated by network outages, flash points, etc.<br\/><br\/>The intellectual merit of the proposed research is that it will take coarse geometry?which has over the past few years silently pervaded such diverse fields as wired and wireless networks, autonomous agents, cooperative control, even biochemistry?along its scale-specific reformulation relevant to complex real-life networks, which do not quite fit the mathematical idealization of Gromov hyperbolic graphs. Certainly, the most transformative part of the research is the load-balancing based on routing on a modified positively curved network. The latter will bring to the real world such curvature smoothing algorithms as the Yamabe flow, which was instrumental in the proof of one of the most celebrated mathematical puzzles of all times?the Poincar\u00b4e conjecture.<br\/><br\/>The broader impact of the proposed activity is that it will foster a well-focused, application-driven multidisciplinary collaboration between the Department of Electrical Engineering, the Computer Engineering group, and the most theoretical geometry\/topology group of the Department of Mathematics. Joint seminars, group meetings, new course development, etc. will create a new breed of engineering students, knowledgeable in coarse geometry, which has so far not been part of the traditional engineering curriculum. Extensive collaboration with Bell Labs will be maintained throughout the project","title":"NetSE: Small: Load Balancing by Network Curvature Control","awardID":"1017881","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":["480349",451466,"531832"],"PO":["565090"]},"167471":{"abstract":"Computer software has become highly complex, with many applications now in the millions of lines of code, and program features often spread across many files and directories. As a result of this, software engineers today spend a significant amount of time struggling with navigating through all of this code as they add new features to complex applications . This research project will develop a new user interface for programmers called Code Bubbles that visualizes the many pieces of code they need to work with and reference on the screen simultaneously, along with the interrelationships between these code fragments. Providing such a \"working set\" of all the relevant code fragments on one screen marks a radical departure from the way software development environments work today, where programmers can only see or work with a few locations in the code at a time and have to rely either on memory or on continuous navigation between files to examine their working set. Preliminary user studies with software developers using an early version of Code Bubbles indicate that the approach has the potential to revolutionize the way programmers think about software development and lead to more efficient and more robust applications, and to lower development costs. <br\/><br\/>Code Bubbles is based on a user interface design that facilitates the simultaneous viewing and manipulation of the fine-grained fragments of information needed to perform tasks. Unlike conventional Interactive Development Environments, which are grounded upon the notion of viewing code at the granularity of a file, Code Bubbles displays fragments of code, documentation, and other artifacts as small bubbles which avoid occlusion by pushing each other out of the way across a large virtual display. Text is automatically reflowed within bubbles for maximum efficiency in using screen space, and \"chrome\", i.e., controls for manipulating a bubble, is kept to a minimum, again to preserve space for content. This project will apply the Code Bubbles concept to a wide range of programming artifacts including scripting, debugging, code review, and collaboration. An iterative design process will be used that involves professional developers as well as students giving feedback on visualization and user interface designs throughout the design process. The experiments to be conducted as part of the evaluation plan will include longitudinal studies in which developers \"take home\" the Code Bubbles system and use it on their projects and then give feedback and participate in regular interviews. A version of Code Bubbles will be implemented for educational purposes (high school through college) and will be integrated into Computer Science courses at Brown University and the University of Central Florida. Finally, Code Bubbles will be distributed freely, as open source, so that software developers can begin using it and also contribute their own features.","title":"SHF: Large: A Working Set Approach to Integrated Development Environments","awardID":"1012056","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[448702,448703],"PO":["564388"]},"165293":{"abstract":"Incubation, taking time out of a difficult task to do something else, is one of the recognized activities that contributes to creativity. Its effect on software designers and in particular on their design cognition, ie their thinking processes, is unknown. This project aims to determine the effects of incubation on software designers? cognition and creativity.<br\/><br\/>The results from this project will provide the empirical grounding necessary to determine the cognitive and design effects of incubation on the creativity of software designers and will form the basis of potential tools to aid creative design and the basis to compare and develop approaches to teaching creative design at both college and high school levels. The results can be used as the foundation of brain studies of software designers.","title":"Pilot: The Design Cognition of Incubation-Induced Creativity of Software Designers","awardID":"1002079","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":["564585"],"PO":["565227"]},"168340":{"abstract":"Rapid advancement of the wireless technologies provide new opportunities for mobile users to have easy access to real-time data, derive useful social information, and stay connected with business partners, colleagues and friends. Towards this end, mobile social networking applications have recently emerged to meet these needs. Current mobile social networking applications do not support advanced context-based services. Additionally, serious security and privacy concerns have been raised when accessing social networking applications either from fixed locations or on-the-go. This project aims to build a secure mobile information sharing system (SEMOIS) that supports secure and privacy-preserving real-time information sharing. SEMOIS has the ability to store secure data items with flexible access control at insecure storage nodes and enables users to send context-based messages with late-binding features. SEMOIS achieves data confidentiality and privacy-preserving through data encryption and encrypted search, and enables intentional name based message dissemination without apriori knowledge of recipients. Additionally, a set of smart learning methods are developed to extract short-term and long-term geo-social patterns from multimodal sensing data collected by mobile devices for social networking purposes, e.g., geo-social patterns are used to derive hidden communities. <br\/><br\/>Project results are expected to advance the state of the art techniques for supporting secure and privacy-preserving mobile social networks with a variety of innovative features. The project equips both graduate and undergraduate students with the necessary background and practical skills for survival in the emerging job market and further contributes to the development of the pervasive computing field. In addition, SEMOIS can be used by middle and high school students from the Tri-State area that participate in the CHOICES and NSF-funded STEM program organized by Lehigh University.","title":"CSR: Small: Collaborative Research: SEMOIS: Secure Mobile Information Sharing System","awardID":"1016303","effectiveDate":"2010-08-01","expirationDate":"2013-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550063"],"PO":["563661"]},"168351":{"abstract":"Many application domains, such as intelligence, counter-terrorism,<br\/>forensics, disease control, often need to cross-match multiple very<br\/>large datasets, such as watch lists. Because those datasets may <br\/>contain privacy-sensitive or confidential information, the use of efficient<br\/>privacy-preserving protocols for cross-matching different datasets is<br\/>crucial. The problem of privacy-preserving record matching has been<br\/>addressed by the use of Secure Multi-party Computation (SMC) protocols.<br\/>Under these protocols, the data are converted to series of functions<br\/>with private inputs. However a major drawback of SMC-based protocols<br\/>is that they involve extensive cryptographic primitives such as<br\/>homomorphic encryption which do not scale to the size of practical<br\/>problems. As a result, SMC-based protocols cannot be used for resource<br\/>constrained data-intensive privacy-preserving record matching approaches <br\/>directly. This project develops a novel approach based on the observation<br\/>that to apply SMC to practical applications, one needs to bridge the gap <br\/>between the size of the datasets that can efficiently be matched using <br\/>SMC protocols and the size of the datasets seen in practice. The approach <br\/>taken by the project tackles the problem from a novel angle by developing <br\/>techniques to reduce the size of practical problems by employing <br\/>privacy-preserving data sanitization methods. The project thus solves <br\/>the privacy-preserving data matching problems through the following<br\/>steps. First, to protect the privacy of data subjects, useful statistics <br\/>about data is gathered using differential privacy. Second, differentially <br\/>private statistics are shared among the parties involved in data matching. <br\/>These parties then identify potential matching pairs where fruitful matching <br\/>may occur. Such a step is referred to as data blocking. Finally, SMC <br\/>techniques are applied to these candidates to accurately cross-match<br\/>information. In addition to syntactic matching, semantic matching is supported <br\/>by which records are compared according to some semantic similarity functions.<br\/>The semantic matching protocols includes techniques for matching and<br\/>aligning ontologies, as the use of ontologies is crucial for an effective<br\/>semantic matching. This project is the first to use differential privacy for<br\/>efficient privacy-preserving record matching that also leverages semantics-based<br\/>approach and a privacy-preserving approach to ontology alignment. The techniques<br\/>developed in the project are the first to achieve efficient privacy-preserving<br\/>matching of large scale data sets using differential privacy, thus overcoming <br\/>the scalability problems of conventional SMC techniques. The approach developed <br\/>in this project expands the opportunities and contexts for data use by enabling <br\/>the cross-match of multiple data archives, possibly owned by different parties, <br\/>without violating the privacy of the data. Many applications, of interest <br\/>for our society, will benefit by such opportunities.<br\/>For further information see the project web site at the URL:<br\/>http:\/\/www.cs.purdue.edu\/homes\/bertino\/prirelink","title":"TC: Small: Collaborative: Protocols for Privacy-Preserving Scalable Record Matching and Ontology Alignment","awardID":"1016343","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["551857","558641"],"PO":["565327"]},"168494":{"abstract":"Verifying temporal properties of non-linear hybrid systems, such as embedded control and mixed-signal systems, is currently one of the hardest challenges in verification research. In practice, these systems are \"verified\" using simulations. Nevertheless, extensive simulation is well known to be inadequate for guaranteeing safety. Harmful defects often remain undetected. These defects may manifest themselves during deployment as rare events with a tiny, but non-zero chance of occurrence.<br\/><br\/>This project investigates stochastic verification techniques for embedded and mixed-signal systems based on rare event simulations. Rare event simulations have been used successfully in areas such as mathematical finance, reliability theory and queuing theory for analyzing events in stochastic models with vanishing probabilities. This work adapts ideas from rare event simulations and extreme value theory to detect property violations in non-linear hybrid systems. Furthermore, our research revisits fundamental concepts such as the Boolean semantics of temporal logics. It investigates real-valued metric semantics of temporal logics, which generalize the standard true-false interpretation over simulation traces.<br\/><br\/>The research program is expected to yield useful tools for verifying embedded and mixed-signal systems. These tools can be readily integrated inside model-based development environments. As a result, the techniques that are being investigated will be directly applicable to embedded control and mixed-signal systems to improve the reliability of the designs. Additionally, the research outcomes are being included in course curricula centered on the application of semi-formal testing techniques to various software\/hardware engineering applications for undergraduate as well as graduate students.","title":"SHF: Small: Collaborative Research: Statistical Techniques for Verifying Temporal Properties of Embedded and Mixed-Signal Systems","awardID":"1016994","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550702"],"PO":["565255"]},"174072":{"abstract":"This project focuses on developing the infrastructure for a self-sustaining organization that can manage, grow, and evangelize olympiads that involve young students (middle and junior high) in computational thinking. A large component is the creation of pilot olympiads in a select few cities in the United States. Specific goals of this project include: (1) identifying a set of foundational skills that underlie computational thinking that can be taught before college and high school; (2) identifying a style of problems and scenarios that engage a wide variety of students; and (3) implementing a curriculum of training sessions and contest questions that exemplify those foundational skills. <br\/><br\/>There are two broad reasons for creating a Computational Thinking Olympiad. First, to expose the fundamentals of computational thinking to a broad audience of potential researchers and practitioners in the field, thus increasing participation and diversity in computing. Second, to ensure long-lasting impact beyond of this project. <br\/><br\/>The success of the Computational Thinking Olympiad will have a significant impact on our society by introducing middle school students to computational thinking in its breadth and depth: (1) encouraging students to have fun with the computational thinking in an arena that is both cooperative and competitive; (2) encouraging students to pursue education in computing; (3) introducing the unplugged parts of computing to those who have not had access to the plugged-in parts; and (4) showing that computational thinking is not ``just'' programming.","title":"Collaborative Research: EAGER: Computational Thinking Olympiad","awardID":"1048413","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[466933],"PO":["565215"]},"168440":{"abstract":"From the ?smart grid? to healthcare to national security systems,<br\/>wireless devices are playing an increasing role in technological<br\/>solutions. Their security and trustworthiness should be a major<br\/>concern.<br\/><br\/>Fingerprinting is an important technique in the cyber-defender<br\/>arsenal, because it helps expose deceptions essential to modern<br\/>multi-step network attacks. We develop methods and tools for wireless<br\/>physical (PHY) layer testing, thus improving trustworthiness of<br\/>wireless devices and equipping cyber-defenders with the tools they<br\/>need to protect wireless networks.<br\/><br\/>Active fingerprinting methods are the most direct and effective ones,<br\/>because they allow the administrators to initiate fingerprinting when<br\/>necessary, probe for a broader range of expected behaviors (thus<br\/>increasing the attacker?s workload to fake behaviors in order to<br\/>escape detection), and are easily tweaked (further increasing said<br\/>workload).<br\/><br\/>We develop robust fingerprinting techniques for wireless devices<br\/>that are based on active probing -- in the physical layer of 802.11<br\/>and 802.15.4 networks.<br\/><br\/>To empower exploration of the attack surface of actual wireless<br\/>networks and to facilitate active physical-layer testing of wireless<br\/>devices, we also will develop a framework for crafting and injecting<br\/>\"marginally\" malformed physical layer signals that correspond to<br\/>common 802.11 and 802.15.4 frames. In particular, we will facilitate<br\/>fuzz-testing of wireless devices. <br\/><br\/>Both fingerprintable responses and (possibly exploitable)<br\/>vulnerabilities of wireless devices amount to differences in<br\/>implementations of protocol logic. We will develop methods for testing<br\/>this logic in the wireless PHY layer, and look for potentially harmful<br\/>security vulnerabilities in its common implementations.","title":"TC: Small: Active physical layer fingerprinting of 802.11 and 802.15.4 wireless devices","awardID":"1016782","effectiveDate":"2010-08-01","expirationDate":"2013-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[450930],"PO":["565327"]},"168594":{"abstract":"Beyond Tit-for-Tat: New Techniques for Collaboration in Network Security Games<br\/>Motivation and Problem: How can we ensure collaboration on the Internet, where populations are highly fluctuating, selfish, and unpredictable? This project will explore a new algorithmic technique for enabling collaboration in network security games. The new technique, Secure Multiparty Mediation (SMM), improves on past approaches such as tit-for-tat in the following ways: (1) it works even in single round games; (2) it works even when the actions of the players of the game are never revealed; (3) it works even in the presence of churn, i.e. players joining and leaving the game.<br\/>In the SMM approach, advice is generated by using a mediator: an algorithm that generates private advice for each player. After receiving advice, each player decides on an action. The players retain free-will and so will follow the advice only if it is their best interest to do so. Preliminary work shows that this approach significantly improves social welfare in a well-studied network security game, the virus inoculation game. The SMM approach does not require the exchange of money or other resources. Moreover, the mediator can be implemented in a completely distributed fashion using algorithmic techniques from distributed computing.<br\/><br\/>Intellectual Merit: This project will push the frontier of knowledge about collaboration in game theory. The concept of a mediator has been known in the economics community for many years. However, the project will result in mediators that are more sophisticated algorithmically than those proposed in the economics literature: they will be specifically designed to work for large-scale networks, and will use tools from distributed computing, cryptography and randomized algorithms. Moreover, the project will use analytical tools from the algorithmic game theory community to quantify the success of the mediators designed. <br\/><br\/>Broader Impact: Success in this project will lead to development of a new mathematical tool for addressing security problems. The power of collaborative techniques will likely increase as the world's information infrastructure becomes increasingly connected. Thus, such a tool could significantly improve the security of the world's computer networks.<br\/><br\/>Research results from this proposal will be incorporated into lectures and a project in a security class that has just been created in the researcher's department. Game theory is beguiling to students and attracts attention in popular culture (cf. the movie ?A Beautiful Mind\"). Thus, success in incorporating game theory into a security class will likely attract and retain more talented students into security research. Game theory is also an important conduit between academic disciplines. Significant mathematical results in this area are likely to be of interest to other disciplines such as economics and biology.<br\/>New Mexico is an EPSCoR state and the University of New Mexico is one of only two universities in the nation that is both a Minority Serving Institution and a Carnegie Very High Research Activity university. The researcher will make every effort to include women and minority students in this project.","title":"NetSE: Small: Beyond Tit-for-Tat: New Techniques for Collaboration in Network Security Games","awardID":"1017509","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":["551090"],"PO":["565090"]},"169100":{"abstract":"This project will improve the state of the art of the implementation and optimization of algorithms for exact linear algebra computation. With exact computation, solving systems of linear equations is advanced from limited accuracy to exact solutions. This greatly increases the scope of accessible applications and allows matrix invariants such as rank, determinant, characteristic and minimal polynomial, and Smith and Frobenius normal forms to be computed.<br\/><br\/>We will combine a newly developed theoretical basis for block blackbox methods in linear algebra with high performance implementation, in hardware and software, of the computational kernels from which these implementations are constructed. The resulting implementations will be made publicly available in the framework of the LinBox software library. A system for automatically tuning the underlying computer algebra kernels will be developed and distributed as part of the LinBox library. The autotuning framework will benefit other computer algebra systems as well. The resulting advances for computation in finite domains, such as modular numbers and finite algebraic field extensions, will benefit many areas including cryptography and coding theory.<br\/><br\/>The project has many practical impacts as follows:<br\/><br\/>Experimental mathematics will be enhanced. In experimental mathematics, symbolic computation provides for testing of conjectures. And, perhaps more importantly, data from symbolic computations can guide the formulation of conjectures that are then candidates for formal proof. By permitting larger exact linear algebra computations, this project will increase the usefulness of such computation in mathematics.<br\/><br\/>The broadest, and perhaps most significant, outcome of this project is an ability to solve many problems which currently have no solution method at all. This project will make it possible to efficiently solve linear systems where numerical methods fail due to ill-condition of the problem instance, yet the exact result could it be obtained is valid and meaningful despite the approximate nature of the data.","title":"AF: Small: Collaborative Research: High Performance Exact Linear Algebra Kernels","awardID":"1019966","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7933","name":"NUM, SYMBOL, & ALGEBRA COMPUT"}}],"PIcoPI":[452562],"PO":["565251"]},"168264":{"abstract":"Project Abstract for NSF Proposal 1016018<br\/> Visual Recognition and Restoration In Concert<br\/> Peyman Milanfar<br\/> Electrical Engineering Department<br\/> University of California at Santa Cruz<br\/><br\/>In this research effort a central challenge in computer vision is addressed: Namely, to recognize and enhance objects in complex visual scenes given imperfect images, and more generally, video data. This effort strengthens the theoretical and practical foundations for generic visual object recognition systems that can deal with significant variations in visual appearance, a large number of categories, and stochastically and systematically degraded data. Data imperfections can include random noise, blur, and environmental degradations. The approach has transformative potential for a broad range of practical applications such as scalable image search and retrieval, automatic annotation, surveillance and security, video forensics, and medical image analysis for computer-aided<br\/>diagnosis.<br\/><br\/>The research advances the state-of-the-art in two important ways: (a) a unified and robust framework is derived for both (2-D) object and (3-D) action recognition, even when the data is subject to significant distortions, and (b) recognition and restoration from degraded data are treated in a common, statistically optimal setting. Traditionally, recognition and restoration have been addressed with limited awareness of each other?s techniques and of potential commonalities in approach. By improving, generalizing, and refining previously separate approaches to recognition with degraded data in an adaptive, non-parametric setting, for both 2-D and 3-D, this project contributes to the technical foundations and toolkits that can connect computer vision and image processing<br\/>intelligently.","title":"CIF: Small: Visual Recognition and Restoration In Concert","awardID":"1016018","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[450496],"PO":["564898"]},"168385":{"abstract":"Reliability is extremely important for some software and hardware <br\/>applications. One approach is to use a programming language with a <br\/>mechanized logic---a so-called theorem-prover---to prove theorems that <br\/>establish some critical behavioral characteristics. Among theorem-provers, <br\/>ACL2 has found use with several industrial suppliers of high assurance <br\/>software and hardware. ACL2 does not support component-oriented software <br\/>development, however, making it difficult to use with large and complex <br\/>projects. <br\/><br\/>This research project has three goals: to add a pragmatic module system to <br\/>ACL2; to equip it with a hygienic macro system; and to investigate a type <br\/>system that accommodates ACL2's programming idioms. The project team <br\/>employs a cyclic, three-step exploration method. The first step is to <br\/>adapt constructs from existing, similar languages to ACL2, especially a <br\/>logical meaning consistent with the theorem prover of ACL2. The second <br\/>step is to explore the pragmatics of the design with a wide range of <br\/>examples. The third step is to add implementations to a pedagogic, <br\/>interactive development environment for ACL2 and to evaluate their <br\/>usefulness in software engineering courses. The results of this last step <br\/>are used to re-start the cycle. <br\/><br\/>The work will contribute to the dissemination of theorem provers in <br\/>classrooms and industry. The research team expects to expose college <br\/>students to the use of theorem proving in the design and development of <br\/>complex systems with dozens, and possibly hundreds, of reliable <br\/>components. The team also hopes to improve the ability of industrial ACL2 <br\/>programmers to tackle complex component-oriented systems.","title":"SHF: SMALL: Collaborative Research: Modular ACL2","awardID":"1016532","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[450799],"PO":["565264"]},"164492":{"abstract":"Human computation studies how to collect useful data as a by-product of another activity in which people are interested (e.g., playing games). A popular example is the ESP Game, where two players are shown the same image and must independently generate tags; tags that match become labels for the image. ESP Game players have generated millions of labels that help improve image search engines.<br\/><br\/>Currently, little is understood about how to capitalize on each person's individual expertise to produce the best results in human computation systems. For example, the ESP Game could generate better results if automotive enthusiasts labeled images of cars while biologists labeled images of animals. This project aims to better understand how each individual's different capabilities can be assessed, dynamically leveraged, and even improved for the purposes of human-driven data collection. <br\/><br\/>Intellectual Merit: Improved understanding of the strengths and weaknesses of human users as teachers and data sources; an intelligent new objective-driven model of data collection; novel opportunities to study machine learning algorithms that capitalize on human teachers' abilities; and an analysis of learning opportunities as incentives for people to participate in human computation systems.<br\/><br\/>Broader Impact: Distribution of large new data sets (e.g., Wikipedia articles in multiple languages); several Internet-based human computation systems for large-scale evaluation of machine learning and other algorithms; a new course called ?Human-in-the-Loop Systems?; workshops held in conjunction with major conferences; and outreach activities (e.g., summer projects) that introduce female undergraduate students to interdisciplinary research.","title":"SoCS: Effectively Leveraging Contributions in Human Computation Systems","awardID":"0968487","effectiveDate":"2010-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["530563","533288"],"PO":["565215"]},"174073":{"abstract":"This project focuses on developing the infrastructure for a self-sustaining organization that can manage, grow, and evangelize olympiads that involve young students (middle and junior high) in computational thinking. A large component is the creation of pilot olympiads in a select few cities in the United States. Specific goals of this project include: (1) identifying a set of foundational skills that underlie computational thinking that can be taught before college and high school; (2) identifying a style of problems and scenarios that engage a wide variety of students; and (3) implementing a curriculum of training sessions and contest questions that exemplify those foundational skills.<br\/><br\/>There are two broad reasons for creating a Computational Thinking Olympiad. First, to expose the fundamentals of computational thinking to a broad audience of potential researchers and practitioners in the field, thus increasing participation and diversity in computing. Second, to ensure long-lasting impact beyond of this project.<br\/><br\/>The success of the Computational Thinking Olympiad will have a significant impact on our society by introducing middle school students to computational thinking in its breadth and depth: (1) encouraging students to have fun with the computational thinking in an arena that is both cooperative and competitive; (2) encouraging students to pursue education in computing; (3) introducing the unplugged parts of computing to those who have not had access to the plugged-in parts; and (4) showing that computational thinking is not ``just'' programming.","title":"Collaborative Research: EAGER: Computational Thinking Olympiad","awardID":"1048415","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["559402",466936],"PO":["565215"]},"168881":{"abstract":"CIF:Small:Wavelets on Graphs -- Theory and Applications -- 1018977<br\/>PI: Antonio Ortega (University of Southern California) <br\/><br\/>A key recent trend has been towards dramatic increases in the amounts of data that can be gathered for analysis. Examples include online social networks, online search logs, DNA analysis, surveillance, among many others. A major challenge is to extract useful information from these large data-sets, to the point that there is a risk that much of these data could be underutilized. This research aims at developing innovative data representation tools to enable significantly faster and more accurate analysis of these emerging data-sets.<br\/><br\/>This research is motivated by two observations: i) data points in these emerging data-sets can often be seen as part of a large graph and ii) tools for analysis of data on graphs tend to be global in nature, making it difficult to identify trends that manifest themselves in relatively small regions of the graph. Inspired by wavelet techniques developed over the past 20 years, this work studies a new class of wavelets that are defined on graphs. This project studies the underlying theory for these wavelets on graphs, including the design of localized, invertible and critically sampled transforms. The team is also addressing two concrete applications to illustrate the potential benefits of these methods. In one application these new tools are applied to Genomic data sets (where graphs correspond to genetic pathways) and to analysis of data in social networks. The second class of applications consists of applying these new transforms to the development of new tools for image and video processing.","title":"CIF: Small: Wavelets on Graphs - Theory and Applications","awardID":"1018977","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":[451991],"PO":["564898"]},"168320":{"abstract":"This project studies the fundamental statistical laws governing the mobility patterns of humans. The prior NSF-funded studies conducted by the PI, which are based on GPS traces of 100 people in five different settings including university campuses, New York City, Disney World, and State Fair, revealed that many important fundamental statistical properties of human mobility, namely heavy-tail flight distributions, self-similar dispersion of visit points, and least-action principle for trip planning. Most of all, they find that peo-ple tend to optimize their trips in a way to minimize their discomfort or cost of trips (e.g., distance). The current project is extending this work by considering the effect of temporal constraints. When two persons meet, they have to be at the same location, and also at the same time. A realistic human mobility model must capture these spatial and temporal constraints and dependencies. The major goal of the project is to statistically capture the fundamental laws of these properties which are invariant of specifics in mobility scenarios, and represent them realistically in diverse mobility scenarios including user-created virtual en-vironments. The results from this project include (1) understandings of the fundamental human mobility characteristics and (2) statistical representation of them in synthetic mobility traces. Realistic human mobility models can improve our current practice of evaluating the performance of mobile networks. The models can also be used for other disciplines such as civil engineering for city planning and escape planning, critical disease control for studying the patterns of virus spread, and biology and sociology.","title":"NeTS: Small: Investigation of Human Mobility: Measurement, Modeling, Analysis, Application and Protocols","awardID":"1016216","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[450636],"PO":["564993"]},"168562":{"abstract":"As multi-core and many-core processors are consensually becoming the de facto standard for all types of computing platforms, two techniques, Speculative Multithreading (SpecMT) and Transactional Memory (TM), have been intensively investigated on such platforms to enhance single-thread performance and to simplify the parallel programming model. These two computing models, proposed separately, share many common features in their underlying implementations. In this research, we give a holistic view and investigate a unified many-core architecture to support both technologies under one implementation. We first map SpecMT onto a hardware-based TM architecture and develop enabling techniques to showcase the performance potential while maintaining the benefit of a TM programming model. Each thread spawned speculatively or non-speculatively is transactified into a transaction. Compilers are used to determine when and where to initiate SpecMT with hardware to dynamically shepherd the decisions and throttle the extent of SpecMT. These transactions, executing different code regions, can be launched out of the sequential program order but must be committed in the original order with hardware support. This research focuses on (1) architectural support for enabling both SpecMT and TM, (2) compiler's support for SpecMT and thread transactification, (3) quantifying the benefits of different types of transactions, and (4) economical and feasible mechanisms to support SpecMT on heterogeneous many-core platforms such as the incoming integrated CPU-GPU systems. The success of such a unified many-core architecture will provide a foundation for delivering high performance for single-thread applications while improving the productivity for software developers substantially.","title":"CSR: Small: A Unified Many-Core Architecture for Enabling Speculative Multithreading and Transactional Memory","awardID":"1017297","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["526309"],"PO":["565255"]},"168694":{"abstract":"Many real-world problems, ranging from scheduling in industrial production lines, planning for intelligent robots, protein structure predication, resource allocation, to various network optimization problems are combinatorial search problems. Constraint Programming (CP) and Answer Set Programming (ASP) are emerging techniques for solving these problems. CP over Finite Domains (FD) has had great successes in many application areas, such as scheduling, where use of global constraints is very effective. ASP has been found amenable to knowledge-intensive search problems such as planning and configuration problems. Recently, there has been great interest in parallelizing CP and ASP solvers to take advantage of the power provided by multi-core processors. <br\/><br\/>This research aims to develop an integrated parallel constraint programming platform for combinatorial search problems. It entails three tasks. Firstly, this research will enhance the power of CLP(FD) (Constraint Logic Programming over FD) by enabling constraints over Composite Finite Domains (CFD). The resulting language, CLP(CFD), allows for natural and efficient modeling of problems with multi-attributed objects. Action Rules (AR), a successful language developed by the PI, will be enhanced and used to implement CLP(CFD). Secondly, this research will develop a compiler to translate ASP programs into AR. For an ASP program, the generated program maintains a partial answer set as a pair of disjoint tuple sets and uses labeling and propagation to compute answer sets. Unlike most ASP solvers, the AR-based solver requires no prior grounding of programs. Thirdly, this research will parallelize AR. Since AR is used as a common intermediate language for both CLP(CFD) and ASP, a parallel implementation of AR will directly result in parallel solvers for CLP(CFD) and ASP. This research will advance the implementation techniques for constraint languages and the resulting system will benefit a wide range of real-world applications.","title":"SHF: Small: An Integrated Parallel Constraint Programming Platform for Combinatorial Search Problems","awardID":"1018006","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[451527],"PO":["564588"]},"168463":{"abstract":"Software maintenance and evolution is a vital and resource consuming phase of the software lifecycle. Introducing software changes is a particularly complex phenomenon in case of long-lived, large-scale, and globally distributed systems. Years of research efforts have recognized three core tasks to support developers during software maintenance: feature location (a starting point of a change in source code), impact analysis (other software entities that are also change prone), and expert developer recommendations (appropriate developers to implement changes). The research will develop a novel one-stop solution for these tasks by integrating and mining the latent information cluttered in structured and unstructured software artifacts produced and constantly changed during evolution of software systems, which are largely untapped in current solutions.<br\/><br\/>This research program has three main goals: 1) Define a new integrated framework SE2 for a comprehensive analysis of software evolution, based on conceptual and evolutionary information, under a single umbrella, 2) Define new methodologies for software maintenance tasks based on SE2, and 3) Perform empirical studies to evaluate SE2 and supported methodologies. Central to our solution are the state of the art data mining, information retrieval, and program analysis methods. The research will formulate both theoretical foundations and deliver novel practical solutions to uniformly represent, analyze, and use them within the SE2 framework. Among the broader impacts the project includes production of software tools under open source licenses and collaboration with industry to transfer technology.","title":"III: Small: Collaborative Research: An Inductive Framework to Support Software Maintenance","awardID":"1016868","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["534438"],"PO":["564388"]},"168111":{"abstract":"In today's information-driven economy, organizations are finding it increasingly difficult to control the flow of sensitive information. <br\/>This project is building a system called PDC (for Practical Data Confinement), a novel information security architecture built around coarse-grained isolation, fine-grained dynamic information flow tracking (DIFT), and informed policy enforcement. In broad terms, PDC provides mechanisms for tracking the movement of confidential data and enforcing dissemination restrictions that specify how, when, and to whom the data may be externalized. PDC requires no changes to applications or the operating system, and is thus readily deployable in existing IT environments. PDC makes use of an augmented hypervisor that enforces inter-VM isolation and carefully tracks the movement of sensitive data between the virtual CPU registers, memory, and disk within each VM. The hypervisor also intercepts all externally observable output actions (e.g., network communication, writing data to a mobile storage medium, sending data to a printer) and enforces security policies, allowing or denying specific application requests to externalize sensitive data. The main focus of the current work is to overcome the performance barriers that have hindered similar approaches in the past.","title":"TC: Small: Practical Data Confinement","awardID":"1015470","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["560562"],"PO":["565327"]},"168485":{"abstract":"This research addresses issues arising from the convergence of two important trends in embedded systems. One, many safety-critical applications are subject to certification requirements. Two, there is an increasing trend towards integrated architectures that support multiple functionalities, often of different criticalities, upon a single computing platform. As such systems become increasingly more complex, obtaining required certifications becomes more challenging. This project investigates the following thesis: Scheduling theory in its current form is unsuited to the design of such mixed-criticality (MC) systems that are subject to multiple certification requirements; efficient resource use in such systems requires the development of fundamentally new scheduling techniques. The methodology adopted in investigating this thesis is to first identify major weaknesses with current approaches, that render certification cumbersome. Once these weaknesses are understood, new models are proposed for representing MC systems, and metrics derived for quantifying the effectiveness of techniques for building these systems. A systematic study of resource allocation and scheduling issues in certifiable systems is then conducted, aimed at providing quantitatively superior resource allocation methodologies.<br\/><br\/>The outcomes of this project will enable embedded safety-critical systems designers to provide systems that make far more efficient use of platform resources than is currently possible, and that pass certification at a significantly lower cost. Technology transfer will be achieved via ongoing and new industrial collaborations, and by continued participation in industry initiatives aimed at improving the certification process. Results will be disseminated via publications, presentations and tutorials, and distribution of software over the internet.","title":"CSR: Small: Formal Foundations of Certifiable Mixed-criticality Systems","awardID":"1016954","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["31436","518412"],"PO":["564778"]},"168375":{"abstract":"Programmers usually modify a program intending to fix a bug or to add a new feature. While they often have a strong understanding of the actual changes they are making to a program, the dynamic effects of these changes on the run-time behavior of the program can be harder to comprehend. The approach helps developers identify when their changes to the source code and the changes in the consequent executable behavior are inconsistent: that is, the change in the source is not apparent in the behavior, or vice versa. <br\/><br\/>The approach identifies specific program elements and dependencies that likely account for the inconsistent nature of the change. Using a static and a dynamic dependence graph from each of two program versions, the dependences are partitioned according to their presence or absence in each of the four graphs. Particular partitions contain dependencies that are likely to represent inconsistent parts of a change; these partitions provide insight into the change that would be otherwise difficult to obtain.<br\/>The partitions allow distinctions to be made that cannot be made using the static dependence graphs alone, the dynamic dependence graphs alone, nor using a static and dynamic graph pair from a given version; much of the power of the approach arises because the cross-version variations in the dependence graphs are small, reducing information provided to the programmer. The intellectual merit includes empirical assessment over a broad set of programs and changes, ?value propositions? for using this information, applications of the approach, and use of the partitioning to augment conventional approaches to assessing software complexity. <br\/><br\/>The project addresses two categories of broader impacts: the people directly involved in the research, and the potential for the research to positively affect society through increased programmer productivity.","title":"SHF: Small: Partitioning Static and Dynamic Dependences Across Versions","awardID":"1016490","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[450771,"483628"],"PO":["564388"]},"168496":{"abstract":"Tactile communication systems represent a promising arena for enhancing human-computer interactions by using a relatively underused sensory system, the sense of touch, to present information. The goal of the proposed research is to understand how the sense of touch can be used to facilitate human-computer interactions by presenting information using tactile cues that is usually presented in the visual or auditory modalities. The tactile signals are similar to those produced by a vibrating cell phone or pager. One of the challenges in using a tactile display that has an array of vibrating motors distributed over the skin is in determining what type of information can be presented tactually, which aspects of vibrotactile stimulation can be used to convey this information effectively, and what tasks benefit from tactile cues. One objective of the proposed research is to develop a conceptual framework that provides guidance to computer-interface designers on how tactile communication systems can be created that reduce work load, decrease errors associated with information overload and diminish the time taken to complete tasks.<br\/><br\/>The broader impact and application of this research extends well beyond computer interfaces, to all tactile interfaces that are used to display information to users, particularly those with visual and auditory impairments. Tactile displays are being developed to aid navigation in the visually impaired and to assist the hearing impaired in learning to lip read; the proposed research directly impacts the development of these displays by determining the optimal properties of vibrotactile signals that users can easily learn and identify. In addition to these applications, the present research is highly relevant to the implementation of tactile feedback in mobile devices and provides guidance as to the types of signals that are processed most efficiently by users.","title":"HCC:Small: Tactile communication in human-computer Interactions","awardID":"1016998","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["549792"],"PO":["565227"]},"168386":{"abstract":"Modern computer systems consume substantial amounts of energy, and energy costs for large computing facilities can reach into the billions of dollars. At the same time, battery power is a limiting factor for cellular phones and other small mobile devices. This proposal aims to design and implement scheduling and load-balancing algorithms which better optimize for energy-efficiency without sacrificing quality of service. Since these algorithms can be implemented in software (without the design or construction of new devices), they are a promising direction to deal with the growing demand for energy to power computation.<br\/><br\/>Scheduling and load-balancing are naturally online problems, where tasks arrive during the run of the algorithm and are not known in advance. The intellectual merit of this proposal includes improving our techniques for producing provably competitive results in such online problems, as well as exploring new hybrid models in cases where the tasks are partially predictable. The proposal aims to produce algorithms under very general relationships between energy and task completion rate (prior work has generally assumed a quadratic relationship, which is not realistic) and to permit more general representations of quality of service. Algorithmic techniques for these problems include linear program rounding, online primal-dual, and flow-based analysis for variants of online weighted matching.<br\/><br\/>The broader impact of this proposal involves the implementation and testing of algorithms, potentially leading to substantial savings in energy. The implementations also require dealing with a number of practical problems, such as collecting data about tasks on arrival (algorithms typically assume that information like priorities and workloads are known) and designing effective user interfaces. These will lead to a number of excellent undergraduate projects in which students can be exposed to advanced theoretical techniques in algorithm design while also producing energy-conserving software for real devices.","title":"CIF: Small: Energy-Efficient Scheduling and Load Balancing","awardID":"1016540","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["533831",450802],"PO":["565251"]},"167066":{"abstract":"Web Science is an emerging discipline that studies the Web: how human activity is shaped by Web interactions, how the Web can benefit society, and how Web technologies can be improved. Central to Web Science is access to data that records the history of the Web, as well as data that records human activity (e.g., posed queries, tagged pages, Twitter updates). It is currently very difficult for academic researchers to obtain such Web data because it is hard to locate, it is fragmented across diverse sites, and is recorded using inconsistent formats and strategies. This project will build a Web Archive Cooperative (WAC) that will integrate existing archives (repositories of Web data), making it feasible to access large volumes of data in a simplified fashion. The WAC will be a virtual service, providing search facilities and access mechanisms to existing resources. These resources will not just be Web pages, but all types of available Web information, such as query logs, tag annotations, blogs, profiles and Twitter updates. Furthermore, resources will also include the software tools for building and managing Web archives.<br\/><br\/>The project will explore three goals for a resource discovery service: (1) the manual or automated discovery of entire existing Web related archives; (2) the selection among known archives of the ones that support a specific research question; and (3) the identification of individual resources from within the selected archives. Tools for characterizing discovered archives, especially for the case where the archive does not provide rich descriptive metadata, will also be developed. Characterization of an archive includes elements such as an estimate of the archive's coverage, particulars of the crawling parameters, like dates\/frequencies, crawl duration, depth, per-site ceiling on the number of collected pages, content statistics, and link structure. Mechanisms for integrating diverse archives will be developed, and the mechanisms will be applied to site reconstruction (from various archives) and archive views (a logical fusion of resources from multiple sources). Since integration issues are so challenging, an experimental testbed will be set up with small but diverse resources. The testbed will contain several crawls of the same target sites, each obtained with different crawlers and using different parameters. The testbed will also contain related resources. Storage trading schemes will be developed, allowing members to trade local backup space for remote space. A Web archive replication tool will be developed based on existing notions for self-preserving objects. Alternatives for replica synchronization will be studied.<br\/><br\/>Workshops to bring together key Web Science researchers will be organized to discuss available resources and impediments to sharing. These workshops will drive research and identify needed tools and protocols. With small groups of participants, challenge problems will be established, e.g., combining a set of Web archives. Reports of these results at future workshops can incentivize others to participate in the WAC. In addition, an Advisory Board of industrial, government, and academic experts has been set up to guide the project. A Summer Institute for Web Science graduate students will be held. At this Institute, students will learn to use the latest tools and will learn from each other's experiences in dealing with Web data. In addition, a one-day workshop will be developed, to be offered at Web Science conferences (WWW, SIGIR, etc.) to educate participants about WAC resources. An undergraduate Web Sciences track for computer science majors will be set up, taking advantage of WAC resources. The project will have impact in two ways. First, it will provide tools and services that facilitate access to Web resources. Any researcher, from a computer scientist studying efficient Web search, to a social scientist studying how human beliefs are changing today, to a historian studying how the early Web evolved, to a biologist understanding how disease spreads, will benefit from the work. Second, the project motivates students and young researchers to stay in academia. Currently top talent is flowing to industry because only they have comprehensive Web data, and it is so hard to do significant Web Science at universities. The WAC can provide an alternative, attracting more researchers and teachers to this important area.","title":"III: Large: Collaborative Research: Web Archive Cooperative","awardID":"1009916","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[447448],"PO":["565136"]},"159113":{"abstract":"Center Name: Center for Science of Information <br\/>Center Director: W. Szpankowski, <br\/>Lead Institution: Purdue University <br\/><br\/>The foundations of modern communications and ancillary trillion plus dollar economic windfall were laid in 1948 by Claude Shannon who introduced a general mathematical theory of the inherent information content in data and its reliable communication in the presence of noise. While Shannon?s Theory has had a profound impact, its application beyond storage and point-to-point communication, e.g., to the Internet, poses fundamental challenges, among the most vexing facing today?s scientists and engineers. The overarching vision of the proposed Center for Science of Information is to develop a new science of information that incorporates common features generally associated with data\/information, such as space, time, structure, semantics and context that are not addressed by Shannon?s Theory. The realization of this vision requires a center-level environment that can focus the efforts of a sizeable (and diverse) group of researchers, for a protracted period of time, on these critically important challenges, which could have far reaching societal impact and enormous economic ramifications. Under the umbrella of this overarching vision, the proposed center will explore the following fundamental issues: (i) modeling complex systems and development of analytical techniques for information flow (e.g., understanding Darwinian selection); (ii) quantification and extraction of informative substructures in complex systems (e.g., discovering functionally relevant structures in gene regulatory networks or modular entities in social networks); (iii) understanding of spatio-temporal coding used to exchange information through timing and localization in complex systems (e.g., building more efficient ad hoc networks and understanding neuronal activity); (iv) data-driven knowledge discovery based on formal information-theoretic measures (e.g., finding semantically relevant information in unstructured repositories); (v) steganography, data obfuscation and hiding as mechanisms for robustness (e.g., developing secure systems for monitoring and surveillance); and (vi) discovering principles of redundancy and fault tolerance in diverse natural systems (e.g., understanding the interplay between erasure coding and distributed system design). <br\/><br\/>The intellectual merits of the proposed center include the community of students and academic and industrial scholars it seeks to sustain, the theoretical advances it hopes to achieve, and the novel insights and tools it hopes to provide to explicate a myriad of diverse systems, ranging from the life sciences through business applications. The broader impacts of this Center extend beyond the potential scientific, societal and economic ramifications and include the creation of an ?active and thriving community of students and scholars? who will train the next generation of scientists and engineers, enlighten the public, and ultimately pave the way for the next information revolution. The Center team is composed of over 40 investigators, many having already made significant accomplishments in multiple research areas relevant to the Science of Information. The Center team is a very diverse group: it has a mix of junior and senior researchers, including several members of underrepresented groups. They bring expertise in all essential areas of research, including Computer Science, Chemistry, Economics, Statistics, Environmental Science, Information Theory, Life Sciences, and Physics. The institutional partners include nine premier institutions (Purdue, Bryn Mawr, Howard, MIT, Princeton, Stanford, UC Berkeley, UCSD, and UIUC), two of which have significant underrepresented student populations. The academic institutions are complemented by the Center?s industrial partners (Amgen, Bell Labs, Configuersoft, Google, HP, Lilly, NEC, Qualcomm, and Yahoo) and by world-renowned researchers at international institutions.","title":"Emerging Frontiers of Science of Information","awardID":"0939370","effectiveDate":"2010-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8005","name":"STCs - 2010 Class"}}],"PIcoPI":["521186","553691",425343,"518163",425345,"550344"],"PO":["564898"]},"174492":{"abstract":"Synthesis is the process of computing an implementation from a specification of the desired behavior, performance, and security and privacy properties. This ideal form of system design has been a long-standing dream in computer science. The goal of this project is to realize the dream of synthesis, in the same way that current tools for program analysis and model checking realize the dream of verifying program correctness. This is feasible today due to the enormous computing power of today's platforms and due to the recent significant technical advances in the underlying technologies for program analysis and verification. Synthesis, supported by powerful computational tools, and integrated into system design, can have a transformative effect by enabling the construction of more complex and more robust systems than are currently possible or cost effective.<br\/><br\/>The focus of this project is the development of algorithmic tools for assertion-based design synthesis. We will develop automata- and game-theoretic approach to assertion-based intentional system design. The intellectual merit of this project is the interplay between games, automata, and logic. Based on deep theoretical foundations, prototype synthesis tools will be developed. These are crucial for the development of reliable, secure, and scalable computing systems.","title":"Eager: Automated Synthesis for System Design","awardID":"1049862","effectiveDate":"2010-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["565263"],"PO":["565264"]},"164460":{"abstract":"Many scientists today face the unprecedented challenge of managing and analyzing a rapidly growing set of complex data. This international PIRE project aims to narrow the growing gap between the capability of modern scientific instruments to produce data and the ability of researchers to manage, analyze, and share those data in a reliable and timely manner. The emerging technology of cloud computing is a step forward from the current cyberinfrastructure. Cloud computing involves clusters (the \"clouds\") of distributed computers that provide potentially less expensive, more flexible, and more powerful on-demand resources and services over a network, usually the Internet, while providing the scale and the reliability of a data center. This PIRE team intends to help develop large-scale distributed computing capabilities - the Open Science Data Cloud (OSDC) - to provide long term persistent storage for scientific data and state-of-the-art services for integrating, analyzing, sharing and archiving scientific data. The group proposes to study and strengthen storage systems that integrate specialized network protocols and support data transport over wide-area, high-performance networks. As data grows in size, the only practical means to analyze it is to use parallel programming, but until recently it has been time consuming for a domain scientist to take advantage of parallel programming. Another research focus will be to develop new classes of cloud-based parallel programming frameworks and to integrate them into the cloud infrastructure so that this technology is more broadly available to scientists. In addition to the research dimensions of this project, another key aspect is the involvement, in workshops and in subsequent use of the cloud cyberinfrastructure, of many domain scientists and their students. These groups will be trained in the basics of cloud computing and then will work to ensure that the cloud computing research advances maximize the manageability and analytical power of the complex datasets unique to their disciplines. <br\/><br\/>This PIRE project embraces cloud computing as a global issue and so taps the cloud computing, high performance networking, domain science, e-Science, education and outreach expertise of its many collaborators in Europe, Asia and South America. Foreign partners also provide a natural mechanism to engage international scientific datasets and distributed networks, and accommodate different international standards to guarantee interoperability. The international collaborators can provide an entry into international collaborations for U.S. graduate students and early career scientists and can also serve as global ambassadors for this new cyberinfrastructure, helping to garner widespread support that will be critical to its future adoption.<br\/><br\/>The project will build a strong cadre of students with a global perspective on scientific data management in many research areas vital to U.S. and international scientific collaborations. The project will provide U.S. graduate students and early career scientists with international research and education experiences with leading scientists via research and training at foreign institutions and participation in annual workshops. As a group, the PIRE students will share an interest in data intensive computing but will be drawn from fields as diverse as computer science, physics, astronomy, geosciences, chemistry, engineering, and biology, lending an interdisciplinary vigor to their training. The PIRE team members will also develop 1-2 day and 1-2 week courses on data intensive computing, with hands-on exercises developed by U.S. and international faculty in computer science and the domain sciences.<br\/><br\/>This PIRE project is likely to have numerous impacts above the level of the individual collaborators. For the U.S. PIRE institutions, it will strengthen current linkages and collaborations in the global Cloud Computing community and engage more U.S. students in international interdisciplinary research teams for the service, support and analysis of large scientific datasets. The project will enhance internationalizing efforts both at the University of Illinois at Chicago and at Florida International University by providing opportunities for short term research abroad and other academic experiences to a diverse group of students. This project will increase the virtual international engagement of the U.S. institutions via distributed research collaborations, courses with transcontinental participation, global web discussions, and focused social networking forums. Increasing the number of scientists with expertise in managing and analyzing very large datasets is also vital to the future of our nation. Finally, since this transformative technology is broadly applicable to any scientific project struggling to manage and analyze the volume of data produced, the OSDC and its facilitative impacts are likely to persist long after the PIRE pr","title":"PIRE: Training and Workshops in Data Intensive Computing Using The Open Science Data Cloud","awardID":"0968341","effectiveDate":"2010-08-01","expirationDate":"2011-04-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"7742","name":"PIRE"}},{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"8004","name":"Software Institutes"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1221","name":"ELEMENTARY PARTICLE ACCEL USER"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7793","name":"DATA-INTENSIVE COMPUTING"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0700","name":"Division of A\/D FUND","abbr":"A\/D"},"pgm":{"id":"1217","name":"EXTRAGALACTIC ASTRON & COSMOLO"}}],"PIcoPI":["533353","564025",440546,"492421"],"PO":["531406"]},"165472":{"abstract":"This project studies the creative practices of artists and computer scientists\/engineers as they work independently to develop novel computing technologies. The goals are to use knowledge gained through this study to both inform computing research as well as to create and evaluate an educational framework for fostering innovation within computing, information science, and engineering (CISE) and science, technology, engineering, and mathematics (STEM) education. At Georgia Institute of Technology, artists are contributing to the future of computing through the doctoral program in Digital Media. Some of these artists are developing novel computing technologies that parallel work being done in computer science and engineering labs at Georgia Tech. For example, a computer scientist who has developed a wearable sensor network for Navy soldiers is adapting his technology into a SIDS monitoring garment for infants. Separately, an artist is adapting technology she has used in an interactive sculpture to an infant swaddler for SIDS prevention. These common technologies and goals place their work in dialog, allowing direct comparison of creative work practices and outcomes. This project will study pairs of artists and computer scientists\/engineers working independently on parallel computing projects to find similarities and differences in their creative work.<br\/><br\/>This study will pose questions such as ?What common ground may be found between an artist?s and a computer scientist?s prototyping methods?? and ?How do artists and computer scientists incorporate or defy disciplinary training in their approach to innovation?? The study will establish a methodology and framework for understanding the mindsets and work approaches of creative practice in computing, engineering, and art and design. The knowledge resulting from this study will help researchers and educators in computer science, digital media, engineering, and the arts orchestrate creative projects in their own disciplines while incorporating knowledge and practices from other disciplines. In the second year, the study researchers will implement a course at Georgia Institute of Technology?s undergraduate Computational Media program that embodies knowledge gained about creativity across the disciplinary boundaries of art and computer science. This work in curriculum design will contribute in tangible ways to the NSF STEM initiative through publication of both the curriculum and classroom results.","title":"Pilot: Qualitative Analysis of Creative Practices in Parallel IT and Art Projects","awardID":"1002820","effectiveDate":"2010-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":[443269,443270],"PO":["564456"]},"168640":{"abstract":"This project develops a geometric approach to the P vs. NP problem, called geometric complexity theory.<br\/><br\/>The P vs. NP problem is the foundational conjecture of mathematical sciences, which says that theorem proving cannot be automated. Geometric complexity theory (GCT) is an approach to this and related problems via algebraic geometry and representation theory. This approach reduces variants of these problems over the field of complex numbers to the problem of proving existence of obstructions, which are objects in algebraic geometry and representation theory that serve as proof certificates of hardness in the lower bound problems under consideration. The existence of these obstructions is deeply connected with certain positivity hypotheses in algebraic geometry and representation theory. The goal of this project is to study this connection between hardness in complexity theory and positivity in mathematics in depth and thereby extend the approach further.<br\/><br\/>Since the boundary between the tractable and intractable problems in science and engineering depends on the P vs. NP problem, the study undertaken in this proposal is of central intellectual relevance to complexity theory as well as several other areas of science and engineering. Furthermore, since GCT reveals a deep connection between the P vs. NP and related problems in complexity theory and fundamental positivity problems in pure mathematics, this study could lead to extensive collaboration between complexity theory and several branches mathematics, such as algebraic geometry and representation theory.","title":"AF: Small: Geometric Complexity Theory Approach to the P vs NP problem","awardID":"1017760","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7927","name":"COMPLEXITY & CRYPTOGRAPHY"}}],"PIcoPI":[451397],"PO":["565157"]},"168651":{"abstract":"Across many scientific disciplines, the availability of very large<br\/>amounts of data is creating a paradigm shift. The goal of this project<br\/>is to develop Scolopax, a tool for finding interesting patterns in<br\/>classification and prediction models trained on large high-dimensional<br\/>data. These patterns can capture previously unknown relationships<br\/>between the variables of a complex process, hence are essential for<br\/>exploratory analysis and scientific discovery.<br\/><br\/>This project explores several research directions to lay the<br\/>foundations for Scolopax: (1) Design of a new query language that can<br\/>express all common pattern search preferences. (2) Algorithms for<br\/>learning a query so that even non-technical users can formulate<br\/>non-trivial queries through an interactive process. (3) New rewrite<br\/>rules and efficient data management approaches to automatically<br\/>transform queries into fast implementations on a cluster or Cloud. (4)<br\/>Design of new semi-parametric data mining techniques that are amenable<br\/>to scalable training, evaluation, and pattern confidence computation.<br\/><br\/>User-friendly query writing functionality makes Scolopax accessible to<br\/>scientists and citizen scientists alike. Its planned deployment<br\/>through popular Web sites, e.g., those hosted by the Cornell Lab of<br\/>Ornithology, has the potential to enable new scientific<br\/>discoveries. By letting citizen scientists not only contribute data,<br\/>but also make their own discoveries using the data, Scolopax also<br\/>serves as an important enabler and motivator for outreach programs and<br\/>greater involvement of citizen scientists. For further information see<br\/>the project web site at the URL:<br\/>http:\/\/www.ccs.neu.edu\/home\/mirek\/Projects\/Scolopax","title":"III: Small: A Scalable Search Tool for Interesting Patterns in Scientific Data","awardID":"1017793","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[451421],"PO":["565136"]},"177242":{"abstract":"A brain-computer interface (BCI) is a system that allows users, especially individuals with severe neuromuscular disorders, to communicate and control devices using their brain waves. There are over two million people in the United States afflicted by such disorders, many of whom could greatly benefit from assistive devices controlled by a BCI. Over the past two years, it has been demonstrated that a non-invasive, scalp-recorded electroencephalography (EEG) based BCI paradigm can be used by a disabled individual for long-term, reliable control of a personal computer. This BCI paradigm allows users to select from a set of symbols presented in a flashing visual matrix by classifying the resulting evoked brain responses. One of the goals of this project is to establish that the same BCI paradigm and techniques used for the aforementioned demonstration can be straightforwardly implemented to generate high-level commands for controlling a robotic manipulator in three dimensions according to user intent, and that such a BCI can provide superior dimensional control over alternative BCI techniques currently available, as well as a wider variety of practical functions for performing everyday tasks.<br\/><br\/>Electrocorticography (ECoG), electrical activity recorded directly from the surface of the brain, has been demonstrated in recent preliminary work to be another potentially viable control for a BCI. ECoG has been shown to have superior signal-to-noise ratio, and spatial and spectral characteristics, compared to EEG. But the EEG signals used at present to operate BCIs have not been characterized in ECoG. The PI believes ECoG signals can be used to improve the speed and accuracy of BCI applications, including for example control of a robotic manipulator. Thus, additional goals of this project are to characterize evoked responses obtained from ECoG, to use them as control signals to operate a simulated robotic manipulator, and to assess the level of control (speed and accuracy) between the two recording modalities and compare the results to competitive BCI techniques. Because this is a collaborative effort with the Departments of Neurology and Neurosurgery at the Mayo Clinic in Jacksonville, the PI team will have access to a pool of ECoG grid patients from which to recruit participants for this study.<br\/><br\/>Broader Impacts: This research will make a number of contributions in the emerging field of BCI and thus will serve as a step toward providing severely disabled individuals with a new level of autonomy for communicating with others and for performing everyday tasks, which will ultimately dramatically improve their quality of life.","title":"HCC: Medium: Control of a Robotic Manipulator via a Brain-Computer Interface","awardID":"1064912","effectiveDate":"2010-08-06","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["557173"],"PO":["565227"]},"165384":{"abstract":"As a ubiquitous creative endeavor across all human cultures, musical composition is an effective microcosm for the study of creativity in general. This project will impact computer science by showing through assisted musical composition how computers can genuinely improve upon the creative capabilities of humans alone. By introducing an interactive framework that enables even inexperienced users to realize their creative vision, this project also helps pave the way for such systems to amplify our creative potential in other areas in the future, such as in engineering and design. The technology that will be developed, called Functional Scaffolding for Musical Composition (FSMC), takes the unique approach of computing accompaniment for existing musical tracks (called the ?scaffold?) by generating special functions that take the scaffold as input and output accompanying tracks. In this way, generated tracks are in effect transformations of the scaffold, allowing them to inherit the global structure and implicit nuance of the preexisting music. The implication for computational creativity in general is thus to harness the richness of preexisting human-generated content as a seed for further elaboration. Furthermore, the user will be provided an interactive evolutionary interface that makes it possible to search the space of such transforming functions, in effect allowing the user to continually breed and elaborate new concepts that build upon preexisting incomplete works.<br\/><br\/>The primary target audience for FSMC as a practical technology will be musicians who lack the resources, collaborators, or expertise to produce complete musical compositions. For example, while a hobbyist with a keyboard might be able to compose a compelling melody, lack of expertise in other instruments may prohibit adding accompanying guitar or base. In addition, even more experienced musicians may benefit from the capability to quickly propose accompaniment as a new means of concept generation. In fact, existing computer programs that aid in musical composition often register millions of downloads online, demonstrating broad public interest in applications that enhance musical creativity. In addition to dissemination through scientific conferences focusing on computational creativity, the results of this research will be released in a form compatible with such programs, thereby directly impacting the public with a practical utility and consequently raising awareness of the potential for artificial intelligence and machine learning to enhance creativity in general.","title":"Pilot: Assisted Musical Composition through Functional Scaffolding","awardID":"1002507","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7788","name":"CreativeIT"}}],"PIcoPI":[443029],"PO":["565227"]},"168794":{"abstract":"Algorithms and software for geometric problems are usually designed and implemented in several layers of abstraction: For example, a map in a GPS navigation unit may be represented as a road network topology (just the interconnections) on top of the road geometry (a collection of line segments), which is represented with coordinates (as a sequence of points in a standard geodesic coordinate system), which are stored as numbers in a computer memory (which have a relatively small number of bits). At times, assumptions at higher levels of abstraction (e.g., lines are continuous, straight, and infinitely thin) are broken by the realities of the underlying levels (e.g., most points fall off a line when rounded to \"machine precision\"). Examples can be found in geometric algorithms for motion capture, robot simulation, x-ray crystallography, video tracking, and many other applications.<br\/><br\/>Sophisticated implementers of geometric algorithms will identify exactly what properties one level needs from its underlying levels, and carefully implement the underlying levels to provide these. The increasing amounts of geometric data mean that most implementers do not have sophistication in geometric algorithms, either because they are more focused on the sophisticated knowledge of their own domain, or because they are students who have not yet reached that level of sophistication.<br\/><br\/>Computer Scientists are accustomed to designing algorithms to optimize running time and memory space -- two resources that are limited, but whose limits may not be known in advance. This project adds arithmetic precision to this list of resources. This resource can be measured, up to constants, by the degree of polynomials in predicates and constructions. Restricting designers to low degree predicates forces creative new solutions to standard problems that can be guaranteed correct in machine precision. The result will be a codebook of algorithms that have been developed and tested by graduate and undergraduate students in this project, and can be the basis for robust primitives or further exploration in education and practical settings.","title":"AF: Small: Degree-Driven Design of Geometric Algorithms","awardID":"1018498","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7929","name":"COMPUTATIONAL GEOMETRY"}}],"PIcoPI":["554471"],"PO":["565157"]},"167243":{"abstract":"Simple contagion processes underlie various phenomena on complex networks, such as the spread of diseases on social-contact networks and information in communication networks; understanding their dynamics and developing control mechanisms are key issues in numerous applications. The goals of this proposal are: (i) Developing methods to construct synthetic relational networks using partial and noisy data; (ii) Understanding the structure of these networks and the contagion processes, and especially important network properties and typical patterns that have an impact on the dynamics of contagion; (iii) Developing techniques to control the spread of contagion processes, and to detect, prevent and arrest cascading failures in coupled socio-technical networks; and (iv) Understanding the co-evolution between the networks and dynamics, and using this to refine their models, and the strategies to control them.","title":"NetSE: Large: Collaborative Research: Contagion in large socio-communication networks","awardID":"1010904","effectiveDate":"2010-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7794","name":"NETWORK SCIENCE & ENGINEERING"}}],"PIcoPI":["532398"],"PO":["565090"]},"168464":{"abstract":"The World Wide Web and other networked information systems provide enormous benefits by enabling access to unprecedented amounts of information. However, for many years, users have been frustrated by the fact that these systems also create significant problems. Sensitive personal data are disclosed, confidential corporate data are stolen, copyrights are infringed, and databases owned by one government organization are accessed by members of another in violation of government policy. The frequency of such incidents continues to increase, and an incident must now be truly outrageous to be considered newsworthy. This project takes the view that when security violations occur, it should be possible to punish the violators in some fashion. <br\/><br\/>Although \"accountability\" is widely agreed to be important and desirable, there has been little theoretical work on the subject; indeed, there does not even seem to be a standard definition of \"accountability,\" and researchers in different areas use it to mean different things. This project addresses these issues, the relationship between accountability and other goals (such as user privacy), and the requirements (such as identifiability of violators and violations) for accountability in real-world systems. This clarification of the important notion of accountability will help propel a next generation of network-mediated interaction and services that users understand and trust.<br\/><br\/>The project's technical approach to accountability as an essential component of trustworthiness involves two intertwined research thrusts. The first thrust focuses on definitions and foundational theory. Intuitively, accountability is present in any system in which actions are governed by well defined rules, and violations of those rules are punished. Project goals are to identify ambiguities and gaps in this intuitive notion, provide formal definitions that capture important accountability desiderata, and explicate relationships of accountability to well studied notions such as identifiability, authentication, authorization, privacy, and anonymity. The second thrust focuses on analysis, design, and abstraction. The project studies fundamental accountability and identifiability requirements in real-world systems, both technological and social. One project goal is to use the resulting better understanding of the extent to which accountability is truly at odds with privacy and other desirable system properties to design new protocols with provable accountability properties. Building on that understanding and insights gained in designing protocols, the project also addresses fundamental trade-offs and impossibility results about accountability and identifiability in various settings. The broader impacts of the work include not only engagement with students but also a new perspective on real world accountability in trustworthy systems.","title":"TC: Small: Collaborative Research: Accountability and Identifiability","awardID":"1016875","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[450982],"PO":["565136"]},"168365":{"abstract":"Reliability is extremely important for some software and hardware<br\/>applications. One approach is to use a programming language with a<br\/>mechanized logic---a so-called theorem-prover---to prove theorems that<br\/>establish some critical behavioral characteristics. Among theorem-provers,<br\/>ACL2 has found use with several industrial suppliers of high assurance<br\/>software and hardware. ACL2 does not support component-oriented software<br\/>development, however, making it difficult to use with large and complex<br\/>projects.<br\/><br\/>This research project has three goals: to add a pragmatic module system to<br\/>ACL2; to equip it with a hygienic macro system; and to investigate a type<br\/>system that accommodates ACL2's programming idioms. The project team<br\/>employs a cyclic, three-step exploration method. The first step is to<br\/>adapt constructs from existing, similar languages to ACL2, especially a<br\/>logical meaning consistent with the theorem prover of ACL2. The second<br\/>step is to explore the pragmatics of the design with a wide range of<br\/>examples. The third step is to add implementations to a pedagogic,<br\/>interactive development environment for ACL2 and to evaluate their<br\/>usefulness in software engineering courses. The results of this last step<br\/>are used to re-start the cycle.<br\/><br\/>The work will contribute to the dissemination of theorem provers in<br\/>classrooms and industry. The research team expects to expose college<br\/>students to the use of theorem proving in the design and development of<br\/>complex systems with dozens, and possibly hundreds, of reliable<br\/>components. The team also hopes to improve the ability of industrial ACL2<br\/>programmers to tackle complex component-oriented systems.","title":"SHF: Small: Collaborative Research: Modular ACL2","awardID":"1016418","effectiveDate":"2010-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["475166"],"PO":["565264"]},"168376":{"abstract":"How to popularize parallel programming is one of the Computing Research Association's Grand Research Challenges for the systems community. Being able to leverage the full potential of multi-core systems would put us back into exponential growth of usable performance, as well as lead to significant power savings. Facilitating correct and efficient execution of multithreaded programs would have a transformative effect in the IT industry, as it attacks a problem at the heart of the programmability issues in multiprocessor systems. With ubiquitous multicores and emerging parallel programs, the IT industry is now dealing with far harder reliability problems. <br\/><br\/>Concurrency errors are hard to understand, are typically non-deterministic and manifest themselves way past the point of their occurrence. Moreover, they have major implications for programming language semantics. Recent work on support for concurrency debugging has made good progress, but has often focused on best-effort techniques for bug detection with probabilistic guarantees. This research takes a direct approach to the problem: making concurrency errors fail-stop by delivering an exception before the error manifests itself. In other words, the system detects that a concurrency error is about to happen and will raise an exception before the code with an error is allowed to execute. The investigators call this mechanism concurrency exceptions. Concurrency exceptions will allow concurrency errors to be handled as conveniently as division by zero and segmentation fault.","title":"SHF: Small: Precise Concurrency Exceptions: Architecture Support, Semantics and System Implications","awardID":"1016495","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[450774,"555936"],"PO":["565272"]},"168277":{"abstract":"The investigators study a new class of statistical methods for learning time series and graphical models. Their approach is based on spectral analysis and matrix decomposition methods that have enjoyed tremendous success in applications, but their use in graphical models has drawn less attention. The goal of this investigation is to extend the enormous previous successes of matrix decomposition methods to the realm of more complicated time series and certain graphical models, which will lead to new statistical machine learning algorithms with important practical applications. <br\/><br\/>In the information age, an important measure of computer intelligence is the ability to analyze huge amount of data that become available electronically, and make critical decisions under uncertain environment. Statistical machine learning is the main technique for analyzing electronic data, and graphical models are mathematical tools for understanding these complex data both by computer systems and by human operators in order to facilitate decision making. However, traditional algorithms for learning graphical models have limitations that restrict capabilities of modern computing systems. The current research attempts a new class of mathematical algorithms that can be used to design more effective graphical models, which in turn allows modern computers to analyze data more accurately and achieve higher level of intelligence.","title":"RI: Small: Spectral Methods for Learning Time Series and Graphical Models","awardID":"1016061","effectiveDate":"2010-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["360575"],"PO":["562760"]}}